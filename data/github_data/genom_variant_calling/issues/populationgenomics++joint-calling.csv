id,title,state,created_at,updated_at,closed_at,body,user,url
MDU6SXNzdWU3MzQxMjM4MDk=,Verify Hail's vcf_combiner functionality,CLOSED,2020-11-02T02:01:14Z,2021-07-12T04:07:49Z,2021-07-12T04:07:49Z,"Make sure we can run Hail's `vcf_combiner` (https://hail.is/docs/0.2/experimental/vcf_combiner.html) as expected:

- For a relatively small dataset, e.g. 100 GVCFs, can we schedule the combiner both with Hail on dataproc and with Hail Batch?
- Once a MatrixTable has been generated, what is the workflow to *incrementally* merge additional GVCFs? E.g. run `vcf_combiner` on 90 GVCFs and then try to merge the remaining 10 GVCFs. Is the idea to create two independent sparse MatrixTables and merge those?",lgruen,https://github.com/populationgenomics/joint-calling/issues/1
MDU6SXNzdWU3MzQxMjYxODg=,Implement basic sample and variant QC,CLOSED,2020-11-02T02:09:43Z,2021-07-12T04:06:35Z,2021-07-12T04:06:35Z,"Based on gnomAD, replicate the main features of its sample and variant QC pipeline (https://gnomad.broadinstitute.org/blog/2020-10-gnomad-v3-1-new-content-methods-annotations-and-data-availability/#sample-and-variant-quality-control) in our own pipeline.

Currently there are many open questions:

- How to generate sample-specific files, like metadata and coverage stats used in the QC pipeline. What's the schema of these files?
- In which order should the QC components ideally be arranged in a workflow, to make sure dependencies are met at every stage and parallelization is as efficient as possible?
- We'd probably want to skip the random forest implementation for variant QC altogether and move directly to AS VSQR. However, it's not clear how that's integrated with the sparse Hail MatrixTable generated by merging GVCFs. Was the functionality reimplemented in Hail? Or was a separate ""site/allele-only"" VCF generated on which ""standard"" AS VSQR was run, with the results used to filter the MatrixTable?",lgruen,https://github.com/populationgenomics/joint-calling/issues/2
MDU6SXNzdWU4MTE2MTE3MjU=,Adding VQSR to the QC pipeline,CLOSED,2021-02-19T01:44:48Z,2021-03-22T09:04:17Z,2021-03-22T09:04:17Z,"This issue is primarily about pulling required documentation together for this issue in one place.

## Goal
To perform filtering through variant quality score recalibration to ensure high quality variants

## Background

Variant quality score recalibration occurs in two steps:

1. Build a recalibration model to score variant quality
1. Apply score cutoff to filter variants based on the previously generated recalibration model.

### Definitions

- A VQSLOD (variant quality score log-odds) is a multi-dimensional score for a variant.
- The *tranche sensitivity threshold* determines the maximum VQSLOD score for which  `$THRESHOLD` percentage of variants in the truth set are included
- A tranche is a partition of variants (buckets) with each bucket having an increasing minimum sensitivity threshold. 

### Theory

1. Build a recalibration model to score variant quality:

  - Takes an overlap of the training/truth resource sets and of your callset.
  - Use the characteristic of the true positives within your callset to build a ~~random forest~~ gaussian mixture model to find good variants within the callset.
  - Use this statistical model to for each variant calculate a VQSLOD score, and add this into the INFO field.

2. Apply score cutoff to filter variants based on the previously generated recalibration model.


#### Random forest model

_Edit: this section was added on March 1st_

As Peter pointed out below, the current GATK variant recalibration (in 4.1) builds a Gaussian mixture model, the gnomAD team use a [random forest model](https://macarthurlab.org/2017/02/27/the-genome-aggregation-database-gnomad/) ([what is a random forest model](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)).

The Random Forest model the gnomAD team developed is available in GitHub: [broadinstitute/gnomad_methods:variant_qc/random_forest.py](https://github.com/broadinstitute/gnomad_methods/blob/master/gnomad/variant_qc/random_forest.py), and [API docs](https://broadinstitute.github.io/gnomad_methods/api_reference/variant_qc/random_forest.html).

This gnomad_methods RF pair (`train_rf` and `apply_rf_model`) would likely replace the previous pair of `gatk` `VariantRecalibration` and `ApplyVQSR`. Here's a paper that discussed the effectiveness of RFs: [ForestQC: Quality control on genetic variants from next-generation sequencing data using random forest](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6938691/), however there was internal conversation that suggested that RFs may be worse at detecting low quality variants than original gaussian models.


### Other notes

As variant recalibration isn't native to Hail, from the matrix table, we need to export a sites-only (to reduce size) VCF to perform this external analysis (sites-only to reduce space.

SNPs and indels must be recalibrated in separate runs.

We'll use the allele-specific version of the recalibrator.

### Points to clarify

- Do we need the model reporting plots (we need Rscript in the path if so)?
- Should we discard the variants that do NOT pass the quality score recalibration?
- What tranche sensitivity threshold should we use, IE 99.0%?

## Analysis

We're taking at least portions of an existing WDL workflow that runs these steps.

![image](https://user-images.githubusercontent.com/22381693/108642052-23b8f480-74f7-11eb-8243-a2f48fc08a34.png)


Note, for performance reasons the SNPsVariantRecalibrator is ran twice, once to create the model, and then again in parallel (with GatherTranches -> ApplyVQSR run later). I don't see this documented in the docs for [GATK 4.1.9 VariantRecalibrator](https://gatk.broadinstitute.org/hc/en-us/articles/360050815872-VariantRecalibrator), but there is documentation about it in the original java code: 

> > This argument is intended to be used in a more complicated VQSR scheme meant for very large WGS callsets that require a prohibitive amount of memory for classic VQSR. Given that training data is downsampled once it exceeds --max-num-training-data, reading in additional data to build the model only serves to consume resources. However, with this argument the output recal file will also be downsampled. The recommended VQSR procedure when using this argument is to run VariantRecalibrator once with sampling and designate an --output-model file. Then VariantRecalibrator can be run a second time scattered using the -scatterTranches argument and that file as an --input-model.  The scattered recal files can be gathered with the GatherVcfs tool and the scattered tranches can be gathered with the GatherTranches tool.

Source: https://github.com/broadinstitute/gatk/blob/ae06fb7345dd8bdef5e5dc0364416e81ab3622ec/src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/VariantRecalibrator.java#L280-L296

Params:
- `--output-model`
- `sample-every-Nth-variant`
- `output-tranches-for-scatter` / `-scatterTranches`
- maybe more...

## Sources

- https://gatk.broadinstitute.org/hc/en-us/articles/360035531612
- VariantRecalibrator: https://gatk.broadinstitute.org/hc/en-us/articles/360036510892-VariantRecalibrator
- ApplyVQSR: https://gatk.broadinstitute.org/hc/en-us/articles/360037056912-ApplyVQSR
- A guide to GATK4 best practice pipeline performance and optimization: https://www.ibm.com/downloads/cas/ZJQD0QAL
- Genome Aggregation Database (gnomAD): https://macarthurlab.org/2017/02/27/the-genome-aggregation-database-gnomad/",illusional,https://github.com/populationgenomics/joint-calling/issues/6
MDU6SXNzdWU4MTk0NzQ0Njg=,Convert VQSR pipeline to Batch,CLOSED,2021-03-02T00:57:00Z,2021-03-22T22:44:05Z,2021-03-22T22:44:05Z,"The existing uk-biobank is written in WDL, and it would be awesome if we could convert it to batch.

I've already written a very toy implementation for WDL -> Janis: https://github.com/PMCC-BioinformaticsCore/janis-core/pull/68, and now if I can write a small janis -> Hail Batch converter, then I'm making good strides.

The difficult thing is going to be mapping all the concepts, back to Batch, especially around command line generation.",illusional,https://github.com/populationgenomics/joint-calling/issues/9
MDU6SXNzdWU4MjA2NjY1MDk=,Upload Processor ,OPEN,2021-03-03T04:10:43Z,2021-03-11T05:39:29Z,,"### Background 
Sequencing providers, using a service account, will upload a range of files into the gs://cpg-#STACK-upload bucket. These files include CRAM files, gVCF files, etc. * These files will need to be processed into appropriate buckets for further downstream analysis & archival storage. 

WIP: https://lucid.app/lucidchart/invitations/accept/8f56b7e5-6be5-45f2-a2fc-518d48ce23ab

### Functional Requirements 
The upload processor pipeline should:

_[Outdated] 2nd March_ 
- ~~Version & move appropriate gVCF files into the gs://cpg-$STACK-main bucket.~~
- ~~Version & move all other files (e.g. CRAM files) into gs://cpg-$STACK-archive.~~
- ~~Validate QC run completion & perform a subsequent ‘Clean Up’ of the gs://cpg-$STACK-main bucket~~

**Update 3rd March** 
- Validate QC run completion, modify input to QC pipeline if required. 
- ‘Clean up’ gc://cpg-$STACK-upload  - version & move files to gs://cpg-$STACK-archive. 

**Inputs:** 
$STACK 
Airtable Table
QC Outputs & Exit Status**

**Trigger:** 
Run within a batch workflow, manually triggered. 


**Current Questions:** 
*Confirmation of all of the input files + organization. I.e. folder per sample? 
**Exploration into how QC outputs will impact the upload processor pipeline. How should that information feedback in? 
",vivbak,https://github.com/populationgenomics/joint-calling/issues/11
MDU6SXNzdWU4MjE2MzczMjQ=,Scale VCF combiner workflow,OPEN,2021-03-04T00:33:39Z,2021-03-04T00:33:39Z,,"Implement combining by chunks, inspired by https://github.com/broadinstitute/hail-ukbb-200k-callset",vladsavelyev,https://github.com/populationgenomics/joint-calling/issues/13
MDU6SXNzdWU4MjE2Mzk5NzM=,Sample QC / joint caller workflow tests,OPEN,2021-03-04T00:39:00Z,2021-03-04T00:39:00Z,,Add unit tests for different stages of the sample QC workflow.,vladsavelyev,https://github.com/populationgenomics/joint-calling/issues/14
MDU6SXNzdWU5NDE2NTI2ODI=,Broad issue to track the joint-calling workflow improvement,OPEN,2021-07-12T04:17:14Z,2021-10-26T04:18:03Z,,"- [ ] Merge the Rmd QC report generation into the workflow. Currently, it's [a part]( https://github.com/populationgenomics/tob-wgs/tree/main/reports/qc) of the TOB-WGS repo.
- [ ] Optimise extending an existing dataset.
- [ ] Don't show old batches in a QC report (keep 2 levels: full dataset and new batch).
- [ ] Add the number of novel variants to gnomAD.
- [x] Merge Kat's [ancestry work](https://github.com/populationgenomics/ancestry/tree/main/scripts/hail_batch) into the workflow?
- [ ] Contact Laura about the ReblockGVCF and GnarlyGenotyper.
- [x] Communicate with the sample metadata server: get sample list to be processed, update sample status.
- [ ] Document the workflow.
- [ ] Add a linter check for exposing sample names.
- [ ] Add QC after VQSR: number of variants filtered, etc. Look into `hail.variant_qc`.",vladsavelyev,https://github.com/populationgenomics/joint-calling/issues/56
I_kwDOEkHKQ85Cm5yz,[population pipeline] Benchmark NAGIM run,OPEN,2022-01-28T14:52:19Z,2022-01-31T00:31:21Z,,"First draft:
https://docs.google.com/spreadsheets/d/1bIU9tFvBuwSbc5oxAtflw-fKgljWGKXIGoXteShSgRE/edit#gid=1759300377

Need to double check it's correct",vladsavelyev,https://github.com/populationgenomics/joint-calling/issues/86
I_kwDOEkHKQ85Cm56m,[population pipeline]  NAGIM: prepare amp-pd+mgrb subset for Joe Copty,OPEN,2022-01-28T14:52:51Z,2022-01-28T14:57:12Z,,"* Added functionality into https://github.com/populationgenomics/joint-calling
* Written to gs://cpg-nagim-main/mt/v1-1-amp-pd-mgrb.mt/
* Counting variant numbers didn't sum up, need to double check",vladsavelyev,https://github.com/populationgenomics/joint-calling/issues/87
I_kwDOEkHKQ85Cm592,[large-cohort] Document variant filtering,OPEN,2022-01-28T14:53:03Z,2022-10-14T02:56:16Z,,"- [ ] Sample-level filtering
- [ ] AS-VQSR (copy from here: https://centrepopgen.slack.com/archives/C01R7CKJGHM/p1643281605009400?thread_ts=1643251715.009000&cid=C01R7CKJGHM)
- [ ] Variant annotations (we remove RF code which helped it clean up stuff a bit, now just add more docstrings and try to generate some documentation, or make some nice graph - so users of MatrixTable can understand what's inside)
- [ ] Make a page similar to https://gnomad.broadinstitute.org/help/hgdp-1kg-annotations (from https://gnomad.broadinstitute.org/news/2021-10-gnomad-v3-1-2-minor-release/#addition-of-a-help-page-describing-annotations-on-the-release-files)
 ",vladsavelyev,https://github.com/populationgenomics/joint-calling/issues/88
I_kwDOEkHKQ85DWyOm, [join-calling]: use transmitted singletons,OPEN,2022-02-10T13:06:15Z,2022-03-01T08:04:08Z,,"For variant QC useful to use trios. Transmitted singletons act as ""real"" de novo variants, and known de novo variants are useful to train and validate VQSR model.",vladsavelyev,https://github.com/populationgenomics/joint-calling/issues/89
