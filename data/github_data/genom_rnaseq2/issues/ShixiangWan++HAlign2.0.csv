id,title,state,created_at,updated_at,closed_at,body,user,url
MDU6SXNzdWUyNDU5MjQyMTM=,missing MSA,OPEN,2017-07-27T04:42:34Z,2017-10-11T12:30:22Z,,"Dears, the output of halign is not an aligment in fasta format, can you add this to next version?, will be great.

Regards",Sanrrone,https://github.com/ShixiangWan/HAlign2.0/issues/1
MDU6SXNzdWUyODA0OTk2NTc=,Wrong id access ?,OPEN,2017-12-08T14:05:15Z,2017-12-08T15:51:32Z,,"Hi, I'm actually trying to use HAlign-II on a spark installation.
I get a problem, given a fasta file: I got an exception of wrong access to a string (trying to get char further the string size ?).

> axverdier@spark:~/Tools/HAlign2.0$ ./testCmd.sh 
>
> spark-submit --master yarn --deploy-mode client --num-executors 1 --executor-memory 5G --driver-java-options '-Dmapr.library.flatclass' --name HAlign2.0 --class main maven-src/target/malab-1.0-SNAPSHOT.jar -sparkMSA /mapr/user/axverdier/data/genome.fasta maprfs://spark-ics/user/axverdier/test_genome_halign 0
>
> 17/12/08 14:58:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
> 894ms
> 17/12/08 14:58:55 WARN TaskSetManager: Stage 0 contains a task of very large size (5541 KB). The maximum recommended task size is 100 KB.
> 2709ms                                                                          
> >>Converting results ... 
> 17/12/08 14:58:56 WARN TaskSetManager: Stage 1 contains a task of very large size (5541 KB). The maximum recommended task size is 100 KB.
> [Stage 1:=============================>                             (1 + 1) / 2]17/12/08 14:59:12 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 3, spark05, executor 1): java.lang.StringIndexOutOfBoundsException: String index out of range: 16633
> 	at java.lang.String.substring(String.java:1963)
> 	at halign.suffix.GenAlignOut.get_every_sequeces(GenAlignOut.java:35)
> 	at halign.suffix.SparkDNAMSA.lambda$start$208d0b67$1(SparkDNAMSA.java:89)
> 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
> 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
> 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
> 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:150)
> 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
> 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
> 	at org.apache.spark.scheduler.Task.run(Task.scala:100)
> 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)
> 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
> 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
> 	at java.lang.Thread.run(Thread.java:748)
> 
> 17/12/08 14:59:13 ERROR TaskSetManager: Task 1 in stage 1.0 failed 4 times; aborting job
> org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 6, spark05, executor 1): java.lang.StringIndexOutOfBoundsException: String index out of range: 16633
> 	at java.lang.String.substring(String.java:1963)
> 	at halign.suffix.GenAlignOut.get_every_sequeces(GenAlignOut.java:35)
> 	at halign.suffix.SparkDNAMSA.lambda$start$208d0b67$1(SparkDNAMSA.java:89)
> 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
> 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
> 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
> 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:150)
> 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
> 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
> 	at org.apache.spark.scheduler.Task.run(Task.scala:100)
> 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)
> 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
> 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
> 	at java.lang.Thread.run(Thread.java:748)
> 
> Driver stacktrace:
> 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1436)
> 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1424)
> 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
> 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
> 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1423)
> 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
> 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
> 	at scala.Option.foreach(Option.scala:257)
> 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
> 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1651)
> 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1606)
> 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1595)
> 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
> 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
> 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
> 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
> 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
> 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
> 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
> 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
> 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
> 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
> 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
> 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361)
> 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)
> 	at halign.suffix.SparkDNAMSA.start(SparkDNAMSA.java:94)
> 	at main.main(main.java:102)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
> 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:498)
> 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:733)
> 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177)
> 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202)
> 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116)
> 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
> Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 16633
> 	at java.lang.String.substring(String.java:1963)
> 	at halign.suffix.GenAlignOut.get_every_sequeces(GenAlignOut.java:35)
> 	at halign.suffix.SparkDNAMSA.lambda$start$208d0b67$1(SparkDNAMSA.java:89)
> 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
> 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
> 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
> 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:150)
> 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:97)
> 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
> 	at org.apache.spark.scheduler.Task.run(Task.scala:100)
> 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)
> 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
> 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
> 	at java.lang.Thread.run(Thread.java:748)
> 

I got some errors of file access when I run in cluster deployed mode, but I'm currently investigating it.",AxVE,https://github.com/ShixiangWan/HAlign2.0/issues/3
