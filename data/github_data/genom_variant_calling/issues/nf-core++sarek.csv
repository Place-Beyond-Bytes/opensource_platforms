id,title,state,created_at,updated_at,closed_at,body,user,url
MDU6SXNzdWU0NTkxMjg2NjU=,Optimize (pre)processing,CLOSED,2019-06-21T10:26:31Z,2021-07-15T15:27:07Z,2021-07-15T15:27:07Z,"Using the ""old"" 2.3 release version of Sarek on a 48-core node with +700G memory, the preprocessing of a 45x/45x tumour/normal WGS pair took 2d 1h 11m 24s  - that is actually pretty good. OTOH, there are pretty long parts using only a single CPU  [Grafana showing on Munin CPU utilisation graph](https://i.imgur.com/AAYRaWu.png) I know @MaxUlysse already managed to speed up recalibration, would be nice to 

- [x] split fastq files with https://www.nextflow.io/docs/latest/operator.html#splitfastq
- [x] have a look at gains after recalibration speedup
- [ ] optimise some of the QC steps
- [ ] once we are happy with preprocessing, have a look at the variant call/annotations parts also

EDIT: add splitting fastq files",szilvajuhos,https://github.com/nf-core/sarek/issues/15
MDU6SXNzdWU0NjA0NDc2MDU=,Use HaplotypeCaller calls QC metrics,OPEN,2019-06-25T14:08:23Z,2024-08-19T13:14:19Z,,"Issue by @szilvajuhos, moved from https://github.com/SciLifeLab/Sarek/issues/205
> In the HaplotypeCaller calls we can check the common SNPs whether the two are the same: it can serve as a QC metric, i.e.to make sure the patients are not mixed up",maxulysse,https://github.com/nf-core/sarek/issues/17
MDU6SXNzdWU0ODE2MTU5NjY=,Create germlineResource for GRCh37,CLOSED,2019-08-16T13:43:00Z,2022-01-12T04:36:35Z,2019-09-30T08:23:30Z,"We don't have any `germlineResource` for `GRCh37`
Can you please create one?
cf #24",maxulysse,https://github.com/nf-core/sarek/issues/25
MDU6SXNzdWU0ODUxNjk0MTE=,Cleanup issues before release,CLOSED,2019-08-26T10:39:21Z,2019-08-26T14:11:41Z,2019-08-26T14:11:41Z,"- [ ] rename wgs_calling_regions_CAW.list for GRCh37
- [x] QC processes do launch with '--noReports' (should not)
- [ ] add check for misspelled parameteres (i.e. adding '--pom file.vcf' is silently ignored)
... more to come as I am testing",szilvajuhos,https://github.com/nf-core/sarek/issues/28
MDU6SXNzdWU1MDY1MTM1NzY=,Checking read counts for raw FASTQ and deduplicated BAM,CLOSED,2019-10-14T08:32:47Z,2019-10-15T08:44:15Z,2019-10-15T08:44:15Z,"Short answer is yes, all the input reads are in the deduplicated BAM, even the unmapped ones. Note, the recalibrated BAM does _not_ contain all the reads.

We are expecting that deduplicated BAMs are containing all the reads (even unmapped ones). Checking on munin for /data1/szilva/test/results/Preprocessing/B/DuplicateMarked :

There are 443237637 raw read pairs considering ""B"" FASTQs in /data1/szilva/test/BTB0001.tsv . In read group 7_11 there are 209807914 reads, and all of them are paired (bwa would bail out for non-interlaced FASTQs anyway):
```
$ echo `zcat 7_170815_H57FTCCXY-11_BTB0001-B_AGCGATAG_1.fastq.gz| wc -l`/4|bc
209807914
$ echo `zcat 7_170815_H57FTCCXY-11_BTB0001-B_AGCGATAG_2.fastq.gz| wc -l`/4|bc
209807914
```
It can be seen that there are umapped reads (checking for a single read group):
```
$ samtools view -R 7_11.rg B.md.bam > 7_11.sam
$ wc -l 7_11.sam
426207196 7_11.sam
$ tail 7_11.sam | awk 'BEGIN{OFS=""\t""}{print $1,$2,$3,$4}'
ST-E00269:210:H57FTCCXY:7:1224:24545:73159      77      *       0
ST-E00269:210:H57FTCCXY:7:1224:24545:73159      141     *       0
ST-E00269:210:H57FTCCXY:7:1224:28706:73159      77      *       0
ST-E00269:210:H57FTCCXY:7:1224:28706:73159      141     *       0
```
And either the first or the second in the pair is exported at least once:
```
$ awk '{print $1}' 7_11.sam > 7_11.reads
$ sort --parallel=24 -u 7_11.reads | wc -l
209807914
```
Still, there can be reads, where only one part of the pair is saved, and the other is missing. Certainly there are multi-mapping reads (for SAM flags see https://broadinstitute.github.io/picard/explain-flags.html , 64 stands for *first* in pair, 128 is *second* in pair):  
``` 
$ samtools view -@16 -c -R 7_11.rg -f 64 B.md.bam 
213004053
$ samtools view -@16 -c -R 7_11.rg -f 128 B.md.bam 
213203143
```
When unique read names are checked for the first and the second pair, the number of printed out reads matches to the number of reads in the initial FASTQ for that read group:
```
 $ time samtools view -@16 -R 7_11.rg -f 64 B.md.bam | awk '{print $1}'| sort --parallel=16 -u | wc -l
209807914
real    110m24.646s
user    129m8.126s
 $ time samtools view -@16 -R 7_11.rg -f 128 B.md.bam | awk '{print $1}'| sort --parallel=16 -u | wc -l
209807914
real    108m56.789s
user    127m5.951s
```
The ultimate count considering all the read groups in the sample is also all right (a single pipe can kill /tmp, hence the two-step processing):
```
$ samtools view -@32 -f 64 B.md.bam | awk '{print $1}' > 64.reads 
$ samtools view -@32 -f 128 B.md.bam | awk '{print $1}' > 128.reads 
$ for f in 64.reads 128.reads; do echo -n $f"" "";sort --parallel=32 -u $f| wc -l; done
64.reads 443237637
128.reads 443237637
```
@alneberg I think we can safely say that all there reads are there in the _deduplicated_ BAM. 

",szilvajuhos,https://github.com/nf-core/sarek/issues/47
MDU6SXNzdWU1MDY3MDc3ODk=,Add `singularity.autoMounts = true` to config or put `autoMounts = true` in scope,CLOSED,2019-10-14T14:57:54Z,2019-10-22T13:26:48Z,2019-10-22T13:26:48Z,"Hi,

after my `sarek` runs aborted due to what looked like an issue with `singularity` autoMounts, I figured this line in `nextflow.config` 

https://github.com/nf-core/sarek/blob/508735eb09d2fe33e904516db8db128b0a808dd2/nextflow.config#L118 

should either be `singularity.autoMounts = true` or put in a scope.

Hope this helps.
Federico
",FedericoComoglio,https://github.com/nf-core/sarek/issues/48
MDU6SXNzdWU1MDcxNzQ1OTI=,Release 2.5 pulls `dev` SNPEff Containers,CLOSED,2019-10-15T11:10:48Z,2019-10-22T13:26:49Z,2019-10-22T13:26:49Z,https://github.com/nf-core/sarek/blob/2.5/conf/base.config ,apeltzer,https://github.com/nf-core/sarek/issues/49
MDU6SXNzdWU1MDc4MjE2NDc=,Fix path for `ControlFREEC`,CLOSED,2019-10-16T12:32:28Z,2019-10-22T13:26:49Z,2019-10-22T13:26:49Z,"Hi everyone,

running `sarek` v2.5 with `ControlFREEC` among the tools, returned

```
[0;35m[nf-core/sarek] Pipeline completed with errors
Error executing process > 'ControlFreecViz (T_vs_N)'

Caused by:
  Process `ControlFreecViz (T_vs_N)` terminated with an error exit status (1)

Command executed:

  cat /opt/conda/envs/sarek-2.5/bin/assess_significance.R | R --slave --args T.pileup.gz_CNVs T.pileup.gz_ratio.txt
  cat /opt/conda/envs/sarek-2.5/bin/assess_significance.R | R --slave --args T.pileup.gz_normal_CNVs T.pileup.gz_normal_ratio.txt
  cat /opt/conda/envs/sarek-2.5/bin/makeGraph.R | R --slave --args 2 T.pileup.gz_ratio.txt T.pileup.gz_BAF.txt
  cat /opt/conda/envs/sarek-2.5/bin/makeGraph.R | R --slave --args 2 T.pileup.gz_normal_ratio.txt N.pileup.gz_BAF.txt
  perl /opt/conda/envs/sarek-2.5/bin/freec2bed.pl -f T.pileup.gz_ratio.txt > T.bed
  perl /opt/conda/envs/sarek-2.5/bin/freec2bed.pl -f T.pileup.gz_normal_ratio.txt > N.bed

Command exit status:
  1

Command output:
  (empty)

Command error:
  cat: /opt/conda/envs/sarek-2.5/bin/assess_significance.R: No such file or directory
```

The reason is that `/opt/conda/envs/sarek-2.5/bin/assess_significance.R` is actually `/opt/conda/envs/nf-core-sarek-2.5/bin/assess_significance.R`

A quick check suggests to fix the path in:

```
grep 'envs/sarek-' main.nf
    cat /opt/conda/envs/sarek-${workflow.manifest.version}/bin/assess_significance.R | R --slave --args ${cnvTumor} ${ratioTumor}
    cat /opt/conda/envs/sarek-${workflow.manifest.version}/bin/assess_significance.R | R --slave --args ${cnvNormal} ${ratioNormal}
    cat /opt/conda/envs/sarek-${workflow.manifest.version}/bin/makeGraph.R | R --slave --args 2 ${ratioTumor} ${bafTumor}
    cat /opt/conda/envs/sarek-${workflow.manifest.version}/bin/makeGraph.R | R --slave --args 2 ${ratioNormal} ${bafNormal}
    perl /opt/conda/envs/sarek-${workflow.manifest.version}/bin/freec2bed.pl -f ${ratioTumor} > ${idSampleTumor}.bed
    perl /opt/conda/envs/sarek-${workflow.manifest.version}/bin/freec2bed.pl -f ${ratioNormal} > ${idSampleNormal}.bed
    genesplicer = params.genesplicer ? ""--plugin GeneSplicer,/opt/conda/envs/sarek-${workflow.manifest.version}/bin/genesplicer,/opt/conda/envs/sarek-${workflow.manifest.version}/share/genesplicer-1.0-1/human,context=200,tmpdir=\$PWD/${reducedVCF}"" : ""--offline""
    genesplicer = params.genesplicer ? ""--plugin GeneSplicer,/opt/conda/envs/sarek-${workflow.manifest.version}/bin/genesplicer,/opt/conda/envs/sarek-${workflow.manifest.version}/share/genesplicer-1.0-1/human,context=200,tmpdir=\$PWD/${reducedVCF}"" : ""--offline""
```

The run completed successfully after fixing this.

Federico",FedericoComoglio,https://github.com/nf-core/sarek/issues/50
MDU6SXNzdWU1MTQwNTMzODE=,wholegenome.interval_list not recognized by CreateIntervalsBed process,CLOSED,2019-10-29T16:26:43Z,2020-01-27T12:39:15Z,2020-01-27T12:39:15Z,"I may be overlooking something but Sarek does not seem to document the input file formats/purposes of the genome files. For *most* files, the purpose is obvious but some, at least for me, aren’t. And for others it isn’t clear what file format is expected.

For instance, I had assumed that the `wholegenome.interval_list` Picard-formatted file from the GATK resource bundle would be valid as a  `genomes.intervals` file, but the result is a cryptic error message, as well as a work directory full of weird BED files:

> java.lang.NumberFormatException: For input string: ""VN"" […]

It turns out that [the relevant Sarek process](https://github.com/nf-core/sarek/blob/05c6e0b63251af83b9e7d5b259cec9e679fccf57/main.nf#L526-L552) only supports two out of the [three formats described by the GATK documentation](https://software.broadinstitute.org/gatk/documentation/article?id=11009), and notably does not support the Picard-format file, which is included in the official GATK bundle.",klmr,https://github.com/nf-core/sarek/issues/56
MDU6SXNzdWU1MTg0MzQwODQ=,Why are we using -B 3 (tumor mismatch penalty)?,OPEN,2019-11-06T12:15:42Z,2022-07-15T19:44:20Z,,"We should have more information on why we're doing that.
I do believe it's a good idea, but can we back it up?
https://github.com/nf-core/sarek/blob/05c6e0b63251af83b9e7d5b259cec9e679fccf57/main.nf#L673-L681",maxulysse,https://github.com/nf-core/sarek/issues/62
MDU6SXNzdWU1MjA5OTA2NjE=,Add support for CRAM,CLOSED,2019-11-11T14:20:41Z,2021-11-24T10:20:36Z,2021-11-24T10:20:35Z,cf https://github.com/SciLifeLab/Sarek/issues/511,maxulysse,https://github.com/nf-core/sarek/issues/63
MDU6SXNzdWU1MjE0MDUyNDg=,Use spark for GATK,CLOSED,2019-11-12T08:40:53Z,2020-01-27T12:38:56Z,2020-01-27T12:38:56Z,"From @apeltzer cf https://github.com/SciLifeLab/Sarek/issues/730

> Might simply work like this: https://software.broadinstitute.org/gatk/documentation/article?id=11245
> 
> Copying the text from there over here:
> 
> > You don't need a Spark cluster to run Spark-enabled GATK tools!
> >
> > If you're working on a ""normal"" machine (even just a laptop) with multiple CPU cores, the GATK engine can still use Spark to create a virtual standalone cluster in place, and set it to take advantage of however many cores are available on the machine -- or however many you choose to allocate. See the example parameters below and the local-Spark tutorial for more information on how to control this. And if your machine only has a single core, these tools can always be run in single-core mode -- it'll just take longer for them to finish.
> >
> > To be clear, even the Spark-only tools can be run on regular machines, though in practice a few of them may be prohibitively slow (SV tools and PathSeq). See the Tool Docs for tool-specific recommendations.
> > 
> > If you do have access to a Spark cluster, the Spark-enabled tools are going to be extra happy but you may need to provide some additional parameters to use them effectively. See the cluster-Spark tutorial for more information.
> > Example command-line parameters
> > 
> > Here are some example arguments you would give to a Spark-enabled GATK tool:
```
    --spark-master local[*] -> ""Run on the local machine using all cores""
    --spark-master local[2] -> ""Run on the local machine using two cores"" 
```
---
> Should be possible to run e.g. `picard markduplicates` like this, same for some other stuff such as `BaseRecalibrator` and `HaplotypeCaller`.
---
> https://gatkforums.broadinstitute.org/gatk/discussion/23441/markduplicatespark-is-slower-than-normal-markduplicates
>
> We should really look into this!
---
> Apparently it benefits when not sorting reads prior `MarkDuplicates` and takes direct `BWA mem` BAM output. So we should really try it :-)
---
> We should move this over to nf-core/sarek ;)
---
Moved over ;-)",maxulysse,https://github.com/nf-core/sarek/issues/64
MDU6SXNzdWU1MjE1MDQyNDA=,Enable FilterMutect2Call without --pon,CLOSED,2019-11-12T11:52:09Z,2020-02-04T10:52:11Z,2020-02-04T10:52:10Z,"from @lconde-ucl (cf https://github.com/SciLifeLab/Sarek/issues/814)
> Hi Max,
>
> Thanks a lot. Yes, I understand that using a PON is recommended and is definitely of great help when you don't have matching normals. But when you do have matching tumour/normal pairs, it will just be another filter for systematic sequencing/protocol/pipeline artifacts.
>
> But regardless if you use a PON or not, the filtering (""FilterMutectCalls"" step) has to be done anyway, otherwise ""mutect2"" would only emit an unfiltered VCF. ""FilterMutectCalls"" provides the annotations to the variants called by mutect2 (either PASS or failed because of ""germline_risk"", ""multiallelic"", ""clustered_events"", ""bad_haplotype"", etc), so irrespective of the use of a PON or not, it is needed to get a properly annotated VCF. 
>
> As an example, this is a VCF file that you would get after running mutect2 without any PON:
>
```
> gatk Mutect2 -R $ref -I $tumor.bam -I $normal.bam -tumor T -normal N  -O unfiltered.vcf --germline-resource ...

[...]
chr1	84490478	.	C	CT	.	.	DP=60;ECNT=1;NLOD=5.02;N_ART_LOD=-8.630e-01;POP_AF=1.000e-06;REF_BASES=CCCAAGTATCCTTTTTTTTTT;RPA=11,12;RU=T;STR;TLOD=10.72	GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB	
0/1:16,6:0.377:8,2:8,4:34,34:121,122:60:30:0.192,0.263,0.273:0.046,0.013,0.941	0/0:17,0:0.127:11,0:6,0:35,0:181,0:0:0
chr1	100206310	.	TAA	T,TA,TAAA	.	.	DP=263;ECNT=1;NLOD=-2.149e+00,-2.977e+01,4.27;N_ART_LOD=12.16,31.15,8.65;POP_AF=1.000e-06,4.452e-03,1.000e-06;REF_BASES=ATCTATTTTTTAAAAAAAAAA;RPA=13,11,12,14;RU=A;STR;TLOD=7.59,23.90,1
1.86	GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB	0/1/2/3:51,7,16,9:0.135,0.212,0.158:15,4,4,3:36,3,12,6:29,28,29,23:122,100,107,104:60,60,60:17,34,17:0.182,0.172,0.193:0.019,0.011,0.970	0/0:59,8,22,12:0.119,0.223,0.158:20,5,6,
3:39,3,16,9:32,33,32,32:186,193,183,154:60,60,60:29,25,26
chr1	102889509	.	T	G	.	.	DP=47;ECNT=1;NLOD=5.72;N_ART_LOD=-1.301e+00;POP_AF=1.000e-06;REF_BASES=CTCGGTCACCTTTTTCCCCTT;TLOD=25.83	GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB	0/1:19,9:0.321:14,6:5,3:
34,35:103,100:60:27:0.303,0.273,0.321:0.014,0.034,0.952	0/0:19,0:1.054e-04:9,0:10,0:35,0:180,0:0:0
[...]
```
>
> And this is what you get after running filterMutect2Calls on the above file:
>
```
> gatk FilterMutectCalls -V unfiltered.vcf -O final.vcf

[...]
chr1	84490478	.	C	CT	.	str_contraction	DP=60;ECNT=1;NLOD=5.02;N_ART_LOD=-8.630e-01;POP_AF=1.000e-06;P_CONTAM=0.00;P_GERMLINE=-5.511e+00;REF_BASES=CCCAAGTATCCTTTTTTTTTT;RPA=11,12;RU=T;STR;TLOD=10.72	GT:AD:AF:F1R2:F2R1:MBQ:M
FRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB	0/1:16,6:0.377:8,2:8,4:34,34:121,122:60:30:0.192,0.263,0.273:0.046,0.013,0.941	0/0:17,0:0.127:11,0:6,0:35,0:181,0:0:0
chr1	100206310	.	TAA	T,TA,TAAA	.	artifact_in_normal;germline_risk;multiallelic	DP=263;ECNT=1;NLOD=-2.149e+00,-2.977e+01,4.27;N_ART_LOD=12.16,31.15,8.65;POP_AF=1.000e-06,4.452e-03,1.000e-06;P_CONTAM=0.00;P_GERMLINE=-5.710e+0
0,0.00,-1.101e+01;REF_BASES=ATCTATTTTTTAAAAAAAAAA;RPA=13,11,12,14;RU=A;STR;TLOD=7.59,23.90,11.86	GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB	0/1/2/3:51,7,16,9:0.135,0.212,0.158:15,4,4,3:36,3,12,6:29,28,29,23:122,100,107,104:60,60
,60:17,34,17:0.182,0.172,0.193:0.019,0.011,0.970	0/0:59,8,22,12:0.119,0.223,0.158:20,5,6,3:39,3,16,9:32,33,32,32:186,193,183,154:60,60,60:29,25,26
chr1	102889509	.	T	G	.	PASS	DP=47;ECNT=1;NLOD=5.72;N_ART_LOD=-1.301e+00;POP_AF=1.000e-06;P_CONTAM=0.00;P_GERMLINE=-6.212e+00;REF_BASES=CTCGGTCACCTTTTTCCCCTT;TLOD=25.83	GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:S
A_POST_PROB	0/1:19,9:0.321:14,6:5,3:34,35:103,100:60:27:0.303,0.273,0.321:0.014,0.034,0.952	0/0:19,0:1.054e-04:9,0:10,0:35,0:180,0:0:0
[...]
```
>
> The difference is that now the FILTER column (column 7) is filled in with annotations, and the variants that pass (only the last one in this case) can be identified. This is independent of the PON.
>
> I hope this makes sense?
>
> That's why I think the FilterMutectCalls steps should be always done. If you feel you would like to enable it with a warning that's fine. Or if you prefer to disable it by default and have an option to enable it without a PON I think is fine to.. Whatever you think is best, but it would be great to give the user the option to use it without a PON.
>
> Sorry for the long reply!
> Many thanks as always,
> Lucia
",maxulysse,https://github.com/nf-core/sarek/issues/65
MDU6SXNzdWU1MjcyMDAwMDg=,Add deepTools to get bigWig coverage files,CLOSED,2019-11-22T13:37:49Z,2022-02-11T17:16:22Z,2022-02-11T17:16:22Z,"bigWig is a compact format to get coverage information, and we can add it to IGV (https://github.com/igvteam/igv.js/wiki/Wig-Track) . With deepTools (https://deeptools.readthedocs.io/en/develop/index.html) we can easily calculate this file like

```bamCoverage -bs 10 -v -b Preprocessing/Recalibrated/sample.recal.bam --ignoreDuplicates --numberOfProcessors 48 --outFileName coverageDir/sample.coverage.bigWig``` 

It would be nice to have it among the results (and deepTools in the container, see conda install at: https://deeptools.readthedocs.io/en/develop/content/installation.html) . Maybe it can replace QualiMap and provide the same QC stuff.",szilvajuhos,https://github.com/nf-core/sarek/issues/70
MDU6SXNzdWU1MzIxMTIxOTA=,BaseRecalibrator is eating up all the memory,CLOSED,2019-12-03T16:35:53Z,2019-12-06T13:40:06Z,2019-12-06T13:40:06Z,"Looks we were left with the old memory settings with BaseRecalibrator, and it is happily grabs all the memory.",szilvajuhos,https://github.com/nf-core/sarek/issues/72
MDU6SXNzdWU1MzU2OTg3NjY=,Enable joint variant calling for germline samples,CLOSED,2019-12-10T12:29:40Z,2022-07-21T08:20:33Z,2022-07-21T08:20:33Z,"Allow calling of germline short variants jointly on multiple samples as outlined here https://software.broadinstitute.org/gatk/documentation/article?id=11019. This approach is expected to increase accuracy and sensitivity as it makes use of population-wide information.

Each sample is first called individually using HaplotypeCaller in -ERC GVCF mode. GenotypeGVCF is then run on all g.vcfs together. See GATK documentation here: https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145",ikeller,https://github.com/nf-core/sarek/issues/75
MDU6SXNzdWU1MzkwNDE3NzE=,Collect MarkDuplicatesSpark metrics with the standalone Picard tool EstimateLibraryComplexity,CLOSED,2019-12-17T12:40:27Z,2021-07-15T15:14:14Z,2021-07-15T15:14:13Z,"cf #64 
cf https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_transforms_markduplicates_MarkDuplicatesSpark.php
> Collecting duplicate metrics slows down performance and thus the metrics collection is optional and must be specified for the Spark version of the tool with '-M'. It is possible to collect the metrics with the standalone Picard tool EstimateLibraryComplexity.",maxulysse,https://github.com/nf-core/sarek/issues/81
MDU6SXNzdWU1Mzk3ODkwOTM=,Singularity image - too many levels of symbolic links,CLOSED,2019-12-18T16:31:32Z,2019-12-18T23:58:10Z,2019-12-18T23:58:10Z,"XBIF04:Desktop $ singularity pull --name nf-core/sarek:2.5.2.img docker://nfcore/sarek:2.5.2

INFO:    Converting OCI blobs to SIF format
INFO:    Starting build...
Getting image source signatures
Copying blob sha256:c5e155d5a1d130a7f8a3e24cee0d9e1349bff13f90ec6a941478e558fde53c14
 43.24 MiB / 43.24 MiB [===================================================] 42s
Copying blob sha256:a5eecf513b481c8a6b4251fd2a356dd9a719faa9458bf052014b56d1b760ea21
 90.70 MiB / 90.70 MiB [=================================================] 1m41s
Copying blob sha256:a2f1f1a8c0a9ef4422f59ec97c292815684a9f18d2b8a557d3b2ddfc96fc1401
 99.07 MiB / 99.07 MiB [=================================================] 1m55s
Copying blob sha256:f827e546ce315d340e846de1af32a6177d9c3d894dd69bfec358196401d0975a
 1.03 MiB / 1.03 MiB [======================================================] 0s
Copying blob sha256:7eb6a47b847d5fb772d28529dc4e43628d4dacf8dadcb3806f8d90c4e32fd04d
 12.82 MiB / 12.82 MiB [===================================================] 15s
Copying blob sha256:54030feaba2f217c1c7c1b9d40630384f9724748734e1957ed7014768dad36d6
 421 B / 421 B [============================================================] 0s
Copying blob sha256:d1eda63fd41c73c659b5479c67d70b538fda017f2fe191f05c966fd27f0f6745
 1.20 GiB / 1.20 GiB [==================================================] 30m16s
Copying config sha256:53abadc78ffac411938e957de42e3d95b1914d74132e6bb7964e56fb34bf3abd
 4.12 KiB / 4.12 KiB [======================================================] 0s
Writing manifest to image destination
Storing signatures
FATAL:   While making image from oci registry: while building SIF from layers: packer failed to pack: while unpacking tmpfs: unpack: error extracting layer: link /var/folders/78/rjtn6lb534v9n8kxwm56fl_m0000gn/T/sbuild-316747039/fs/opt/conda/pkgs/ncurses-6.1-he6710b0_1/share/terminfo/N/NCR260VT300WPP /var/folders/78/rjtn6lb534v9n8kxwm56fl_m0000gn/T/sbuild-316747039/fs/opt/conda/share/terminfo/N/NCR260VT300WPP: too many levels of symbolic links",blancha,https://github.com/nf-core/sarek/issues/82
MDU6SXNzdWU1NTQyMjkxNjc=,Rename GRCh38 > hg38 in igenomes.config,CLOSED,2020-01-23T15:27:13Z,2021-10-05T12:26:23Z,2021-10-05T12:26:23Z,"The Broad/GATK ships their genome resource bundles relative to UCSC builds i.e. `hg19` and `hg38` and these are also used in Sarek. However, they have been named according to the Ensembl naming convention i.e. `GRCh38` and `GRCh37`. This is slightly misleading and also adds to the confusion when trying to figure out how the pipeline is using and providing its genome reference data. 

The keys in `igenomes.config` for these assemblies will have to be changed from `GRCh38` > `hg38` and `GRCh37` > `hg19`, and any custom files added for use with the pipeline will also have to be renamed appropriately (and uploaded to AWS iGenomes!).",drpatelh,https://github.com/nf-core/sarek/issues/86
MDU6SXNzdWU1NTU1NDg1MjI=,Final delivery checks,CLOSED,2020-01-27T12:41:36Z,2021-07-15T09:06:40Z,2021-07-15T09:06:40Z,"Issue by @szilvajuhos, moved from SciLifeLab#448

> We are delivering variant calls and annotations at the very end.
> We have to have a final step/util to check all the VCF/annotation/QC/alignment files are there.

Do we really need that?",maxulysse,https://github.com/nf-core/sarek/issues/88
MDU6SXNzdWU1NTU1NTA1MjY=,Do VQSR for HaplotypeCaller calls,CLOSED,2020-01-27T12:45:38Z,2022-07-21T08:20:06Z,2022-07-21T08:20:06Z,"Issue by @malinlarsson, moved from SciLifeLab#513

> Filter the calls from HaplotypeCaller with Variant Quality Score Recalibration according to GATK best practise (Tools VariantRecalibrator, ApplyRecalibration, see https://gatkforums.broadinstitute.org/gatk/discussion/39/variant-quality-score-recalibration-vqsr, cf https://gatk.broadinstitute.org/hc/en-us/articles/360037594511-VariantRecalibrator or a more recent version)

Useful comment by @apeltzer 

> Keep in mind, that you'd need a ""bit"" of data for doing VQSR properly. The recommendation was to use at least 30 WES or 1WGS sample for performing VQSR.

> I started working on a solution (in NGI-ExoSeq) to automatically download 35 of the 1000G Exome Datasets, run HaplotypeCaller on them and use them for analysis procedures with less than the minimum required 30 WES samples.",maxulysse,https://github.com/nf-core/sarek/issues/89
MDU6SXNzdWU1NTU1NTEzNjk=,Add a MultiQC report as an example,CLOSED,2020-01-27T12:47:19Z,2021-07-15T09:06:31Z,2021-07-15T09:06:31Z,"Issue by @MaxUlysse, moved from SciLifeLab#550",maxulysse,https://github.com/nf-core/sarek/issues/90
MDU6SXNzdWU1NTU1NTMxMTc=,gnomAD and ExAC from VEP,CLOSED,2020-01-27T12:50:48Z,2022-06-10T10:03:05Z,2022-06-10T10:03:05Z,"Issue by @szilvajuhos, moved from SciLifeLab#622

> **Add this AF metrics to annotation output**

> In the ""gnomAD and ExAC"" section of https://www.ensembl.org/info/docs/tools/vep/script/vep_example.html it is stated that gnomAD replacing ExAC, and we should add this feature as it is not enabled by default.
> Also see: https://www.ensembl.org/info/docs/tools/vep/script/vep_options.html#opt_af_gnomad


",maxulysse,https://github.com/nf-core/sarek/issues/91
MDU6SXNzdWU1NTU1NTM5NDQ=,Clean up selectROI.py files,CLOSED,2020-01-27T12:52:20Z,2020-07-16T06:57:37Z,2020-07-16T06:57:37Z,"Issue by @szilvajuhos, moved from SciLifeLab#623

> There a quite a lot of chr*bam files left after running selectROI.py.
> It is actually a source of error and ugly.
> Clean it up.",maxulysse,https://github.com/nf-core/sarek/issues/92
MDU6SXNzdWU1NTU1NTQ3MjY=,Add check for unique lanes in sample when parsing TSV files from FASTQ samples,CLOSED,2020-01-27T12:53:50Z,2022-07-08T14:15:48Z,2022-07-08T14:15:48Z,"Issue by @MaxUlysse, moved from SciLifeLab#626

> We should have a warning when at least one sample has more than 2 FASTQ with the same lane.
> Right now, it's failing at the merging BAMs step.

> **Describe the solution you'd like**

> Add some checks when parsing the TSV file

> **Describe alternatives you've considered**

> No alternatives considered at the moment

> **Additional context**

> Issue discovered by Katja when running some WES project

Issue by @nicweb, moved from SciLifeLab#714

> When there is no lane information given in the sample.tsv markduplicates will fail with

> ```
>htsjdk.samtools.SAMFormatException: Error parsing SAM header. Problem
>parsing @rg key:value pair.
>```
> as readgroup ID is empty.

> Would be nice to check TSV before execution or autoset empty lane with sampleID or similar to avoid downstream execution halt, or update documentation that field should not be empty.",maxulysse,https://github.com/nf-core/sarek/issues/93
MDU6SXNzdWU1NTU1NTc1NTk=,Add Adaptor Clipping / Quality Trimming Step,CLOSED,2020-01-27T12:59:26Z,2020-02-27T13:14:42Z,2020-02-27T13:14:42Z,"Issue by @apeltzer, moved from SciLifeLab#664

> **Is your feature request related to a problem? Please describe.**

> Not really - but we had some discussions about it on the Gitter channel for example that it might be helpful in certain cases to provide means to clip adaptors and also quality trim reads before mapping.
> I guess TrimGalore would be nice to have but I also hear very good things about ""Atropos"". 

> **Describe the solution you'd like**

> An optional step to perform read clipping/trimming before mapping in Sarek :-) 

> **Describe alternatives you've considered**

> Externalize the process, but as it is probably easy to built-in and some people might want it too - why not have it internalized + made optional :-) ",maxulysse,https://github.com/nf-core/sarek/issues/94
MDU6SXNzdWU1NTU1NTg1MjI=,Community feedback needed: Consider other tools,OPEN,2020-01-27T13:01:19Z,2024-11-05T08:13:23Z,,"Issue by @MaxUlysse, moved from SciLifeLab#666

- [ ] [ExpansionHunter](https://github.com/Illumina/ExpansionHunter) for estimating repeat sizes
- [ ] [QDNAseq](https://github.com/ccagc/QDNAseq) CNVs for shallow WGS
- [x] [CNVkit](https://github.com/etal/cnvkit) for CNVs
- [ ] [deconstructSigs](https://github.com/raerose01/deconstructSigs) for mutational signatures
- [ ] ditto [mutation-signatures](https://github.com/mskcc/mutation-signatures) 
- [ ] [SomaticSeq](https://github.com/bioinform/somaticseq) ensemble caller and machine learning
- [x] [MSIsensor](https://github.com/ding-lab/msisensor) for replication slippage variants at microsatellite regions
- [ ] [Oncotator](https://portals.broadinstitute.org/oncotator/) for annotating human point mutations and indels
- [ ] [MutSig](https://software.broadinstitute.org/cancer/cga/mutsig) for Mutation Significance checks
- [ ] [TrackSig](https://github.com/morrislab/TrackSig) to infer mutational signatures in cancer over time
- [ ] [DriverPower](https://github.com/smshuai/DriverPower) to discover potential coding and non-coding cancer driver elements from tumour whole-genome or whole-exome somatic mutations
- [ ] [SVclone](https://github.com/mcmero/SVclone) for inferring the cancer cell fraction of tumour structural variation from whole-genome sequencing data
- [ ] [HsMetrics](https://broadinstitute.github.io/picard/javadoc/picard/picard/analysis/directed/HsMetrics.html) better QC
- [x] [CNNScoreVariants](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.0.3.0/org_broadinstitute_hellbender_tools_walkers_vqsr_CNNScoreVariants.php) Annotate a VCF with scores from a Convolutional Neural Network (CNN)
- [ ] [SignatureAnalyzer](https://software.broadinstitute.org/cancer/cga/msp) yet an other mutation signature analyser from Broad
- [ ] [PROSIC2](https://github.com/prosic/prosic2) is a caller for somatic variants in tumor-normal sample pairs
- [ ] [seqCAT](https://github.com/fasterius/seqCAT) Evaluation of SNV profiles
- [ ] [varlociraptor](https://github.com/varlociraptor/varlociraptor) A Rust library for variant calling using a latent variable model
- [x] [GATK CNN](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_vqsr_CNNScoreVariants.php) Convolutional Neural Net to filter annotated variants
- [x] [Google DeepVariant](https://github.com/google/deepvariant) Deep neural network to call genetic variants from next-generation DNA sequencing data
- [ ] [Octopus](https://github.com/luntergroup/octopus)
- [x] [mosdepth](https://github.com/brentp/mosdepth) fast BAM/CRAM depth calculation for WGS, exome, or targeted sequencing
- [ ] [rock](https://github.com/pdiakumis/rock) circos plot out of manta
- [ ] DeepTrio
- [ ] [SmuRF](https://github.com/skandlab/SMuRF#input-alt) for Random Forest Ensembl Somatic Calling
- [x] [PureCN](https://github.com/lima1/PureCN) for estimating purity/ploidy
- [ ] [Lofreq](https://csb5.github.io/lofreq/) a fast and sensitive variant-caller for inferring SNVs and indels from next-generation sequencing data. LoFreq is very sensitive; most notably, it is able to predict variants below the average base-call quality (i.e. sequencing error rate).
- [ ] [DeepSomatic](https://github.com/google/deepsomatic) extension of deep learning-based variant caller DeepVariant
- [ ] [MEDICC2](https://bitbucket.org/schwarzlab/medicc2/src/master/) Minimum Event Distance for Intra-tumour Copy-number Comparisons
- [ ] [DPClust](https://github.com/Wedge-lab/dpclust) Dirichlet Process based methods for subclonal reconstruction of tumours
- [ ] [LINX](https://github.com/hartwigmedical/hmftools/tree/master/linx) annotation, interpretation and visualisation tool for structural variants
- [ ] [FindDNAFusion](https://github.com/jml-bioinfo/FindDNAFusion) a combinatorial pipeline for the detection of cancer-associated gene fusions in next-generation DNA sequencing data",maxulysse,https://github.com/nf-core/sarek/issues/95
MDU6SXNzdWU1NTU1NjAxNTQ=,Target Coverage in BAMQC,CLOSED,2020-01-27T13:04:30Z,2020-02-12T16:20:25Z,2020-02-12T16:20:24Z,"Issue by @apeltzer, moved from SciLifeLab#670

> **Is your feature request related to a problem? Please describe.**

> The target coverage is computed in BAMQC based on the entire genome.
> For exome data (even with specified BED file and therefore regions, Sarek doesn't specify the coverage on covered sites but instead the overall coverage on the entire genome. 

> **Describe the solution you'd like**

> Use the `--gff` switch in QualiMap2 to run with the specified BED file.
> That provides more accurate coverage on target capture coverage.

Comments:
@szilvajuhos

> Cool, other groups were also requesting something similar, I already made some prototype, we can also add the QualiMap2 part. Will work on that.

@apeltzer

> Just make sure to use a more updated QualiMap2 version.
> The possibility to use BED-3 instead of BED-6 format, was just introduced fairly recently (upon my request... https://bitbucket.org/kokonech/qualimap/commits/all ).
> I don't know which version of QualiMap2 is shipped with the current Sarek container(s), so we should make sure that this works :-)",maxulysse,https://github.com/nf-core/sarek/issues/96
MDU6SXNzdWU1NTU1NjEzMDc=,Check BED compatibility,OPEN,2020-01-27T13:06:48Z,2024-08-19T13:12:24Z,,"Issue by @apeltzer, moved from SciLifeLab#687

> **Is your feature request related to a problem? Please describe.**

> Not really ;-)
> But this happens quite often when people use a target capture method. 

> **Describe the solution you'd like**

> A small extra process that checks BED files for consistency with the utilized reference genome.
> Either check only and issue a warning or also offer to remove the `chr`prefix for example ;-) 

Comments:

@apeltzer

> Can do that if you guys don't mind :-)

@MaxUlysse

> That would be amazing \o/",maxulysse,https://github.com/nf-core/sarek/issues/97
MDU6SXNzdWU1NTU1NjgwNTU=,Sex determination Option,OPEN,2020-01-27T13:19:11Z,2024-08-19T13:12:24Z,,"Issue by @apeltzer, moved from SciLifeLab#688

> **Is your feature request related to a problem? Please describe.**

> If you don#t know the sex of an individual, one could run simple scripts to perform sex determination.
> This is commonly done in population genetics and there are quite a bunch of scripts to perform that automatically, more or less checking chrX/chrY ratios with thresholds.

> **Describe the solution you'd like**

> Having an optional step may be to run sex determination, as we do have cases where we don't have the information.... ",maxulysse,https://github.com/nf-core/sarek/issues/98
MDU6SXNzdWU1NTU1NjkzMTc=,Add variants comparison between multiple T/N pairs from the same patient,OPEN,2020-01-27T13:21:26Z,2024-08-19T13:12:25Z,,"Issue by @MaxUlysse, moved from SciLifeLab#761

> It would be good to have a script that show which variants are added or lost between a tumor and a relapse.

> I would like a script to compare variants between multiple T/N pairs from the same patient.

> Requested by Teresita",maxulysse,https://github.com/nf-core/sarek/issues/99
MDU6SXNzdWU1NTU1NzI4NTY=,Provide reference files for WES for ASCAT,OPEN,2020-01-27T13:27:17Z,2024-08-20T17:39:54Z,,"Issue by @Anuragksingh, moved from SciLifeLab#763

> Hello, 
> I had some doubts over ascat.
> It would be really helpful if you can provide some insights about that.

> Can we use the available ascat directly on WES data?
> I guess the creation of  BAF and LogR  files from bam  would be same for WES data also.
> Or do we need to apply additional filters to use ascat on WES data?
> For reference kindly see this link:
> https://github.com/Crick-CancerGenomics/ascat/issues/19
> Also does we need to add --exome flag when running ascat on WES data?

> Any suggestions on above aspects would be really helpful.

> Thanks

Comments:

@MaxUlysse

>  Running ASCAT for WES is definitively something that we're thinking about.
> I remember reading about this link before, I'll definitively look more into it.
> Thanks again for all your interest in this project

@szilvajuhos

> To run ASCAT on exome you will need a separate file of SNPs that is covering the targeted exons only

@dingxm

> Hi,
> We are trying to analyze exome data for copy number.
> The exome data has extended probes covering regions outside of the exomes.
> It seems that last year ASCAT could not use exome data without pre-processing, is this still true or is there a version that deals with Exome data now?
> Thank you

@szilvajuhos

> Hej,
> If you have a BED file that is covering your WES target, I would recommend to make a .loci file i.e. by filtering the latest dbSNP for common biallelic SNPs that are overlapping your target.
> You have to have quite a few loci positions (since the target is smaller and the genome is not covered evenly) to make sure you will still have enough SNPs.
> @malinlarsson is better in playing with it, the .loci file I have created contains ~16M lines:

> ```
> $ wc -l dbSNP_151_S04380110_AllExonV5_hg38.loci
> 16200625
> $ head -3 dbSNP_151_S04380110_AllExonV5_hg38.loci
> chr1    65529
> chr1    65552
> chr1    65591
> ```

> We have managed to make meaningful charts with this loci file (I can share it if you want to give a try), but certainly it is only the beginning of finding the right settings.
",maxulysse,https://github.com/nf-core/sarek/issues/100
MDU6SXNzdWU1NTU1NzM2MjQ=,Should we use -Y option for bwa mem?,CLOSED,2020-01-27T13:28:32Z,2022-07-17T13:57:04Z,2022-07-17T13:57:03Z,"Issue by @MaxUlysse, moved from SciLifeLab#770

> > Use soft clipping CIGAR operation for supplementary alignments.
> > By default, BWA-MEM uses soft clipping for the primary alignment and hard clipping for supplementary alignments.

> We're not using this -Y option for `bwa mem`, but I looked more into the GTAK best practice workflow, and they're using it.

> So I'm wondering, should we use that or not?",maxulysse,https://github.com/nf-core/sarek/issues/101
MDU6SXNzdWU1NTU1NzQxNTk=,Look into MultiQC report for multi patients TSV,OPEN,2020-01-27T13:29:25Z,2022-06-10T07:25:23Z,,"Issue by @MaxUlysse, moved from SciLifeLab#771

> Look how it's like and fix whatever needs fixing",maxulysse,https://github.com/nf-core/sarek/issues/102
MDU6SXNzdWU1NTYwODUxMDI=,FilterMutect2Calls complains about '.' when expecting a numeric value in MPOS fields,CLOSED,2020-01-28T09:32:57Z,2020-06-15T14:20:45Z,2020-06-15T14:20:45Z,Mutect2 complains about '.' when it expects a numeric value during the FilterMutect2Calls process. This appears to be caused by large negative values assigned to the MPOS Info field being converted to '.' by bcftools. We have verified that replacing these '.' with numeric values (we arbitrarily used 5) fixes the problem but this is obviously not an ideal solution.,DrJCampbell,https://github.com/nf-core/sarek/issues/103
MDU6SXNzdWU1NTcwNDQ4ODM=,Deepvariant support,CLOSED,2020-01-29T18:41:36Z,2020-01-30T09:04:13Z,2020-01-30T09:04:13Z,Hi! It would be amazing if you added deepvariant support :),kokyriakidis,https://github.com/nf-core/sarek/issues/105
MDU6SXNzdWU1NTc0Mzk0NTA=,snpEff report bug,CLOSED,2020-01-30T11:43:07Z,2020-02-04T10:54:11Z,2020-02-04T10:54:11Z,"Hi, I found a small bug.

In the report generated by snpEff at the end of it there is a link to ~/results/Reports/SAMPLE/snpEff/Strelka_SAMPLE_variants_snpEff.genes.txt

however, the txt file in this directory is named ~/results/Reports/SAMPLE/snpEff/Strelka_SAMPLE_variants_snpEff.txt

(without the genes part, therefore the link doesn't work)
",egenomics,https://github.com/nf-core/sarek/issues/106
MDU6SXNzdWU1NTc1OTA1MjQ=,amplicon mode,CLOSED,2020-01-30T16:01:15Z,2020-05-24T14:42:22Z,2020-05-24T14:42:22Z,"Hi,
It would be a cool enhancement to have an ""amplicon mode"" for targeted sequencing (i.e. illumina cancer panels, etc).
Probably the big differences with what there is the inclusion of a manifest (bed file with targeted regions). 

QC [fastqc+multiqc]
alignment [bwa-mem]
[deduplication?]<-probably not needed
variant calling [gatk,strelka]

Quality control for target capture experiments:
https://bioconductor.org/packages/release/bioc/html/TEQC.html

CNV:
Currently using CoNVaDING. I am sure there are better alternatives out there.
https://github.com/molgenis/CoNVaDING 
Probably a good alternative is CNVkit: https://cnvkit.readthedocs.io/en/stable/

Calculation of coverages in the targeted regions, gene level and exon level.",egenomics,https://github.com/nf-core/sarek/issues/108
MDU6SXNzdWU1NjA5MjM4Njk=,Handle UMIs,CLOSED,2020-02-06T10:38:19Z,2020-08-31T09:15:04Z,2020-08-31T09:15:04Z,,maxulysse,https://github.com/nf-core/sarek/issues/112
MDU6SXNzdWU1NjQxNTgwNzE=,Missing Variant depths in MulitQC Report - Bcftools,CLOSED,2020-02-12T17:37:43Z,2022-07-18T14:55:06Z,2022-07-18T14:55:06Z,"With Sarek v2.5.2 on WES data (`--genome 'GRCh38' --input sample_all.tsv --tools HaplotypeCaller,Strelka,Manta --targetBED exome_capture.bed`) the Bcftools 'Variant depths' plot  in the MulitQC report is empty. This would be nice to have in the future.",skrakau,https://github.com/nf-core/sarek/issues/114
MDU6SXNzdWU1Njc1MDgxNzQ=,Mutect2 process skipped,CLOSED,2020-02-19T11:36:33Z,2020-08-03T20:16:54Z,2020-05-24T10:18:43Z,"Hello,

I'm trying to run this pipeline with as many annotators as possible. However I can only output a couple of them. One that I cannot output is Mutect2 - it seems sarek skips that process, even if I explicit my germlineResource and pon files (including both indexes). What am I missing?

Code line:
`nextflow run nf-core/sarek --input FASTQ_SUB --tools Mutect2 --germlineResource somatic-b37_af-only-gnomad.raw.sites.vcf --germlineResourceIndex somatic-b37_af-only-gnomad.raw.sites.vcf.idx --pon somatic-b37_Mutect2-WGS-panel-b37.vcf.gz --pon_index somatic-b37_Mutect2-WGS-panel-b37.vcf.idx -profile singularity -c sarek.config
`

Thank you very much,
Pedro R.",PedroRaposo,https://github.com/nf-core/sarek/issues/118
MDU6SXNzdWU1NjkzNzU4Njg=,SamToFastq to unaligned BAM complains about unpaired mates,CLOSED,2020-02-22T18:41:25Z,2022-02-11T17:23:28Z,2022-02-11T17:23:28Z,"Hi, when starting `sarek` with unaligned `BAM` files, I get the following error in the `MapReads` process due to `SamToFastq`. 

```Using GATK jar /opt/conda/envs/nf-core-sarek-2.5.2/share/gatk4-4.1.2.0-1/gatk-package-4.1.2.0-local.jar
Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx372g -jar /opt/conda/envs/nf-core-sarek-2.5.2/share/gatk4-4.1.2.0-1/gatk-package-4.1.2.0-local.jar SamToFastq --INPUT=db2fe200f284c9d024ace1e110d4efa6.CHC1754N.sorted.bam --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true
11:40:52.633 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/envs/nf-core-sarek-2.5.2/share/gatk4-4.1.2.0-1/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
[Sat Feb 22 11:40:52 UTC 2020] SamToFastq  --INPUT db2fe200f284c9d024ace1e110d4efa6.CHC1754N.sorted.bam --FASTQ /dev/stdout --INTERLEAVE true --INCLUDE_NON_PF_READS true  --OUTPUT_PER_RG false --COMPRESS_OUTPUTS_PER_RG false --RG_TAG PU --RE_REVERSE true --CLIPPING_MIN_LENGTH 0 --READ1_TRIM 0 --READ2_TRIM 0 --INCLUDE_NON_PRIMARY_ALIGNMENTS false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false
[Sat Feb 22 11:40:52 UTC 2020] Executing as root@ip-172-31-21-99.ec2.internal on Linux 4.14.165-133.209.amzn2.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.2.0
[M::bwa_idx_load_from_disk] read 3171 ALT contigs
[W::main_mem] when '-p' is in use, the second query file is ignored.
[M::process] read 795736 sequences (100000250 bp)...
[M::process] 0 single-end sequences; 795736 paired-end sequences
INFO	2020-02-22 11:40:59	SamToFastq	Processed     1,000,000 records.  Elapsed time: 00:00:07s.  Time for last 1,000,000:    7s.  Last read position: chr1:3,679,276
...
...
INFO	2020-02-22 16:15:53	SamToFastq	Processed 1,004,000,000 records.  Elapsed time: 04:35:00s.  Time for last 1,000,000:   25s.  Last read position: chrY:59,079,046
[Sat Feb 22 16:15:54 UTC 2020] picard.sam.SamToFastq done. Elapsed time: 275.03 minutes.
Runtime.totalMemory()=144277241856
To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp
htsjdk.samtools.SAMFormatException: SAM validation error: ERROR: Found 6534 unpaired mates
	at htsjdk.samtools.SAMUtils.processValidationError(SAMUtils.java:467)
	at picard.sam.SamToFastq.doWork(SamToFastq.java:211)
	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295)
	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25)
	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162)
	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205)
	at org.broadinstitute.hellbender.Main.main(Main.java:291)
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (2, 449, 0, 0)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (410, 495, 597)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (36, 971)
[M::mem_pestat] mean and std.dev: (496.58, 152.28)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1158)
[M::mem_pestat] skip orientation RF as there are not enough pairs
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::process] read 658604 sequences (82750538 bp)...
[M::mem_process_seqs] Processed 795956 reads in 1069.883 CPU sec, 22.743 real sec
[M::process] 0 single-end sequences; 658604 paired-end sequences
[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (9, 47804, 251, 4)
[M::mem_pestat] skip orientation FF as there are not enough pairs
[M::mem_pestat] analyzing insert size distribution for orientation FR...
[M::mem_pestat] (25, 50, 75) percentile: (433, 512, 600)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (99, 934)
[M::mem_pestat] mean and std.dev: (515.67, 131.15)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 1101)
[M::mem_pestat] analyzing insert size distribution for orientation RF...
[M::mem_pestat] (25, 50, 75) percentile: (1392, 2857, 3468)
[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 7620)
[M::mem_pestat] mean and std.dev: (2465.89, 1275.67)
[M::mem_pestat] low and high boundaries for proper pairs: (1, 9696)
[M::mem_pestat] skip orientation RR as there are not enough pairs
[M::mem_pestat] skip orientation RF
[M::mem_process_seqs] Processed 658604 reads in 1479.307 CPU sec, 30.950 real sec
[main] Version: 0.7.17-r1188
[main] CMD: bwa mem -K 100000000 -R @RG\tID:1\tPU:1\tSM:P1N\tLB:P1N\tPL:illumina -t 48 -M -p Homo_sapiens_assembly38.fasta /dev/stdin -
[main] Real time: 16546.077 sec; CPU: 697892.779 sec
[bam_sort_core] merging from 144 files and 48 in-memory blocks...
```

The command executed `command.sh` was:
```#!/bin/bash -euo pipefail
gatk --java-options -Xmx372g SamToFastq --INPUT=db2fe200f284c9d024ace1e110d4efa6.CHC1754N.sorted.bam --FASTQ=/dev/stdout --INTERLEAVE=true --NON_PF=true | \
bwa mem -K 100000000 -R ""@RG\tID:1\tPU:1\tSM:P1N\tLB:P1N\tPL:illumina""  -t 48 -M Homo_sapiens_assembly38.fasta         -p /dev/stdin - 2> >(tee db2fe200f284c9d024ace1e110d4efa6.CHC1754N.sorted.bam.bwa.stderr.log >&2) |         samtools sort --threads 48 -m 2G - > P1N_1.bam
```
I found some help from Samtools that in these cases either the BAM can be cleaned of unpaired reads before running SamToFastq or that VALIDATION_STRINGENCY=SILENT flag can be used when running SamToFastq.

Is there any reason why you are not doing that? I can make a PR otherwise to fix that. 

It would also be great to have the SamToFastq step separated from the mapping, so we can cache this process in case the mapping fails. Don't you think?

I'm happy to help if you think this is a good idea!",ggabernet,https://github.com/nf-core/sarek/issues/122
MDU6SXNzdWU1Njk3MDA5OTY=,Sarek documentation: broken link,CLOSED,2020-02-22T20:11:04Z,2020-02-24T08:45:31Z,2020-02-24T08:45:13Z,"I get a 404 error

```
Sorry, that page could not be found.

It looks like https://nf-co.re/sarek/docs/use_cases/docs/input doesn't exist. 
```

When clicking on the link named ""input files and documentation"" here: https://nf-co.re/sarek/docs/use_cases#starting-from-raw-fastq-pair-of-fastq-files",winni2k,https://github.com/nf-core/sarek/issues/124
MDU6SXNzdWU1Njk3MDA4MTE=,Sarek documentation: `gender` should be `sex`,CLOSED,2020-02-22T20:44:38Z,2020-02-25T08:18:33Z,2020-02-25T08:18:33Z,"The `samples.tsv` has a second column titled ""gender"" (https://nf-co.re/sarek/docs/input#information-about-the-tsv-files), but the column actually refers to the sex chromosomes (XX, XY, and perhaps in the future even sex chromosome aneuploidies such as X, XXY, XXXY, etc.?). The gender of the patient is irrelevant. Also, this pipeline runs on mice, and mice don't have gender.",winni2k,https://github.com/nf-core/sarek/issues/123
MDU6SXNzdWU1Njk3MDk2ODc=,ASCAT executing R files,CLOSED,2020-02-24T08:42:55Z,2020-02-28T08:11:33Z,2020-02-28T08:11:33Z,"Hi, when running Sarek with multiple variant callers, it seems like the first one is picked and the rest are ignored. I run it indicating Strelka and ASCAT, and ASCAT was just ignored. 

```#!/bin/bash

nextflow run ggabernet/nf-core-sarek -r v2.5.2-branch \
--outdir 's3://qbic-bucket-virginia/resultsdirsarekicgc1' \
-w 's3://qbic-bucket-virginia/workdirsarekicgc1' \
--tracedir 's3://qbic-bucket-virginia/tracesarekicgc1' \
--input 's3://qbic-bucket-virginia/icgc-sarek/input-icgc-1.tsv' \
--genome 'GRCh38' \
--tools 'Strelka,ASCAT,snpEff' \
-c awsbatch.config \
--awsregion 'us-east-1' \
--igenomes_base 's3://qbic-bucket-virginia/references' \
--awscli '/home/ec2-user/miniconda/bin/aws' -resume
```",ggabernet,https://github.com/nf-core/sarek/issues/125
MDU6SXNzdWU1NzE2NDQ1NzI=,No more task to compute but following nodes are still active,CLOSED,2020-02-26T20:37:54Z,2021-09-06T14:30:01Z,2021-07-15T09:03:57Z,"I tried resuming a Sarek call on my server at 3pm today, and now it's 9:30pm, and nothing seems to be happening. This is the output of the pipeline on my screen: 
![image](https://user-images.githubusercontent.com/304958/75385161-8b863080-58df-11ea-9901-ae7787c4258d.png)

This is my nextflow log:
[nextflow.log](https://github.com/nf-core/sarek/files/4258049/nextflow.log)

Nothing seems to be running when I look at htop. Any ideas what might be going on?",winni2k,https://github.com/nf-core/sarek/issues/130
MDU6SXNzdWU1NzI1OTY0OTI=,Switch sarek to DSL 2,CLOSED,2020-02-28T08:10:46Z,2021-07-15T09:02:28Z,2021-07-15T09:02:28Z,,maxulysse,https://github.com/nf-core/sarek/issues/132
MDU6SXNzdWU1NzM5MTEzNDE=,mutect2 fails with --no_intervals option ,CLOSED,2020-03-02T11:29:29Z,2020-03-06T15:03:01Z,2020-03-06T15:03:01Z,"Hi,
I'm trying the pipeline (dev version) with some exome data and there seems to be an issue when running it with the ""--no_intervals"" option. The mutect2 step fails because it tries to use an interval file ""-L no_intervals.bed"" that does not exist. This is an example .command.sh generated in the mutect2 process:

```
#!/bin/bash -euo pipefail
# Get raw calls
gatk --java-options ""-Xmx7g""  Mutect2  \
-R Homo_sapiens_assembly38.fasta  \
-I NET03-A3_Tumour.recal.bam  -tumor NET03-A3_Tumour   \
-I NET03-A7_Ctr.recal.bam -normal NET03-A7_Ctr      \
-L no_intervals.bed    \
--germline-resource gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz   \
-O no_intervals_NET03-A3_Tumour_vs_NET03-A7_Ctr.vcf
```

I understand the reasoning of using the -L argument with the intervals when the pipeline is run in normal mode (i.e., without the --no_intervals"" option), but when the user selects the --no-intervals option, shouldn't this be changed so that the **-L argument in mutect2 points to the --targetBed file** (or nothing for whole genome)?

I guess that what I'm trying to understand too is why the --targetBed targets are being passed to BamQC, manta and strelka, but not to mutect2? Is it because variants called with mutect2 outside the targets are filtered anyway in the concatVCF step later on? 

Thanks
Lucia",lconde-ucl,https://github.com/nf-core/sarek/issues/135
MDU6SXNzdWU1NzQ2NTEzNjk=,Ascat computes all possible N-T combinations,CLOSED,2020-03-03T13:00:21Z,2020-03-06T15:03:07Z,2020-03-06T15:03:06Z,"Hi, I was wondering why 25 ascat processes were started when I had just 5 Tumor-Normal pairs, and it turns out that ascat computes all possible N-T combinations. Eg:

Ascat_CHC2113T_vs_CHC2113N
Ascat_CHC2113T_vs_CHC2115N
Ascat_CHC2113T_vs_CHC912N

I can also help with fixing that, just wanted to document it in an issue :)",ggabernet,https://github.com/nf-core/sarek/issues/136
MDU6SXNzdWU1NzUzMzExMTM=,Fix VEP CI tests,CLOSED,2020-03-04T11:33:33Z,2020-03-05T10:45:19Z,2020-03-05T10:45:19Z,,maxulysse,https://github.com/nf-core/sarek/issues/137
MDU6SXNzdWU1NzU0NDgxMzc=,Fix TIDDIT recipe on bioconda,CLOSED,2020-03-04T14:30:54Z,2021-02-19T12:10:10Z,2021-02-19T12:10:10Z,https://github.com/bioconda/bioconda-recipes/tree/master/recipes/tiddit,maxulysse,https://github.com/nf-core/sarek/issues/139
MDU6SXNzdWU1NzY5MzkxMzA=,Create Tests for Mutect2,CLOSED,2020-03-06T13:41:39Z,2021-10-05T12:23:11Z,2021-10-05T12:23:11Z,"As agreed with @MaxUlysse @cgpu opening this issue to discuss how to perform larger scale testing for Mutect2, and make sure it runs as expected.
We can discuss here what datasets to use, where to place the data and how to run the tests.",lescai,https://github.com/nf-core/sarek/issues/148
MDU6SXNzdWU1NzY5NjA5MjM=,Add script for the CreateSomaticPanelOfNormals workflow,CLOSED,2020-03-06T14:18:01Z,2022-06-10T07:45:40Z,2021-11-09T10:26:28Z,"This will enable users to create their own PON that is specific to their samples processing workflow. Quoting the relevant GATK article:

>  _their main purpose [of PON resources] is to capture recurrent technical artifacts in order to improve the results of the variant calling analysis._


- [ ] Discuss how to make available the **AF-only gnomAD** resource (it's not quite the same as [`genomes.genome.germlineResource`](https://github.com/nf-core/sarek/blob/b952fe2b3fcd3541237c9f4a9f27e1852f537967/conf/igenomes.config#L23), I'll have to check this further)
- [ ] Replace [`bam.simpleName`](https://github.com/nf-core/sarek/blob/b7a1a4d9b27e0bc92c345de5a363f6399f5fe95b/create_pon.nf#L188) with a variable that holds the extracted sample name from the respective normal bam
- [ ] Replace clunky old bash with groovy [here](https://github.com/nf-core/sarek/blob/b7a1a4d9b27e0bc92c345de5a363f6399f5fe95b/create_pon.nf#L214-L223) probably remove collect from input declaration and implement the multi vcf definition like:

```groovy
 ${inputVcfCreateGenomicsDB.collect {""-V $it ""}.sort().join()}
```

- [ ] Add input parameter docs

- [ ] Make/Find tiny adequate bam files for testing",cgpu,https://github.com/nf-core/sarek/issues/149
MDU6SXNzdWU1ODY0NzM3NTE=,"For input string: ""status""",CLOSED,2020-03-23T19:44:16Z,2020-04-06T16:24:38Z,2020-04-06T16:24:37Z,"Hi Sarek Team,

I am having a little trouble getting off the ground here. I have sarek installed in the cluster with a conda setup. I would like to run on some already aligned BAM Files from a targeted sequencing panel.

Whenever I run I keep on getting this in the error report:
`For input string: ""status""`
My input tsv looks to be set up correctly. I have attached that and the pipeline report. Appreciate your insight.

Thanks,
Michael
[pipeline_report.txt](https://github.com/nf-core/sarek/files/4371586/pipeline_report.txt)
[P-0006113-T01-IM5.txt](https://github.com/nf-core/sarek/files/4371588/P-0006113-T01-IM5.txt)


",mkinnaman,https://github.com/nf-core/sarek/issues/165
MDU6SXNzdWU1ODkxMTAxNzA=,Param --markdup_java_options missing from docs,CLOSED,2020-03-27T12:47:37Z,2020-03-30T09:08:02Z,2020-03-30T09:08:02Z,I just realized about this when I wanted to increase the memory. I can also make a PR for this myself :),ggabernet,https://github.com/nf-core/sarek/issues/166
MDU6SXNzdWU2MDEwNzk2MDU=,Better support for hg19 ,CLOSED,2020-04-16T13:59:50Z,2021-10-05T12:26:18Z,2021-10-05T12:26:18Z,Currently hg19 is not fully supported and requires the user to input known regions and other necessary files manually. ,jfnavarro,https://github.com/nf-core/sarek/issues/184
MDU6SXNzdWU2MDEwODEwMTA=,Support for joint-analysis,CLOSED,2020-04-16T14:01:27Z,2020-04-16T14:20:55Z,2020-04-16T14:20:35Z,"It would be nice if Sarek would support to perform a joint analysis, for example with Strelka2 and HaplotypeCaller since they both support it. It would also be nice to combine variants for each caller (all samples). ",jfnavarro,https://github.com/nf-core/sarek/issues/185
MDU6SXNzdWU2MDEwODI4Mzc=,Variant missing in HaplotypeCaller but present in HaplotypeCallerGVCF,CLOSED,2020-04-16T14:03:40Z,2022-06-10T08:17:01Z,2022-06-10T08:17:00Z,"The variant is a ""control"" variant that should always be detected. It appears in the results of HaplotypeCallerGVCF but not in HaplotypeCaller. This may be a bug? ",jfnavarro,https://github.com/nf-core/sarek/issues/186
MDU6SXNzdWU2MDE2ODU1Nzg=,FreeEbayes fails with -no_interval command,CLOSED,2020-04-17T04:02:57Z,2020-04-24T07:18:34Z,2020-04-23T16:58:26Z,"Hi, 
When I start sarek with the --no_interval option as below

`./nextflow run sarek/main.nf -profile singularity --sample UWO23.tsv --custom_config_base sarek/conf --genome GRCh38 -c nextflow.slurm.v2.config --tools HaplotypeCaller,ASCAT,ControlFREEC,FreeBayes,Manta,Strelka,TIDDIT,merge --no_intervals`

I get the following error

```
Error executing process > 'FreeBayes (MS6_vs_325B-no_intervals)

Caused by:
  Process `FreeBayes (MS6_vs_325B-no_intervals)` terminated with an error exit status (1)

Command executed:
  freebayes -f Homo_sapiens_assembly38.fasta  --pooled-continuous  --pooled-discrete   --genotype-qualities  --report-genotype-likelihood-max   --allele-balance-priors-off  --min-alternate-fraction 0.03 --min-repeat-entropy 1  --min-alternate-count 2 -t no_intervals.bed         MS6.recal.bam  325B.recal.bam > no_intervals_MS6_vs_325B.vcf

Command exit status:
  1

Command output:
  (empty)

Command error:
  bed targets file is not open

Work dir:
  /gpfs/fs0/scratch/n/nicholsa/zyfniu/work/0a/adcc1bf7e2663ef9ff50c39b84423a
```

I think this is related to #135 

Thanks for your help",peter-yufan-zeng,https://github.com/nf-core/sarek/issues/187
MDU6SXNzdWU2MDI3NjgwNjA=,controlFREEC assess_significance.R fails,CLOSED,2020-04-19T17:04:41Z,2020-05-24T14:42:37Z,2020-05-24T14:42:37Z,"```
$ nextflow run nf-core/sarek -r dev -profile munin --step variantcalling --tools manta,controlfreec --skip_qc all --input results/Preprocessing/TSV/recalibrated.tsv -resume
N E X T F L O W  ~  version 19.10.0
Launching `nf-core/sarek` [evil_booth] - revision: d553d0f8df [dev]
Pulling Singularity image docker://nfcore/sarek:dev [cache /data1/containers/nfcore-sarek-dev.img]
[...]
Caused by:
  Process `ControlFreecViz (A_vs_B)` terminated with an error exit status (1)
[...]
Command error:
  Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :   
    line 753 did not have 9 elements
[...]
```
Maybe @malinlarsson knows the missing part

",szilvajuhos,https://github.com/nf-core/sarek/issues/190
MDU6SXNzdWU2MDMxMzI1MTg=,Use mappability file in ControlFREEC,CLOSED,2020-04-20T10:36:46Z,2020-05-24T10:17:41Z,2020-05-24T10:17:41Z,"from @szilvajuhos in #190 

> [...] We also have a mappability file at `/data1/references/annotations/mappability_ControlFREEC_GRCh38_out100m2.gem` that we also should use.
> 
> ```
> [...]
> gemMappabilityFile = /home/teresita/Documents/Control-FREEC/out100m2_hg38.gem
> [...]
> ```

",maxulysse,https://github.com/nf-core/sarek/issues/191
MDU6SXNzdWU2MDM3NTAwNTM=,Can not find BAM in Sentieon branch,CLOSED,2020-04-21T06:45:44Z,2024-12-09T16:22:52Z,2020-04-21T10:32:40Z,"My guess it is a typo

```
szilva@munin /data2/sentieon/P2233/P2233_101T_P2233_120N $ nextflow run  MaxUlysse/nf-core_sarek -r Sentieon -profile munin --input ../tsv/P2233_101T_P2233_120N.tsv --steps mapping,variantcalling --tools TNScope --skip_qc all --sentieon
N E X T F L O W  ~  version 19.10.0
Launching `MaxUlysse/nf-core_sarek` [backstabbing_colden] - revision: 428036b186 [Sentieon]

...

[fc/b4266c] Submitted process > MergeBamMapped (Sent_P2233_101T_120N-P2233_120N)
Error executing process > 'IndexBamMergedForSentieon (Sent_P2233_101T_120N-P2233_120N)'

Caused by:
  Unknown variable 'bam' -- Make sure it is not misspelt and defined somewhere in the script before using it

Source block:
  """"""
      samtools index ${bam}
      """"""
```",szilvajuhos,https://github.com/nf-core/sarek/issues/192
MDU6SXNzdWU2MDM4Njk3MjQ=,Update MultiQC,CLOSED,2020-04-21T09:59:40Z,2020-05-08T13:49:44Z,2020-04-21T11:08:24Z,"There seems to be an issue with the MultiQC report as most of the stats are not being shown (JavaScript error). Apparently, it is an issue with the parsing of the JSON data. I attached the report from a test run. 
[report.zip](https://github.com/nf-core/sarek/files/4509163/report.zip)
",jfnavarro,https://github.com/nf-core/sarek/issues/193
MDU6SXNzdWU2MTMzNTAzNDY=,Error calling FilterMutectCalls on FreeBayes output,CLOSED,2020-05-06T14:09:29Z,2020-07-13T12:06:27Z,2020-07-13T12:06:27Z,"I am trying the current dev branch and I stumbled upon this issue. If I use the callers Mutect2 and FreeBayes, sarek tries to somehow call FilterMutectCalls on the FreeBayes output causing an error. If I exclude FreeBayes or Mutect2 from list of callers then sarek completes fine.

The command that I used was (slurm as executor):

`nextflow run nf-core/sarek -r dev --tools FreeBayes,Mutect2,Strelka,Manta,snpeff --genome GRCh37 --species homo_sapiens --max_memory '148.GB' --max_time '3.d' --max_cpus '20' --input samples.tsv -profile conda --outdir 1273_OUT_CANCER --snpeff_cache ~/snpEff_cache/data --annotation_cache --snpeff_db GRCh37.75 --igenomes_base ~/shared/iGenome`",jfnavarro,https://github.com/nf-core/sarek/issues/200
MDU6SXNzdWU2MTQ3MjgyNzY=,Option to skip MarkDuplicates step,CLOSED,2020-05-08T13:00:12Z,2020-05-24T10:17:28Z,2020-05-24T10:17:28Z,It would be nice to have a flag to skip the marking of duplicates for instance when using data from amplicons/PCR. ,jfnavarro,https://github.com/nf-core/sarek/issues/203
MDU6SXNzdWU2MjY0ODkxNzM=,Warning  Invalide tag value for CreateIntervalBeds,CLOSED,2020-05-28T13:00:02Z,2020-07-06T09:33:42Z,2020-07-06T09:33:42Z,"I am running sarek `2.6` with nextflow `20.04` and got this `WARN: Invalid tag value for process: CreateIntervalBeds -- A string is expected instead of type: sun.nio.fs.UnixPath; offending value=wgs_calling_regions.hg38.bed`

Pipeline seems to run regardless so far.",FriederikeHanssen,https://github.com/nf-core/sarek/issues/209
MDU6SXNzdWU2Mjg2NDI1NDY=,Update Mutect2 version to 4.1.7.0,CLOSED,2020-06-01T18:35:20Z,2020-06-22T08:56:00Z,2020-06-22T08:56:00Z,"I ran sarek with `mutect2` and ran into this: 
```Runtime.totalMemory()=934281216
  java.lang.IllegalStateException: Smith-Waterman alignment failure. Cigar = 219M with reference length 219 but expecting reference length of 275 ref = TGCCTGAGCACCTGCCCCACCAGAGCGCCACTGCTGGCCTCACGAGCGCCGCTGGGTGCCTGAGCACCTGCCCCACCAGAGCGCCACTGCTGGCCTCACGAGCGCCGCTGGGTGCCTGAGCACCTGCCCCACCAGAGCGCCACTGCTGGCCTCACGAGCACCGCTGGGTGCCTGAGCACCTGCCCCACCAGAGCGCCACTGCTGGCCTCATGAGCACCGCTGGGTGCCTGAGCACCTGCCCCACCAGAGCGCCACTGCTCTGACTTCCACGCTGG path TGCCTGAGCACCTGCCCCACCAGAGCGCCACTGCTGGCCTCACGAGCGCCGCTGGGTGCCTGAGCACCTGCCCCACCAGAGCGCCACTGCTGGCCTCACGAGCACCGCTGGGTGCCTGAGCACCTGCCCCACCAGAGCGCCACTGCTGGCCTCACGAGCACCGCTGGGTGCCTGAGCACCTGCCCCACCAGAGCGCCACTGCTCTGACTTCCACGCTGG
        at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.findBestPaths(ReadThreadingAssembler.java:354)
        at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.assembleKmerGraphsAndHaplotypeCall(ReadThreadingAssembler.java:196)
        at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.readthreading.ReadThreadingAssembler.runLocalAssembly(ReadThreadingAssembler.java:146)
        at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:269)
        at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:226)
        at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:299)
        at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:200)
        at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173)
        at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048)
        at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139)
        at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191)
        at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210)
        at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163)
        at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206)
        at org.broadinstitute.hellbender.Main.main(Main.java:292)
```

Looks like this is a known issue (https://github.com/broadinstitute/gatk/issues/6533) and fixed in the new release 4.1.7 : https://github.com/broadinstitute/gatk/releases",FriederikeHanssen,https://github.com/nf-core/sarek/issues/210
MDU6SXNzdWU2MzM5ODY2NDc=,TrimGalore Error due to python version,CLOSED,2020-06-08T02:00:22Z,2022-05-11T07:36:54Z,2022-05-11T07:36:54Z,"There seems to be a problem with enabling trim-galore in Sarek.

Running on both an cluster and a local computer, sarek 2.6 throws error 
`Now performing quality (cutoff '-q 20') and adapter trimming in a single pass for the adapter sequence: 'AGATCGGAAGAGC' from file HPV5N_2_R1.fastq.gz 
  ERROR: Running in parallel is not supported on Python 2
  Cutadapt terminated with exit signal: '256'.
  Terminating Trim Galore run, please check error message(s) to get an idea what went wrong...`

From the error, this seems to be an issue with the python version used. ",peter-yufan-zeng,https://github.com/nf-core/sarek/issues/215
MDU6SXNzdWU2MzQ4NjQ2NzY=,"--skip_qc ""BaseRecalibrator"" doesn't actually skip BaseRecalibrator...",CLOSED,2020-06-08T19:06:09Z,2021-10-05T03:02:00Z,2020-06-11T03:47:41Z,"Hello! I'm trying to skip the ""BaseRecalibrator"" qc step because it's producing a strange ""java.lang.OutOfMemoryError"", but when I use --skip_qc ""BaseRecalibrator"", it still goes right ahead and runs it. Is this intentional? Or am I doing something incorrectly? Thanks for your help!

-TF",tefirman,https://github.com/nf-core/sarek/issues/216
MDU6SXNzdWU2MzUyNzIyMDc=,Singularity - pulling container failed,CLOSED,2020-06-09T09:33:56Z,2020-06-09T11:49:52Z,2020-06-09T11:49:52Z,"Command:

`singularity pull --name nfcore-sarek-2.6.img docker://nfcore/sarek:2.6`

Result:

```
RuntimeError: maximum recursion depth exceeded
Cleaning up...
ERROR: pulling container failed!
```

Anyone seeing this as well?

Bests,
Felix",fbemm,https://github.com/nf-core/sarek/issues/217
MDU6SXNzdWU2Mzg4OTc5Mjk=,Adding options for GATK,CLOSED,2020-06-15T14:26:34Z,2020-07-14T11:31:17Z,2020-07-14T11:31:17Z,"We noticed that using the default Mutect2 options, a large number of insertions were being called based solely on evidence from soft clipped bases. Adding the option '--dont-use-soft-clipped-bases true' got rid of these.

Is there a way to add specific options to parts of the pipeline whilst using the official version of sarek or do we need to fork the project, make changes and merge in future updates?

Thanks
James",DrJCampbell,https://github.com/nf-core/sarek/issues/218
MDU6SXNzdWU2NDk3MjY5MTM=,Control-FREEC restart is flaky,CLOSED,2020-07-02T08:07:41Z,2020-07-14T11:31:32Z,2020-07-14T11:31:32Z,"```
 $ nextflow run nf-core/sarek -r dev -profile munin --step Control-FREEC --input sample.tsv 
N E X T F L O W  ~  version 20.04.1
Launching `nf-core/sarek` [crazy_bardeen] - revision: 2e39582d29 [dev]
WARN: Unknown parameter: c
Unknown tool(s), see --help for more information
$ cat sample.tsv
subject        XY      0       normal      /path/to/normal.pileup
subject        XY      1       tumor       /path/to/tumor.pileup
```

",szilvajuhos,https://github.com/nf-core/sarek/issues/225
MDU6SXNzdWU2NDk5NzEyNzU=,Look into GATK Best Practices Pipeline for $5 per genome,CLOSED,2020-07-02T14:22:49Z,2024-08-19T13:05:37Z,2024-08-19T13:05:37Z,"https://gatkforums.broadinstitute.org/gatk/discussion/11415/run-the-germline-gatk-best-practices-pipeline-for-5-per-genome

https://github.com/gatk-workflows/gatk4-genome-processing-pipeline",maxulysse,https://github.com/nf-core/sarek/issues/226
MDU6SXNzdWU2NTEzOTc5ODE=,Typo for Mutect2,CLOSED,2020-07-06T09:41:11Z,2020-07-16T06:37:05Z,2020-07-16T06:37:05Z,"When running sarek `2.6.1` with:

``` 
nextflow run nf-core/sarek -profile cfc -r 2.6.1 \
--outdir 'sarek_patients_1_10' \
--input 'sarek_patients_1_10.tsv' \
--genome 'GRCh38' \
--tools 'Strelka,ASCAT,snpEff,Mutect2' \
-c sarek.config \
--no_gatk_spark \
-resume
```

I get:
```
 No such variable: variantcaller
 -- Check script '/home-link/iizha01/.nextflow/assets/nf-core/sarek/main.nf' at line: 3387 or see '.nextflow.log' file for more details
```

The respective line is:

```
variantCaller, idPatient, idSample, vcf, tbi, tsv ->
```
Embedded within:
```
vcfKeep = Channel.empty().mix(
    filteredMutect2Output.map{
        variantCaller, idPatient, idSample, vcf, tbi, tsv ->
        [variantcaller, idSample, vcf]
    },
    vcfConcatenated.map{
        variantcaller, idPatient, idSample, vcf, tbi ->
        [variantcaller, idSample, vcf]
    },
```
I think it is a typo in the variable `variantCaller`  and should be `variantcaller` instead.


",FriederikeHanssen,https://github.com/nf-core/sarek/issues/227
MDU6SXNzdWU2NTQwMjI1NDk=,BWA Mem2 Stable release,CLOSED,2020-07-09T12:30:14Z,2020-07-14T11:31:07Z,2020-07-14T11:31:07Z,"Fresh release: 

Stable now: https://github.com/bwa-mem2/bwa-mem2/releases/tag/v2.0 :wink:

Claiming identical results with major speedups:

https://github.com/bwa-mem2/bwa-mem2#performance

 

",apeltzer,https://github.com/nf-core/sarek/issues/228
MDU6SXNzdWU2NTYwNjA5NTg=,Analysis of single end (Unpaired) fastq files,CLOSED,2020-07-13T18:44:31Z,2020-08-31T09:12:33Z,2020-08-31T09:12:33Z,"Hi,

I have a fastq file (single end) raw files from the RNAseq data. How can I process this using this workflow. I have aligned and indexed my BAM file (to get .bam and .bai). will it be okay for me to use these bam files for variant calling?",Jokendo-collab,https://github.com/nf-core/sarek/issues/231
MDU6SXNzdWU2NTY0NTkwOTY=,Checking the sample sheet in DSL2,CLOSED,2020-07-14T09:12:39Z,2022-06-17T08:00:57Z,2022-06-17T08:00:57Z,"The DSL2 template foresees a Python script to check the structure of the sample sheet.
However Sarek uses Nextflow / Groovy code for that. How do we want to implement this?",ggabernet,https://github.com/nf-core/sarek/issues/235
MDU6SXNzdWU2NjgyMDU1NTI=,Broken links in docs,CLOSED,2020-07-15T12:46:16Z,2020-08-31T08:05:13Z,2020-08-31T08:05:13Z,"Hi - In the documentation here:
https://nf-co.re/sarek/docs/usage#--tools

There are broken links to:
https://nf-co.re/sarek/docs/usage/variant_calling
https://nf-co.re/sarek/docs/usage/annotation

I found the info I was after in the docs directory in github:
https://github.com/nf-core/sarek/tree/master/docs",DrJCampbell,https://github.com/nf-core/sarek/issues/258
MDU6SXNzdWU2NjQ0MTcyNjE=,"markdown_to_html.py: Failed loading extension ""pymdownx.extra"".",CLOSED,2020-07-23T12:02:00Z,2020-07-23T15:04:46Z,2020-07-23T14:52:15Z,"I tried to run with provided bam file:
```
nextflow run nf-core/sarek/ --custom_config_base nf-core/sarek/configs-master/ --input bam_files.tsv --step variantcalling --genome hg19

The printed error message is:
[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'Output_documentation'

Caused by:
  Process `Output_documentation` terminated with an error exit status (1)

Command executed:

  markdown_to_html.py output.md -o results_description.html

Command exit status:
  1

Command output:
  (empty)

...

  ImportError: Failed loading extension ""pymdownx.extra"".
```
I can give more details if needed.

Edit for better markdown rendering by @MaxUlysse  
",bimbam23,https://github.com/nf-core/sarek/issues/254
MDU6SXNzdWU2NzQyMDM5MDc=,Pipeline consistently crashes when using Mutect2,CLOSED,2020-08-06T10:23:56Z,2020-08-06T10:36:56Z,2020-08-06T10:36:56Z,"I've been trying to run the 2.6.1 version on a set of paired WGS cancer/normal samples. As soon as a FilterMutectCalls-process finishes, the pipeline crashes with the following message:

> WARN: Killing pending tasks (2)
> No such variable: variantcaller
> 
>  -- Check script '/proj/sens2019036/nobackup/Sarek/default/main.nf' at line: 3387 or see '.nextflow.log' file for more details
> 
> 

Looking at line 3387 in the source code this seems to be the culprit.


EDIT: Nevermind, only just noticed this has been solved in the dev branch.

`    filteredMutect2Output.map{
        variantCaller, idPatient, idSample, vcf, tbi, tsv ->
        [variantcaller, idSample, vcf]
    },`",sabackman,https://github.com/nf-core/sarek/issues/265
MDU6SXNzdWU2NzQ5ODgyNzk=,Issue with prepare_recalibration and custom genome,CLOSED,2020-08-07T12:39:12Z,2020-09-16T06:53:29Z,2020-09-16T06:53:29Z,"command line:
```
nextflow run nf-core/sarek -r 2.6.1 --igenomes_ignore --genome custom --input bam.tsv --step prepare_recalibration -profile conda --tools FreeBayes,HaplotypeCaller,mpileup
```
custom genome:
```
--fasta genome.fa
--dict genome.dict
--fasta_fai genome.fa.fai
--intervals genome.bed
```
tsv:
```
PATIENT  XX  0   SAMPLE /path/to/bam    /path/to/bam.bai
```",maxulysse,https://github.com/nf-core/sarek/issues/267
MDU6SXNzdWU2ODE2ODYwMTk=,ConcatVCF_Mutect2 recurring SIGPIPE,CLOSED,2020-08-19T09:22:53Z,2020-08-31T08:05:49Z,2020-08-31T08:05:49Z,"I've been running Sarek 2.6.1 on a number of normal-tumour sample pairs and quite often the runs crash due to a SIGPIPE in one or more of the ConcatVCF_Mutect2 jobs:

>  Process `ConcatVCF_Mutect2 (SJ-2335-10703_vs_SJ-2335-1647)` terminated with an error exit status (141)

Rerunning with the -resume always works, but typically some results are lost and need to be computed again. Some additions to the configuration file should solve the issue (similar to ConcatVCF). ",sabackman,https://github.com/nf-core/sarek/issues/268
MDU6SXNzdWU2ODcwNTkyMzc=,Issues regarding full runs with Sentieon,CLOSED,2020-08-27T08:42:53Z,2024-12-09T16:22:52Z,2022-07-22T12:34:04Z,"I am trying to make a full run to get clinically relevant VCFs, using https://github.com/szilvajuhos/btb-scripts/blob/master/gms/runAll.sh as frontend. Issues so far are:

- we should _not_ run annotations for all files, because it takes ages 
- have to disable GVCF during runs (use `--no_gvcf`) 
- do not run variant callers in a single command, because we are going to run out of inodes/open files/whatever (see #268)
- PON file is run-dependent, we have to remove from configs, as some runs has no PON (have to disable for these)
- VQSR for HaplotypeCaller (#89) ",szilvajuhos,https://github.com/nf-core/sarek/issues/273
MDU6SXNzdWU2ODg1MTYxNzE=,Manta looks broken,CLOSED,2020-08-29T11:31:34Z,2022-04-11T20:22:22Z,2020-09-01T09:01:00Z,"Apparently the index of deduped and recalibrated BAMs are mixed up:

```
szilva@munin /data2/GMS_test/BTB0001 $ nextflow pull nf-core/sarek -r dev
Checking nf-core/sarek ...
 Already-up-to-date - revision: 7b9a46dfe5 [dev]
szilva@munin /data2/GMS_test/BTB0001 $ nextflow -log bugger.manta.log run nf-core/sarek -r dev -profile munin --step variantcalling --tools manta --input results/Preprocessing/TSV/sentieon_recalibrated.tsv --no_gvcf
[...]
Caused by:
  Process `MantaSingle (BTB0001T)` terminated with an error exit status (2)
Command executed:
  configManta.py         --tumorBam BTB0001T.recal.bam         --reference Homo_sapiens_assembly38.fasta                  --runDir Manta
[...]

Command exit status:
  2
Command output:
  (empty)
Command error:
  Usage: configManta.py [options]

  configManta.py: error: Can't find any expected BAM/CRAM index files for: 'BTB0001T.recal.bam'

Work dir:
  /data2/GMS_test/BTB0001/work/3e/124c23495fcfbe601212da18e7f22a

szilva@munin /data2/GMS_test/BTB0001 $ cd /data2/GMS_test/BTB0001/work/3e/124c23495fcfbe601212da18e7f22a
szilva@munin /data2/GMS_test/BTB0001/work/3e/124c23495fcfbe601212da18e7f22a $ ls -l 
total 0
lrwxrwxrwx 1 szilva btb  95 Aug 29 13:24 BTB0001T.deduped.bam.bai -> /data2/GMS_test/BTB0001/results/Preprocessing/BTB0001T/DedupedSentieon/BTB0001T.deduped.bam.bai
lrwxrwxrwx 1 szilva btb  87 Aug 29 13:24 BTB0001T.recal.bam -> /data2/GMS_test/BTB0001/results/Preprocessing/BTB0001T/RecalSentieon/BTB0001T.recal.bam
lrwxrwxrwx 1 szilva btb 107 Aug 29 13:24 Homo_sapiens_assembly38.fasta -> /data1/references/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta
lrwxrwxrwx 1 szilva btb 111 Aug 29 13:24 Homo_sapiens_assembly38.fasta.fai -> /data1/references/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai
lrwxrwxrwx 1 szilva btb  74 Aug 29 13:24 input.5 -> /data2/GMS_test/BTB0001/work/tmp/57/89166200088726eafdcf9159a819b3/input.5
```",szilvajuhos,https://github.com/nf-core/sarek/issues/274
MDU6SXNzdWU2ODg3NjczNTQ=,404 in sarek documentation,CLOSED,2020-08-30T17:16:11Z,2020-08-31T07:19:51Z,2020-08-31T07:19:51Z,"Hi

I saw a 404 error in https://nf-co.re/sarek/2.6.1/docs/input . 

As instructed, I open the issue.

Cheers",davidmasp,https://github.com/nf-core/sarek/issues/275
MDU6SXNzdWU2ODkwNDkwMDg=,refactor docs,CLOSED,2020-08-31T09:08:34Z,2020-09-16T06:53:27Z,2020-09-16T06:53:27Z,cf #275 #258 ,maxulysse,https://github.com/nf-core/sarek/issues/276
MDU6SXNzdWU2ODkyNzg3NTg=,MultiQC Report - GATK Section (BQSR),CLOSED,2020-08-31T15:05:28Z,2022-04-21T09:37:41Z,2022-04-21T09:37:41Z,"I noticed that the description says, that this section shows 'base quality scores in each sample before and after BQSR'. However, I only see the tag `Pre` in the plot. I suppose there are three options:

1. Some file is not displayed
2. The module description does not fit this use case 
3. I missed setting some option or sth in that direction

Attached a screen shot from the report. It was generated using `2.6.1` 
<img width=""698"" alt=""Screenshot 2020-08-31 at 16 46 34"" src=""https://user-images.githubusercontent.com/12273093/91735083-12c4a180-ebac-11ea-8d3d-c857913d6f59.png"">
 ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/277
MDU6SXNzdWU2OTUwMDI4MTA=,Update indices to use with BWAMem2,CLOSED,2020-09-07T11:25:15Z,2021-07-15T09:00:01Z,2021-07-15T09:00:01Z,"For BWA-Mem2, new indices are required, as described here: https://github.com/bwa-mem2/bwa-mem2/tree/cbcc183c0843d20d45c84e066177eb8d58be2f9b

Additionally, for a future version upgrade of this tool, we may once again have to update the indices: https://github.com/bwa-mem2/bwa-mem2

In the long run, this requires a recomputation of the indices stored in igenomes. For the short term, a quick fix could be to not specify the index paths in igenomes anymore, forcing a computation for each run. This would increase the runtime however. ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/281
MDU6SXNzdWU3MTE4NDMyNTE=,Malformed tsv file,CLOSED,2020-09-30T10:52:39Z,2020-09-30T12:21:23Z,2020-09-30T12:21:22Z,"I got a error message that I cannot not run the nextflow anymore.
This error is caused by `Cannot invoke method toInteger() on null object`

Can anyone help this issue?


the log is following:

```
Sep-30 17:54:29.985 [main] DEBUG nextflow.cli.Launcher - $> nextflow run nf-core/sarek --input /mscdb/bioinfo/hts/ineo/sample.tsv -profile docker --fasta /home/bioinfo/ngs/References/b37/human_g1k_v37_decoy.fasta --dict /home/bioinfo/ngs/References/b37/human_g1k_v37_decoy.dict --fasta_fai /home/bioinfo/ngs/References/b37/human_g1k_v37_decoy.fasta.fai --bwa '/home/bioinfo/ngs/References/b37/human_g1k_v37_decoy.fasta.64.{amb,ann,bwt,pac,sa}' --dbsnp /home/bioinfo/ngs/References/b37/dbsnp_138.b37.vcf --dbsnp_index /home/bioinfo/ngs/References/b37/dbsnp_138.b37.vcf.idx --known_indels /home/bioinfo/ngs/References/b37/Mills_and_1000G_gold_standard.indels.b37.vcf --known_indels_index /home/bioinfo/ngs/References/b37/Mills_and_1000G_gold_standard.indels.b37.vcf.idx --genome GRCh37 -resume --tools HaplotypeCaller --max_cpus 8 --max_memory 24GB -with-report --sequencing_center GGA --max_time 168h --skip_qc all
Sep-30 17:54:30.395 [main] INFO  nextflow.cli.CmdRun - N E X T F L O W  ~  version 20.04.1
Sep-30 17:54:32.722 [main] DEBUG nextflow.scm.AssetManager - Git config: /home/bioinfo/.nextflow/assets/nf-core/sarek/.git/config; branch: master; remote: origin; url: https://github.com/nf-core/sarek.git
Sep-30 17:54:32.749 [main] DEBUG nextflow.scm.AssetManager - Git config: /home/bioinfo/.nextflow/assets/nf-core/sarek/.git/config; branch: master; remote: origin; url: https://github.com/nf-core/sarek.git
Sep-30 17:54:33.093 [main] DEBUG nextflow.scm.AssetManager - Git config: /home/bioinfo/.nextflow/assets/nf-core/sarek/.git/config; branch: master; remote: origin; url: https://github.com/nf-core/sarek.git
Sep-30 17:54:33.093 [main] INFO  nextflow.cli.CmdRun - Launching `nf-core/sarek` [maniac_lichterman] - revision: bce378e09d [master]
Sep-30 17:54:34.832 [main] DEBUG nextflow.config.ConfigBuilder - Found config base: /home/bioinfo/.nextflow/assets/nf-core/sarek/nextflow.config
Sep-30 17:54:34.833 [main] DEBUG nextflow.config.ConfigBuilder - Parsing config file: /home/bioinfo/.nextflow/assets/nf-core/sarek/nextflow.config
Sep-30 17:54:34.864 [main] DEBUG nextflow.config.ConfigBuilder - Applying config profile: `docker`
Sep-30 17:54:37.362 [main] DEBUG nextflow.config.ConfigBuilder - Available config profiles: [cfc_dev, denbi_qbic, bi, genotoul, bigpurple, test_trimming, test_annotation, uppmax, test_tool, docker, gis, utd_ganymede, conda, singularity, icr_davros, munin, prince, czbiohub_aws, hebbe, cfc, uzh, ccga_med, debug, test, genouest, cbe, ebc, ccga_dx, crick, google, test_split_fastq, kraken, phoenix, shh, test_no_gatk_spark, awsbatch, pasteur, uct_hpc, test_targeted, binac]
Sep-30 17:54:37.642 [main] DEBUG nextflow.Session - Session uuid: 62fe3436-8d20-450b-8d7b-f08e174753e5
Sep-30 17:54:37.643 [main] DEBUG nextflow.Session - Run name: maniac_lichterman
Sep-30 17:54:37.645 [main] DEBUG nextflow.Session - Executor pool size: 24
Sep-30 17:54:37.683 [main] DEBUG nextflow.cli.CmdRun -
  Version: 20.04.1 build 5335
  Created: 03-05-2020 19:37 UTC (04-05-2020 03:37 CDT)
  System: Linux 3.10.0-1127.13.1.el7.x86_64
  Runtime: Groovy 2.5.8 on OpenJDK 64-Bit Server VM 1.8.0_252-b09
  Encoding: UTF-8 (UTF-8)
  Process: 10946@MSC-Mining01 [192.168.7.12]
  CPUs: 24 - Mem: 30.9 GB (559.4 MB) - Swap: 15.6 GB (13.1 GB)
Sep-30 17:54:37.786 [main] DEBUG nextflow.Session - Work-dir: /mscdb/bioinfo/hts/ineo/work [xfs]
Sep-30 17:54:37.891 [main] DEBUG nextflow.Session - Observer factory: TowerFactory
Sep-30 17:54:37.894 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory
Sep-30 17:54:38.569 [main] DEBUG nextflow.Session - Session start invoked
Sep-30 17:54:38.576 [main] DEBUG nextflow.trace.TraceFileObserver - Flow starting -- trace file: /mscdb/bioinfo/hts/ineo/results/pipeline_info/execution_trace.txt
Sep-30 17:54:44.236 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution
Sep-30 17:54:44.270 [main] DEBUG nextflow.Session - Workflow process names [dsl1]: FreebayesSingle, VEP, Mpileup, MapReads, MergeMutect2Stats, ApplyBQSR, IndexBamRecal, Ascat, AlleleCounter, Vcftools, Sentieon_BQSR, BuildGermlineResourceIndex, TrimGalore, Snpeff, BuildKnownIndelsIndex, StrelkaSingle, Sentieon_TNscope, HaplotypeCaller, Sentieon_DNAseq, StrelkaBP, Output_documentation, ControlFREEC, BuildIntervals, PileupSummariesForMutect2, MantaSingle, BuildPonIndex, ControlFreecViz, CompressVCFvep, BuildFastaFai, CalculateContamination, VEPmerge, IndexBamFile, BcftoolsStats, Strelka, MergePileupSummaries, IndexBamMergedForSentieon, MergeBamMapped, BuildDict, FastQCFQ, Manta, BamQC, GenotypeGVCFs, CompressVCFsnpEff, CreateIntervalBeds, MergeMpileup, Get_software_versions, GatherBQSRReports, BuildDbsnpIndex, TIDDIT, FastQCBAM, MarkDuplicates, MSIsensor_msi, BaseRecalibrator, CNVkit, MultiQC, MergeBamRecal, SamtoolsStats, BuildBWAindexes, FreeBayes, CompressSentieonVCF, ConvertAlleleCounts, Sentieon_MapReads, Mutect2, ConcatVCF, Sentieon_DNAscope, FilterMutect2Calls, MSIsensor_scan, Sentieon_Dedup, ConcatVCF_Mutect2
Sep-30 17:54:44.728 [Actor Thread 2] ERROR nextflow.extension.OperatorEx - @unknown
java.lang.NullPointerException: **Cannot invoke method toInteger() on null object**
        at org.codehaus.groovy.runtime.NullObject.invokeMethod(NullObject.java:91)
        at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:43)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
        at org.codehaus.groovy.runtime.callsite.NullCallSite.call(NullCallSite.java:34)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
        at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:55)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:119)
        at Script_9abbae70$_extractFastq_closure9.doCall(Script_9abbae70:4175)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:101)
        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)
        at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:263)
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1041)
        at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:37)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
        at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:52)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:127)
        at nextflow.extension.MapOp$_apply_closure1.doCall(MapOp.groovy:56)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:101)
        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)
        at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:263)
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1041)
        at groovy.lang.Closure.call(Closure.java:405)
        at groovyx.gpars.dataflow.operator.DataflowOperatorActor.startTask(DataflowOperatorActor.java:120)
        at groovyx.gpars.dataflow.operator.DataflowOperatorActor.onMessage(DataflowOperatorActor.java:108)
        at groovyx.gpars.actor.impl.SDAClosure$1.call(SDAClosure.java:43)
        at groovyx.gpars.actor.AbstractLoopingActor.runEnhancedWithoutRepliesOnMessages(AbstractLoopingActor.java:293)
        at groovyx.gpars.actor.AbstractLoopingActor.access$400(AbstractLoopingActor.java:30)
        at groovyx.gpars.actor.AbstractLoopingActor$1.handleMessage(AbstractLoopingActor.java:93)
        at groovyx.gpars.util.AsyncMessagingCore.run(AsyncMessagingCore.java:132)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Sep-30 17:54:44.741 [main] DEBUG nextflow.file.FileHelper - Creating a file system instance for provider: S3FileSystemProvider
Sep-30 17:54:44.788 [Actor Thread 2] DEBUG nextflow.Session - Session aborted -- Cause: Cannot invoke method toInteger() on null object
Sep-30 17:54:44.788 [main] DEBUG nextflow.file.FileHelper - AWS S3 config details: {}
Sep-30 17:54:44.849 [Actor Thread 2] DEBUG nextflow.Session - The following nodes are still active:
  [operator] map
  [operator] map
```

EDIT: code block for Markdown",yaowei2010,https://github.com/nf-core/sarek/issues/286
MDU6SXNzdWU3MTI3MDM4NjU=,Fail to annotate with VEP and snpEff (merge tool),CLOSED,2020-10-01T10:17:49Z,2020-10-02T11:59:05Z,2020-10-02T11:59:05Z,"Good morning,

I am testing your sarek pipeline and I am experiencing some problems with the annotation step. Until now, I am just using the test dataset.

After the whole pipeline failed to annotate my variants, I tried to only run the annotation steps, using the commands from `annotation.md`. First, I downloaded the snpEff and VEP cache files. Then I tried to annotate a VCF (one of the files generated with the test dataset) using different tools. If I annotate with snpEff or VEP alone, the pipeline works without any error:

```
nextflow run nf-core/sarek -profile docker --step annotate --tools snpEff --input results/VariantCalling/1234N/HaplotypeCallerGVCF/HaplotypeCaller_1234N.g.vcf.gz --genome GRCh37 --snpeff_cache ./references/snpEff/cache/ --vep_cache ./references/vep/cache/ --annotation_cache

nextflow run nf-core/sarek -profile docker --step annotate --tools VEP --input results/VariantCalling/1234N/HaplotypeCallerGVCF/HaplotypeCaller_1234N.g.vcf.gz --genome GRCh37 --snpeff_cache ./references/snpEff/cache/ --vep_cache ./references/vep/cache/ --annotation_cache
```

However, if I try to use the merge option, `Snpeff` process fails to execute:

```
nextflow run nf-core/sarek -profile docker --step annotate --tools merge --input results/VariantCalling/1234N/HaplotypeCallerGVCF/HaplotypeCaller_1234N.g.vcf.gz --genome GRCh37 --snpeff_cache ./references/snpEff/cache/ --vep_cache ./references/vep/cache/ --annotation_cache
```
When examining the .command.log:
```
WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.
00:00:00        SnpEff version SnpEff 4.3t (build 2017-11-24 10:18), by Pablo Cingolani
00:00:00        Command: 'ann'
00:00:00        Reading configuration file 'snpEff.config'. Genome: 'null'
00:00:00        Reading config file: /home/genetica/Carlos/sarek_test/work/a6/7d025354942ad6cf19a46ce18d28b2/snpEff.config
00:00:00        Reading config file: /opt/conda/envs/nf-core-sarek-2.6.1/share/snpeff-4.3.1t-3/snpEff.config
java.lang.RuntimeException: Property: 'null.genome' not found
        at org.snpeff.interval.Genome.<init>(Genome.java:106)
        at org.snpeff.snpEffect.Config.readGenomeConfig(Config.java:681)
        at org.snpeff.snpEffect.Config.readConfig(Config.java:649)
        at org.snpeff.snpEffect.Config.init(Config.java:480)
        at org.snpeff.snpEffect.Config.<init>(Config.java:117)
        at org.snpeff.SnpEff.loadConfig(SnpEff.java:451)
        at org.snpeff.snpEffect.commandLine.SnpEffCmdEff.run(SnpEffCmdEff.java:1000)
        at org.snpeff.snpEffect.commandLine.SnpEffCmdEff.run(SnpEffCmdEff.java:984)
        at org.snpeff.SnpEff.run(SnpEff.java:1183)
        at org.snpeff.SnpEff.main(SnpEff.java:162)
00:00:01        Logging
00:00:02        Done.
```
This error is not happening when runing snpEff alone. Do you have any idea of how can this be solved?

Thanks,
 
",yocra3,https://github.com/nf-core/sarek/issues/287
MDU6SXNzdWU3MTk5NTk2NzA=,Instructions not clear enough for using custom genome,CLOSED,2020-10-13T07:35:35Z,2022-07-20T12:28:31Z,2022-07-20T12:28:31Z,"when I run the commands:
```
nextflow run -bg nf-core/sarek -profile docker \
        --input /home/ye/Work/BioAligment/Variant-Calling/sample.tsv \
        --no_gatk_spark --save_bam_mapped --no_intervals \
        --genome  ""GRCh38""  \
        --fasta ""/home/ye/Data/genome/GRCh38.primary_assembly.genome.fa"" \
        --fasta_fai ""/home/ye/Data/genome/GRCh38.primary_assembly.genome.fa.fai"" \
        --igenomes_ignore --dbsnp ""/home/ye/Data/genome/dbsnp_146.hg38.vcf.gz"" \
        --known_indels ""/home/ye/Data/genome/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz"" \
        --email ""ryanyip_@hotmail.com"" --email_on_fail --max_cpus 16 --publish_dir_mode  ""copy"" \
        --outdir sarek-results -w sarek-works -with-report report.html
```

I always got the error:
```
WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted.
Memory limited without swap.
[E::bwa_idx_load_from_disk] fail to locate the index files.
```
And my sample.tsv is:

```
L9527   M       0       Liusheng        CL1     /home/ye/Data/dataset/totalRNA/Liusheng/C_L1_382382_R1_001.fastq.gz     /home/ye/Data/dataset/totalRNA/Liusheng/C_L1_382382_R2_001.fastq.gz
```

EDIT: formatting",RyanYip-Kat,https://github.com/nf-core/sarek/issues/291
MDU6SXNzdWU3MjI0NTAyODE=,small bug  in main.nf line 439,CLOSED,2020-10-15T15:40:14Z,2020-10-22T10:47:06Z,2020-10-22T10:47:06Z,"https://github.com/nf-core/sarek/blob/0e111ebead1e8176c0a8f8300ff0b13b2ba4e253/main.nf#L439

should be 

```if (params.annotate_tools) summary['Tools to annotate'] = params.annotate_tools.join(', ')```

I think",chelauk,https://github.com/nf-core/sarek/issues/292
MDU6SXNzdWU3MjgzNDQwODc=,Error in GetPileupSummaries  when using --no_intervals option,CLOSED,2020-10-23T16:11:35Z,2022-06-20T13:33:27Z,2022-06-20T13:33:27Z,"I got this error message

`2020-10-23T12:17:09.403+02:00	A USER ERROR has occurred: Argument intervals was missing: Argument 'intervals' is required.`

The command executed was:

```
Command executed:
  gatk --java-options ""-Xmx7g""         GetPileupSummaries         -I 3109.recal.bam         -V gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz                  -O no_intervals_3109_pileupsummaries.table
But I can see in the GATK documentation that the argument -L --intervals is required.
```

But I can see in the GATK documentation that the argument -L --intervals is required.",jfnavarro,https://github.com/nf-core/sarek/issues/299
MDU6SXNzdWU3MzcwNzAwNjc=,"awsbatch profile does not run, cannot find aws cli",CLOSED,2020-11-05T16:15:19Z,2020-11-27T08:45:39Z,2020-11-05T21:26:56Z,"With a fresh install of Nextflow v20.10.0, I am trying to run the Sarek test pipeline using the awsbatch executor. My command line is:

```
nextflow run nfcore/sarek -profile test,awsbatch --awsqueue my-test-queue --awsregion us-east-1 -w s3://my-test-bucket/workdir --outdir s3://my-test-bucket/outdir
```

Nextflow is able to submit jobs to the Batch queue, but they all fail with the message

```
bash: /home/ec2-user/miniconda/bin/aws: No such file or directory
bash: line 1: /home/ec2-user/miniconda/bin/aws: No such file or directory
```

This is happening because Nextflow is specifying the following command to the container when it starts:

```
[""bash"",""-o"",""pipefail"",""-c"",""trap \""{ ret=$?; /home/ec2-user/miniconda/bin/aws --region us-east-1 s3 cp --only-show-errors .command.log s3://my-test-bucket/workdir/fa/9bafeb75c35a243f6923cbdd26ead8/.command.log||true; exit $ret; }\"" EXIT; /home/ec2-user/miniconda/bin/aws --region us-east-1 s3 cp --only-show-errors s3://my-test-bucket/workdir/fa/9bafeb75c35a243f6923cbdd26ead8/.command.run - | bash 2>&1 | tee .command.log""]
```

When I launch the `nfcore/sarek:2.6.1` Docker container manually, I can see that the `/home` directory is empty, and the AWS CLI does not seem to be installed anywhere.

Should the AWS CLI be added to the list of packages installed by Conda? Or am I expected to build a custom container image including this tool? The [Sarek documentation on AWS Batch](https://nf-co.re/sarek/2.6.1/docs/usage#awsbatch-specific-parameters) implies a custom AMI, which doesn't seem to make sense in this case.

Thanks for your help!",kgutwin,https://github.com/nf-core/sarek/issues/301
MDU6SXNzdWU3Mzg4Mjk3MjU=,Update TSV docs for gender,CLOSED,2020-11-09T08:55:10Z,2021-07-15T08:55:28Z,2021-07-15T08:55:28Z,"cf conversation on Slack:

> Hi all, I have a question about gender designation in the tsv manifest file.
> I have some samples where this is unknown.
> Is there a way to bypass this or place the equivalent of N/A?
> not sure if it affects the run or not.
> I don't believe we need ASCAT or CNV as we are doing a WES run with HaplotypeCaller

Yes, it's actually just a string, so you can put ZZ if you want, it won't matter unless you ran `ASCAT` or `ControlFreec`",maxulysse,https://github.com/nf-core/sarek/issues/303
MDU6SXNzdWU3NDY2NTk0NzA=,Invalid S3 URL when running ControlFREEC,CLOSED,2020-11-19T14:59:20Z,2020-12-09T13:52:03Z,2020-12-09T13:52:03Z,"When running ControlFREEC I got the following error message:

```
[null] NOTE: Can't stage file s3:///ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Control-FREEC/out100m2_hg38.gem -- file does not exist -- Execution is retried (1)
-[nf-core/sarek] Pipeline completed with errors-
[null] NOTE: Can't stage file s3:///ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Control-FREEC/out100m2_hg38.gem -- file does not exist -- Execution is retried (1)
Unable to re-submit task `ControlFREEC (a35760da-3f13-40a3-8537-e7e841baa6a1_sample_vs_a35760da-3f13-40a3-8537-e7e841baa6a1_control)`
Unable to re-submit task `ControlFREEC (e8f63b36-a0b9-406e-905e-69b7621f33ba_sample_vs_e8f63b36-a0b9-406e-905e-69b7621f33ba_control)`
```

Indeed the mentioned URL seems to refer to a non-existing file:

```
aws s3 ls s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/
                           PRE ASCAT/
                           PRE GATKBundle/
                           PRE GermlineResource/
                           PRE intervals/
```
This URL seems to be defined in the conf/igenomes.config file:
```
mappability             = ""${params.igenomes_base}/Homo_sapiens/GATK/GRCh38/Annotation/Control-FREEC/out100m2_hg38.gem""
```
My guess is that it used to work but then somebody rearranged files on the S3 bucket and now it does not. You probably need to update the URLs.",szymonwieloch,https://github.com/nf-core/sarek/issues/305
MDU6SXNzdWU3NDY2ODMyNjY=,ConcatVCF process is run for a single sample,CLOSED,2020-11-19T15:27:29Z,2022-05-16T18:24:09Z,2022-05-16T18:24:09Z,"I run the pipeline with only tumor samples and with the ControlFREEC tool. My input file is something like this:

```
e8f63b36-a0b9-406e-905e-69b7621f33ba    XX      1       e8f63b36-a0b9-406e-905e-69b7621f33ba_sample     1       e8f63b36-a0b9-406e-905e-69b7621f33ba_R1.fastq.gz        e8f63b36-a0b9-406e-905e-69b7621f33ba_R2.fastq.gz
a35760da-3f13-40a3-8537-e7e841baa6a1    XX      1       a35760da-3f13-40a3-8537-e7e841baa6a1_sample     1       a35760da-3f13-40a3-8537-e7e841baa6a1_R1.fastq.gz        a35760da-3f13-40a3-8537-e7e841baa6a1_R2.fastq.gz
```
While running the pipeline, the process ConcatVCF is run twice (once per each sample, the failed process was probably just a random error):
```
[50/c0e2d7] process > ConcatVCF (FreeBayes-a35760da-3f13-40a3-8537-e7e841baa6a1_sample)                                                           [100%] 3 of 3, failed: 1 ✔
```

I am no a great expert but I had a discussion with @apeltzer and he suggested that this shouldn't happen - when you have tumor samples without control samples, then no VCF concatenation should take place. He suggested that you may want to know, so I am just letting you know that you have a potential issue.",szymonwieloch,https://github.com/nf-core/sarek/issues/306
MDU6SXNzdWU3NTM0NDM1Mzg=,FreeBayes requires unique lane IDs,CLOSED,2020-11-30T12:36:42Z,2022-05-23T13:57:40Z,2022-05-23T13:57:40Z,"While running FreeBayes tool I get the following error message:

```
ERROR(freebayes): multiple samples (SM) map to the same read group (RG)

  samples e8f63b36-a0b9-406e-905e-69b7621f33ba_sample and e8f63b36-a0b9-406e-905e-69b7621f33ba_control map to 1

  As freebayes operates on a virtually merged stream of its input files,
  it will not be possible to determine what sample an alignment belongs to
  at runtime.

  To resolve the issue, ensure that RG ids are unique to one sample
  across all the input files to freebayes.

  See bamaddrg (https://github.com/ekg/bamaddrg) for a method which can
  add RG tags to alignments.
```
I've done some experimentation and consultation and the reason for this problem seems to be my input file configuration:

```
e8f63b36-a0b9-406e-905e-69b7621f33ba    XX      1       e8f63b36-a0b9-406e-905e-69b7621f33ba_sample     1       e8f63b36-a0b9-406e-905e-69b7621f33ba_R1.fastq.gz        e8f63b36-a0b9-406e-905e-69b7621f33ba_R2.fastq.gz
e8f63b36-a0b9-406e-905e-69b7621f33ba    XX      0       e8f63b36-a0b9-406e-905e-69b7621f33ba_control    1       d7caf077-cf7c-4272-a74e-3d31ab61852c_R1.fastq.gz        d7caf077-cf7c-4272-a74e-3d31ab61852c_R2.fastq.gz
a35760da-3f13-40a3-8537-e7e841baa6a1    XX      1       a35760da-3f13-40a3-8537-e7e841baa6a1_sample     1       a35760da-3f13-40a3-8537-e7e841baa6a1_R1.fastq.gz        a35760da-3f13-40a3-8537-e7e841baa6a1_R2.fastq.gz
a35760da-3f13-40a3-8537-e7e841baa6a1    XX      0       a35760da-3f13-40a3-8537-e7e841baa6a1_control    1       a80d728b-8dc0-428d-acdb-07d03bfe19a3_R1.fastq.gz        a80d728b-8dc0-428d-acdb-07d03bfe19a3_R2.fastq.gz
```

Strictly speaking, the problem is related to the lane column (5th). After replacing 1, 1, 1, 1 with 1, 2, 3, 4 - the pipeline works fine. 

@apeltzer suggested  that this may be a feature, not a bug:

> I suspect that Sarek merges the two resulting BAM files (one for the tumor, one for normal) into a single BAM file and thus needs unique read groups to do so.

Still, we would prefer to consult that because the documentation is not clear about it (at least I haven't found anything that would explain this behavior). A fix in the docs would be really nice too. At the moment my observation is that the FreeBayes tool requires a unique lane column value for each subject. I was told that @MaxUlysse may be the right person to ask about it.

Big thanks in advance!",szymonwieloch,https://github.com/nf-core/sarek/issues/311
MDU6SXNzdWU3NTk5Njg0NDk=,control-FREEC reference file out100m2_hg38.gem doesn't exist in s3/igenomes,CLOSED,2020-12-09T03:53:05Z,2020-12-09T09:25:45Z,2020-12-09T08:33:12Z,"Hello, 

I have encountered a problem to run control-FREEC on S3.

In nextflow.config, the default `igenomes_base` = `s3://ngi-igenomes/igenomes/`

In conf/igenomes.config, 
`mappability             = ""${params.igenomes_base}/Homo_sapiens/GATK/GRCh38/Annotation/Control-FREEC/out100m2_hg38.gem""`
but `s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Control-FREEC/out100m2_hg38.gem` actually doesn't exist. 

Can you upload the required file? Thank you in advance.
",XLuyu,https://github.com/nf-core/sarek/issues/312
MDU6SXNzdWU3NjIyNTU3ODM=,Redesign of the logic for tumor only and germline samples,CLOSED,2020-12-11T11:24:35Z,2022-05-11T07:29:25Z,2022-05-11T07:29:25Z,"The current approach is to perform variant calling (germline mode) for the samples even
if they are paired (tumor-normal). So, for instance if we have

SAMPLE1 0
SAMPLE2 1 
SAMPLE3 1

Sarek will perform variant calling in somatic mode for SAMPLE2_vs_SAMPLE1 and in germline mode
for SAMPLE1, SAMPLE2 and SAMPLE3. I believe it is not necessary to perform variant calling
in germline mode for SAMPLE1 and SAMPLE2 (specially for SAMPLE1). By changing this behaviour
we would save a considerable amount of computational time and resources (specially for big datasets). 

This, of course, may vary depending on the tools used. Any thoughts on this? Technically, it would not
require a lot of changes in the code to change the current behaviour. ",jfnavarro,https://github.com/nf-core/sarek/issues/313
MDU6SXNzdWU3NzQzMTg4MTM=,How to change MarkDup java option ?,CLOSED,2020-12-24T10:29:25Z,2021-01-07T08:45:45Z,2021-01-05T13:57:17Z,"I'm using  nf-core/sarek v2.6.1 with Nextflow-v20.10.0

I tried to change values of java memory for markdup without success.

`nextflow run nf-core/sarek -c ../genotoul_1.config --input input.tsv ... -resume --tools HaplotypeCaller --no_gatk_spark ` 

In config I tried with and without following lines
`
withName:MarkDuplicates {
  memory = 24.GB
}
`

And in command line I tried with and without the following option: 
`--markdup_java_options ""-Xms8000m -Xmx15g"" `

The command of markduplicate in command.sh never changed and job is always killed by slurm : 
`
#!/bin/bash -euo pipefail
gatk --java-options ""-Xms3g -Xmx6g""         MarkDuplicates         --MAX_RECORDS_IN_RAM 50000         --INPUT ....
`
What am I doing wrong?",noirot,https://github.com/nf-core/sarek/issues/316
MDU6SXNzdWU3ODUxNjU0NDE=,extractInfos return empty genderMap & statusMap ,CLOSED,2021-01-13T14:44:58Z,2021-01-13T15:52:07Z,2021-01-13T15:52:07Z,"I learn to write my own nf file, I found the `extractInfos` function return empty maps(genderMap & statusMap ), inputSample channel is just right.

I test the map size in the loops, it looks right, but the returned map size is zero.

```
def extractInfos(channel) {
    def genderMap = [:]
    def statusMap = [:]
    channel = channel.map{ it ->
        def idPatient = it[0]
        def gender = it[1]
        def status = it[2]
        def idSample = it[3]
        genderMap[idPatient] = gender
        statusMap[idPatient, idSample] = status
        [idPatient] + it[3..-1]
    }
    [genderMap, statusMap, channel]
}
```

I googled a lot, I could not figure it out. Could anybody help me, thanks.",ray1919,https://github.com/nf-core/sarek/issues/318
MDU6SXNzdWU3ODU1NzM2OTI=,offline download error,CLOSED,2021-01-14T01:17:30Z,2021-01-14T10:18:43Z,2021-01-14T10:18:43Z,"I tried to download sarek, but meet errors like this:

```
nf-core download sarek --singularity

                                          ,--./,-.
          ___     __   __   __   ___     /,-._.--~\
    |\ | |__  __ /  ` /  \ |__) |__         }  {
    | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                          `._,._,'

    nf-core/tools version 1.12.1



INFO     Saving sarek                                               download.py:77
          Pipeline release: 2.6.1
          Pull singularity containers: Yes
          Output file: nf-core-sarek-2.6.1.tar.gz
INFO     Downloading workflow files from GitHub                     download.py:85
INFO     Downloading centralised configs from GitHub                download.py:89
INFO     Downloading 3 singularity containers                      download.py:101
INFO     Building singularity image from Docker Hub:               download.py:279
         docker://nfcore/sarek:2.6.1
INFO:    Converting OCI blobs to SIF format
INFO:    Starting build...
Getting image source signatures
Copying blob b8f262c62ec6 done
Copying blob 0a43c0154f16 done
Copying blob 906d7b5da8fb done
Copying blob 067a79e18102 done
Copying blob 5276f70a05fe done
Copying blob 936f436e39d0 done
Copying blob 61fb5912a8dd done
Copying config 763d76be71 done
Writing manifest to image destination
Storing signatures
2021/01/14 09:06:13  info unpack layer: sha256:b8f262c62ec67f02536f49654de586c022043652bbb6bbf76a8dab1542627a8d
2021/01/14 09:06:14  info unpack layer: sha256:0a43c0154f168ca6fe36f31e366dc85ba2f95da95a9bf69399d927d513d501f9
2021/01/14 09:06:17  info unpack layer: sha256:906d7b5da8fb08f9d6c98d3e7df1d621d03d249c26524b64b09641b1f70fc27e
2021/01/14 09:06:20  info unpack layer: sha256:067a79e18102653b2ddd70ba3301753367b9bd8d605531cf835158d6763e7ad5
2021/01/14 09:06:20  info unpack layer: sha256:5276f70a05fe1e730dfb4560d6cb6a66f1e43b1ca09e02b4d3f2dc6041eb5017
2021/01/14 09:06:20  info unpack layer: sha256:936f436e39d088a5b3793d50dd0c35e6a0d1a591636b14bbf837a1089d0120c8
2021/01/14 09:07:23  info unpack layer: sha256:61fb5912a8dda8efae721b9c57b9ad79db36f5766170591d25a0e2b08cfe270a
INFO:    Creating SIF file...
INFO     Building singularity image from Docker Hub:               download.py:279
         docker://{(params.annotation_cache &&
         params.snpeff_cache) ? 'nfcore/sarek:2.6.1' :
         ""nfcore/sareksnpeff:2.6.1.${params.genome}""}
FATAL:   While making image from oci registry: error fetching image to cache: failed to get checksum for docker://{(params.annotation_cache && params.snpeff_cache) ? 'nfcore/sarek:2.6.1' : ""nfcore/sareksnpeff:2.6.1.${params.genome}""}: unable to parse image name docker://{(params.annotation_cache && params.snpeff_cache) ? 'nfcore/sarek:2.6.1' : ""nfcore/sareksnpeff:2.6.1.${params.genome}""}: invalid reference format
INFO     Building singularity image from Docker Hub:               download.py:279
         docker://{(params.annotation_cache && params.vep_cache) ?
         'nfcore/sarek:2.6.1' :
         ""nfcore/sarekvep:2.6.1.${params.genome}""}
FATAL:   While making image from oci registry: error fetching image to cache: failed to get checksum for docker://{(params.annotation_cache && params.vep_cache) ? 'nfcore/sarek:2.6.1' : ""nfcore/sarekvep:2.6.1.${params.genome}""}: unable to parse image name docker://{(params.annotation_cache && params.vep_cache) ? 'nfcore/sarek:2.6.1' : ""nfcore/sarekvep:2.6.1.${params.genome}""}: invalid reference format
INFO     Compressing download..                                    download.py:117
INFO     Command to extract files: tar -xzf                        download.py:303
         nf-core-sarek-2.6.1.tar.gz
INFO     MD5 checksum for nf-core-sarek-2.6.1.tar.gz:              download.py:344
         53f8a8c8d01017ed48e167fd7b4e0816
```",ray1919,https://github.com/nf-core/sarek/issues/319
MDU6SXNzdWU3ODY3NzEwMDI=,Sarek should have an error or at least a warning when no fasta params,CLOSED,2021-01-15T10:08:39Z,2023-03-27T09:30:07Z,2022-06-17T07:45:48Z,"In my samplesheet I have 15 lines (for my 15 samples) looking like:
`FS_WT_C	XX	0	SUB1152A31	L007	/absolute/path/SUB1152A31_S91_L007_R1_001.fastq.gz	/absolute/path/SUB1152A31_S91_L007_R2_001.fastq.gz`

Sarek version is 2.6.1

Command line:
```
ml load Nextflow/19.04.0
ml load Singularity/3.3.0
nextflow run nf-core/sarek -profile crick --input 'samplesheet.tsv' --igenomes_ignore --genome custom /absolute/path/GCF_001722355.1_ASM172235v1_genomic.fasta
```

I have `results/Preprocessing/TSV` and that TSV folder is empty.
In `results/Reports` I have folders for each of my samples but there's only fastqc files

In my MultiQC report I have General Statistics and then I have FastQC tests like Sequence Counts, Per Sequence Quality Score... and others.

[nextflow_log.txt](https://github.com/nf-core/sarek/files/5819800/nextflow_log.txt)

",pablosoro,https://github.com/nf-core/sarek/issues/321
MDU6SXNzdWU3ODkyNDY3MTM=,Small mistake in usage.md,CLOSED,2021-01-19T18:13:23Z,2021-01-20T17:39:55Z,2021-01-20T17:39:55Z,"The paragraph talks about`star`, the code example uses `VEP`.

![image](https://user-images.githubusercontent.com/22882929/105075675-5b60f680-5a8a-11eb-93ba-3bcfff5567a8.png)
",heseber,https://github.com/nf-core/sarek/issues/327
MDU6SXNzdWU3ODk4MDEyNzU=,Error in test_annotation VEP,CLOSED,2021-01-20T09:27:15Z,2021-01-20T09:32:55Z,2021-01-20T09:32:55Z,"Hi there,

I tested the annotation part of the pipeline using the test_annotation profile and following command: 

`nextflow run nf-core/sarek -profile test_annotation,docker --tools vep,snpeff`

```
...
-------------------- EXCEPTION --------------------
MSG: ERROR: Cache assembly version (WBcel235) and database or selected assembly version (GRCh37) do not match

If using human GRCh37 add ""--port 3337"" to use the GRCh37 database, or --offline to avoid database connection entirely
...
```

I attached the failed VEP process log files 

[command.err.txt](https://github.com/nf-core/sarek/files/5841349/command.err.txt)
[command.log.txt](https://github.com/nf-core/sarek/files/5841350/command.log.txt)
[command.sh.txt](https://github.com/nf-core/sarek/files/5841351/command.sh.txt)


[nextflow.log](https://github.com/nf-core/sarek/files/5841337/nextflow.log)

Best,
Momo
",ChillyMomo709,https://github.com/nf-core/sarek/issues/328
MDU6SXNzdWU3OTAwMjUyNTA=,MSIsensor for tumor-only ,OPEN,2021-01-20T14:26:13Z,2024-08-19T13:12:26Z,,"It would be nice to add a process to compute MSIs for tumor-only samples:

https://github.com/niu-lab/msisensor2

I could take care of this. ",jfnavarro,https://github.com/nf-core/sarek/issues/330
MDU6SXNzdWU3OTQxNjg5NDM=,Add a key for VEP genome assembly,CLOSED,2021-01-26T11:48:49Z,2021-07-15T08:54:34Z,2021-07-15T08:54:34Z,"It would be nice to add a params to choose the VEP genome assembly.
In most cases it is the `--genome` params, but I'm assuming it'll be easier to control if we have that as a separate params.
For example, it's currently not working with the `--genome custom` setting.",maxulysse,https://github.com/nf-core/sarek/issues/332
MDU6SXNzdWU3OTQ2MTU2OTI=,Remove files in DSL2,CLOSED,2021-01-26T22:45:27Z,2021-07-15T08:31:08Z,2021-07-15T08:31:08Z,"- [ ] `.github/workflows/push_dockerhub_dev.yml`
- [ ] `.github/workflows/push_dockerhub_release.yml`
- [ ] `Dockerfile`
- [ ] `environment.yml`
- [ ] `bin/scrape_software_versions.py`",maxulysse,https://github.com/nf-core/sarek/issues/335
MDU6SXNzdWU3OTQ2MTYwNDQ=,Need some test data for conf/test_full.config,CLOSED,2021-01-26T22:46:10Z,2021-07-15T08:30:53Z,2021-07-15T08:30:53Z,,maxulysse,https://github.com/nf-core/sarek/issues/336
MDU6SXNzdWU3OTUzMzMyNjc=,Should we keep two version of bwa-mem,CLOSED,2021-01-27T18:15:25Z,2021-10-27T08:01:18Z,2021-10-26T13:11:23Z,"From @FriederikeHanssen 

> Should we keep both aligners or remove bwa-mem?
Maybe depends on whether bwa-mem is further developed and not replaced with bwa-mem2
",maxulysse,https://github.com/nf-core/sarek/issues/337
MDU6SXNzdWU3OTgyMzMyOTE=,[FEATURE] Add Datasets for AWS Full-size test,CLOSED,2021-02-01T10:56:04Z,2022-07-18T15:08:01Z,2022-07-18T15:08:01Z,"I haven't found a related issue, so far. In case I missed it, we can just add this there. 
## Is your feature request related to a problem? Please describe

As of this year, we try to add full-size tests to all pipelines to run with aws and then display the results on the homepage.

## Describe the solution you'd like

One dataset suitable for this could be the one described in this [paper](https://doi.org/10.1038/s42003-020-01460-9): 
 - It is publically available on SRA: SRP159787
 - WGS, 35X coverage
 - Human origin
 - Colon cancer cell line

## Describe alternatives you've considered

Haven't searched for other suitable datasets, but we could use this thread to collect more before deciding on one for the test run.
",FriederikeHanssen,https://github.com/nf-core/sarek/issues/339
MDU6SXNzdWU4MDEyNzM2NzA=,[BUG] BWA-mem2 indices not linked properly,CLOSED,2021-02-04T13:15:55Z,2022-05-23T13:58:01Z,2022-05-23T13:58:01Z,"I have checked the following places for your error:

- [X] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [X] [nf-core/sarek pipeline documentation](https://nf-co.re/nf-core/sarek/usage)

## Description of the bug

When you run sarek2.7 with bwa-mem2, the simlink to the bwa-mem2 indices is located in the temporary folder, but the command is not pointing to that folder with indices therefore, bwa-mem2 expect the index files to be in the same folder as the fastq file.

I tried tried to copy the index files in the run temp folder, and the mapping works.

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line:

```
nextflow run {base_path}/sarek-master \
--custom_config_base {base_path}/sarek-master/conf \
--input {in_folder} \
--igenomes_base {base_path}/igenomes/ \
--genome GRCh38  \
--sequencing_center WGS_center_QA  \
--outdir {out_folder}/results \
--project ngc-bio \
--aligner bwa-mem2 \
-c {base_path}/sarek-master/conf/ngc.config \
-profile singularity \
-with-singularity {base_path}/sarek_simg/nf-core-sarek-2.7.simg \
-with-timeline \
-work-dir {out_folder}/work

```
2. See error:
    
 bwa-mem2 fails because it's missing the indicies files.


## Expected behaviour

I tried tried to copy the index files directly in the run temp folder, and the mapping works.

## Log files

Have you provided the following extra information/files:

- [x] The command used to run the pipeline
- [ ] The `.nextflow.log` file <!-- this is a hidden file in the directory where you launched the pipeline -->

## System

- Hardware: HPC
- Executor: pbs
- OS: CentOS
- Version 2.7

## Nextflow Installation

- Version:  20.10.0

## Container engine

- Engine: Singularity
- version: 3.4.1
- Image tag: nfcore/sarek:2.7

",nicorap,https://github.com/nf-core/sarek/issues/340
MDU6SXNzdWU4MDY2NDkxOTA=,Parallelization of BaseRecalibrator,CLOSED,2021-02-11T18:47:36Z,2021-02-12T07:50:46Z,2021-02-12T07:50:46Z,"Hi,

I am using this pipeline to run exome sequencing analysis on about 1000 samples (tumor-normal pairs) on our cluster (SGE). We are finding that for the steps BaseRecalibrator and ApplyBQSR, the pipeline is splitting the genome into tiny chunks, each taking about 30 seconds to run. However, with SGE job management, it cannot keep up with this rate of job requests and so jobs often spend longer waiting to be started than actually running. Is there a way to modify the size of genome chunk being used? My command is copied below.

Kind regards,

Dr Sam Kleeman
PhD Student
Cold Spring Harbor Laboratory, NY 

`nextflow run -resume nf-core/sarek -profile singularity -c /mnt/grid/janowitz/home/skleeman/nextflow/nextflow.config -r 2.6.1 --tools 'Strelka, Manta, MSIsensor, SnpEff' --genome GRCh38  --target_bed /mnt/grid/janowitz/home/references/exome_regions_padded.bed --skip_qc all --outdir /mnt/grid/janowitz/rdata_norepl/pan_immuno/output/dna --igenomes_base /mnt/grid/janowitz/home/references/human_dna/references/references --step prepare_recalibration`
",samkleeman1,https://github.com/nf-core/sarek/issues/341
MDU6SXNzdWU4MDcxNjUxNDA=,[FEATURE] Make modules for:,CLOSED,2021-02-12T11:36:35Z,2021-03-22T10:09:51Z,2021-03-22T10:09:51Z,"We need modules in [nf-core/modules](https://github.com/nf-core/modules) for:

Please tick a module if you are working on one, and add the link to the PR once you created it

- [ ] allelecounter
- [ ] ascat
- [x] bwa-mem2/index - https://github.com/nf-core/modules/pull/166
- [x] bwa-mem2/mem - https://github.com/nf-core/modules/pull/166
- [x] cnvkit - https://github.com/nf-core/modules/pull/173
- [ ] controlfreec/freec
- [ ] fgbio/callmolecularconsensusreads
- [ ] fgbio/fastqtobam
- [ ] fgbio/groupreadsbyumi
- [ ] freebayes/single
- [ ] freebayes/somatic
- [ ] gatk/calculatecontamination
- [ ] gatk/filtermutectcalls
- [ ] gatk/getpileupsummaries
- [ ] gatk/applybqsr
- [ ] gatk/baserecalibrator
- [x] gatk/createsequencedictionary https://github.com/nf-core/modules/pull/177
- [ ] gatk/fastqtosam
- [ ] gatk/gatherbqsrreports
- [ ] gatk/genotypegvcf
- [ ] gatk/haplotypecaller
- [ ] gatk/markduplicates
- [ ] gatk/mergemutectstats
- [ ] gatk/mutect2
- [x] manta/single
- [ ] manta/single_tumor
- [ ] manta/somatic
- [x] msisensor/msi
- [x] msisensor/scan
- [x] qualimap/bamqc https://github.com/nf-core/modules/pull/129
- [x] samtools/faidx https://github.com/nf-core/modules/pull/129
- [ ] samtools/merge
- [ ] snpeff
- [ ] strelka/germline
- [ ] strelka/somatic
- [ ] strelka/somatic_bp
- [x] tabix/bgzip https://github.com/nf-core/modules/pull/180
- [x] tabix/tabix https://github.com/nf-core/modules/pull/179
- [x] tiddit https://github.com/nf-core/modules/pull/252
- [ ] vcftools
- [ ] vep",maxulysse,https://github.com/nf-core/sarek/issues/343
MDU6SXNzdWU4MDcyNTUxNjM=,Incomplete gVCF files with --target_bed,CLOSED,2021-02-12T13:54:37Z,2022-07-21T08:20:25Z,2022-07-21T08:20:25Z,"## Check Documentation

I have checked the following places for your error:

- [x] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [x] [nf-core/sarek pipeline documentation](https://nf-co.re/nf-core/sarek/usage)

## Description of the bug

I used Sarek to generate gVCF files with the tool HaplotypeCaller, and then planned to do joint genotyping myself on all samples together. As this is exome sequencing, I first used the option --target_bed, but realised that this results in lots of missing genotypes. The reason is that ""bcftools isec"" apparently is run on the gvcf files, which removes all regions where the start of a non-variant block in the gvcf is not within the regions listed in the bed file. This means that many of the regions with reference alleles are removed from the file, even if parts of these blocks are indeed covered by the bed (bcftools does not look at the END tag). The vcf files generated for each sample are fine though.

## Steps to reproduce

Command line:
`nextflow run ~/sarek/main.nf -profile uppmax,singularity -with-singularity /sw/data/ToolBox/nf-core/nfcore-sarek-2.6.1.img --containerPath ~/sarek/containers --custom_config_base ~/configs-master/ --genome_base /sw/data/ToolBox/hg38bundle/ --project XXX --genome GRCh38 --step prepare_recalibration --target_bed Twist_Exome_RefSeq_targets_hg38.bed --input mapped_bam_files.tsv`

## Expected behaviour

Even if it might be better to do the joint genotyping on the full file anyway, I would expect the gvcf files generated to include (at least) the regions in the given bed file when using --target_bed. Or maybe just a note/warning on this in the description of --target_bed?

## Log files

Have you provided the following extra information/files:

- [x] The command used to run the pipeline
- [ ] The `.nextflow.log` file <!-- this is a hidden file in the directory where you launched the pipeline -->

## System

- Hardware: HPC
- Executor: slurm
- Sarek version: 2.6.1

## Nextflow Installation

- Version: 20.10.0.5431

## Container engine

- Engine: Singularity

",jtangrot,https://github.com/nf-core/sarek/issues/344
MDU6SXNzdWU4MDc2MzI0Mzc=,Error with BaseRecalibrator on aws batch,CLOSED,2021-02-13T00:32:27Z,2021-07-15T08:51:39Z,2021-07-15T08:51:39Z,"Run with `nextflow run nf-core/sarek --input ../data/sarek_input.tsv -profile docker -c ./src/aws.config`

```
Error executing process > 'BaseRecalibrator (103-14008-S4-chr21_8310972-8472360)'

Caused by:
  Oops.. something wrong happened while creating task 'BaseRecalibrator' unique id -- Offending keys: [
 - type=java.util.UUID value=776a3081-6868-4476-ad1b-a1bdf0423beb, 
 - type=java.lang.String value=BaseRecalibrator, 
 - type=java.lang.String value=dbsnpOptions = params.dbsnp ? ""--known-sites ${dbsnp}"" : """"
knownOptions = params.known_indels ? knownIndels.collect{""--known-sites ${it}""}.join(' ') : """"
prefix = params.no_intervals ? """" : ""${intervalBed.baseName}_""
intervalsOptions = params.no_intervals ? """" : ""-L ${intervalBed}""
""""""
    gatk --java-options -Xmx${task.memory.toGiga()}g \
        BaseRecalibrator \
        -I ${bam} \
        -O ${prefix}${idSample}.recal.table \
        --tmp-dir . \
        -R ${fasta} \
        ${intervalsOptions} \
        ${dbsnpOptions} \
        ${knownOptions} \
        --verbosity INFO
    """"""
, 
 - type=java.lang.String value=nfcore/sarek:2.7, 
 - type=java.lang.String value=idPatient, 
 - type=java.lang.String value=103-14008, 
 - type=java.lang.String value=idSample, 
 - type=java.lang.String value=S4, 
 - type=java.lang.String value=bam, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/temp-core/nf-work/4f/fe4b466c62399d9d34fcdeb39d3f57/S4.md.bam, storePath:/temp-core/nf-work/4f/fe4b466c62399d9d34fcdeb39d3f57/S4.md.bam, stageName:S4.md.bam)], 
 - type=java.lang.String value=bai, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/temp-core/nf-work/4f/fe4b466c62399d9d34fcdeb39d3f57/S4.md.bam.bai, storePath:/temp-core/nf-work/4f/fe4b466c62399d9d34fcdeb39d3f57/S4.md.bam.bai, stageName:S4.md.bam.bai)], 
 - type=java.lang.String value=intervalBed, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/temp-core/nf-work/a4/4fed99bf0d4b0aac6260efd6d32761/chr21_8310972-8472360.bed, storePath:/temp-core/nf-work/a4/4fed99bf0d4b0aac6260efd6d32761/chr21_8310972-8472360.bed, stageName:chr21_8310972-8472360.bed)], 
 - type=java.lang.String value=dbsnp, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz, storePath:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz, stageName:dbsnp_146.hg38.vcf.gz)], 
 - type=java.lang.String value=dbsnpIndex, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz.tbi, storePath:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz.tbi, stageName:dbsnp_146.hg38.vcf.gz.tbi)], 
 - type=java.lang.String value=fasta, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta, storePath:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta, stageName:Homo_sapiens_assembly38.fasta)], 
 - type=java.lang.String value=dict, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.dict, storePath:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.dict, stageName:Homo_sapiens_assembly38.dict)], 
 - type=java.lang.String value=fastaFai, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai, storePath:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai, stageName:Homo_sapiens_assembly38.fasta.fai)], 
 - type=java.lang.String value=knownIndels, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz, storePath:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz, stageName:Mills_and_1000G_gold_standard.indels.hg38.vcf.gz), FileHolder(sourceObj:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/beta/Homo_sapiens_assembly38.known_indels.vcf.gz, storePath:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/beta/Homo_sapiens_assembly38.known_indels.vcf.gz, stageName:Homo_sapiens_assembly38.known_indels.vcf.gz)], 
 - type=java.lang.String value=knownIndelsIndex, 
 - type=nextflow.util.ArrayBag value=[FileHolder(sourceObj:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi, storePath:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi, stageName:Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi), FileHolder(sourceObj:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/beta/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi, storePath:/ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/beta/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi, stageName:Homo_sapiens_assembly38.known_indels.vcf.gz.tbi)], 
 - type=java.lang.String value=$, 
 - type=java.lang.Boolean value=true, 
 - type=java.util.HashMap$EntrySet value=[params.dbsnp=s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz, params.no_intervals=null, params.known_indels=s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz]]
```",prmac,https://github.com/nf-core/sarek/issues/345
MDU6SXNzdWU4MDc2NzU5Mzk=,Skip BaseRecalibration,CLOSED,2021-02-13T04:49:41Z,2021-09-17T15:23:08Z,2021-02-16T09:06:31Z,"Hi,

We are running some analysis on an inbred mouse strain versus the genome assembly for the specific strain from the Sanger Mouse Genome Project. As a result, it is difficult to define what the 'known SNPs' or 'known indels' would be. I would like to run the pipeline without BaseRecalibration, I was wondering if you can suggest how to do this?

Kind regards,

Sam Kleeman",samkleeman1,https://github.com/nf-core/sarek/issues/346
MDU6SXNzdWU4MDk4OTExODI=,MarkDuplicates configured with invalid memory options (-Xms0g -Xmx-1g),CLOSED,2021-02-17T06:15:56Z,2021-02-17T17:20:58Z,2021-02-17T14:42:18Z,"## Check Documentation

I have checked the following places for your error:

- [x] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [x] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

**Full disclosure:** I'm an absolute novice at Nextflow, so if you think this problem has nothing to do with `sarek`, just let me know and I can migrate the issue to the Nextflow repository. That said, any friendly pointers are well-appreciated! 

For some reason, the `MarkDuplicates` jobs are being configured with invalid Java memory options (`-Xms0g -Xmx-1g`). Unsurprisingly, this results in `Invalid maximum heap size: -Xmx-1g` errors. I've included the full `MarkDuplicates` command below. 

```
gatk --java-options ""-Xms0g -Xmx-1g"" MarkDuplicates --INPUT TCRBOA3-T.bam --METRICS_FILE TCRBOA3-T.bam.metrics --TMP_DIR . --ASSUME_SORT_ORDER coordinate --CREATE_INDEX true --OUTPUT TCRBOA3-T.md.bam
```

## Steps to reproduce

I'm running the `sarek` workflow on some standard exome FASTQ files derived from tumour-normal pairs. The workflow is running locally on an `m5a.8xlarge` EC2 instance (32 cores and 128 GB of memory). I've included the `nextflow` command below. 

Note that I have to include a value for `--max_memory` because I run into errors with the `qualimap` jobs where they attempt to run with 128 GB when only ~123 GB are actually available. I believe this is a separate issue (although let me know if this assumption is wrong), and at least I have a workaround for it. 

```
export NXF_OPTS='-Xms1g -Xmx4g'

nextflow run \
    nf-core/sarek -revision 2.7 -resume fabulous_poincare \
    -profile docker -with-tower \
    -work-dir /home/ubuntu/runs/sarek-tcrb/work/ \
    --outdir /home/ubuntu/runs/sarek-tcrb/results/ \
    --input /home/ubuntu/scripts/nf-core-sarek/sarek-tcrb-samples-local.tsv \
    --step mapping --tools 'Strelka,Mutect2,VEP' \
    --genome GRCh38 --igenomes_base /home/ubuntu/igenomes/ \
    --max_cpus 4 --max_memory 64 GB
```

I tried setting the available CPUs and memory in a `nextflow.config` file (loaded with the `-config` option), to no avail. 

```
executor {
    $local {
        cpus = 16
        memory = '64 GB'
    }
}
```

I also tried setting `--markdup_java_options '""-Xms4000m -Xmx7g""'` in the Nextflow command, which still didn't work. 

Let me know if you could use more details. Thanks! 

## Expected behaviour

I expected Nextflow and/or the `sarek` workflow to provide a minimum amount of memory to the `MarkDuplicates` jobs. 

## Log files

Have you provided the following extra information/files:

- [x] The command used to run the pipeline
- [x] The `.nextflow.log` file ([nextflow.log](https://github.com/nf-core/sarek/files/5993562/nextflow.log))

## System

- Hardware: AWS EC2 instance (m5a.8xlarge)
- Executor: local
- OS: Ubuntu
- Version: 18.04.4

## Nextflow Installation

- Version: 20.10.0 build 5430",BrunoGrandePhD,https://github.com/nf-core/sarek/issues/347
MDU6SXNzdWU4MTE0MjQ0Mjc=,[BUG] No container for snpEff BALB_cJ_v1.99,CLOSED,2021-02-18T20:15:20Z,2021-07-15T08:51:24Z,2021-07-15T08:51:24Z,"Hi,

I am getting repeated errors from singularity when pulling the snpeff image, with both 2.6.1 and 2.7 pipelines.

Run command:

```
nextflow run nf-core/sarek \
--input /mnt/grid/janowitz/rdata_norepl/mouse/results/Preprocessing/TSV/duplicates_marked_no_table.tsv \
-c /mnt/grid/janowitz/home/skleeman/nextflow/nextflow.config -r 2.6.1 \
--tools 'Strelka, Manta, SnpEff' \
--fasta /mnt/grid/janowitz/home/references/balbc/Mus_musculus_balbcj.BALB_cJ_v1.dna.toplevel.fa \
--fasta_fai /mnt/grid/janowitz/home/references/balbc/Mus_musculus_balbcj.BALB_cJ_v1.dna.toplevel.fa.fai \
--intervals /mnt/grid/janowitz/home/references/balbc/Mus_musculus_balbcj.BALB_cJ_v1.dna.toplevel.fa.bed \
--dict /mnt/grid/janowitz/home/references/balbc/Mus_musculus_balbcj.BALB_cJ_v1.dna.toplevel.dict \
--bwa ""/mnt/grid/janowitz/home/references/balbc/BWAIndex/Mus_musculus_balbcj.BALB_cJ_v1.dna.toplevel.fa.{amb,ann,bwt,pac,sa}"" \
--snpeff_db 'BALB_cJ_v1.99' --species 'Mus_musculus_balbcj' --igenomes_ignore --genome custom --step variant_calling
```

Error message:

```
/mnt/grid/janowitz/rdata_norepl/tmp/singularity/nfcore-sareksnpeff-2.6.1.custom.img]
Error executing process > 'Snpeff (XXXX - Strelka - Strelka_XXXXX_variants.vcf.gz)'

Caused by:
  Failed to pull singularity image
  command: singularity pull  --name nfcore-sareksnpeff-2.6.1.custom.img.pulling.1613678997380 docker://nfcore/sareksnpeff:2.6.1.custom > /dev/null
  status : 255
  message:
    FATAL:   While making image from oci registry: error fetching image to cache: failed to get checksum for docker://nfcore/sareksnpeff:2.6.1.custom: Error reading manifest 2.6.1.custom in docker.io/nfcore/sareksnpeff: manifest unknown: manifest unknown

```

",samkleeman1,https://github.com/nf-core/sarek/issues/348
MDU6SXNzdWU4MTI2NTE0NjA=,[FEATURE] Standard Output = standard ouput and error output,CLOSED,2021-02-20T16:10:51Z,2021-03-01T07:39:53Z,2021-03-01T07:39:53Z,"Hi !

I wanted to separate the standard error and the standard output using `1>>` and `2>>` in an ubuntu terminal, but the file generated with `2>>` is empty, and the errors are in the file generated with `1>>`.

## Describe the solution you'd like

I would like to separate the standard output using `1>>` and the standard error output using `2>>`. 

Thanks

## Additional context

I use Sarek v2.7 with this characteristics :
- ubuntu 20.04
- 12 CPUs
- 125 Go of RAM",AxelMaths,https://github.com/nf-core/sarek/issues/350
MDU6SXNzdWU4MTM1Mjg0OTM=,[FEATURE] Extensions understanding,CLOSED,2021-02-22T14:00:52Z,2021-03-04T09:52:52Z,2021-03-04T09:52:52Z,"<!--
# nf-core/sarek feature request

Hi there!

Thanks for suggesting a new feature for the pipeline!
Please delete this text and anything that's not relevant from the template below:
-->

## Is your feature request related to a problem? Please describe

Hi !

After running the profile ""test"", certain html files have a .1 or .2 extensions in the pipeline_info folder (like ""execution_report.html.1""), and they are not recognized by every software.

## Describe the solution you'd like

I would like to have only html files without extensions like .1.

## Describe alternatives you've considered

Maybe we could consider renaming them instead of having .1 and .2 extensions ?

## Additional context

I recently discovered Sarek and I used Sarek v2.7.

From what I've noticed the difference between those files comes from the docker container's name but I'm not sure.
Is this due to Docker ? I didn't really find any info in the documentation.

Could you enlighten me on this subject ?

Thanks
",TaynaP,https://github.com/nf-core/sarek/issues/351
MDU6SXNzdWU4MTQ0OTY4NTQ=,[FEATURE] Filtering of common variants with VEP should be optional,CLOSED,2021-02-23T14:06:02Z,2021-02-23T14:38:51Z,2021-02-23T14:38:48Z,"## Is your feature request related to a problem? Please describe
VEP filters out common variants based on frequency in 1000 genomes project

https://github.com/nf-core/sarek/blob/master/main.nf#L3750

## Describe the solution you'd like
This behaviour should be optional.  Quite often we are interested in common variants in a cohort of samples

",prmac,https://github.com/nf-core/sarek/issues/352
MDU6SXNzdWU4MTU3NjgxMDk=,[FEATURE] Default values of max_cpus and max_memory,CLOSED,2021-02-24T19:16:01Z,2021-03-22T14:17:02Z,2021-03-22T14:17:02Z,"Hi !

I tried to run a pipeline with the profile ""docker"" using the same datas as the profile ""test"", but I had an error. It was because I didn't have enough cpus and ram.

## Describe the solution you'd like
I would like to run the pipeline without having to use ""--max_cpus"" and ""--max_memory"" options or to change the default values in the config file.

## Describe alternatives you've considered
I think that the default values are a bit too high (maybe setting them around 6~8 cpus and 32 gb ram would be more adapted to people that doesn't have access to this kind of computers) but that's just a suggestion.

## Additional context
I use Sarek v2.7 with these characteristics :
- Ubuntu 20.04
- 12 cpus
- 125 GB of RAM

I finally found the ""--max_cpus"" and ""--max_memory"" options in the documentation, but they were hidden and not very easy to find in my opinion.

Thanks
",LeaVandamme,https://github.com/nf-core/sarek/issues/355
MDU6SXNzdWU4MjQ5MTI2MTM=,Learn Orientation Bias Artifacts,CLOSED,2021-03-08T20:13:49Z,2022-02-11T17:18:45Z,2022-02-11T17:18:45Z,"<!--
# nf-core/sarek feature request

Hi there!

Thanks for suggesting a new feature for the pipeline!
Please delete this text and anything that's not relevant from the template below:
-->

To filter out FFPE artifacts with FilterByOrientationBias. Would you add the learn orientation bias tool (LearnReadOrientationModel) to the mutect workflow? ",QiongICR,https://github.com/nf-core/sarek/issues/357
MDU6SXNzdWU4MzI1NDEwMjA=,[BUG] Mutect2 - Error with both 'intervals' and 'no-intervals' options,CLOSED,2021-03-16T08:22:19Z,2022-06-20T13:33:44Z,2022-06-20T13:33:44Z,"Dear sarek community,
I used the pipeline several times and I always found this bugs using Mutect2. 

## Description of the bug

GATK Mutect2

I found that among the pipeline there are two different bug using Mutect2 for somatic Tumor-Normal samples.

1- Mutect2 for Variantcalling using intervals option for parallelization; 
Even if the pipeline is executed correctly I get the following error;

**The exit status of the task that caused the workflow execution to fail was: null.

The full error message was:

No such property: variantcaller for class: Script_a7aea67c**

********************************************************************************************************************************

2- Mutect2 for Variantcalling with no intervals;  there is a problem with the script in handling the ""intervals (-L)"" option:

**Error executing process > 'PileupSummariesForMutect2 (NIST7035_vs_NIST7086-no_intervals)'

Caused by:
Process `PileupSummariesForMutect2 (NIST7035_vs_NIST7086-no_intervals)` terminated with an error exit status (1)

Command executed:
gatk --java-options ""-Xmx25g""    GetPileupSummaries  
-I NIST7035.recal.bam       -V somatic-hg38_af-only-gnomad.hg38.vcf.gz                  
-O no_intervals_NIST7035_pileupsummaries.table

Command exit status:
  1

 A USER ERROR has occurred: Argument intervals was missing: Argument 'intervals' is required.**

## Steps to reproduce

Steps to reproduce the behaviour:
1. Command line: 

nextflow run main.nf --input /hpcshare/genomics/sarek_analyses/results/Preprocessing/TSV/recalibrated.tsv
-profile base,singularity
--step variantcalling 
--tools mutect2
--pon /homenfs/yabili/gatk_bundle/resources/somatic-hg38_1000g_pon.hg38.vcf.gz
--pon_index /homenfs/yabili/gatk_bundle/resources/somatic-hg38_1000g_pon.hg38.vcf.gz.tbi 
--germline_resource /homenfs/yabili/gatk_bundle/resources/somatic-hg38_af-only-gnomad.hg38.vcf.gz 
--germline_resource_index /homenfs/yabili/gatk_bundle/resources/somatic-hg38_af-only-gnomad.hg38.vcf.gz.tbi

(to get the second error I disabled the intervals option in nextflow.config)

## Nextflow Installation
- Version: 2.7

## Container engine
- Image tag: sarek_2.7.sif


",YussAb,https://github.com/nf-core/sarek/issues/359
MDU6SXNzdWU4Mzc3Njk1NTI=,[FEATURE] Have non blocking channel out of bwamem,CLOSED,2021-03-22T14:24:00Z,2021-07-15T15:13:05Z,2021-07-15T15:13:05Z,"Copied the discussion from https://github.com/nf-core/sarek/pull/358 for context.

Basically what would be good is to have non-blocking channel our of the mapping step, so that following steps can happen while mapping is still ongoing.

from @rogerzhanglijie

> how to resolve the groupTuple block problems in line 78, because I can not comment on that line, so I comment here,
> if the mapping step has multiple runs, groupTuple will block all bam files, until all samples have the aligned results, however, we can not set size parameter in groupTuple, because we don't know how many runs in one sample, there may be 2,3,4,.... and so on.
 
> I don't see a problem here actually.
the reads_input.groupTuple(by: [0,1]) ensure that we have only one fastq pairs in the channel emission.
We group the different bams from the same samples with this:
> ```
> MERGE_BAM(bam_bwa_multiple) 
>  bam_mapped       = bam_bwa_single.mix(MERGE_BAM.out.bam) 
>  ```
> But maybe I misunderstood your comment.
> Can you elaborate more if it was the case?

> What I mean is in this part of groupTuple , this part is grouping the align result bams according to bam files size, not the input fastq files. because you need to group the output bam according to bam files size, bam_bwa_to_sort channel will not emit element until all aligned bam has finished. what's more, there is no set 'size' parameter in groupTuple operator, so the groupTuple will block until all samples aligned bam finished in bwa-mem, because groupTuple is a block operator if you don't set 'size' parameter.

> ok, I'll do some more tests for that.
I copied part of this code from dsl1 and tried to update it, it's definitively possible that I created issues.
Thanks for noticing that

> Looking forward to your reply as soon as possible, I have come across the same problem in my code, but I don't know how to solve this problem, because the size numbers are Dynamically changing values for different samples which multiple run data.

> Ok, re-read and tested things, and I understand what you want.
Sorry, I was still in a no coffee mode this morning.",maxulysse,https://github.com/nf-core/sarek/issues/362
MDU6SXNzdWU4NDAxNjMxMzU=,[BUG] Modules BWA and BWamem2 mulled-container  hashes match,CLOSED,2021-03-24T21:04:32Z,2021-03-30T13:31:19Z,2021-03-30T13:31:19Z,"Wanted to copy&paste the hashes for the bwam men and bwa =mem2 mulled containers and realised, that they were both the same. Will try to find out which is which and add it as comment.

<!--
# nf-core/sarek bug report

Hi there!

Thanks for telling us about a problem with the pipeline.
Please delete this text and anything that's not relevant from the template below:
-->

## Check Documentation

I have checked the following places for your error:

- [ ] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [ ] [nf-core/sarek pipeline documentation](https://nf-co.re/nf-core/sarek/usage)

## Description of the bug

<!-- A clear and concise description of what the bug is. -->

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line: <!-- [e.g. `nextflow run ...`] -->
2. See error: <!-- [Please provide your error message] -->

## Expected behaviour

<!-- A clear and concise description of what you expected to happen. -->

## Log files

Have you provided the following extra information/files:

- [ ] The command used to run the pipeline
- [ ] The `.nextflow.log` file <!-- this is a hidden file in the directory where you launched the pipeline -->

## System

- Hardware: <!-- [e.g. HPC, Desktop, Cloud...] -->
- Executor: <!-- [e.g. slurm, local, awsbatch...] -->
- OS: <!-- [e.g. CentOS Linux, macOS, Linux Mint...] -->
- Version <!-- [e.g. 7, 10.13.6, 18.3...] -->

## Nextflow Installation

- Version: <!-- [e.g. 19.10.0] -->

## Container engine

- Engine: <!-- [e.g. Conda, Docker, Singularity or Podman] -->
- version: <!-- [e.g. 1.0.0] -->
- Image tag: <!-- [e.g. nfcore/sarek:2.6] -->

## Additional context

<!-- Add any other context about the problem here. -->
",FriederikeHanssen,https://github.com/nf-core/sarek/issues/364
MDU6SXNzdWU4NjEzNDczNzg=,[BUG] no suitable codecs found,CLOSED,2021-04-19T14:05:17Z,2022-06-20T13:33:06Z,2022-06-20T13:33:06Z,"## Description of the bug

When trying to run sarek (Mutect2) in a cluster, it stops after this error:

```
-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'Mutect2 (lv-2_vs_lv-4-11_1-122082543)'

Caused by:
  Process `Mutect2 (lv-2_vs_lv-4-11_1-122082543)` terminated with an error exit status (2)

Command executed:

  # Get raw calls
  gatk --java-options ""-Xmx32g""       Mutect2       -R genome.fa      -I lv-2.recal.bam  -tumor lv-2       -I lv-4.recal.bam -normal lv-4       -L 11_1-122082543.bed              --germline-resource input.9              -O 11_1-122082543_lv-2_vs_lv-4.vcf

Command exit status:
  2

Command output:
  (empty)

Command error:
  Using GATK jar /opt/conda/envs/nf-core-sarek-2.7/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx32g -jar /opt/conda/envs/nf-core-sarek-2.7/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar Mutect2 -R genome.fa -I lv-2.recal.bam -tumor lv-2 -I lv-4.recal.bam -normal lv-4 -L 11_1-122082543.bed --germline-resource input.9 -O 11_1-122082543_lv-2_vs_lv-4.vcf
  11:26:11.367 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/envs/nf-core-sarek-2.7/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
  Apr 19, 2021 11:26:11 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine
  INFO: Failed to detect whether we are running on Google Compute Engine.
  11:26:11.922 INFO  Mutect2 - ------------------------------------------------------------
  11:26:11.922 INFO  Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.7.0
  11:26:11.922 INFO  Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/
  11:26:11.922 INFO  Mutect2 - Executing as mgrau@hh-yoda-01-07.ebi.ac.uk on Linux v3.10.0-693.5.2.el7.x86_64 amd64
  11:26:11.922 INFO  Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_265-b11
  11:26:11.922 INFO  Mutect2 - Start Date/Time: April 19, 2021 11:26:11 AM UTC
  11:26:11.923 INFO  Mutect2 - ------------------------------------------------------------
  11:26:11.923 INFO  Mutect2 - ------------------------------------------------------------
  11:26:11.923 INFO  Mutect2 - HTSJDK Version: 2.21.2
  11:26:11.923 INFO  Mutect2 - Picard Version: 2.21.9
  11:26:11.923 INFO  Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  11:26:11.923 INFO  Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  11:26:11.923 INFO  Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  11:26:11.923 INFO  Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  11:26:11.923 INFO  Mutect2 - Deflater: IntelDeflater
  11:26:11.923 INFO  Mutect2 - Inflater: IntelInflater
  11:26:11.923 INFO  Mutect2 - GCS max retries/reopens: 20
  11:26:11.923 INFO  Mutect2 - Requester pays: disabled
  11:26:11.924 INFO  Mutect2 - Initializing engine
  11:26:12.544 INFO  Mutect2 - Shutting down engine
  [April 19, 2021 11:26:12 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.02 minutes.
  Runtime.totalMemory()=2359820288
  ***********************************************************************
  
  A USER ERROR has occurred: Cannot read file://input.9 because no suitable codecs found
  
  ***********************************************************************
  Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.
```

On the other hand, Strelka2 finishes without errors.

## Steps to reproduce

1. Command line:
nextflow run nf-core/sarek -profile singularity --genome GRCm38 --input input.tsv --tools 'Mutect2,VEP' --resume

## System

- Hardware: HPC 
- Executor: LSF

## Nextflow Installation

- Version: 20.10.0 build 5430
- 
## Container engine

- Engine: Singularity  
- version: 3.5.0
 
",migrau,https://github.com/nf-core/sarek/issues/367
MDU6SXNzdWU5MDI0OTc4NzQ=,[BUG] Json schema `--tools`,CLOSED,2021-05-26T14:41:58Z,2021-07-15T08:49:10Z,2021-07-15T08:49:10Z,"Hi there,

when trying to launch Sarek 2.7 with the new tower interface or the nf-core website launcher, it is not possible to select multiple tools, because the parameter is defined as an `enum` parameter. Maybe there were alredy some discussions on slack that I missed 😄 

I thought it would be nice to start a discussion here on how to solve that, can also help with this on a PR :)
One option would be to keep the `--tools` param, but then define it as a string. It is not possible though to validate then that the tools provided are valid.

Another option would be to have `--caller` and `--annotate` params, etc. And define those as `enum`. A disadvantage of this approach though is that still providing two different variant callers won't be possible again. So that this is possible the param should also be defined only as `string`.

So not sure what would be the best way to solve this really...",ggabernet,https://github.com/nf-core/sarek/issues/380
MDU6SXNzdWU5MDY1MjIyNzg=,[BUG] Mutect2 - java.lang.StringIndexOutOfBoundsException,CLOSED,2021-05-29T17:35:45Z,2021-05-30T07:55:18Z,2021-05-30T07:55:18Z,"Hello,

please, upgrade GATK to version 4.1.8.0 to fix Mutect2 java.lang.StringIndexOutOfBoundsException as can be seen here:
https://github.com/broadinstitute/gatk/issues/6516
",xhejtman,https://github.com/nf-core/sarek/issues/384
MDU6SXNzdWU5MjI3MTc2Mzg=,"Do not define ""null"" string as a default value at nextflow_schema.json",CLOSED,2021-06-16T14:50:11Z,2021-07-15T08:48:59Z,2021-07-15T08:48:59Z,"## Description
Some fields at `nextflow_schema.json` file define default values like an string (`""default"": ""null""`) this will be a problem in the upcoming version of [tower.nf](https://tower.nf/). The ""null"" string will be set at the launchpad form and send to Nextflow when launching the pipeline. Finally the run will fail because Nextflow will interpret it as a string and not as an empty parameter.

## Solution
Aligned with the discussion [here](https://github.com/nf-core/tools/issues/992) about enforcing stricter rules for initialising params with no default value, I suggest to just set this fields to `null` at `nextflow.config` file and remove the default setting from the schema file. This will be compatible with the future [tower.nf](https://tower.nf/) release.",jordeu,https://github.com/nf-core/sarek/issues/389
MDU6SXNzdWU5MjQxNzE0NDc=,[BUG] Sentieon_BQSR is not skipped when dbsnp and indel files are not provided,CLOSED,2021-06-17T17:24:48Z,2024-12-09T16:22:53Z,2023-08-15T10:50:58Z,"<!--
# nf-core/sarek bug report

Hi there!

Thanks for telling us about a problem with the pipeline.

-->

## Check Documentation

I have checked the following places for your error:

- [ ] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [ ] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

<!-- A clear and concise description of what the bug is. -->

I want the sarek pipeline to SKIP BQRS because I don't have dbsnp and indels for my rice genome. But the pipeline tries to run Sentieon_BQSR anyway and fails.  What am I doing wrong?

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line: <!-- [e.g. `nextflow run ...`] -->

nextflow run ./sarek/ -c mapping.conf -profile uiuc_hpcbio --custom_config_base /home/groups/hpcbio/projects/meksem/2021-June-Rice/results/20210612-11-Samples/configs -resume -qs 4 -with-report -with-trace

2. See error: <!-- [Please provide your error message] -->

<pre>
Meksem_8.deduped.bam`         --algo QualCal         -k input.3         Meksem_8.recal.table

  sentieon driver         -t 24         -r GCF_001433935.1_IRGSP-1.0_genomic.fna         -i Meksem_8.deduped.bam         -q Meksem_8.recal.table         --algo QualCal         -k input.3         Meksem_8.table.post         --algo ReadWriter Meksem_8.recal.bam

  sentieon driver         -t 24         --algo QualCal         --plot         --before Meksem_8.recal.table         --after Meksem_8.table.post         Meksem_8_recal_result.csv

Command exit status:
  1

Command output:
  (empty)

Command error:
  cmdline: /home/apps/software/sentieon/201911/libexec/driver -t 24 -r GCF_001433935.1_IRGSP-1.0_genomic.fna -i Meksem_8.deduped.bam --algo QualCal -k input.3 Meksem_8.recal.table
  This software is licensed to dslater@illinois.edu by Sentieon Inc.
  Error: Unrecognized format for vcf file input.3

Work dir:
  /home/groups/hpcbio/projects/meksem/2021-June-Rice/results/20210612-11-Samples/work/e0/62a1cb946b94d27021fa2333f8cdc5

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`

</pre>


## Expected behaviour

<!-- A clear and concise description of what you expected to happen. -->

I expected the pipieline to skip the Sentieon_BQSR process where it failed

## Log files

Have you provided the following extra information/files:

- [ ] The command used to run the pipeline
- [ ] The `.nextflow.log` file <!-- this is a hidden file in the directory where you launched the pipeline -->

[nextflow.log](https://github.com/nf-core/sarek/files/6672008/nextflow.log)
[samples.tsv.txt](https://github.com/nf-core/sarek/files/6672027/samples.tsv.txt)
[mapping.conf.txt](https://github.com/nf-core/sarek/files/6672028/mapping.conf.txt)




## System

- Hardware: <!-- [e.g. HPC, Desktop, Cloud...] -->
- Executor: <!-- [e.g. slurm, local, awsbatch...] -->
- OS: <!-- [e.g. CentOS Linux, macOS, Linux Mint...] -->
- Version <!-- [e.g. 7, 10.13.6, 18.3...] -->

## Nextflow Installation

- Version: <!-- [e.g. 19.10.0] -->

nextflow/20.11.0-edge-Java_1.8.0_152

## Container engine

- Engine: <!-- [e.g. Conda, Docker, Singularity, Podman, Shifter or Charliecloud] --> singularity
- version: <!-- [e.g. 1.0.0] --> 3.4.1
- Image tag: <!-- [e.g. nfcore/sarek:2.7] --> nf-core-sarek-2.7

## Additional context

<!-- Add any other context about the problem here. -->

Processing a non-human genome that does not have dbsnp or indel files
",grendon,https://github.com/nf-core/sarek/issues/391
MDU6SXNzdWU5Mjc2MjI3OTg=,[FEATURE] GATK GenotypeGVCFs option?,CLOSED,2021-06-22T20:51:00Z,2022-07-21T08:19:58Z,2022-07-21T08:19:58Z,"It doesn't appear GenotypeGVCFs included in `--tools`? Could it be included in a future version? It's an important step for some larger scale studies. GATK documentation and description: https://gatk.broadinstitute.org/hc/en-us/articles/360037057852-GenotypeGVCFs

Thanks!",mvelinder,https://github.com/nf-core/sarek/issues/392
MDU6SXNzdWU5MzE1MTUwOTg=,[BUG] Wrong references for files in nextflow.config ,CLOSED,2021-06-28T12:23:50Z,2021-06-28T12:45:16Z,2021-06-28T12:26:26Z,"https://github.com/nf-core/sarek/blob/master/nextflow.config contains wrong references:
timeline {
  enabled = true
  file = ""${params.tracedir}/execution_timeline_${trace_timestamp}.html""
}
report {
  enabled = true
  file = ""${params.tracedir}/execution_report_${trace_timestamp}.html""
}
timeline {
  enabled = true
  file = ""${params.tracedir}/execution_trace_${trace_timestamp}.txt""
}
trace {
  enabled = true
  file = ""${params.tracedir}/pipeline_dag_${trace_timestamp}.svg""
}
dag {
  enabled = true
  file = ""${params.tracedir}/pipeline_dag_${trace_timestamp}.svg""
}

trace should not be in .svg file? While timeline is present twice. ",xhejtman,https://github.com/nf-core/sarek/issues/393
MDU6SXNzdWU5Mzk4NTA1ODQ=,adaptation for custom read names (specifically MGISEQ),CLOSED,2021-07-08T13:17:41Z,2023-05-26T11:27:07Z,2023-05-26T11:27:06Z,"I'm attempting to call variants on MGISEQ .fq, although very similar to Illumina, there are some differences, for example this is read header for mgiseq ""@V300082373L1C001R0061260954/1"".

May i ask what adaptations should be made for a successful execution, if any?

 ",raimondsre,https://github.com/nf-core/sarek/issues/395
MDU6SXNzdWU5NDEyODgzMjI=,[BUG] Cutadapt (trim_galore) issue: ERROR: Running in parallel is not supported on Python 2,CLOSED,2021-07-10T17:06:57Z,2021-07-16T12:17:27Z,2021-07-12T09:47:21Z,"
While running sarek with option --trim_fastq i encounter a following error:

` ERROR: Running in parallel is not supported on Python 2 `

The issue could be with singularity, could you suggest a solution?

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line: 
```
 nextflow run nf-core/sarek \
--input 1test.tsv \
--split_fastq 2000000 \
--tools 'HaploTypeCaller' \
--skip_qc 'FastQC, MultiQC, bamQC, BCFtools, BaseRecalibrator' \
--trim_fastq \
--genome GRCh38 \
--save_reference \
--species 'homo_sapiens' \
-profile singularity 
```
2. here is the .command.err file:  
[.command.err](https://github.com/nf-core/sarek/files/6795661/command_err.txt)


## Log files

 The `.nextflow.log` file 
[.nextflow.log](https://github.com/nf-core/sarek/files/6795662/nextflow_log.txt)


## System

- Hardware: HPC
- Executor: PBS
- OS:  CentOS Linux
- Version 3.10.0-957.21.3.el7.x86_64

## Nextflow Installation

- Version: 2.7.1

## Container engine

- Engine: singularity
- version: 3.2.1-7.1.ohpc.1.3.8
- Image tag: nfcore/sarek:2.7 (?)

",raimondsre,https://github.com/nf-core/sarek/issues/396
MDU6SXNzdWU5NDQ5NTQ3NzM=,Cannot find `nf-core/sarek`,CLOSED,2021-07-15T03:07:12Z,2021-07-15T07:58:15Z,2021-07-15T07:58:15Z,"nextflow pull nf-core/sarek
Checking nf-core/sarek ...
WARN: Cannot read project manifest -- Cause: Connection reset
Cannot find `nf-core/sarek` -- Make sure exists a GitHub repository at this address `https://github.com/nf-core/sarek`

Hello, I can't use it normally after I input the download command. Would you like to ask if I can download the file to the local separately and then install it later?
",ld9866,https://github.com/nf-core/sarek/issues/399
MDU6SXNzdWU5NDUxNzUxOTU=,Add tumor only mode variant calling to DSL2,CLOSED,2021-07-15T09:16:30Z,2022-02-11T17:18:31Z,2022-02-11T17:17:56Z,,maxulysse,https://github.com/nf-core/sarek/issues/400
MDU6SXNzdWU5NDUxNzU5Mjk=,Add CNV to DSL2,CLOSED,2021-07-15T09:17:17Z,2022-07-13T09:36:54Z,2022-07-13T09:36:54Z,,maxulysse,https://github.com/nf-core/sarek/issues/401
MDU6SXNzdWU5NDUxODcwNDQ=,Update Sarek docs,CLOSED,2021-07-15T09:30:37Z,2022-07-20T12:52:52Z,2022-07-20T12:29:55Z,"In general the docs need to be overhauled, including:

- [x] #502
- [x] #503
- [x] #504",FriederikeHanssen,https://github.com/nf-core/sarek/issues/402
MDU6SXNzdWU5NDUxODk4OTc=,Add a UMI subworkflow to DSL2,CLOSED,2021-07-15T09:34:14Z,2022-02-11T17:17:42Z,2022-02-11T17:17:42Z,,maxulysse,https://github.com/nf-core/sarek/issues/403
MDU6SXNzdWU5NDUxOTAzMDE=,Finally solve Spark issue,CLOSED,2021-07-15T09:34:45Z,2022-06-09T15:10:44Z,2022-06-09T15:10:44Z,,maxulysse,https://github.com/nf-core/sarek/issues/404
MDU6SXNzdWU5NDUxOTA2MDM=,Create a sentieon specific subworkflow for DSL2,OPEN,2021-07-15T09:35:06Z,2024-08-19T13:12:26Z,,"EDIT (from @cjfields): 
Modules checklist:

- [x] alignment (accelerated bwa)
- [ ] ~BQSR (optional?) (EDIT: Won't be implemented. It should be possible to use GATK-based BQSR.)~
- [x] DNAseq (SNV/Indel) - including GVCF support
- [ ] DNAscope (SV and SNV/Indel) - including GVCF support
- [ ] TNseq (tumor/normal) - including GVCF support
- [ ] TNscope (tumor/normal)",maxulysse,https://github.com/nf-core/sarek/issues/405
MDU6SXNzdWU5NDUxOTA4ODI=,Add Mutec2 to DSL2,CLOSED,2021-07-15T09:35:26Z,2022-02-11T17:17:30Z,2022-02-11T17:17:30Z,,maxulysse,https://github.com/nf-core/sarek/issues/406
MDU6SXNzdWU5NDUxOTQ2Mjc=,GA4GH compliant,OPEN,2021-07-15T09:39:50Z,2024-08-19T13:12:26Z,,"https://www.ga4gh.org/

https://seqera.io/blog/tower-api-docs-release/

More info to follow ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/407
MDU6SXNzdWU5NDY0ODU0OTc=,[BUG] MARKDUPLICATES -> QC_MARKDUPLICATES,CLOSED,2021-07-16T17:47:17Z,2021-07-19T08:07:54Z,2021-07-16T21:48:30Z,"I recently reinstalled sarek from scratch using the test run command
`nextflow run nf-core/sarek -profile test,singularity -r dsl2 -resume` 

And I believe there is a typo as a result of the recent ""code clean up"" commit (July 15 @FriederikeHanssen ) the `MARKDUPLICATES` subworkflow was renamed to `QC_MARKDUPLICATES` however on lines 341 and 343 of the sarek.nf workflow still use the name `MARKDUPLICATES` to generate the cram_markduplicates channel, and qc_reports. 

As a result, the pipeline can not continue as written. Locally I renamed the commands on lines 341, 343 and was able to get it running again. I imagine this was a missed typo, but regardless I wanted to point it out.

Thanks for all of your hard work, and I look forward to using sarek more and more often.",nickhsmith,https://github.com/nf-core/sarek/issues/409
MDU6SXNzdWU5NTA1Nzk4MTE=,Mutetc2 on aws and ConcateVCF ,CLOSED,2021-07-22T11:56:02Z,2022-05-23T14:01:41Z,2022-05-23T14:01:40Z,"<!--
# nf-core/sarek bug report

Hi there!

Thanks for telling us about a problem with the pipeline.
Please delete this text and anything that's not relevant from the template below:

## Description of the bug

<!-- A clear and concise description of what the bug is. --> I tried to run pipeline on AWS several time but failed due to Mutect2 and concateVCF.
[sarek-issue-mutect2.txt](https://github.com/nf-core/sarek/files/6861985/sarek-issue-mutect2.txt)
[Sarek-issue-concanvcf.txt](https://github.com/nf-core/sarek/files/6861986/Sarek-issue-concanvcf.txt)

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line: <!-- [e.g. `nextflow run ...`] -->nextflow run 'https://github.com/nf-core/sarek'
		 -name tiny_celsius
		 -params-file nf-3fIGCIZ9boB5R5.params.json
		 -with-tower
		 -r 68b9930a74962f3c42eee71f51e6dd2646269199
		 -resume a62adcf8-3b18-4d01-acf9-656cdf20da21
2. See error: <!-- [Please provide your error message] -->Please see attached files.


",varun171,https://github.com/nf-core/sarek/issues/413
MDU6SXNzdWU5NTMwNzUzNTI=,[BUG] using with custom genome,CLOSED,2021-07-26T16:23:25Z,2022-07-04T21:37:14Z,2022-05-11T07:26:33Z,"<!--
# nf-core/sarek bug report

-->

Hi,
I tried to use the Sarek pipe line with costume genome, without dbSNP or known indels. I couldn't make it work with the following, wonder if this pipeline is capable of doing it or did I do it wrong? Thanks!

nextflow run /path/variant_calling/sarek \
-profile docker \
--tools Manta, Strelka \
--input  ${input} \
--outdir ${sarek_result} \
--genome ""custom"" \
--fasta ${ref_fa} \
--bwa $bwa \
--fasta_fai ${fa_fai} \
--email ${email} \
--max_cpus 16 \
--publish_dir_mode ""copy"" \
-with-report report.html
",xiaoyunguo,https://github.com/nf-core/sarek/issues/414
MDU6SXNzdWU5NTM4MzEzNDg=,SNPEff completes successfully for custom genomeswith mismatching chromosome names causing empty reports,CLOSED,2021-07-27T12:28:07Z,2022-07-18T13:35:08Z,2022-07-18T13:35:08Z,"Hi, 

I'm having two different issues with the Sarek MultiQC report that I'm not sure how to solve.

Regarding pipeline versions: I'm using Sarek v2.7.1, with singularity containers (which uses MultiQC1.8).

The first issue is that no results are shown for the SnpEff module and this I observed with two independent analyses already, when using custom genome snpEff cache.

![Screenshot 2021-07-27 at 14 13 26](https://user-images.githubusercontent.com/3973503/127153013-3b000485-e0a0-4c0c-91e6-649c758cb772.png)

The other issue appears for one of the projects for me and not for the other, and it is that the MultiQC report is stuck at ""loading"" the plots, and no plots are shown, only gray boxes. The same issue persists even when making a fresh re-run of the pipeline to generate a new MultiQC report.

![Screenshot 2021-07-27 at 14 28 46](https://user-images.githubusercontent.com/3973503/127153722-d0976099-f4ab-402e-939c-987a8599da5b.png)


I can send you privately the multiqc report, GitHub does not let me upload it here.


Command used to run the analysis:
```
nextflow run nf-core/sarek -r 2.7.1 -profile cfc \
--input input.tsv \
--fasta ""/sfs/7/workspace/ws/qeaga01-QWTPM-0/references/ncbi-genomes-2021-07-16/GCA_000750555.1_ASM75055v1_genomic_Ecoli_BW25113.fa"" \
--tools strelka,snpeff \
--step mapping \
--igenomes_ignore \
--genome custom \
--bwa false \
--no_gatk_spark \
--snpeff_cache /sfs/7/workspace/ws/qeaga01-QWTPM-0/Ecoli_BW25113/reference/ \
--annotation_cache true \
--snpeff_db Escherichia_coli_bw25113
```

NXF_VER=20.10.0",ggabernet,https://github.com/nf-core/sarek/issues/415
MDU6SXNzdWU5NjE0MTg4Mjk=,[BUG] Error downloading nf-core/sarek,CLOSED,2021-08-05T04:19:18Z,2021-08-05T10:00:47Z,2021-08-05T10:00:46Z,"hello! l have installed docker version sarek through print ""docker pull nfcore/sarek"" and it was done successfully.
Besides,  l install nextflow through conda and it can successfully run. But when l print "" nextflow run nf-core/sarek -profile docker"" it showed that
N E X T F L O W  ~  version 21.04.3
Unknown project `nf-core/sarek` -- NOTE: automatic download from remote repositories is disable.
l'd like to ask how can l solve it ?
thank you!",ld9866,https://github.com/nf-core/sarek/issues/421
MDU6SXNzdWU5NjE0OTc3ODE=,[BUG] Error downloading Nextflow pipelines,CLOSED,2021-08-05T06:41:23Z,2021-08-05T09:51:45Z,2021-08-05T09:50:27Z,"hello!
I can't use our software normally because it can't download automatically.
how can l solve this problem？
“nextflow run nextflow-io/hello“

N E X T F L O W  ~  version 21.04.3
Unknown project `nextflow-io/hello` -- NOTE: automatic download from remote repositories is disabled
",ld9866,https://github.com/nf-core/sarek/issues/422
MDU6SXNzdWU5NzAxMDc4OTI=,how to control the number of maximum files in a HPC with lustre and slurm from sarek,CLOSED,2021-08-13T06:37:40Z,2022-05-11T07:25:30Z,2022-05-11T07:25:30Z,"Hi!
I'm trying to process 36 whole-genome samples in an HPC cluster with slurm and a lustre filesystem. Sarek runs perfectly well with one sample but when it comes to analyze the whole cohort then it parallelizes so match that it reaches the maximum number of files allowed for the user, which is currently set to 200,000. Is there any way to control the number of maximum files that it can write with sarek for this type of cluster?

Thanks in advance.

S.
",smzt,https://github.com/nf-core/sarek/issues/423
MDU6SXNzdWU5NzMyNjgzMzY=,Nextfow tower[BUG],CLOSED,2021-08-18T04:43:34Z,2021-09-07T15:09:58Z,2021-09-07T15:09:58Z,"I cannot use Sarek in Nextfow Tower, is this profile available? How can I point the lauchpad to my s3 bucket where I have the tsv? do I need to use fusion for this?
Thank you in advance!
",luantunez,https://github.com/nf-core/sarek/issues/424
MDU6SXNzdWU5NzM0MDQ1NTg=,bamQC runtime exceeded [BUG],CLOSED,2021-08-18T08:21:47Z,2021-09-08T10:11:46Z,2021-09-08T10:11:46Z,"Hi there!

I have a problem running the sarek pipeline, the bamQC jobs get canceled after a day and I'm failing to increase the runtime either via command line or config file.

## Check Documentation

I have checked the following places for your error:

- [x] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [x] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

I'm running the pipeline with either
>--max_time '4.day'

or a config file:

>process { withLabel: 'BamQC' {   time = '96h'  }}

Ressources of the machine seem to be correctly determined:
>Max Resources     : 128 GB memory, 16 cpus, 10d time per job

And bamQC is almost making it, before getting cancled after 1d of runtime (last line from job output):
>Processed 450 out of 485 windows...



## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line: <!-- [e.g. `nextflow run ...`] -->
>nextflow run nf-core/sarek -profile conda -c params.config --input input.tsv --genome GRCh37 --resume
2. See error: <!-- [Please provide your error message] -->
See also nextflow.log (line 4135)


>'BamQC (HCT116-HCT116_1_1)'
>Caused by:
>Process exceeded running time limit (1d)
>Command executed:
>qualimap --java-mem-size=128G         bamqc         -bam HCT116_1_1.bam         --paint-chromosome-limits         --genome-gc-distr HUMAN                  -nt 16         -skip-duplicated         --skip-dup-mode 0         -outdir HCT116_1_1         -outformat HTML


## Expected behaviour

I assumed either command line or config file should be able to change the bamQC runtime.

## Log files

Have you provided the following extra information/files:

- [x] The command used to run the pipeline
- [x] The `.nextflow.log` file <!-- this is a hidden file in the directory where you launched the pipeline -->
[nextflow.log](https://github.com/nf-core/sarek/files/7005681/nextflow.log)

## System

- Hardware: <!-- [e.g. HPC, Desktop, Cloud...] -->
Server, 1TB RAM, 128 cores
- Executor: <!-- [e.g. slurm, local, awsbatch...] -->
Local
- OS: <!-- [e.g. CentOS Linux, macOS, Linux Mint...] -->
Mariux (in-house linux)
- Version <!-- [e.g. 7, 10.13.6, 18.3...] -->

## Nextflow Installation

- Version: <!-- [e.g. 19.10.0] -->
21.04.1.5556

## Container engine

- Engine: <!-- [e.g. Conda, Docker, Singularity, Podman, Shifter or Charliecloud] -->
Conda
- version: <!-- [e.g. 1.0.0] -->
4.9.0
- Image tag: <!-- [e.g. nfcore/sarek:2.7] -->
nf-core/sarek v2.7

## Additional context

<!-- Add any other context about the problem here. -->
",giesselmann,https://github.com/nf-core/sarek/issues/425
MDU6SXNzdWU5NzUxMzQzNTQ=,tsv on tower[BUG],CLOSED,2021-08-19T23:47:40Z,2021-10-26T13:07:16Z,2021-10-26T13:07:16Z,"Hello! I am trying to run nextflow-tower with a custom tsv file, but I am getting ````Cannot invoke method toInteger() on null object````
from line #4183 in main.nf
I think it is because I am not getting the tsv right. I am using 
````
1222	XX	0	1222N	1222N_1 https://raw.githubusercontent.com/nf-core/test-datasets/sarek/testdata/tiny/normal/tiny_n_L001_R1_xxx.fastq.gz	https://raw.githubusercontent.com/nf-core/test-datasets/sarek/testdata/tiny/normal/tiny_n_L001_R2_xxx.fastq.gz
1222	XX	1	9666T	9666T_1 https://raw.githubusercontent.com/nf-core/test-datasets/sarek/testdata/tiny/tumor/tiny_t_L001_R1_xxx.fastq.gz	https://raw.githubusercontent.com/nf-core/test-datasets/sarek/testdata/tiny/tumor/tiny_t_L001_R2_xxx.fastq.gz

````
Could you guide me please? Thanks in advance!",luantunez,https://github.com/nf-core/sarek/issues/426
I_kwDOCvwIC847k4G6,[BUG] Control-FREEC 'Stack Smashing' error with Mouse SNP file,CLOSED,2021-09-17T16:18:07Z,2023-11-14T11:36:05Z,2023-08-15T14:20:49Z,"<!--
# nf-core/sarek bug report

Hi there!

Thanks for telling us about a problem with the pipeline.
Please delete this text and anything that's not relevant from the template below:
-->

## Check Documentation

I have checked the following places for your error:

- [X] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [X] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

We are running mouse data with the full pipeline. 

The recommended SNP file input: `""${params.igenomes_base}/Mus_musculus/Ensembl/GRCm38/MouseGenomeProject/mgp.v5.merged.snps_all.dbSNP142.vcf.gz""` is causing Control-FREEC to crash. 

We have run just FREEC outside of the pipeline, but with pipeline prepared config file to replicate the error: 

```
$ singularity run nfcore-sarek-2.7.1.img freec -conf 846_KB1P_T.config.txt
Control-FREEC v11.6 : a method for automatic detection of copy number alterations, subclones and for accurate estimation of contamination and main ploidy using deep-sequencing data
Multi-threading mode using 8 threads
..consider the sample being female
..Breakpoint threshold for segmentation of copy number profiles is 0.8
..telocenromeric set to 50000
..FREEC is not going to adjust profiles for a possible contamination by normal cells
..Coefficient Of Variation set equal to 0.05
..it will be used to evaluate window size
..Output directory:	.
..Directory with files containing chromosome sequences:	/fastscratch/lloydm/8d7a192defa08d02b633f305e10578/Chromosomes
..Sample file:	/fastscratch/lloydm/8d7a192defa08d02b633f305e10578/846_KB1P_T.pileup
..Sample input format:	pileup
..minimal expected GC-content (general parameter ""minExpectedGC"") was set to 0.35
..maximal expected GC-content (general parameter ""maxExpectedGC"") was set to 0.55
..Polynomial degree for ""ReadCount ~ GC-content"" normalization is 3 or 4: will try both
..Minimal CNA length (in windows) is 1
..File with chromosome lengths:	/fastscratch/lloydm/8d7a192defa08d02b633f305e10578/GRCm38.len
..Using the default minimal mappability value of 0.85
..uniqueMatch = FALSE
..average ploidy set to 2
..break-point type set to 2
..noisyData set to 0
..Control-FREEC will not look for subclones
..will use SNP positions from /fastscratch/lloydm/8d7a192defa08d02b633f305e10578/mgp.v5.merged.snps_all.dbSNP142.vcf.gz to calculate BAF profiles
..Starting reading /fastscratch/lloydm/8d7a192defa08d02b633f305e10578/mgp.v5.merged.snps_all.dbSNP142.vcf.gz to get SNP positions
*** stack smashing detected ***: <unknown> terminated
SIGABRT: abort
PC=0x473c4b m=0 sigcode=0

goroutine 1 [running, locked to thread]:
syscall.RawSyscall(0x3e, 0x1b849, 0x6, 0x0, 0xc000173ef0, 0x49de52, 0x1b849)
	/usr/lib64/go-1.11.1/src/syscall/asm_linux_amd64.s:78 +0x2b fp=0xc000173eb8 sp=0xc000173eb0 pc=0x473c4b
syscall.Kill(0x1b849, 0x6, 0x4388ee, 0xc000173f20)
	/usr/lib64/go-1.11.1/src/syscall/zsyscall_linux_amd64.go:597 +0x4b fp=0xc000173f00 sp=0xc000173eb8 pc=0x4705bb
github.com/sylabs/singularity/internal/app/starter.Master.func2()
	internal/app/starter/master_linux.go:152 +0x3e fp=0xc000173f38 sp=0xc000173f00 pc=0x7395ee
github.com/sylabs/singularity/internal/pkg/util/mainthread.Execute.func1()
	internal/pkg/util/mainthread/mainthread.go:21 +0x2f fp=0xc000173f60 sp=0xc000173f38 pc=0x737dff
main.main()
	cmd/starter/main_linux.go:102 +0x5f fp=0xc000173f98 sp=0xc000173f60 pc=0x91a51f
runtime.main()
	/usr/lib64/go-1.11.1/src/runtime/proc.go:201 +0x207 fp=0xc000173fe0 sp=0xc000173f98 pc=0x430bb7
runtime.goexit()
	/usr/lib64/go-1.11.1/src/runtime/asm_amd64.s:1333 +0x1 fp=0xc000173fe8 sp=0xc000173fe0 pc=0x45c601

goroutine 5 [syscall]:
os/signal.signal_recv(0xaf7ba0)
	/usr/lib64/go-1.11.1/src/runtime/sigqueue.go:139 +0x9c
os/signal.loop()
	/usr/lib64/go-1.11.1/src/os/signal/signal_unix.go:23 +0x22
created by os/signal.init.0
	/usr/lib64/go-1.11.1/src/os/signal/signal_unix.go:29 +0x41

goroutine 7 [chan receive]:
github.com/sylabs/singularity/internal/pkg/util/mainthread.Execute(0xc000242cb0)
	internal/pkg/util/mainthread/mainthread.go:24 +0xb4
github.com/sylabs/singularity/internal/app/starter.Master(0x7, 0x4, 0x1b855, 0xc00000d300)
	internal/app/starter/master_linux.go:151 +0x405
main.startup()
	cmd/starter/main_linux.go:75 +0x4b8
created by main.main
	cmd/starter/main_linux.go:98 +0x35

rax    0x0
rbx    0x0
rcx    0xffffffffffffffff
rdx    0x0
rdi    0x1b849
rsi    0x6
rbp    0xc000173ef0
rsp    0xc000173eb0
r8     0x0
r9     0x0
r10    0x0
r11    0x206
r12    0x24
r13    0x33
r14    0xaeccdc
r15    0x0
rip    0x473c4b
rflags 0x206
cs     0x33
fs     0x0
gs     0x0
```
As you see, it gets to the SNP parsing step, and crashes. 


We raised this with the Control-FREEC development team: https://github.com/BoevaLab/FREEC/issues/92

The reply being to use a cleaned SNP file prepared by their team: 
```
...run with an old SNP file: http://xfer.curie.fr/get/AS9Zncn7yfB/mm10_dbSNP137.ucsc.freec.txt.gz ?
```

We would like to be able to run mouse data through FREEC as part of the pipeline. Given that you are recommending the use of `mgp.v5.merged.snps_all.dbSNP142` as the SNP file, do you have a recommended way to get around this issue? 

",MikeWLloyd,https://github.com/nf-core/sarek/issues/428
I_kwDOCvwIC847qDZ0,[FEATURE] disable BQSR,CLOSED,2021-09-20T11:54:15Z,2021-10-28T14:15:30Z,2021-10-28T14:15:30Z,"Could be a good idea to look at disabling BQSR since it will probably be removed from the GATK BP.

Especially when having deep coverage, this step is particularly slow.

cf: https://twitter.com/VdaGeraldine/status/1296181178534440963

cc @apeltzer @FriederikeHanssen ",maxulysse,https://github.com/nf-core/sarek/issues/429
I_kwDOCvwIC848NsYL,[BUG] BaseRecalibrator errors,CLOSED,2021-09-28T21:21:30Z,2023-03-21T13:15:20Z,2023-03-21T13:15:20Z,"
[nextflow.log](https://github.com/nf-core/sarek/files/7247360/nextflow.log)

## Check Documentation

I have checked the following places for your error:

- [ x ] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [ x ] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

The pipeline stops at BaseRecalibrator step.

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line: `nextflow run nf-core/sarek -r 2.7.1 --cpus 100 --max_cpus 100 --max_memory 120.GB --input wt_etop.vs.wt_unt.tsv -profile singularity -resume`
2. See error:
```bash
-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'BaseRecalibrator (lib1-lib1-chr22_18339130-18433513)'

Caused by:
  Process `BaseRecalibrator (lib1-lib1-chr22_18339130-18433513)` terminated with an error exit status (135)

Command executed:

  gatk --java-options -Xmx7g         BaseRecalibrator         -I lib1.md.bam         -O chr22_18339130-18433513_lib1.recal.table         --tmp-dir .         -R Homo_sapiens_assembly38.fasta         -L chr22_18339130-18433513.bed         --known-sites dbsnp_146.hg38.vcf.gz         --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz         --verbosity INFO

Command exit status:
  135

Command output:
  (empty)

Command error:
  .command.sh: line 2: 66505 Bus error               gatk --java-options -Xmx7g BaseRecalibrator -I lib1.md.bam -O chr22_18339130-18433513_lib1.recal.table --tmp-dir . -R Homo_sapiens_assembly38.fasta -L chr22_18339130-18433513.bed --known-sites dbsnp_146.hg38.vcf.gz --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz --verbosity INFO

Work dir:
  /home/user/sarek_test/src/work/40/723898390643e1286d518afd5e6d68

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`
```


## Expected behaviour

It works without errors in Ubuntu 20.04 with singularity 3.6.4 on a laptop with an i5 processor.

## Log files

Have you provided the following extra information/files:

- [ x ] The command used to run the pipeline
- [ x ] The `.nextflow.log` file

## System

- Hardware: HPC, a compute node equipped with AMD ryzen processor.
- Executor:  an interactive shell through slurm (`srun -c 120 --mem 240G -t 12:00:00 --pty /bin/bash`).
- OS: CentOS
- Version: 7.9.2009

## Nextflow Installation

- Version: 21.04.1.5556

## Container engine

- Engine: Singularity
- version: 3.7.0
- Image tag: nfcore/sarek:2.7.1

## Additional context

Singularity and Java are installed through Spack.
",jacorvar,https://github.com/nf-core/sarek/issues/430
I_kwDOCvwIC848TOmN,nf-core launch sarek has erro,CLOSED,2021-09-30T03:56:25Z,2022-05-11T07:23:52Z,2022-05-11T07:23:51Z,"Hello, I encountered this error when installing the software. I don't know how to solve it?
Thank you!

```
nf-core launch sarek

INFO     NOTE: This tool ignores any pipeline parameter defaults overwritten by Nextflow config files or profiles                launch.py:130
                                                                                                                                              
? Select release / branch: 2.7.1  [release]
╭──────────────────────────────────────────────────── Traceback (most recent call last)  ─────────────────────────────────────────────────────╮
│ /home/lidong/anaconda3/bin/nf-core:8 in <module>                                                                                           │
│                                                                                                                                            │
│   7 │   sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])                                                                   │
│ ❱ 8 │   sys.exit(run_nf_core())                                                                                                            │
│   9                                                                                                                                        │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/nf_core/__main__.py:62 in run_nf_core                                                   │
│                                                                                                                                            │
│    61 │   # Lanch the click cli                                                                                                            │
│ ❱  62 │   nf_core_cli()                                                                                                                    │
│    63                                                                                                                                      │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/click/core.py:1137 in __call__                                                          │
│                                                                                                                                            │
│   1136 │   │   """"""Alias for :meth:`main`.""""""                                                                                               │
│ ❱ 1137 │   │   return self.main(*args, **kwargs)                                                                                           │
│   1138                                                                                                                                     │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/click/core.py:1062 in main                                                              │
│                                                                                                                                            │
│   1061 │   │   │   │   with self.make_context(prog_name, args, **extra) as ctx:                                                            │
│ ❱ 1062 │   │   │   │   │   rv = self.invoke(ctx)                                                                                           │
│   1063 │   │   │   │   │   if not standalone_mode:                                                                                         │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/click/core.py:1668 in invoke                                                            │
│                                                                                                                                            │
│   1667 │   │   │   │   with sub_ctx:                                                                                                       │
│ ❱ 1668 │   │   │   │   │   return _process_result(sub_ctx.command.invoke(sub_ctx))                                                         │
│   1669                                                                                                                                     │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/click/core.py:1404 in invoke                                                            │
│                                                                                                                                            │
│   1403 │   │   if self.callback is not None:                                                                                               │
│ ❱ 1404 │   │   │   return ctx.invoke(self.callback, **ctx.params)                                                                          │
│   1405                                                                                                                                     │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/click/core.py:763 in invoke                                                             │
│                                                                                                                                            │
│    762 │   │   │   with ctx:                                                                                                               │
│ ❱  763 │   │   │   │   return __callback(*args, **kwargs)                                                                                  │
│    764                                                                                                                                     │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/nf_core/__main__.py:203 in launch                                                       │
│                                                                                                                                            │
│   202 │   )                                                                                                                                │
│ ❱ 203 │   if launcher.launch_pipeline() == False:                                                                                          │
│   204 │   │   sys.exit(1)                                                                                                                  │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/nf_core/launch.py:156 in launch_pipeline                                                │
│                                                                                                                                            │
│   155 │   │   │   # Build the schema and starting inputs                                                                                   │
│ ❱ 156 │   │   │   if self.get_pipeline_schema() is False:                                                                                  │
│   157 │   │   │   │   return False                                                                                                         │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/nf_core/launch.py:216 in get_pipeline_schema                                            │
│                                                                                                                                            │
│   215 │   │   try:                                                                                                                         │
│ ❱ 216 │   │   │   self.schema_obj.get_schema_path(self.pipeline,                                                                           │
│       revision=self.pipeline_revision)                                                                                                     │
│   217 │   │   │   self.schema_obj.load_lint_schema()                                                                                       │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/nf_core/schema.py:64 in get_schema_path                                                 │
│                                                                                                                                            │
│    63 │   │   elif not local_only:                                                                                                         │
│ ❱  64 │   │   │   self.pipeline_dir = nf_core.list.get_local_wf(path, revision=revision)                                                   │
│    65 │   │   │   self.schema_filename = os.path.join(self.pipeline_dir,                                                                   │
│       ""nextflow_schema.json"")                                                                                                              │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/nf_core/list.py:53 in get_local_wf                                                      │
│                                                                                                                                            │
│    52 │   wfs = Workflows()                                                                                                                │
│ ❱  53 │   wfs.get_local_nf_workflows()                                                                                                     │
│    54 │   for wf in wfs.local_workflows:                                                                                                   │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/nf_core/list.py:142 in get_local_nf_workflows                                           │
│                                                                                                                                            │
│   141 │   │   for wf in self.local_workflows:                                                                                              │
│ ❱ 142 │   │   │   wf.get_local_nf_workflow_details()                                                                                       │
│   143                                                                                                                                      │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/nf_core/list.py:358 in get_local_nf_workflow_details                                    │
│                                                                                                                                            │
│   357 │   │   │   │   repo = git.Repo(self.local_path)                                                                                     │
│ ❱ 358 │   │   │   │   self.commit_sha = str(repo.head.commit.hexsha)                                                                       │
│   359 │   │   │   │   self.remote_url = str(repo.remotes.origin.url)                                                                       │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/git/refs/symbolic.py:197 in _get_commit                                                 │
│                                                                                                                                            │
│   196 │   │   │   SymbolicReferences. The symbolic reference will be dereferenced                                                          │
│       recursively.""""""                                                                                                                      │
│ ❱ 197 │   │   obj = self._get_object()                                                                                                     │
│   198 │   │   if obj.type == 'tag':                                                                                                        │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/git/refs/symbolic.py:190 in _get_object                                                 │
│                                                                                                                                            │
│   189 │   │   # Our path will be resolved to the hexsha which will be used accordingly                                                     │
│ ❱ 190 │   │   return Object.new_from_sha(self.repo,                                                                                        │
│       hex_to_bin(self.dereference_recursive(self.repo, self.path)))                                                                        │
│   191                                                                                                                                      │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/git/refs/symbolic.py:132 in dereference_recursive                                       │
│                                                                                                                                            │
│   131 │   │   while True:                                                                                                                  │
│ ❱ 132 │   │   │   hexsha, ref_path = cls._get_ref_info(repo, ref_path)                                                                     │
│   133 │   │   │   if hexsha is not None:                                                                                                   │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/git/refs/symbolic.py:181 in _get_ref_info                                               │
│                                                                                                                                            │
│   180 │   │   point to, or None""""""                                                                                                         │
│ ❱ 181 │   │   return cls._get_ref_info_helper(repo, ref_path)                                                                              │
│   182                                                                                                                                      │
│                                                                                                                                            │
│ /home/lidong/anaconda3/lib/python3.8/site-packages/git/refs/symbolic.py:164 in _get_ref_info_helper                                        │
│                                                                                                                                            │
│   163 │   │   if tokens is None:                                                                                                           │
│ ❱ 164 │   │   │   raise ValueError(""Reference at %r does not exist"" % ref_path)                                                            │
│   165                                                                                                                                      │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
ValueError: Reference at 'refs/heads/master' does not exist
```",ld9866,https://github.com/nf-core/sarek/issues/431
I_kwDOCvwIC848kw1I,Local Modules -> nf-core modules,CLOSED,2021-10-05T12:32:05Z,2022-02-15T13:20:35Z,2022-02-15T13:20:35Z,"- [x] BWAmem2/bwa (sync with nf-core/modules version)
- [x] DeepVariant
- [x] FreeBayes 
- [x] GATK4 ones (partially sync with the ones in nf-core/modules)
- [x] Manta
- [x] Strelka
- [x] MSISensor
- [x] QualiMap
- [x] Samtools (partially sync with the ones in nf-core/modules)
",FriederikeHanssen,https://github.com/nf-core/sarek/issues/433
I_kwDOCvwIC848kzHn,Update template branch with newest tool release,CLOSED,2021-10-05T12:41:37Z,2022-01-13T14:28:55Z,2022-01-13T14:28:55Z,,FriederikeHanssen,https://github.com/nf-core/sarek/issues/434
I_kwDOCvwIC848kzUa,Update test data in modules repo to contain cram,CLOSED,2021-10-05T12:42:27Z,2021-10-25T12:11:14Z,2021-10-25T12:10:22Z,,FriederikeHanssen,https://github.com/nf-core/sarek/issues/435
I_kwDOCvwIC848kzkF,Add as many modules to local modules as possible,CLOSED,2021-10-05T12:43:28Z,2022-01-13T14:26:20Z,2022-01-13T14:26:20Z,,FriederikeHanssen,https://github.com/nf-core/sarek/issues/436
I_kwDOCvwIC848k02T,Allow saving of BAM files in 3.0,CLOSED,2021-10-05T12:48:37Z,2022-06-21T18:43:00Z,2022-06-21T18:43:00Z,,FriederikeHanssen,https://github.com/nf-core/sarek/issues/437
I_kwDOCvwIC848rvce,[BUG] BaseRecalibrator terminated,CLOSED,2021-10-06T11:25:26Z,2023-03-21T13:15:02Z,2023-03-21T13:15:02Z,"## Description of the bug

<!-- A clear and concise description of what the bug is. -->
I run Sarek test (on a minimal dataset). It runs ok until BaseRecalibrator step, and it craches with following error: 

```
Error executing process > BaseRecalibrator (1234-1234N-2_1-200000)
Caused by:
  Process `BaseRecalibrator (1234-1234N-2_1-200000)` terminated with an error exit status (3)
```

I've just started using both Singularity and NextFlow. RNA-seq worklow test runs successfully.

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line: 

```
nextflow run -latest nf-core/sarek -profile test,singularity
```

2. See error:

```
-[nf-core/sarek] Pipeline completed with errors-
WARN: To render the execution DAG in the required format it is required to install Graphviz -- See http://www.graphviz.org for more info.
Error executing process > 'BaseRecalibrator (1234-1234N-2_1-200000)'

Caused by:
  Process `BaseRecalibrator (1234-1234N-2_1-200000)` terminated with an error exit status (3)

Command executed:

  gatk --java-options -Xmx6g         BaseRecalibrator         -I 1234N.md.bam         -O 2_1-200000_1234N.recal.table         --tmp-dir .         -R human_g1k_v37_decoy.small.fasta         -L 2_1-200000.bed         --known-sites dbsnp_138.b37.small.vcf.gz         --known-sites Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz         --verbosity INFO

Command exit status:
  3

Command output:
  (empty)

Command error:
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191)
  pression_level=2 -Xmx6g -jar /opt/conda/envs/nf-core-sarek-2.7.1/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar BaseRecalibrator -I 1234N.md.bam -O 2_1-200000_1234N.recal.table --tmp-dir . -R human_g1k_v37_decoy.small.fasta -L 2_1-200000.bed --known-sites dbsnp_138.b37.small.vcf.gz --known-sites Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz --verbosity INFO
  09:55:38.984 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/envs/nf-core-sarek-2.7.1/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
  09:55:39.005 WARN  NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory)
  09:55:39.019 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/envs/nf-core-sarek-2.7.1/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
  09:55:39.019 WARN  NativeLibraryLoader - Unable to load libgkl_compression.so from native/libgkl_compression.so (No such file or directory)
  Oct 06, 2021 9:55:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine
  INFO: Failed to detect whether we are running on Google Compute Engine.
  09:55:39.497 INFO  BaseRecalibrator - ------------------------------------------------------------
  09:55:39.498 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.7.0
  09:55:39.498 INFO  BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/
  09:55:39.498 INFO  BaseRecalibrator - Executing as apfeifer@zmnbio1 on Linux v4.4.0-159-generic amd64
  09:55:39.498 INFO  BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_282-b08
  09:55:39.499 INFO  BaseRecalibrator - Start Date/Time: October 6, 2021 9:55:38 AM UTC
  09:55:39.499 INFO  BaseRecalibrator - ------------------------------------------------------------
  09:55:39.499 INFO  BaseRecalibrator - ------------------------------------------------------------
  09:55:39.500 INFO  BaseRecalibrator - HTSJDK Version: 2.21.2
  09:55:39.500 INFO  BaseRecalibrator - Picard Version: 2.21.9
  09:55:39.500 INFO  BaseRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  09:55:39.500 INFO  BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  09:55:39.500 INFO  BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  09:55:39.500 INFO  BaseRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  09:55:39.500 INFO  BaseRecalibrator - Deflater: JdkDeflater
  09:55:39.500 INFO  BaseRecalibrator - Inflater: JdkInflater
  09:55:39.500 INFO  BaseRecalibrator - GCS max retries/reopens: 20
  09:55:39.500 INFO  BaseRecalibrator - Requester pays: disabled
  09:55:39.500 INFO  BaseRecalibrator - Initializing engine
  09:55:39.572 WARN  IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater
  09:55:39.580 WARN  IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater
  09:55:39.955 WARN  IntelDeflaterFactory - IntelInflater is not supported, using Java.util.zip.Inflater
  09:55:39.974 INFO  BaseRecalibrator - Shutting down engine
  [October 6, 2021 9:55:39 AM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 0.02 minutes.
  Runtime.totalMemory()=1181745152
  org.broadinstitute.hellbender.exceptions.GATKException: Unable to automatically instantiate codec org.broadinstitute.hellbender.utils.codecs.AnnotatedIntervalCodec
  	at org.broadinstitute.hellbender.engine.FeatureManager.getCandidateCodecsForFile(FeatureManager.java:509)
  	at org.broadinstitute.hellbender.engine.FeatureManager.getCodecForFile(FeatureManager.java:456)
  	at org.broadinstitute.hellbender.engine.FeatureDataSource.getCodecForFeatureInput(FeatureDataSource.java:354)
  	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:334)
  	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282)
  	at org.broadinstitute.hellbender.engine.FeatureManager.addToFeatureSources(FeatureManager.java:247)
  	at org.broadinstitute.hellbender.engine.FeatureManager.initializeFeatureSources(FeatureManager.java:210)
  	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:157)
  	at org.broadinstitute.hellbender.engine.ReadWalker.initializeFeatures(ReadWalker.java:68)
  	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:706)
  	at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:50)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210)
  	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163)
  	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206)
  	at org.broadinstitute.hellbender.Main.main(Main.java:292)

Work dir:
  /mnt/zmnbio1/ola/NGS kody/instalacja i testowanie NextFlow/work/02/23bdaf801da1e42f619631dbb0397c

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line
```

## Log files

Have you provided the following extra information/files:

- [ ] The command used to run the pipeline
- [ ] The `.nextflow.log` file <!-- this is a hidden file in the directory where you launched the pipeline -->

[nextflow.log](https://github.com/nf-core/sarek/files/7293047/nextflow.log)
## System

- Hardware: (single virtual machine)
- Executor: local
- OS: Ubuntu Linux
- Version: 16.04.6 LTS (Xenial Xerus)

## Nextflow Installation

- Version: 21.04.3.5560

## Container engine

- Engine: Singularity
- version: 3.7.3
- Image tag: nfcore/sarek:2.7.1


",olanio,https://github.com/nf-core/sarek/issues/438
I_kwDOCvwIC8481SN7,Tumor only sample,CLOSED,2021-10-08T01:54:23Z,2021-10-26T13:03:51Z,2021-10-26T13:03:51Z,"Hi I'am interested in analysing breast cancer datasets. Unfortunately i don't have paired normal to compare. Could anyone suggest how to go about in detecting somatic, germline and MSI in this scenario. Is it possible to run only with tumor only datasets ",karthick1087,https://github.com/nf-core/sarek/issues/439
I_kwDOCvwIC849PFSR,Can't download VEP cache from behind proxy [BUG],CLOSED,2021-10-15T11:48:51Z,2022-08-26T09:38:30Z,2022-08-26T09:38:30Z,"## Check Documentation

I have checked the following places for your error:

- [x] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [x] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)
- [x] [VEP cache docs](https://m.ensembl.org/info/docs/tools/vep/script/vep_cache.html#cache)


## Description of the bug
There is a proxy at the HPC I am at, which makes it hard to connect to external sources, for example when trying to download the cache for SnpEff and VEP, using the `./download_cache`.nf script.

I successfully got SnpEff to work by exporting some variables using the .conf file:

```
# Add variable to my local execution directory nextflow.conf (i.e., not in the SAREK root)
singularity.envWhitelist = 'SINGULARITYENV_JAVA_TOOL_OPTIONS' > nextflow.conf

# Then export the variable
export SINGULARITYENV_JAVA_TOOL_OPTIONS=""-Dhttp.proxyHost=xxxx -Dhttp.proxyPort=xxxx -Dhttps.proxyHost=xxxx -Dhttps.proxyPort=xxxx""
```

But for VEP, it appears not possible to forward the right flags:
https://github.com/bcbio/bcbio-nextgen/issues/818


## The solution
I have a change to suggest, both for me to not have to fork and modify the process of ./download_cache, and also potentially make life easier for others with proxy problems.

Why not add an option to the script for pointing to a local directory to search instead of the remote cache at ensembl. The process would then look something like this:
```
  vep_install \
    -a cf \
    -c . \
    -s ${species} \
    -y ${genome} \
    -u ${local_vep_cache_dir} \
    --CACHE_VERSION ${vep_cache_version} \
    --CONVERT \
    --NO_HTSLIB --NO_TEST --NO_BIOPERL --NO_UPDATE
```

Then running ./download_cache would look like this:
```
# Download the cache for VEP GRCh38(VEP version 104)
mkdir -p local_vep_cache_dir_tmp
wget -P local_vep_cache_dir_tmp ftp://ftp.ensembl.org/pub/release-104/variation/indexed_vep_cache/homo_sapiens_vep_104_GRCh38.tar.gz

nextflow \
  run repos/sarek/download_cache.nf \
    -with-singularity simgs/sarek.2.7.1.sif \
    --vep_cache annotation_cache/VEPeff_cache \
    --species homo_sapiens \
    --vep_cache_version 104 \
    --genome GRCh38 \
    --local_vep_cache_dir local_vep_cache_dir_tmp
```

Let me know if you think this is a good solution. In the meantime, I will prepare for a PR. 

## Nextflow Installation

- Version:20.10.0

## Container engine

- Image tag: nfcore/sarek:2.7
sha256:09da1f431aebe8b61da6b989ed2adf17edd03492408d403f87d26b543bd0a365
",pappewaio,https://github.com/nf-core/sarek/issues/441
I_kwDOCvwIC849Ub88,How to remove temp dir(work dir) in one command,CLOSED,2021-10-18T08:07:52Z,2021-10-20T11:26:19Z,2021-10-20T11:26:18Z,"Hello,
I am studying bioinformatics with your sarek pipeline and it has been helpful for me so far!
While using your sarek, I have faced a problem that needs your attention.
I want to properly deal with the disc space by removing temp dir(work dir) in one command line like below!
`
nextflow run nf-core/sarek \
-r 2.7.1 \
-profile test, docker \
**--clean true**
`
So, I tried using `hidden_params` parameter, but there were no parameters related with my concern about disc.
Can you recommend any other options? I would really appreciate your precious piece of advice.",parkjaeming,https://github.com/nf-core/sarek/issues/443
I_kwDOCvwIC849bcv8,[FEATURE] Add target intervals to HaplotypeCaller & BQSR,CLOSED,2021-10-19T18:13:20Z,2022-02-11T17:17:01Z,2022-02-11T17:17:01Z,"GATK states here that capture target files should be supplied when running Base Recalibration:

```
By definition, exome sequencing and other targeted sequencing data don’t cover the entire genome, so most analyses can be
restricted to just the capture targets (genes or exons) to save processing time and enable scatter gather parallelism. In 
addition, there are some processing steps, such as BQSR, that should be restricted to the capture targets in order to 
eliminate off-target sequencing data, which is uninformative and is a source of noise.
```
https://gatk.broadinstitute.org/hc/en-us/articles/360035889551-When-should-I-restrict-my-analysis-to-specific-intervals-

Same for HaplotypeCaller:

https://gatk.broadinstitute.org/hc/en-us/community/posts/4403947098395-What-intervals-to-use-in-HaplotypeCaller-GenotypeGVCF-for-exome-data-sequenced-with-different-capture-kits


So far we are not doing this as far as I see.",FriederikeHanssen,https://github.com/nf-core/sarek/issues/445
I_kwDOCvwIC849dQvA,string to number error in json schema[BUG],CLOSED,2021-10-20T07:39:06Z,2021-10-21T07:38:29Z,2021-10-20T11:22:56Z,"<!--
# nf-core/sarek bug report

Hi there!

Thanks for telling us about a problem with the pipeline.
Please delete this text and anything that's not relevant from the template below:
-->


## Description of the bug

I think there is a bug in the json checker that forces `vep_cache_version` to be string, but then later it has to be a number. And therefore it doesn't work.

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line: 
```
nextflow run repos/fork/sarek \
  -with-singularity simgs/sarek.2.7.1.sif \
  --step annotate \
  --tools ""snpEff,VEP"" \
  --annotation_cache \
  --snpeff_cache annotation_cache/snpeff_cache \
  --vep_cache annotation_cache/VEP_cache \
  --vep_cache_version 104 \
  --genome GRCh38 \
  --species homo_sapiens \
  --input ""test/small_vcf_example/1kgp/GRCh38_example_data.vcf.gz"" \
  --outdir ""out""
```

2. See error: 

```
ERROR: Validation of pipeline parameters failed!
* --vep_cache_version: expected type: String, found: Integer (104)
```

## Solution

Change ""string -> number"" in nextflow_schema.json, see below for correct format.
```
""vep_cache_version"": {
                    ""type"": ""number"",
                    ""fa_icon"": ""fas fa-tag"",
                    ""description"": ""VEP cache version""
                },

```

This is a simple change that makes it work, and I will prepare a PR for this.

## Nextflow Installation

- Version:20.10.0

## Container engine
- Image tag: nfcore/sarek:2.7
sha256:09da1f431aebe8b61da6b989ed2adf17edd03492408d403f87d26b543bd0a365
",pappewaio,https://github.com/nf-core/sarek/issues/446
I_kwDOCvwIC84-SPH6,[BUG],CLOSED,2021-11-04T16:36:03Z,2021-11-08T10:13:34Z,2021-11-08T10:13:34Z,"Hello,

I am new to nf-core's Sarek pipeline. Currently I am simply trying the test configurations to make sure that its processes run on our cluster. All test configurations appear to run except 'test_full':

`nextflow run nf-core/sarek -revision 2.7.1 -profile test_full,singularity`

This generates the following message:

N E X T F L O W  ~  version 21.04.3
Launching `nf-core/sarek` [mad_kirch] - revision: 68b9930a74 [2.7.1]
Unknown configuration profile: 'test_full'

Did you mean one of these?
    test_tool
",j-schoenebeck,https://github.com/nf-core/sarek/issues/450
I_kwDOCvwIC84-oILd,[BUG] step functionality,CLOSED,2021-11-11T08:37:14Z,2021-11-17T16:21:04Z,2021-11-17T16:21:04Z,"## Description
**In the dev branch**:
When using the `step` functionality, I noticed a disconnect between the allowed values in `nextflow_schema.json` and the `worfkflows/sarek.nf` code. In the nextflow_schema, values have an '_' separating `variant_calling`, and `prepare_recalibration` while in the workflow they are designated without the separator (variantcalling, and preparerecalibration). This leads to an enum error when designating steps.

## Steps to reproduce

nextflow run nf-core/sarek -profile test,conda --step variantcalling -r dev

## Suggestion
By editing my local version of `nextflow_schema.json` (can I have a 2nd one that overrides the version packaged with SAREK?) to remove the `_` things work as expected.

## Other note
The sarek code seems to allow for an automatic csv input detection when using the step functionality to restart a run for just a given step. However the input is a mandatory input, and SAREK won't run without it, rendering this part essentially useless as far as I can tell. Let me know if I've misread the step case switching block in `workflows/sarek.nf` lines 48-60


Thanks

",nickhsmith,https://github.com/nf-core/sarek/issues/451
I_kwDOCvwIC84-zrUe,Error of Java Runtime Environment,CLOSED,2021-11-15T14:32:45Z,2021-11-16T11:36:28Z,2021-11-16T11:34:51Z,"A fatal error has been detected by the Java Runtime Environment:
SIGBUS (0x7) at pc=0x00007fd2a496e74a, pid=13426, tid=0x00007fd23caf1700
JRE version: OpenJDK Runtime Environment (8.0_275-b01) (build 1.8.0_275-b01)
Java VM: OpenJDK 64-Bit Server VM (25.275-b01 mixed mode linux-amd64 compressed oops)
 Problematic frame:
C  [libc.so.6+0x15c74a]
Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again

`ulimit -c unlimited`
module load nextflow/v21.04.3
module load singularity/v3.8.1
nextflow run nf-core/sarek -profile singularity   --input sample.tsv --genome GRCh38

Server is HPC, slurm, Debian4.19

",idewdewi,https://github.com/nf-core/sarek/issues/452
I_kwDOCvwIC84-85fu,[FEATURE] gvcf option for freebayes,OPEN,2021-11-17T13:47:18Z,2024-08-19T13:12:26Z,,"Will be useful to have --gvcf option for freebayes, to combine samples that were aligned separately to the same reference. TIA",annat22,https://github.com/nf-core/sarek/issues/453
I_kwDOCvwIC84_MZCI,[BUG] concat_vcf tool packaging,CLOSED,2021-11-22T14:19:10Z,2021-11-24T14:19:19Z,2021-11-24T14:19:19Z,"## Check Documentation

I have checked the following places for your error:

- [X] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [X] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

```
GERMLINE_VARIANT_CALLING:CONCAT_GVCF 
Command executed:

  concatenateVCFs.sh -i Homo_sapiens_assembly38.fasta.fai -c 6 -o haplotypecaller_gvcf_A006850052.vcf -t Agilent_v7_ucsc.bed

Command exit status:
  127

Command output:
  Target is Agilent_v7_ucsc.bed - Selecting subset...

Command error:
  /<my_path>/.nextflow/assets/nf-core/sarek/bin/concatenateVCFs.sh: line 101: bcftools: command not found
```

## Steps to reproduce

Steps to reproduce the behaviour:

define `params.target_bed` and run variant_calling using `haplotypecaller`. The helper function `sarek/bin/concatenateVCFs.sh` uses `bcftools` but the CONCAT_GVCF module does not requrire bcftools in either Conda, Singularity, or Docker environment. 

nextflow run nf-core/sarek -r dev -c <myConfig.config> 

**myConfig.config**
params.input = 'PATH_TO_FILE.csv'
params.tools = ""haplotypecaller""
params.step=""variant_calling""
params.target_bed=""PATH_TO_FILE.bed""

## Expected behavior

ignore/eliminate variants outside of the target region.

## Potential Solution
Modify the underlying tool, or update the environment to include `bcftools` in the file `sarek/modules/local/concat_vcf/main.nf`
",nickhsmith,https://github.com/nf-core/sarek/issues/456
I_kwDOCvwIC84_bXI5,"[BUG] Root filesystem extraction failed, during BaseRecalibrator step",CLOSED,2021-11-26T06:17:47Z,2023-03-21T13:14:48Z,2023-03-21T13:14:48Z,"## Description of the bug

While running a full exome sequencing workflow, the pipeline crashes at the BaseRecalibrator step, with the following error message:

```
-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'BaseRecalibrator (3809-3809_180406_S16-chr2_89753993-90402511)'

Caused by:
  Process `BaseRecalibrator (3809-3809_180406_S16-chr2_89753993-90402511)` terminated with an error exit status (255)

Command executed:

  gatk --java-options -Xmx7g         BaseRecalibrator         -I 3809_180406_S16.md.bam         -O chr2_89753993-90402511_3809_180406_S16.recal.table         --tmp-dir .         -R Homo_sapiens_assembly38.fasta         -L chr2_89753993-90402511.bed         --known-sites dbsnp_146.hg38.vcf.gz         --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz         --verbosity INFO

Command exit status:
  255

Command output:
  (empty)

Command error:
  INFO:    Converting SIF file to temporary sandbox...
  FATAL:   while extracting /mnt/store3/local/proj/si-net2/Pipelines/exome/variants_nextflow/work/singularity/nfcore-sarek-2.7.1.img: root filesystem extraction failed: extract command failed: WARNING: passwd file doesn't exist in container, not updating
  WARNING: group file doesn't exist in container, not updating
  WARNING: Skipping mount /etc/hosts [binds]: /etc/hosts doesn't exist in container
  WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container
  WARNING: Skipping mount proc [kernel]: /proc doesn't exist in container
  WARNING: Skipping mount /home/joakim/bin/miniconda3/envs/sinet2_nextflow/var/singularity/mnt/session/tmp [tmp]: /tmp doesn't exist in container
  WARNING: Skipping mount /home/joakim/bin/miniconda3/envs/sinet2_nextflow/var/singularity/mnt/session/var/tmp [tmp]: /var/tmp doesn't exist in container
  WARNING: Skipping mount /home/joakim/bin/miniconda3/envs/sinet2_nextflow/var/singularity/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container

  Write on output file failed because No space left on device

  FATAL ERROR:writer: failed to write file /image/root/opt/conda/pkgs/freebayes-1.3.2-py27h49fb759_2/bin/bamleftalign
  Parallel unsquashfs: Using 56 processors
  180749 inodes (130153 blocks) to write

  : exit status 1

Work dir:
  /mnt/store3/local/proj/si-net2/Pipelines/exome/variants_nextflow/work/c0/ac0184acf57bedd973c353303fec54

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`
```
The command used was:

`nextflow run nf-core/sarek --input samples.tsv -profile singularity   -resume  --target_bed /home/joakim/proj/project_1/Pipelines/exome/preprocessing/data/Agilent_SureSelect_Clinical_Research_Exome_V2_S30409818_hs_hg38/S30409818_Padded.reformatted.bed     --fasta /home/joakim/store1/pinbot/data1/reference/GATK_resource_bundle/hg38/hg38/Homo_sapiens_assembly38.fasta     --fasta_fai /home/joakim/store1/pinbot/data1/reference/GATK_resource_bundle/hg38/hg38/Homo_sapiens_assembly38.fasta.fai     --dbsnp /home/joakim/store1/pinbot/data1/reference/GATK_resource_bundle/hg38/hg38/dbsnp_146.hg38.vcf.gz     --dbsnp_index /home/joakim/store1/pinbot/data1/reference/GATK_resource_bundle/hg38/hg38/dbsnp_146.hg38.vcf.gz.tbi     --dict /home/joakim/store1/pinbot/data1/reference/GATK_resource_bundle/hg38/hg38/Homo_sapiens_assembly38.dict     --germline_resource /home/joakim/store1/pinbot/data1/reference/GATK_resource_bundle/hg38/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz     --germline_resource_index /home/joakim/store1/pinbot/data1/reference/GATK_resource_bundle/hg38/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi     --known_indels /home/joakim/store1/pinbot/data1/reference/GATK_resource_bundle/hg38/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz     --known_indels_index /home/joakim/store1/pinbot/data1/reference/GATK_resource_bundle/hg38/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi     --mappability /home/joakim/proj/si-net2/Pipelines/exome/variants_nextflow/out100m2_hg38.gem     --vep_cache_version GRCh38.104     --snpeff_db GRCh38.92     --cpus 51`

The errors seems to suggest that space is running out somewhere, and it appears that it is the /tmp space that is running out of storage. All main data volumes have plenty of storage left, as shown below. Output of ""df -h"":

```
devtmpfs                   189G      0   189G   0% /dev
tmpfs                      189G    68K   189G   1% /dev/shm
tmpfs                      189G   2,3M   189G   1% /run
tmpfs                      189G      0   189G   0% /sys/fs/cgroup
/dev/mapper/fedora-root    100G    17G    84G  17% /
tmpfs                      189G   108G    81G  58% /tmp
/dev/sdh2                  976M   203M   707M  23% /boot
/dev/sdh1                  200M   7,8M   193M   4% /boot/efi
/dev/sdd                    11T   9,2T   955G  91% /mnt/store4
/dev/sdb                    11T   9,1T   1,1T  90% /mnt/store2
/dev/sdf                    11T   8,1T   2,1T  80% /mnt/store6
/dev/sdg                    11T   9,8T   303G  98% /mnt/store7
/dev/sdc                    11T   7,6T   2,6T  76% /mnt/store3
/dev/sda2                  5,4T   5,0T   133G  98% /mnt/store1
/dev/sda1                  5,4T   4,7T   440G  92% /home
/dev/sde                    11T   7,1T   3,1T  71% /mnt/store5
tmpfs                       38G      0    38G   0% /run/user/1000
```
I'm not entirely sure how got get around this issue. I've tried a number of things, such as setting different environmental variables in an attempt to change the temporary directory that the pipeline tries to extract the images to. For instance, as follows:

```
TMPDIR=`pwd`/tmp
mkdir -p $TMPDIR
export NXF_SINGULARITY_CACHEDIR=$TMPDIR
export NXF_TEMP=$TMPDIR
export SINGULARITY_CACHEDIR=$TMPDIR
export SINGULARITY_TMPDIR=$TMPDIR
```

This likewise resulted in the following error (running the same command):

```
-[nf-core/sarek] Pipeline completed with errors-
Waiting files transfer to complete (1 files)
Error executing process > 'BaseRecalibrator (3791-3791_REPTILS_DNA_S2-chr4_49486925-49658100)'

Caused by:
  Process `BaseRecalibrator (3791-3791_REPTILS_DNA_S2-chr4_49486925-49658100)` terminated with an error exit status (255)

Command executed:

  gatk --java-options -Xmx7g         BaseRecalibrator         -I 3791_REPTILS_DNA_S2.md.bam         -O chr4_49486925-49658100_3791_REPTILS_DNA_S2.recal.table         --tmp-dir .         -R Homo_sapiens_assembly38.fasta         -L chr4_49486925-49658100.bed         --known-sites dbsnp_146.hg38.vcf.gz         --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz         --verbosity INFO

Command exit status:
  255

Command output:
  (empty)

Command error:
  INFO:    Converting SIF file to temporary sandbox...
  FATAL:   while extracting /home/joakim/proj/si-net2/Pipelines/exome/variants_nextflow/tmp/nfcore-sarek-2.7.1.img: root filesystem extraction failed: extract command failed: WARNING: passwd file doesn't exist in container, not updating
  WARNING: group file doesn't exist in container, not updating
  WARNING: Skipping mount /etc/hosts [binds]: /etc/hosts doesn't exist in container
  WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container
  WARNING: Skipping mount proc [kernel]: /proc doesn't exist in container
  WARNING: Skipping mount /home/joakim/bin/miniconda3/envs/sinet2_nextflow/var/singularity/mnt/session/tmp [tmp]: /tmp doesn't exist in container
  WARNING: Skipping mount /home/joakim/bin/miniconda3/envs/sinet2_nextflow/var/singularity/mnt/session/var/tmp [tmp]: /var/tmp doesn't exist in container
  WARNING: Skipping mount /home/joakim/bin/miniconda3/envs/sinet2_nextflow/var/singularity/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container

  Write on output file failed because No space left on device

  FATAL ERROR:writer: failed to write file /image/root/opt/conda/envs/nf-core-sarek-2.7.1/lib/python2.7/site-packages/Bio/SeqIO/__init__.py
  Parallel unsquashfs: Using 56 processors
  180749 inodes (130153 blocks) to write

  : exit status 1

Work dir:
  /mnt/store3/local/proj/si-net2/Pipelines/exome/variants_nextflow/work/5b/3973280939e7b369bdb643afd5829c

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`
```

Output of ""df -h"" for the second run:

```
devtmpfs                   189G      0   189G   0% /dev
tmpfs                      189G    68K   189G   1% /dev/shm
tmpfs                      189G   2,3M   189G   1% /run
tmpfs                      189G      0   189G   0% /sys/fs/cgroup
/dev/mapper/fedora-root    100G    17G    84G  17% /
tmpfs                      189G   125G    64G  67% /tmp
/dev/sdh2                  976M   203M   707M  23% /boot
/dev/sdh1                  200M   7,8M   193M   4% /boot/efi
/dev/sdd                    11T   9,2T   955G  91% /mnt/store4
/dev/sdb                    11T   9,1T   1,1T  90% /mnt/store2
/dev/sdf                    11T   8,1T   2,1T  80% /mnt/store6
/dev/sdg                    11T   9,8T   303G  98% /mnt/store7
/dev/sdc                    11T   7,7T   2,5T  77% /mnt/store3
/dev/sda2                  5,4T   5,0T   133G  98% /mnt/store1
/dev/sda1                  5,4T   4,7T   440G  92% /home
/dev/sde                    11T   7,1T   3,1T  71% /mnt/store5
tmpfs                       38G      0    38G   0% /run/user/1000
```

## Log files

Log files for the two runs that were described above:

[nextflow.log.first_run.txt](https://github.com/nf-core/sarek/files/7606674/nextflow.log.first_run.txt)
[nextflow.log.second_run.txt](https://github.com/nf-core/sarek/files/7606675/nextflow.log.second_run.txt)

## System

- Hardware: Local server, 28 core / 56 threads Intel CPU, 376GB RAM -->
- Executor: local
- OS: Fedora
- Version 30, kernel 5.6.13-100.fc30.x86_64

## Nextflow Installation

- Version: 21.10.3.5655

## Container engine

- Engine: Singularity
- version: 3.8.4
- Image tag: nfcore/sarek:2.7.1
",jowkar,https://github.com/nf-core/sarek/issues/458
I_kwDOCvwIC84_m1xP,snpEff GRCm38 singularity image error,CLOSED,2021-11-30T11:54:48Z,2021-11-30T12:19:07Z,2021-11-30T12:19:07Z,"Hello sarek maintainers,

I'm trying to run snpEff (sarek v2.7.1, Nextflow v21.04.3) on mouse WGS data aligned to the GRCm38 reference. I saw in another issue reported here that snpEff this reference genome should be supported, but I got the following error - what's the best way to solve this?

Thanks very much!

- Mike



---

Workflow execution completed unsuccessfully!

The exit status of the task that caused the workflow execution to fail was: null.

The full error message was:

Error executing process > 'Snpeff (BKO_tumour - Strelka - Strelka_BKO_tumour_variants.vcf.gz)'

Caused by:
  Failed to pull singularity image
  command: singularity pull  --name nfcore-sareksnpeff-2.7.1.GRCm38.img.pulling.1638179895467 docker://nfcore/sareksnpeff:2.7.1.GRCm38 > /dev/null
  status : 143
  message:
    [34mINFO:   [0m Converting OCI blobs to SIF format
    [34mINFO:   [0m Starting build...
    Getting image source signatures
    Copying blob sha256:68ced04f60ab5c7a5f1d0b0b4e7572c5a4c8cce44866513d30d9df1a15277d6b
    Copying blob sha256:9c388eb6d33c40662539172f0d9a357287bd1cd171692ca5c08db2886bc527c3
    Copying blob sha256:96cf53b3a9dd496f4c91ab86eeadca2c8a31210c2e5c2a82badbb0dcf8c8f76b
    Copying blob sha256:3fbba84366ddeb8b6dece9fb1c547db6afa6e727c4bfe599a5ca2cfdff4c964e
    Copying blob sha256:dab4755957b222abf14671238b44d656dda9912c5cfce0181b0f0f6752e6cc7f
    Copying blob sha256:65f4224a243e8f0db3777f98e62dd2a01d18fb0f2db90188b0c7a04fd49967ec
    Copying blob sha256:3a96f9b571ed978d10eff21993389ffa94fc0931fa91654573e08f26a45921b5
    Copying blob sha256:b427d20953ff85692680e09f7cd56bb2c485243e615146f7700b0446a672b6b9
    Copying config sha256:124da0780772bc69760ba31827ce13ca0fee9bd6b6cd2f945871c613e372aa0f
    Writing manifest to image destination
    Storing signatures
    2021/11/29 11:00:04  info unpack layer: sha256:68ced04f60ab5c7a5f1d0b0b4e7572c5a4c8cce44866513d30d9df1a15277d6b
    2021/11/29 11:00:08  warn xattr{etc/gshadow} ignoring ENOTSUP on setxattr ""user.rootlesscontainers""
    2021/11/29 11:00:08  warn xattr{/omics/groups/OE0014/internal/BCAT1_TME_Pavle/analysis/nf-sarek/work/rootfs-e0c384a7-50fa-11ec-a8db-e41f13814edf/etc/gshadow} destination filesystem does not support xattrs, further warnings will be suppressed
    2021/11/29 11:01:52  info unpack layer: sha256:9c388eb6d33c40662539172f0d9a357287bd1cd171692ca5c08db2886bc527c3
    2021/11/29 11:01:54  warn xattr{etc/gshadow} ignoring ENOTSUP on setxattr ""user.rootlesscontainers""
    2021/11/29 11:01:54  warn xattr{/omics/groups/OE0014/internal/BCAT1_TME_Pavle/analysis/nf-sarek/work/rootfs-e0c384a7-50fa-11ec-a8db-e41f13814edf/etc/gshadow} destination filesystem does not support xattrs, further warnings will be suppressed
    2021/11/29 11:06:16  info unpack layer: sha256:96cf53b3a9dd496f4c91ab86eeadca2c8a31210c2e5c2a82badbb0dcf8c8f76b
    2021/11/29 11:09:18  info unpack layer: sha256:3fbba84366ddeb8b6dece9fb1c547db6afa6e727c4bfe599a5ca2cfdff4c964e
    2021/11/29 11:09:20  warn xattr{var/cache/apt/archives/partial} ignoring ENOTSUP on setxattr ""user.rootlesscontainers""
    2021/11/29 11:09:20  warn xattr{/omics/groups/OE0014/internal/BCAT1_TME_Pavle/analysis/nf-sarek/work/rootfs-e0c384a7-50fa-11ec-a8db-e41f13814edf/var/cache/apt/archives/partial} destination filesystem does not support xattrs, further warnings will be suppressed
    2021/11/29 11:09:21  info unpack layer: sha256:dab4755957b222abf14671238b44d656dda9912c5cfce0181b0f0f6752e6cc7f
    2021/11/29 11:09:21  info unpack layer: sha256:65f4224a243e8f0db3777f98e62dd2a01d18fb0f2db90188b0c7a04fd49967ec
    2021/11/29 11:16:50  info unpack layer: sha256:3a96f9b571ed978d10eff21993389ffa94fc0931fa91654573e08f26a45921b5
    2021/11/29 11:17:08  info unpack layer: sha256:b427d20953ff85692680e09f7cd56bb2c485243e615146f7700b0446a672b6b9
    [34mINFO:   [0m Creating SIF file...

",mncfletcher,https://github.com/nf-core/sarek/issues/461
I_kwDOCvwIC85ANE_H,Add basic error checking for input csv samplesheet,CLOSED,2021-12-10T19:38:08Z,2022-07-13T07:13:18Z,2022-06-23T14:36:17Z,"EDIT by @maxulysse to add some checkbox to track the progress:
we should have proper check to see if data is correct for step:
- [x] `nextflow run . -profile test,docker --step mapping`
  - [x] other input data should fail
  - [x] proper warning
- [x] `nextflow run . -profile test,docker --step markduplicates`
  - [x] fails
  - [x] proper warning
- `nextflow run . -profile test,docker --step prepare_recalibration`
  - [x] fails
  - [x] proper warning
- `nextflow run . -profile test,docker --step recalibrate`
  - [x] fails
  - [x] proper warning
- `nextflow run . -profile test,docker --step variant_calling`
  - [x] fails
  - [x] proper warning
- `nextflow run . -profile test,docker --step annotate`
  - [x] fails
  - [x] proper warning

Some additional ideas from slack (edited by @FriederikeHanssen )

- [x] If `--step variant_calling` or `--step_annotation` , then `--tools` should not be empty
- [x] #603

## Is your feature request related to a problem? Please describe

It's frustrating to have the pipeline run through completely, only to tell me at the end that there are file name clashes based on me (as a new user of the pipeline) having set up my TSV file incorrectly.

## Describe the solution you'd like

To help with this, it would be useful for the pipeline to do some basic error checking on the TSV file before starting any other steps; this means checking things such as: are values in columns where each row needs to be unique actually unique?  Do the columns provided (at least) seem to match what should go into them based on the start point given (for example, does a file name look like a fastq / bam / bai file?  Does the file actually exist?)?  It would also be helpful if the pipeline supported a header row in the TSV file, as I suspect part of the reason I messed up was due to not having a header row to refer to.  

## Describe alternatives you've considered

Not sure if there would be any good alternatives to this; it's mostly a convenience feature to get the pipeline to throw an error before it runs all night and then needs to be restarted from scratch in the morning.  
",bkohrn,https://github.com/nf-core/sarek/issues/469
I_kwDOCvwIC85ARjyh,[BUG] Tools using Log4j,CLOSED,2021-12-13T10:22:43Z,2022-02-11T17:16:42Z,2022-02-11T17:16:42Z,"- [ ] GAWK
- [ ] BCFTools (no, c)
- [ ] Deepvariant (python/c++, not)
- [x] GATK4  ( https://github.com/broadinstitute/gatk/search?q=log4j, issue already here: https://github.com/broadinstitute/gatk/pull/)
- [ ] MSISensor (not, c++)
- [x] Qualimap (maybe, maybe not)
- [ ] Samtools ( not, c++)
- [ ] BWA/BWAMEM2 ( not, c+=)
- [ ] Ensembl VEP ( not, perl)
- [x] FastQC ( not, maybe upstream dependency on picard?)
- [ ] FreeBayes ( not, c++)
- [ ] Manta ( not, c++)
- [ ] MultiQC ( not , python)
- [ ] Seqkit ( not, go)
- [ ] Strelka ( not, c++)
- [ ] Tabix ( not, c)
- [ ] TrimGalore (not, perl)

Looked a bit into the repositories of the tools we have on `dev` currently. I think the biggest problem is gatk4, but please correct me if I am wrong, certainly no expert here. Maybe FastQC and Qualimap as well
",FriederikeHanssen,https://github.com/nf-core/sarek/issues/470
I_kwDOCvwIC85AR528,[BUG] modules.config: publishDirmode: 'copy',CLOSED,2021-12-13T11:51:47Z,2022-02-24T15:00:12Z,2022-02-24T15:00:12Z,"When changing the publishDir mode, `copy` needs to be stated explicitly everywhere.

see: https://nfcore.slack.com/archives/CGFUX04HZ/p1639391495442600?thread_ts=1639041878.432300&cid=CGFUX04HZ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/471
I_kwDOCvwIC85C4ym9,[BUG] htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Invalid file pointer: 101939186246823 for gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz,CLOSED,2022-02-02T17:11:01Z,2023-09-25T18:58:29Z,2022-02-03T17:49:41Z,"Hello,
first of all, thanks for the great pipeline.
I'm trying to process two samples, one normal and one tumour, using the following command:
```
nextflow run nf-core/sarek -r 2.7.1 --input ./samples.tsv -profile singularity --tools Mutect2,Strelka,Manta,TIDDIT,ASCAT,ControlFREEC,snp
Eff,VEP --genome GRCh38  
```
the samples.tsv (names simplified) is like:
```
TestSample	XY	0	TestSample_0	1	/data/exome/fastq/testA_1.fastq.gz	/data/exome/fastq/testA_2.fastq.gz
TestSample	XY	1	TestSample_1	1	/data/exome/fastq/testB_1.fastq.gz	/data/exome/fastq/testB_2.fastq.gz
```
From the reports, it looks like both MantaSingle and Mutect2Single are failing, but I get only a Mutec2Single error log:
```
Error executing process > 'Mutect2Single (WCMC4_1_C_1-chr6_61371373-95020790)'

Caused by:
  Process `Mutect2Single (WCMC4_1_C_1-chr6_61371373-95020790)` terminated with an error exit status (3)

Command executed:

  # Get raw calls
  gatk --java-options ""-Xmx7g""       Mutect2       -R Homo_sapiens_assembly38.fasta      -I WCMC4_1_C_1.recal.bam  -tumor WCMC4_1_C_1       -L chr6_61371373-95020790.bed              --germline-resource gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz              -O chr6_61371373-95020790_WCMC4_1_C_1.vcf

Command exit status:
  3

Command output:
  (empty)

Command error:
  15:04:28.146 INFO  Mutect2 - Inflater: IntelInflater
  15:04:28.146 INFO  Mutect2 - GCS max retries/reopens: 20
  15:04:28.146 INFO  Mutect2 - Requester pays: disabled
  15:04:28.146 INFO  Mutect2 - Initializing engine
  15:04:28.731 INFO  FeatureManager - Using codec VCFCodec to read file file://gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz
  15:04:28.890 INFO  FeatureManager - Using codec BEDCodec to read file file://chr6_61371373-95020790.bed
  15:04:28.896 INFO  IntervalArgumentCollection - Processing 33649418 bp from intervals
  15:04:28.951 INFO  Mutect2 - Done initializing engine
  15:04:29.045 INFO  NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/opt/conda/envs/nf-core-sarek-2.7.1/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_utils.so
  15:04:29.056 INFO  NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/opt/conda/envs/nf-core-sarek-2.7.1/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so
  15:04:29.095 INFO  IntelPairHmm - Using CPU-supported AVX-512 instructions
  15:04:29.095 INFO  IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM
  15:04:29.096 INFO  IntelPairHmm - Available threads: 80
  15:04:29.096 INFO  IntelPairHmm - Requested threads: 4
  15:04:29.096 INFO  PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation
  15:04:29.163 INFO  ProgressMeter - Starting traversal
  15:04:29.163 INFO  ProgressMeter -        Current Locus  Elapsed Minutes     Regions Processed   Regions/Minute
  15:04:29.575 INFO  VectorLoglessPairHMM - Time spent in setup for JNI call : 0.0
  15:04:29.575 INFO  PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 0.0
  15:04:29.575 INFO  SmithWatermanAligner - Total compute time in java Smith-Waterman : 0.00 sec
  15:04:29.576 INFO  Mutect2 - Shutting down engine
  [February 2, 2022 3:04:29 PM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.03 minutes.
  Runtime.totalMemory()=2185232384
  htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Invalid file pointer: 101939186246823 for gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz
  	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:48)
  	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.readNextRecord(TabixFeatureReader.java:170)
  	at htsjdk.tribble.TabixFeatureReader$FeatureIterator.(TabixFeatureReader.java:159)
  	at htsjdk.tribble.TabixFeatureReader.query(TabixFeatureReader.java:133)
  	at org.broadinstitute.hellbender.engine.FeatureDataSource.refillQueryCache(FeatureDataSource.java:567)
  	at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:536)
  	at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:353)
  	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:173)
  	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.isActive(Mutect2Engine.java:389)
  	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:136)
  	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112)
  	at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35)
  	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192)
  	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173)
  	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210)
  	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163)
  	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206)
  	at org.broadinstitute.hellbender.Main.main(Main.java:292)
  Caused by: java.io.IOException: Invalid file pointer: 101939186246823 for gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz
  	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:385)
  	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:427)
  	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46)
  	... 20 more

```

Any help is appreciated.

singularity version: 3.8.5-2.el8
Workflow repository: https://github.com/nf-core/sarek, revision 2.7.1 (commit hash 68b9930a74962f3c42eee71f51e6dd2646269199)
Workflow profile: singularity
Workflow container:  nfcore/sarek:2.7.1
Container engine: singularity
Nextflow version:  version 21.10.6, build 5660 (21-12-2021 16:55 UTC)",CloXD,https://github.com/nf-core/sarek/issues/480
I_kwDOCvwIC85ELovf,Get Software Versions crashes -- unknown error,CLOSED,2022-02-18T22:57:11Z,2022-02-28T15:57:53Z,2022-02-28T15:57:53Z,"> /pine/scr/k/w/kwamek/AlexRotation/nextflow -bg run nf-core/sarek --input Illumina_whole-genome_seq_data.tsv -profile singularity --max_cpus 10


Error executing process > 'get_software_versions'
Caused by:
 Process requirement exceed available CPUs -- req: 8; avail: 1
Command executed:
 alleleCounter --version &> v_allelecount.txt 2>&1 || true
 bcftools --version &> v_bcftools.txt 2>&1 || true
 bwa version &> v_bwa.txt 2>&1 || true
 cnvkit.py version &> v_cnvkit.txt 2>&1 || true
 configManta.py --version &> v_manta.txt 2>&1 || true
 configureStrelkaGermlineWorkflow.py --version &> v_strelka.txt 2>&1 || true
 echo ""2.7.1"" &> v_pipeline.txt 2>&1 || true
 echo ""21.10.6"" &> v_nextflow.txt 2>&1 || true
 snpEff -version &> v_snpeff.txt 2>&1 || true
 fastqc --version &> v_fastqc.txt 2>&1 || true
 freebayes --version &> v_freebayes.txt 2>&1 || true
 freec &> v_controlfreec.txt 2>&1 || true
 gatk ApplyBQSR --help &> v_gatk.txt 2>&1 || true
 msisensor &> v_msisensor.txt 2>&1 || true
 multiqc --version &> v_multiqc.txt 2>&1 || true
 qualimap --version &> v_qualimap.txt 2>&1 || true
 R --version &> v_r.txt 2>&1 || true
 R -e ""library(ASCAT); help(package='ASCAT')"" &> v_ascat.txt 2>&1 || true
 samtools --version &> v_samtools.txt 2>&1 || true
 tiddit &> v_tiddit.txt 2>&1 || true
 trim_galore -v &> v_trim_galore.txt 2>&1 || true
 vcftools --version &> v_vcftools.txt 2>&1 || true
 vep --help &> v_vep.txt 2>&1 || true
 scrape_software_versions.py &> software_versions_mqc.yaml
Command exit status:
 -
Command output:
 (empty)
Work dir:
 /pine/scr/k/w/kwamek/AlexRotation/work/f1/9d44ad544ccdac7d463c51f7456441
Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`
Execution cancelled -- Finishing pending tasks before exit
",KwameForbes,https://github.com/nf-core/sarek/issues/486
I_kwDOCvwIC85EgmJC,[BUG] restart without input,CLOSED,2022-02-24T15:01:54Z,2022-06-09T16:12:27Z,2022-06-09T15:04:53Z,"As we're checking for existence of params.input file(s), automatic restart doesn't work.
Need to fix TEMPLATE or find another solution",maxulysse,https://github.com/nf-core/sarek/issues/490
I_kwDOCvwIC85EmIMh,"[BUG] ""pipeline completed with errors"" - but no error given",CLOSED,2022-02-25T20:25:31Z,2022-02-26T11:25:08Z,2022-02-26T11:25:07Z,"Hi,

I am running sarek for the first time, and it is failing quite early. However, I'm not sure what the error is:-

```
WARN: Found unexpected parameters:
* --step-mapping: true
- Ignore this warning: params.schema_ignore_params = ""step-mapping"" 

For input string: """"status""""

 -- Check script '/home/md1mjdx/.nextflow/assets/nf-core/sarek/main.nf' at line: 4183 or see '.nextflow.log' file for more details
Core Nextflow options
  revision              : master
  runName               : confident_euler
  containerEngine       : singularity
  container             : nfcore/sarek:2.7.1
  launchDir             : /mnt/fastdata/md1mjdx/LM
  workDir               : /mnt/fastdata/md1mjdx/LM/work
  projectDir            : /home/md1mjdx/.nextflow/assets/nf-core/sarek
  userName              : md1mjdx
  profile               : singularity
  configFiles           : /home/md1mjdx/.nextflow/assets/nf-core/sarek/nextflow.config, /mnt/fastdata/md1mjdx/Luke_Mansfield/nextflow.config

Input/output options
  input                 : input_7884.tsv
  outdir                : results_7884

Main options
  tools                 : null
  skip_qc               : null

Trim/split FASTQ
  clip_r1               : 0
  clip_r2               : 0
  three_prime_clip_r1   : 0
  three_prime_clip_r2   : 0
  trim_nextseq          : 0

Preprocessing
  markdup_java_options  : ""-Xms4000m -Xmx7g""

Variant Calling
  ascat_ploidy          : null
  ascat_purity          : null
  cf_contamination      : null
  cf_ploidy             : 2
  read_structure1       : null
  read_structure2       : null

Annotation
  annotate_tools        : null
  cadd_indels           : false
  cadd_indels_tbi       : false
  cadd_wg_snvs          : false
  cadd_wg_snvs_tbi      : false
  snpeff_cache          : null
  vep_cache             : null

Reference genome options
  genome                : GRCz10
  bwa                   : s3://ngi-igenomes/igenomes//Danio_rerio/Ensembl/GRCz10/Sequence/BWAIndex/genome.fa.{amb,ann,bwt,pac,sa}
  fasta                 : s3://ngi-igenomes/igenomes//Danio_rerio/Ensembl/GRCz10/Sequence/WholeGenomeFasta/genome.fa
  igenomes_base         : s3://ngi-igenomes/igenomes/
  genomes_base          : null

Generic options
  max_multiqc_email_size: 25 MB
  sequencing_center     : null

Max job request options
  single_cpu_mem        : 7 GB
  max_cpus              : 4
  max_memory            : 64.GB
  max_time              : 10d

------------------------------------------------------
 Only displaying parameters that differ from defaults.
------------------------------------------------------
Pipeline Release  : master
Run Name          : confident_euler
Max Resources     : 64.GB memory, 4 cpus, 10d time per job
Container         : singularity - nfcore/sarek:2.7.1
Output dir        : results_7884
Launch dir        : /mnt/fastdata/md1mjdx/LM
Working dir       : /mnt/fastdata/md1mjdx/LM/work
Script dir        : /home/md1mjdx/.nextflow/assets/nf-core/sarek
User              : md1mjdx
Input             : input_7884.tsv
Step              : mapping
Genome            : GRCz10
Nucleotides/s     : 1000
MarkDuplicates    : Options
Java options      : ""-Xms4000m -Xmx7g""
GATK Spark        : No
Save BAMs mapped  : No
Skip MarkDuplicates: No
AWS iGenomes base : s3://ngi-igenomes/igenomes/
Save Reference    : No
BWA indexes       : s3://ngi-igenomes/igenomes//Danio_rerio/Ensembl/GRCz10/Sequence/BWAIndex/genome.fa.{amb,ann,bwt,pac,sa}
fasta reference   : s3://ngi-igenomes/igenomes//Danio_rerio/Ensembl/GRCz10/Sequence/WholeGenomeFasta/genome.fa
Publish dir mode  : copy
Config Profile    : singularity
Config Files      : /home/md1mjdx/.nextflow/assets/nf-core/sarek/nextflow.config, /mnt/fastdata/md1mjdx/LM/nextflow.config
----------------------------------------------------
WARN: Singularity cache directory has not been defined -- Remote image will be stored in the path: /mnt/fastdata/md1mjdx/LM/work/singularity -- Use env variable NXF_SINGULARITY_CACHEDIR to specify a different location
Pulling Singularity image docker://nfcore/sarek:2.7.1 [cache /mnt/fastdata/md1mjdx/LM/work/singularity/nfcore-sarek-2.7.1.img]
-[nf-core/sarek] Pipeline completed with errors-
WARN: To render the execution DAG in the required format it is required to install Graphviz -- See http://www.graphviz.org for more info.
```


I looked at `.nextflow.log` and see the following:-

```
Feb-25 19:40:04.140 [main] DEBUG nextflow.Session - Session await > all process finished
Feb-25 19:40:04.140 [main] DEBUG nextflow.Session - Session await > all barriers passed
Feb-25 19:40:05.386 [Actor Thread 12] DEBUG nextflow.sort.BigSort - Sort completed -- entries: 1; slices: 1; internal sort time: 0.001 s; external sort time: 0.251 s; total time: 0.252 s
Feb-25 19:40:05.389 [Actor Thread 12] DEBUG nextflow.file.FileCollector - >> temp file exists? false
Feb-25 19:40:05.389 [Actor Thread 12] DEBUG nextflow.file.FileCollector - Missed collect-file cache -- cause: java.nio.file.NoSuchFileException: /scratch/8740858.1.all.q/7da7133bd4cee5a77cf7581d5b124130.collect-file
Feb-25 19:40:05.433 [Actor Thread 12] DEBUG nextflow.file.FileCollector - Saved collect-files list to: /scratch/8740858.1.all.q/7da7133bd4cee5a77cf7581d5b124130.collect-file
Feb-25 19:40:05.496 [Actor Thread 12] DEBUG nextflow.file.FileCollector - Deleting file collector temp dir: /scratch/8740858.1.all.q/nxf-3812309998024783298
Feb-25 19:40:05.780 [main] INFO  nextflow.Nextflow - -[nf-core/sarek] Pipeline completed with errors-
Feb-25 19:40:05.798 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=0; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=0ms; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=0; peakCpus=0; peakMemory=0; ]
Feb-25 19:40:05.803 [main] DEBUG nextflow.trace.TraceFileObserver - Flow completing -- flushing trace file
Feb-25 19:40:05.821 [main] DEBUG nextflow.trace.ReportObserver - Flow completing -- rendering html report
Feb-25 19:40:05.822 [main] DEBUG nextflow.trace.ReportObserver - Execution report summary data:
  []
Feb-25 19:40:08.252 [main] DEBUG nextflow.trace.TimelineObserver - Flow completing -- rendering html timeline
Feb-25 19:40:08.620 [main] WARN  nextflow.dag.GraphvizRenderer - To render the execution DAG in the required format it is required to install Graphviz -- See http://www.graphviz.org for more info.
Feb-25 19:40:08.622 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Feb-25 19:40:08.622 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Feb-25 19:40:08.622 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Feb-25 19:40:08.622 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Feb-25 19:40:08.622 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Feb-25 19:40:08.622 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Feb-25 19:40:08.622 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Feb-25 19:40:08.622 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Feb-25 19:40:08.626 [main] DEBUG nextflow.CacheDB - Closing CacheDB done
Feb-25 19:40:08.629 [main] DEBUG nextflow.util.SpuriousDeps - AWS S3 uploader shutdown
Feb-25 19:40:08.807 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye

```

Is it something to do with a temp directory? Do I need to create this?

This is the command I used to run.

```
export SINGULARITY_CACHEDIR=${PWD}/singularity-cache
export SINGULARITY_TMPDIR=${PWD}/singularity-tmp
module load apps/java

/shared/bioinformatics_core1/Shared/software/nextflow/v20.11.0-edge/nextflow run nf-core/sarek \
							-resume \
							--input input_8574.tsv \
							--step-mapping \
							--genome GRCz10	 \
							--outdir results_8574 \
							-profile singularity \
							--max_memory 64.GB \
							--max_cpus 4
```

Thanks in advance for any help",markdunning,https://github.com/nf-core/sarek/issues/491
I_kwDOCvwIC85E4Kgw,[FEATURE] vulnerabilities in docker image,CLOSED,2022-03-01T16:53:52Z,2022-03-02T12:39:26Z,2022-03-02T12:39:26Z,"
## Description of the bug

Hi, a scan on the sarek docker image reveals several critical vulnerabilities. Although it is not a problem in term of scientific results, it may prevent the researcher to be allowed to use it on sensible platforms.

A few updates in the dependenxies should be able to solve it
 

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line:

`docker scan  nfcore/sarek:2.7.1`

3. See error:

``` 
✗ High severity vulnerability found in subversion/libsvn1
  Description: NULL Pointer Dereference
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-SUBVERSION-1071814
  Introduced through: subversion@1.10.4-1+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1
  From: subversion@1.10.4-1+deb10u1
  Fixed in: 1.10.4-1+deb10u2

✗ High severity vulnerability found in p11-kit/libp11-kit0
  Description: Out-of-bounds Write
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-P11KIT-1050833
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > gnutls28/libgnutls30@3.6.7-4+deb10u5 > p11-kit/libp11-kit0@0.23.15-2
  Fixed in: 0.23.15-2+deb10u1

✗ High severity vulnerability found in p11-kit/libp11-kit0
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-P11KIT-1050836
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > gnutls28/libgnutls30@3.6.7-4+deb10u5 > p11-kit/libp11-kit0@0.23.15-2
  Fixed in: 0.23.15-2+deb10u1

✗ High severity vulnerability found in openssl/libssl1.1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENSSL-1075326
  Introduced through: cyrus-sasl2/libsasl2-modules@2.1.27+dfsg-1+deb10u1, openssh/openssh-client@1:7.9p1-10+deb10u2, ca-certificates@20200601~deb10u1, subversion@1.10.4-1+deb10u1, git@1:2.20.1-2+deb10u3, mercurial@4.8.2-1+deb10u1
  From: cyrus-sasl2/libsasl2-modules@2.1.27+dfsg-1+deb10u1 > openssl/libssl1.1@1.1.1d-0+deb10u3
  From: openssh/openssh-client@1:7.9p1-10+deb10u2 > openssl/libssl1.1@1.1.1d-0+deb10u3
  From: ca-certificates@20200601~deb10u1 > openssl@1.1.1d-0+deb10u3 > openssl/libssl1.1@1.1.1d-0+deb10u3
  and 5 more...
  Fixed in: 1.1.1d-0+deb10u5

✗ High severity vulnerability found in openssl/libssl1.1
  Description: Out-of-bounds Read
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENSSL-1569406
  Introduced through: cyrus-sasl2/libsasl2-modules@2.1.27+dfsg-1+deb10u1, openssh/openssh-client@1:7.9p1-10+deb10u2, ca-certificates@20200601~deb10u1, subversion@1.10.4-1+deb10u1, git@1:2.20.1-2+deb10u3, mercurial@4.8.2-1+deb10u1
  From: cyrus-sasl2/libsasl2-modules@2.1.27+dfsg-1+deb10u1 > openssl/libssl1.1@1.1.1d-0+deb10u3
  From: openssh/openssh-client@1:7.9p1-10+deb10u2 > openssl/libssl1.1@1.1.1d-0+deb10u3
  From: ca-certificates@20200601~deb10u1 > openssl@1.1.1d-0+deb10u3 > openssl/libssl1.1@1.1.1d-0+deb10u3
  and 5 more...
  Fixed in: 1.1.1d-0+deb10u7

✗ High severity vulnerability found in openldap/libldap-common
  Description: Reachable Assertion
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064721
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: Out-of-bounds Read
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064724
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: Integer Underflow
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064726
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: Release of Invalid Pointer or Reference
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064733
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: Double Free
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064737
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: Loop with Unreachable Exit Condition ('Infinite Loop')
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064742
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: CVE-2020-36226
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064744
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: Integer Underflow
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064746
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: Access of Resource Using Incompatible Type ('Type Confusion')
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064752
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: Reachable Assertion
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1064754
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u5

✗ High severity vulnerability found in openldap/libldap-common
  Description: Reachable Assertion
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENLDAP-1074919
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4 > openldap/libldap-common@2.4.47+dfsg-3+deb10u4
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > openldap/libldap-2.4-2@2.4.47+dfsg-3+deb10u4
  Fixed in: 2.4.47+dfsg-3+deb10u6

✗ High severity vulnerability found in nettle/libnettle6
  Description: Out-of-bounds Write
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-NETTLE-1090205
  Introduced through: wget@1.20.1-1.1, git@1:2.20.1-2+deb10u3
  From: wget@1.20.1-1.1 > nettle/libnettle6@3.4.1-1
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > nettle/libnettle6@3.4.1-1
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > gnutls28/libgnutls30@3.6.7-4+deb10u5 > nettle/libnettle6@3.4.1-1
  and 4 more...
  Fixed in: 3.4.1-1+deb10u1

✗ High severity vulnerability found in nettle/libnettle6
  Description: Improper Input Validation
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-NETTLE-1301269
  Introduced through: wget@1.20.1-1.1, git@1:2.20.1-2+deb10u3
  From: wget@1.20.1-1.1 > nettle/libnettle6@3.4.1-1
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > nettle/libnettle6@3.4.1-1
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > gnutls28/libgnutls30@3.6.7-4+deb10u5 > nettle/libnettle6@3.4.1-1
  and 4 more...
  Fixed in: 3.4.1-1+deb10u1

✗ High severity vulnerability found in libxml2
  Description: Use After Free
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-LIBXML2-1277346
  Introduced through: libxml2@2.9.4+dfsg1-7+deb10u1, shared-mime-info@1.10-1
  From: libxml2@2.9.4+dfsg1-7+deb10u1
  From: shared-mime-info@1.10-1 > libxml2@2.9.4+dfsg1-7+deb10u1
  Fixed in: 2.9.4+dfsg1-7+deb10u2

✗ High severity vulnerability found in libxml2
  Description: Out-of-bounds Write
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-LIBXML2-1277349
  Introduced through: libxml2@2.9.4+dfsg1-7+deb10u1, shared-mime-info@1.10-1
  From: libxml2@2.9.4+dfsg1-7+deb10u1
  From: shared-mime-info@1.10-1 > libxml2@2.9.4+dfsg1-7+deb10u1
  Fixed in: 2.9.4+dfsg1-7+deb10u2

✗ High severity vulnerability found in libxml2
  Description: Use After Free
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-LIBXML2-1277350
  Introduced through: libxml2@2.9.4+dfsg1-7+deb10u1, shared-mime-info@1.10-1
  From: libxml2@2.9.4+dfsg1-7+deb10u1
  From: shared-mime-info@1.10-1 > libxml2@2.9.4+dfsg1-7+deb10u1
  Fixed in: 2.9.4+dfsg1-7+deb10u2

✗ High severity vulnerability found in krb5/libkrb5support0
  Description: NULL Pointer Dereference
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-KRB5-1320094
  Introduced through: git@1:2.20.1-2+deb10u3, openssh/openssh-client@1:7.9p1-10+deb10u2, subversion@1.10.4-1+deb10u1, krb5/krb5-locales@1.17-3+deb10u1
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > krb5/libgssapi-krb5-2@1.17-3+deb10u1 > krb5/libkrb5support0@1.17-3+deb10u1
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > krb5/libgssapi-krb5-2@1.17-3+deb10u1 > krb5/libk5crypto3@1.17-3+deb10u1 > krb5/libkrb5support0@1.17-3+deb10u1
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > krb5/libgssapi-krb5-2@1.17-3+deb10u1 > krb5/libkrb5-3@1.17-3+deb10u1 > krb5/libkrb5support0@1.17-3+deb10u1
  and 9 more...
  Fixed in: 1.17-3+deb10u2

✗ High severity vulnerability found in gnutls28/libgnutls30
  Description: Out-of-bounds Write
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-GNUTLS28-609778
  Introduced through: apt@1.8.2.1, wget@1.20.1-1.1, git@1:2.20.1-2+deb10u3
  From: apt@1.8.2.1 > gnutls28/libgnutls30@3.6.7-4+deb10u5
  From: wget@1.20.1-1.1 > gnutls28/libgnutls30@3.6.7-4+deb10u5
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > gnutls28/libgnutls30@3.6.7-4+deb10u5
  and 2 more...
  Fixed in: 3.6.7-4+deb10u7

✗ High severity vulnerability found in glib2.0/libglib2.0-0
  Description: Incorrect Conversion between Numeric Types
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-GLIB20-1075023
  Introduced through: glib2.0/libglib2.0-0@2.58.3-2+deb10u2, shared-mime-info@1.10-1, glib2.0/libglib2.0-data@2.58.3-2+deb10u2
  From: glib2.0/libglib2.0-0@2.58.3-2+deb10u2
  From: shared-mime-info@1.10-1 > glib2.0/libglib2.0-0@2.58.3-2+deb10u2
  From: glib2.0/libglib2.0-data@2.58.3-2+deb10u2
  Fixed in: 2.58.3-2+deb10u3

✗ High severity vulnerability found in glib2.0/libglib2.0-0
  Description: Incorrect Conversion between Numeric Types
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-GLIB20-1075027
  Introduced through: glib2.0/libglib2.0-0@2.58.3-2+deb10u2, shared-mime-info@1.10-1, glib2.0/libglib2.0-data@2.58.3-2+deb10u2
  From: glib2.0/libglib2.0-0@2.58.3-2+deb10u2
  From: shared-mime-info@1.10-1 > glib2.0/libglib2.0-0@2.58.3-2+deb10u2
  From: glib2.0/libglib2.0-data@2.58.3-2+deb10u2
  Fixed in: 2.58.3-2+deb10u3

✗ High severity vulnerability found in gcc-8/libstdc++6
  Description: Information Exposure
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-GCC8-347558
  Introduced through: gcc-8/libstdc++6@8.3.0-6, apt@1.8.2.1, icu/libicu63@63.1-6+deb10u1, meta-common-packages@meta
  From: gcc-8/libstdc++6@8.3.0-6
  From: apt@1.8.2.1 > gcc-8/libstdc++6@8.3.0-6
  From: icu/libicu63@63.1-6+deb10u1 > gcc-8/libstdc++6@8.3.0-6
  and 3 more...

✗ High severity vulnerability found in expat/libexpat1
  Description: Resource Exhaustion
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2329087
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ High severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2330888
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ High severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2331795
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ High severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2331796
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ High severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2331820
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ High severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2406126
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u3

✗ High severity vulnerability found in curl/libcurl3-gnutls
  Description: Out-of-bounds Write
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-CURL-1049502
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1
  Fixed in: 7.64.0-4+deb10u2

✗ High severity vulnerability found in curl/libcurl3-gnutls
  Description: Improper Certificate Validation
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-CURL-1049506
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1
  Fixed in: 7.64.0-4+deb10u2

✗ High severity vulnerability found in curl/libcurl3-gnutls
  Description: Arbitrary Code Injection
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-CURL-573151
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1
  Fixed in: 7.64.0-4+deb10u2

✗ High severity vulnerability found in curl/libcurl3-gnutls
  Description: Information Exposure
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-CURL-573153
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1
  Fixed in: 7.64.0-4+deb10u2

✗ High severity vulnerability found in curl/libcurl3-gnutls
  Description: Use After Free
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-CURL-608200
  Introduced through: git@1:2.20.1-2+deb10u3
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1
  Fixed in: 7.64.0-4+deb10u2

✗ Critical severity vulnerability found in openssl/libssl1.1
  Description: Buffer Overflow
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-OPENSSL-1569403
  Introduced through: cyrus-sasl2/libsasl2-modules@2.1.27+dfsg-1+deb10u1, openssh/openssh-client@1:7.9p1-10+deb10u2, ca-certificates@20200601~deb10u1, subversion@1.10.4-1+deb10u1, git@1:2.20.1-2+deb10u3, mercurial@4.8.2-1+deb10u1
  From: cyrus-sasl2/libsasl2-modules@2.1.27+dfsg-1+deb10u1 > openssl/libssl1.1@1.1.1d-0+deb10u3
  From: openssh/openssh-client@1:7.9p1-10+deb10u2 > openssl/libssl1.1@1.1.1d-0+deb10u3
  From: ca-certificates@20200601~deb10u1 > openssl@1.1.1d-0+deb10u3 > openssl/libssl1.1@1.1.1d-0+deb10u3
  and 5 more...
  Fixed in: 1.1.1d-0+deb10u7

✗ Critical severity vulnerability found in lz4/liblz4-1
  Description: Out-of-bounds Write
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-LZ4-1277601
  Introduced through: apt@1.8.2.1, subversion@1.10.4-1+deb10u1, procps@2:3.3.15-2
  From: apt@1.8.2.1 > apt/libapt-pkg5.0@1.8.2.1 > lz4/liblz4-1@1.8.3-1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > lz4/liblz4-1@1.8.3-1
  From: procps@2:3.3.15-2 > procps/libprocps7@2:3.3.15-2 > systemd/libsystemd0@241-7~deb10u4 > lz4/liblz4-1@1.8.3-1
  Fixed in: 1.8.3-1+deb10u1

✗ Critical severity vulnerability found in libx11/libx11-data
  Description: Buffer Overflow
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-LIBX11-1293573
  Introduced through: libxext/libxext6@2:1.3.3-1+b2, libxmu/libxmuu1@2:1.1.2-2+b3, libxrender/libxrender1@1:0.9.10-1, xauth@1:1.0.10-1
  From: libxext/libxext6@2:1.3.3-1+b2 > libx11/libx11-6@2:1.6.7-1+deb10u1 > libx11/libx11-data@2:1.6.7-1+deb10u1
  From: libxext/libxext6@2:1.3.3-1+b2 > libx11/libx11-6@2:1.6.7-1+deb10u1
  From: libxmu/libxmuu1@2:1.1.2-2+b3 > libx11/libx11-6@2:1.6.7-1+deb10u1
  and 2 more...
  Fixed in: 2:1.6.7-1+deb10u2

✗ Critical severity vulnerability found in libbsd/libbsd0
  Description: Out-of-bounds Read
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-LIBBSD-541041
  Introduced through: libedit/libedit2@3.1-20181209-1, libsm/libsm6@2:1.2.3-1, libxext/libxext6@2:1.3.3-1+b2
  From: libedit/libedit2@3.1-20181209-1 > libbsd/libbsd0@0.9.1-2
  From: libsm/libsm6@2:1.2.3-1 > libice/libice6@2:1.0.9-2 > libbsd/libbsd0@0.9.1-2
  From: libxext/libxext6@2:1.3.3-1+b2 > libx11/libx11-6@2:1.6.7-1+deb10u1 > libxcb/libxcb1@1.13.1-2 > libxdmcp/libxdmcp6@1:1.1.2-3 > libbsd/libbsd0@0.9.1-2
  Fixed in: 0.9.1-2+deb10u1

✗ Critical severity vulnerability found in gnutls28/libgnutls30
  Description: Use After Free
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-GNUTLS28-1085094
  Introduced through: apt@1.8.2.1, wget@1.20.1-1.1, git@1:2.20.1-2+deb10u3
  From: apt@1.8.2.1 > gnutls28/libgnutls30@3.6.7-4+deb10u5
  From: wget@1.20.1-1.1 > gnutls28/libgnutls30@3.6.7-4+deb10u5
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > gnutls28/libgnutls30@3.6.7-4+deb10u5
  and 2 more...
  Fixed in: 3.6.7-4+deb10u7

✗ Critical severity vulnerability found in gnutls28/libgnutls30
  Description: Use After Free
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-GNUTLS28-1085097
  Introduced through: apt@1.8.2.1, wget@1.20.1-1.1, git@1:2.20.1-2+deb10u3
  From: apt@1.8.2.1 > gnutls28/libgnutls30@3.6.7-4+deb10u5
  From: wget@1.20.1-1.1 > gnutls28/libgnutls30@3.6.7-4+deb10u5
  From: git@1:2.20.1-2+deb10u3 > curl/libcurl3-gnutls@7.64.0-4+deb10u1 > gnutls28/libgnutls30@3.6.7-4+deb10u5
  and 2 more...
  Fixed in: 3.6.7-4+deb10u7

✗ Critical severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2331803
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ Critical severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2331813
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ Critical severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2331818
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ Critical severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2359258
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ Critical severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2384929
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u2

✗ Critical severity vulnerability found in expat/libexpat1
  Description: Improper Encoding or Escaping of Output
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2403513
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u3

✗ Critical severity vulnerability found in expat/libexpat1
  Description: Exposure of Resource to Wrong Sphere
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2403518
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u3

✗ Critical severity vulnerability found in expat/libexpat1
  Description: Integer Overflow or Wraparound
  Info: https://snyk.io/vuln/SNYK-DEBIAN10-EXPAT-2406128
  Introduced through: git@1:2.20.1-2+deb10u3, subversion@1.10.4-1+deb10u1, mercurial@4.8.2-1+deb10u1
  From: git@1:2.20.1-2+deb10u3 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > expat/libexpat1@2.2.6-2+deb10u1
  From: subversion@1.10.4-1+deb10u1 > subversion/libsvn1@1.10.4-1+deb10u1 > apr-util/libaprutil1@1.6.1-4 > expat/libexpat1@2.2.6-2+deb10u1
  and 1 more...
  Fixed in: 2.2.6-2+deb10u3


```

Thanks a lot for your work on sarek!

",arnaudceol,https://github.com/nf-core/sarek/issues/493
I_kwDOCvwIC85FSKjE,[FEATURE] Tumor mutational burden (TMB) scoring,OPEN,2022-03-08T08:57:03Z,2024-08-19T13:12:27Z,,"# nf-core/sarek feature request

## Is your feature request related to a problem? Please describe

TMB / tumor mutational burden is a key metric for many oncology related projects and thus could be a feature requested by many users of the pipeline at some point to be computed similar to MSI status (which is already done in the pipeline). 

## Describe the solution you'd like

Have some method / way of computing TMB status and reporting this to the user in the main MultiQC table ideally.

## Describe alternatives you've considered

![image](https://user-images.githubusercontent.com/2359510/157202429-9ecbbc57-5ffc-4c87-b8e8-5612b3a0bc9e.png)

",apeltzer,https://github.com/nf-core/sarek/issues/495
I_kwDOCvwIC85FfZOB,[BUG] convertAlleleCounts.r returns logR = 0 when no coverage in tumour sample,CLOSED,2022-03-11T00:09:13Z,2022-07-01T14:26:24Z,2022-07-01T14:26:24Z,"## Description of the bug

convertAlleleCounts.r returns a logR or 0 when tumourcounts is 0.

Line 85 aims at catching log(0/0) cases, but in line 73, any log(n/0) or log(0/n) that should return +/- Infinity are changed to NA. As a result cases where there is coverage in normal only or in tumour only end up with an incorrect logR = 0.

## Additional context

Note that current version of ASCAT ignores non-finite numbers. As such, returning NA or -Infinity doesn't matter at this time, although -Inf gives ASCAT the option to capture these cases in a future version. Alternatively, convertAlleleCounts.r could add a small delta to both tumour and normal counts to avoid +/- Infinity.

In short, there is no perfect solution, at least for now, but returning logR =0 signals lack of CNA instead of, for instance, homozygous deletion. Keeping these logR as -Infinity/+Infinity or simply NA (as for the BAF) would be preferable.",jherrero,https://github.com/nf-core/sarek/issues/496
I_kwDOCvwIC85FpSTF,[BUG] Somatic variant calling with non-unique runID variable error,CLOSED,2022-03-14T14:21:47Z,2022-05-11T07:20:45Z,2022-05-11T07:20:45Z,"<!--
# nf-core/sarek bug report
-->
Hi,

We are experiencing difficulties when running somatic variant detection for samples defined in a input tsv e.g.:

Patient1 XX 1 sample1 HHK55CCXX sample1_wgs_HHK55CCXX_lane1_R1_fastq.gz sample1_wgs_HHK55CCXX_lane1_R2_fastq.gz
Patient1 XX 0 sample2 HHK55CCXX sample2_wgs_HHK55CCXX_lane1_R1_fastq.gz sample2_wgs_HHK55CCXX_lane1_R2_fastq.gz

The variable idRun in main.nf is set as HHK55CCXX for both the tumour and normal samples. This is used as the ID-tag of Read Groups in the MapReads process. A subsequent freebayes analysis produces the following error:

_ERROR(freebayes): multiple samples (SM) map to the same read group (RG)
samples sample1 and sample2 map to HHK55CCXX
As freebayes operates on a virtually merged stream of its input files, it will not be possible to determine what sample an alignment belongs to at runtime.
To resolve the issue, ensure that RG ids are unique to one sample across all the input files to freebayes.
See bamaddrg (https://github.com/ekg/bamaddrg) for a method which can add RG tags to alignments._

A workaround is to add index to flow cell ID like so: HHK55CCXX_1 and HHK55CCXX_2. However, since it is evident from the .tsv that the two samples are tumor/normal (column 3 - 1/0), we believe that a more foolproof solution would be to build the RGID (idRun variable) in a more unique way, e.g. in the same way, as it is built when specifying input in the ""path to files"" form: https://nf-co.re/sarek/2.7.1/usage#input-ltsamplegt---step-mapping
...
RGID = ""sample_lib_flowcell_index_lane""

Kuba
",Hynst,https://github.com/nf-core/sarek/issues/499
I_kwDOCvwIC85FyNo8,Update nextflow.json,CLOSED,2022-03-16T10:00:18Z,2022-07-18T06:43:59Z,2022-07-18T06:43:59Z,"- [ ] All parameters have a good explanation at hand
- [ ] All parameters are sorted in an order that is intuitive and makes sense to the user",maxulysse,https://github.com/nf-core/sarek/issues/502
I_kwDOCvwIC85FyOE0,Add a tube map to visualize data flow,CLOSED,2022-03-16T10:01:53Z,2022-03-29T16:22:23Z,2022-03-29T16:22:23Z,happy to have a go at this if no one has started yet,chelauk,https://github.com/nf-core/sarek/issues/503
I_kwDOCvwIC85FyVur,Document how to use sarek without an igenomes genome,CLOSED,2022-03-16T10:27:15Z,2022-07-20T17:05:57Z,2022-07-20T17:05:57Z,"Document how to use sarek without an igenomes genome in different stages:
- [x] no BWA Indices
- [x] no known indels
- [x] snpEff/VEP on another organism.

Lots of people have problems figuring out how to do this.
We should come up with a better of explaining it.",FriederikeHanssen,https://github.com/nf-core/sarek/issues/504
I_kwDOCvwIC85F4FcI,[FEATURE] Investigate Fastp usage for speed improvement,CLOSED,2022-03-17T12:39:40Z,2022-06-24T19:23:16Z,2022-06-24T19:23:16Z,"cc @marchoeppner 
<!--
# nf-core/sarek feature request

Hi there!

Thanks for suggesting a new feature for the pipeline!
Please delete this text and anything that's not relevant from the template below:
-->

## Is your feature request related to a problem? Please describe

<!-- A clear and concise description of what the problem is. -->

<!-- e.g. [I'm always frustrated when ...] -->

## Describe the solution you'd like

<!-- A clear and concise description of what you want to happen. -->

## Describe alternatives you've considered

<!-- A clear and concise description of any alternative solutions or features you've considered. -->

## Additional context

<!-- Add any other context about the feature request here. -->
",FriederikeHanssen,https://github.com/nf-core/sarek/issues/508
I_kwDOCvwIC85F91Wg,Select only somatic/germline[FEATURE],CLOSED,2022-03-18T17:28:01Z,2022-05-11T07:18:35Z,2022-05-11T07:18:35Z,"In order to speed up the (wonderful!) _Sarek_ pipeline, could be it considered to add an option to process somatic *OR* germline variants and not all indistinctly?

Maybe something like `--step somatic/germline`?

Thanks",jgarces02,https://github.com/nf-core/sarek/issues/509
I_kwDOCvwIC85Gyl92,[FEATURE] RNA-Seq Support,CLOSED,2022-03-31T07:57:38Z,2022-03-31T08:01:23Z,2022-03-31T08:01:23Z,"<!--
# nf-core/sarek feature request

Hi there!

Thanks for suggesting a new feature for the pipeline!
Please delete this text and anything that's not relevant from the template below:
-->
<!-- A clear and concise description of what the problem is. -->
<!-- e.g. [I'm always frustrated when ...] -->
Hey, all. I'm working on a project with tumor-only samples. These tumor samples have been sequenced using WES and RNA-Seq, however we don't have WES data for all samples. Therefore, I would like to be able to do variant calling on the RNA-Seq samples to see if we can find some interesting variants in those samples.

<!-- A clear and concise description of what you want to happen. -->
I already have an idea how to do this. Since sarek already follows the GATK best practices for pre-processing I thought it would be best to also follow the same GATK best practices for RNA-Seq variant calling described [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035531192-RNAseq-short-variant-discovery-SNPs-Indels-).
This includes mapping with STAR 2-Pass, Marking Duplicates (already in sarek), and SplitNCigarReads.

<!-- A clear and concise description of any alternative solutions or features you've considered. -->
STAR could also be replaced by HISAT2, since STAR is quite an computational intensive tool. However, then Mutect2 might not be able to run anymore..

<!-- Add any other context about the feature request here. -->
After these pre-processing steps the workflow should run as if it is using WES data or would that introduce problems I'm currently not foreseeing?",JasperO98,https://github.com/nf-core/sarek/issues/517
I_kwDOCvwIC85G7_3j,"[BUG] * --vep_cache_version: expected type: String, found: Integer (99)",CLOSED,2022-04-01T18:20:45Z,2022-07-13T09:33:50Z,2022-07-13T09:33:50Z,"
## Check Documentation

I have checked the following places for your error:

- [X] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [X] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

`* --vep_cache_version: expected type: String, found: Integer (99)`

## Steps to reproduce

Steps to reproduce the behaviour:
```
./nextflow run sarek-2.7.1  \
--input tiny-manta-https.gcs.tsv 	 \
--step 'mapping'	 \
--outdir './results'			 \
--nucleotides_per_second 1000.0												 \
--aligner 'bwa-mem'	 \
--markdup_java_options '-Xms4000m -Xmx7g'						 \
--cf_coeff 0.05			 \
--cf_ploidy 2.0																				 \
--genome 'smallGRCh37'																	 \
--snpeff_db 'WBcel235.86'	 \
--species 'caenorhabditis_elegans'	 \
--igenomes_base 's3://ngi-igenomes/igenomes'	 \
--genomes_base 'raw.githubusercontent.com/nf-core/test-datasets/sarek/reference'	 \
--igenomes_ignore  		 \
--publish_dir_mode 'copy'		 \
--validate_params  			 \
--max_multiqc_email_size '25.MB'			 \
--tracedir './results/pipeline_info'			 \
--cpus 8	 \
--single_cpu_mem '7 GB'	 \
--max_cpus 16	 \
--max_memory '128.GB'	 \
--max_time '240.h'	 \
--custom_config_version 'master'	 \
--vep_cache_version '99'		 \
--custom_config_base 'https://raw.githubusercontent.com/nf-core/configs/master'	 \
-w gs://somebucket
```

## Expected behaviour

not treat `vep_cache_version` as numeric when it is quoted

## Log files

- [X] The command used to run the pipeline

## System

- Hardware: GCP
- Executor: <!-- [e.g. slurm, local, awsbatch...] -->
- OS: <!-- [e.g. CentOS Linux, macOS, Linux Mint...] -->
- Version <!-- [e.g. 7, 10.13.6, 18.3...] -->

## Nextflow Installation

- Version: nextflow version 21.10.6.5660

## Container engine

- Engine: Docker in GCP LS
- Image tag: sarek 2.7.1 

## Additional context

there is no type of quote that will solve this to my knowledge",leipzig,https://github.com/nf-core/sarek/issues/518
I_kwDOCvwIC85HMrYt,[BUG],CLOSED,2022-04-06T11:54:40Z,2022-04-08T08:23:34Z,2022-04-08T08:18:41Z,"Hello,
I am trying to run the pipeline with my own dataset but I get the following error:

Unexpected error [UnsupportedOperationException]

 -- Check script '/Users/christossynodinos/.nextflow/assets/nf-core/sarek/main.nf' at line: 211 or see '.nextflow.log' file for more details

Command line: nextflow run nf-core/sarek --input sarek_input.tsv -profile docker

- Hardware: Desktop
- OS: MacOS
- Nextflow version: 21.04.0
- Engine: Docker
- version: 4.5.0

The dataset comes from an Illumina MiSeq machine.

[nextflow.log](https://github.com/nf-core/sarek/files/8426577/nextflow.log)

I was able to run the test dataset using docker.",CSynodinos,https://github.com/nf-core/sarek/issues/519
I_kwDOCvwIC85IFJ_A,[FEATURE] Haplotype aware effect prediction,OPEN,2022-04-20T08:02:06Z,2024-08-19T13:12:27Z,,"# nf-core/sarek feature request

## Is your feature request related to a problem? Please describe

Typical effect prediction (SNPEff, VEP) will not take into consideration variants ""in phase"", i.e. each entry in a VCF file is interpreted individually. This can lead to certain pathogenic combinations of variants to be missed. One example is the inability of Deepvariant to call MNPs. As a result, the effect prediction will miss pathogenic di-nucleotide variants. However, this also extends to certain pharmacogentic haplotypes etc.

To be clear, this issue is not widely discussed, so it would be nice if the Sarek team could look into it and possibly add some support for this kind of analysis.

## Describe the solution you'd like

Add haplotype aware effect prediction. 

Possible tool chains:
vcf+bam -> Whatshap (phasing) -> Bcftools csq / EnsEMBL Haplosaurus
vcf+bam -> Shapeit4 -> Whatshap -> Bcftools csq / EnsEMBL Haplosaurus

## Describe alternatives you've considered

I am not sure whether adding Shapeit4 is strictly necessary (have not worked with it myself). So the most basic approach would be to add phasing information to the VCF file and run this through an effect predictor which interprets phasing information (Bcftools csq, or Haplosaurus).

Haplosaurus is part of the VEP package, so no extra effort needed (uses the same cache). Bcftools requires a compatible GFF3 file (iGenomes would work). 

## Additional context

At this stage I think a discussion over the ""added value"" of this approach would be useful. The implementation as such would be quite straight forward. 
",marchoeppner,https://github.com/nf-core/sarek/issues/523
I_kwDOCvwIC85I0KH2,The pipeline (sarek 2.7.1) stopped working once I updated to Nextflow version 22.04.0,CLOSED,2022-04-29T22:30:17Z,2022-05-02T07:22:52Z,2022-05-02T02:47:55Z,"The error message I received once I updated to Nextflow version 22.04.0:

> No such variable: ch_software_versions_yaml
>  -- Check script '.nextflow/assets/nf-core/sarek/main.nf' at line: 450 or see '.nextflow.log' file for more details

Seemed like `ch_software_versions_yaml` is not defined correctly. I copied `software_versions_mqc.yaml` into sarek/assets and added the following line based on nf-core/rnaseq pipeline code (which runs fine with the new Nextflow version) and commented out line 450:

>ch_software_versions_yaml = file(""$projectDir/assets/software_versions_mqc.yaml"", checkIfExists: true)

After this I reran the test, and received this error message:

>No such variable: bwa_built
> -- Check script 'sarek/main.nf' at line: 484 or see '.nextflow.log' file for more details

`bwa_built` variable is also defined by the command `file   into bwa_built`, similarly to `ch_software_versions_yaml`, maybe this type of variable definition is not supported in Nextflow 22.04.0 anymore.

If I run `NXF_VER=21.10.4 nextflow run nf-core/sarek -profile test,docker` the test pipeline runs fine.",SzilviaKiriakov,https://github.com/nf-core/sarek/issues/530
I_kwDOCvwIC85I8Yfq,[BUG] Scatter/gather of intervals uses misleading filename (content is fine),OPEN,2022-05-03T07:52:11Z,2024-02-08T15:12:31Z,,"The module `create_intervals_bed` uses `awk` magic to split up interval files into batches of similar genome size. However, for the naming of the file the first interval is being used, which can lead to the misleading impression, that some intervals are discarded. (They are not!) Fixing this would be great, i.e. by using the first and last position in the file for naming.",FriederikeHanssen,https://github.com/nf-core/sarek/issues/534
I_kwDOCvwIC85JNmy2,PBS killing job because VMEM exceeding allotted resources,CLOSED,2022-05-06T19:12:20Z,2022-05-12T07:05:00Z,2022-05-12T07:05:00Z,"Hi there,

I'm trying to run sarek through our institution HPC. In my bash script I have assigned:

nodes=1:ppn=20
gres=localhd:100
mem=200g
vmem=200g

I'm launching the pipeline in my bash script using this command:

nextflow run nf-core/sarek --input ./results/Preprocessing/TSV/duplicates_marked_no_table.tsv --step 'prepare_recalibration'  --genome GRCm38 --path/to/ref -profile singularity --cpus 16 --single_cpu_mem '7 GB'

The --max_memory and --max_cpus are left at default and this should be well within the 200g limit I've set.

However my job is quickly killed by PBS for exceeding resource limit. I've tried to look through the documentation but I'm not sure how to limit the memory allocation further so that this doesn't happen.

> PBS Job Id: 75736704
Job Name:   sarekRecal
Exec host:  node450/0-19
Aborted by PBS Server
Job exceeded a memory resource limit (vmem, pvmem, etc.). Job was aborted
See Administrator for help
Exit_status=-10
resources_used.cput=00:17:35
resources_used.vmem=286793240kb
resources_used.walltime=00:12:06
resources_used.mem=37296368kb
resources_used.energy_used=0
req_information.task_count.0=1
req_information.lprocs.0=20
req_information.total_memory.0=209715200kb
req_information.memory.0=209715200kb
req_information.total_swap.0=209715200kb
req_information.swap.0=209715200kb
req_information.thread_usage_policy.0=allowthreads
req_information.hostlist.0=node450:ppn=20
req_information.task_usage.0.task.0={""task"":{""cpu_list"":""0-19"",""mem_list"":""0"",""cores"":0,""threads"":20,""host"":""node450""}}",vincentye2,https://github.com/nf-core/sarek/issues/541
I_kwDOCvwIC85JbUXM,[BUG] iGenomes genome index directory does not exist,CLOSED,2022-05-11T01:54:34Z,2022-05-11T07:42:42Z,2022-05-11T07:42:42Z,"In the `dev` branch, **bwamem2** index directory below does not exist on iGenomes bucket:

https://github.com/nf-core/sarek/blob/3bcf3bbcb6c2bece3c69e4b18558ac1f54107861/conf/igenomes.config#L39",bounlu,https://github.com/nf-core/sarek/issues/542
I_kwDOCvwIC85Jc-hE,[BUG] No variantcalling output is generated,CLOSED,2022-05-11T09:41:51Z,2022-05-24T14:54:23Z,2022-05-24T12:42:53Z,"<!--
# nf-core/sarek bug report

Hi there!

Thanks for telling us about a problem with the pipeline.
Please delete this text and anything that's not relevant from the template below:
-->

## Check Documentation

I have checked the following places for your error:

- [ ] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [ ] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)
- [ ] https://github.com/nf-core/sarek/issues/118

## Description of the bug

Although I specify the tools for variant calling as below, no relevant output is generated, and no error/log is displayed as well.

## Steps to reproduce

I run it on AWS Batch with the following command and configs and input:

```
nextflow run nf-core/sarek \
-profile docker \
--genome 'hg38' \
--input 'samplesheet.csv' \
--step mapping \
--aligner bwa-mem2 \
--tools mpileup,mutect2,snpeff,strelka,vep \
--outdir 's3://omeran/nextflow/sarek/results/' \
-work-dir 's3://omeran/nextflow/sarek/work/' \
-c '/Users/omeran/Desktop/aws/sarek/custom.config' \
-r dev \
-resume
```

`custom.config` file:
```
process.executor	= 'awsbatch'
process.queue		= 'omeran-nextflow-sarek-spot'
aws.batch.cliPath	= '/home/ec2-user/miniconda/bin/aws'
```
`samplesheet.csv` file:
```
subject,sex,status,sample,lane,fastq1,fastq2
DNBSEQ,XX,1,DNBSEQ_tumour,1,s3://fastq/xxx/V350040544_L01_111-606_1.fq.gz,s3://fastq/xxx/V350040544_L01_111-606_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,2,s3://fastq/xxx/V350040544_L01_111-607_1.fq.gz,s3://fastq/xxx/V350040544_L01_111-607_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,3,s3://fastq/xxx/V350040544_L01_111-608_1.fq.gz,s3://fastq/xxx/V350040544_L01_111-608_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,4,s3://fastq/xxx/V350040544_L01_111-609_1.fq.gz,s3://fastq/xxx/V350040544_L01_111-609_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,5,s3://fastq/xxx/V350040544_L01_111-610_1.fq.gz,s3://fastq/xxx/V350040544_L01_111-610_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,6,s3://fastq/xxx/V350040544_L02_111-606_1.fq.gz,s3://fastq/xxx/V350040544_L02_111-606_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,7,s3://fastq/xxx/V350040544_L02_111-607_1.fq.gz,s3://fastq/xxx/V350040544_L02_111-607_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,8,s3://fastq/xxx/V350040544_L02_111-608_1.fq.gz,s3://fastq/xxx/V350040544_L02_111-608_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,9,s3://fastq/xxx/V350040544_L02_111-609_1.fq.gz,s3://fastq/xxx/V350040544_L02_111-609_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,10,s3://fastq/xxx/V350040544_L02_111-610_1.fq.gz,s3://fastq/xxx/V350040544_L02_111-610_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,11,s3://fastq/xxx/V350040544_L03_111-606_1.fq.gz,s3://fastq/xxx/V350040544_L03_111-606_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,12,s3://fastq/xxx/V350040544_L03_111-607_1.fq.gz,s3://fastq/xxx/V350040544_L03_111-607_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,13,s3://fastq/xxx/V350040544_L03_111-608_1.fq.gz,s3://fastq/xxx/V350040544_L03_111-608_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,14,s3://fastq/xxx/V350040544_L03_111-609_1.fq.gz,s3://fastq/xxx/V350040544_L03_111-609_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,15,s3://fastq/xxx/V350040544_L03_111-610_1.fq.gz,s3://fastq/xxx/V350040544_L03_111-610_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,16,s3://fastq/xxx/V350040601_L01_111-606_1.fq.gz,s3://fastq/xxx/V350040601_L01_111-606_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,17,s3://fastq/xxx/V350040601_L01_111-607_1.fq.gz,s3://fastq/xxx/V350040601_L01_111-607_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,18,s3://fastq/xxx/V350040601_L01_111-608_1.fq.gz,s3://fastq/xxx/V350040601_L01_111-608_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,19,s3://fastq/xxx/V350040601_L01_111-609_1.fq.gz,s3://fastq/xxx/V350040601_L01_111-609_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,20,s3://fastq/xxx/V350040601_L01_111-610_1.fq.gz,s3://fastq/xxx/V350040601_L01_111-610_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,21,s3://fastq/xxx/V350040601_L02_111-606_1.fq.gz,s3://fastq/xxx/V350040601_L02_111-606_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,22,s3://fastq/xxx/V350040601_L02_111-607_1.fq.gz,s3://fastq/xxx/V350040601_L02_111-607_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,23,s3://fastq/xxx/V350040601_L02_111-608_1.fq.gz,s3://fastq/xxx/V350040601_L02_111-608_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,24,s3://fastq/xxx/V350040601_L02_111-609_1.fq.gz,s3://fastq/xxx/V350040601_L02_111-609_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,25,s3://fastq/xxx/V350040601_L02_111-610_1.fq.gz,s3://fastq/xxx/V350040601_L02_111-610_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,26,s3://fastq/xxx/V350040601_L03_111-606_1.fq.gz,s3://fastq/xxx/V350040601_L03_111-606_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,27,s3://fastq/xxx/V350040601_L03_111-607_1.fq.gz,s3://fastq/xxx/V350040601_L03_111-607_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,28,s3://fastq/xxx/V350040601_L03_111-608_1.fq.gz,s3://fastq/xxx/V350040601_L03_111-608_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,29,s3://fastq/xxx/V350040601_L03_111-609_1.fq.gz,s3://fastq/xxx/V350040601_L03_111-609_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,30,s3://fastq/xxx/V350040601_L03_111-610_1.fq.gz,s3://fastq/xxx/V350040601_L03_111-610_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,31,s3://fastq/xxx/V350040622_L01_111-606_1.fq.gz,s3://fastq/xxx/V350040622_L01_111-606_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,32,s3://fastq/xxx/V350040622_L01_111-607_1.fq.gz,s3://fastq/xxx/V350040622_L01_111-607_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,33,s3://fastq/xxx/V350040622_L01_111-608_1.fq.gz,s3://fastq/xxx/V350040622_L01_111-608_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,34,s3://fastq/xxx/V350040622_L01_111-609_1.fq.gz,s3://fastq/xxx/V350040622_L01_111-609_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,35,s3://fastq/xxx/V350040622_L01_111-610_1.fq.gz,s3://fastq/xxx/V350040622_L01_111-610_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,36,s3://fastq/xxx/V350040622_L02_111-606_1.fq.gz,s3://fastq/xxx/V350040622_L02_111-606_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,37,s3://fastq/xxx/V350040622_L02_111-607_1.fq.gz,s3://fastq/xxx/V350040622_L02_111-607_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,38,s3://fastq/xxx/V350040622_L02_111-608_1.fq.gz,s3://fastq/xxx/V350040622_L02_111-608_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,39,s3://fastq/xxx/V350040622_L02_111-609_1.fq.gz,s3://fastq/xxx/V350040622_L02_111-609_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,40,s3://fastq/xxx/V350040622_L02_111-610_1.fq.gz,s3://fastq/xxx/V350040622_L02_111-610_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,41,s3://fastq/xxx/V350040622_L03_111-606_1.fq.gz,s3://fastq/xxx/V350040622_L03_111-606_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,42,s3://fastq/xxx/V350040622_L03_111-607_1.fq.gz,s3://fastq/xxx/V350040622_L03_111-607_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,43,s3://fastq/xxx/V350040622_L03_111-608_1.fq.gz,s3://fastq/xxx/V350040622_L03_111-608_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,44,s3://fastq/xxx/V350040622_L03_111-609_1.fq.gz,s3://fastq/xxx/V350040622_L03_111-609_2.fq.gz
DNBSEQ,XX,1,DNBSEQ_tumour,45,s3://fastq/xxx/V350040622_L03_111-610_1.fq.gz,s3://fastq/xxx/V350040622_L03_111-610_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,1,s3://fastq/yyy/V350040544_L01_222-611_1.fq.gz,s3://fastq/yyy/V350040544_L01_222-611_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,2,s3://fastq/yyy/V350040544_L01_222-612_1.fq.gz,s3://fastq/yyy/V350040544_L01_222-612_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,3,s3://fastq/yyy/V350040544_L01_222-613_1.fq.gz,s3://fastq/yyy/V350040544_L01_222-613_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,4,s3://fastq/yyy/V350040544_L01_222-614_1.fq.gz,s3://fastq/yyy/V350040544_L01_222-614_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,5,s3://fastq/yyy/V350040544_L01_222-615_1.fq.gz,s3://fastq/yyy/V350040544_L01_222-615_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,6,s3://fastq/yyy/V350040544_L02_222-611_1.fq.gz,s3://fastq/yyy/V350040544_L02_222-611_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,7,s3://fastq/yyy/V350040544_L02_222-612_1.fq.gz,s3://fastq/yyy/V350040544_L02_222-612_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,8,s3://fastq/yyy/V350040544_L02_222-613_1.fq.gz,s3://fastq/yyy/V350040544_L02_222-613_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,9,s3://fastq/yyy/V350040544_L02_222-614_1.fq.gz,s3://fastq/yyy/V350040544_L02_222-614_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,10,s3://fastq/yyy/V350040544_L02_222-615_1.fq.gz,s3://fastq/yyy/V350040544_L02_222-615_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,11,s3://fastq/yyy/V350040544_L03_222-611_1.fq.gz,s3://fastq/yyy/V350040544_L03_222-611_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,12,s3://fastq/yyy/V350040544_L03_222-612_1.fq.gz,s3://fastq/yyy/V350040544_L03_222-612_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,13,s3://fastq/yyy/V350040544_L03_222-613_1.fq.gz,s3://fastq/yyy/V350040544_L03_222-613_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,14,s3://fastq/yyy/V350040544_L03_222-614_1.fq.gz,s3://fastq/yyy/V350040544_L03_222-614_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,15,s3://fastq/yyy/V350040544_L03_222-615_1.fq.gz,s3://fastq/yyy/V350040544_L03_222-615_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,16,s3://fastq/yyy/V350040601_L01_222-611_1.fq.gz,s3://fastq/yyy/V350040601_L01_222-611_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,17,s3://fastq/yyy/V350040601_L01_222-612_1.fq.gz,s3://fastq/yyy/V350040601_L01_222-612_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,18,s3://fastq/yyy/V350040601_L01_222-613_1.fq.gz,s3://fastq/yyy/V350040601_L01_222-613_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,19,s3://fastq/yyy/V350040601_L01_222-614_1.fq.gz,s3://fastq/yyy/V350040601_L01_222-614_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,20,s3://fastq/yyy/V350040601_L01_222-615_1.fq.gz,s3://fastq/yyy/V350040601_L01_222-615_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,21,s3://fastq/yyy/V350040601_L02_222-611_1.fq.gz,s3://fastq/yyy/V350040601_L02_222-611_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,22,s3://fastq/yyy/V350040601_L02_222-612_1.fq.gz,s3://fastq/yyy/V350040601_L02_222-612_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,23,s3://fastq/yyy/V350040601_L02_222-613_1.fq.gz,s3://fastq/yyy/V350040601_L02_222-613_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,24,s3://fastq/yyy/V350040601_L02_222-614_1.fq.gz,s3://fastq/yyy/V350040601_L02_222-614_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,25,s3://fastq/yyy/V350040601_L02_222-615_1.fq.gz,s3://fastq/yyy/V350040601_L02_222-615_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,26,s3://fastq/yyy/V350040601_L03_222-611_1.fq.gz,s3://fastq/yyy/V350040601_L03_222-611_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,27,s3://fastq/yyy/V350040601_L03_222-612_1.fq.gz,s3://fastq/yyy/V350040601_L03_222-612_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,28,s3://fastq/yyy/V350040601_L03_222-613_1.fq.gz,s3://fastq/yyy/V350040601_L03_222-613_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,29,s3://fastq/yyy/V350040601_L03_222-614_1.fq.gz,s3://fastq/yyy/V350040601_L03_222-614_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,30,s3://fastq/yyy/V350040601_L03_222-615_1.fq.gz,s3://fastq/yyy/V350040601_L03_222-615_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,31,s3://fastq/yyy/V350040622_L01_222-611_1.fq.gz,s3://fastq/yyy/V350040622_L01_222-611_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,32,s3://fastq/yyy/V350040622_L01_222-612_1.fq.gz,s3://fastq/yyy/V350040622_L01_222-612_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,33,s3://fastq/yyy/V350040622_L01_222-613_1.fq.gz,s3://fastq/yyy/V350040622_L01_222-613_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,34,s3://fastq/yyy/V350040622_L01_222-614_1.fq.gz,s3://fastq/yyy/V350040622_L01_222-614_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,35,s3://fastq/yyy/V350040622_L01_222-615_1.fq.gz,s3://fastq/yyy/V350040622_L01_222-615_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,36,s3://fastq/yyy/V350040622_L02_222-611_1.fq.gz,s3://fastq/yyy/V350040622_L02_222-611_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,37,s3://fastq/yyy/V350040622_L02_222-612_1.fq.gz,s3://fastq/yyy/V350040622_L02_222-612_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,38,s3://fastq/yyy/V350040622_L02_222-613_1.fq.gz,s3://fastq/yyy/V350040622_L02_222-613_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,39,s3://fastq/yyy/V350040622_L02_222-614_1.fq.gz,s3://fastq/yyy/V350040622_L02_222-614_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,40,s3://fastq/yyy/V350040622_L02_222-615_1.fq.gz,s3://fastq/yyy/V350040622_L02_222-615_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,41,s3://fastq/yyy/V350040622_L03_222-611_1.fq.gz,s3://fastq/yyy/V350040622_L03_222-611_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,42,s3://fastq/yyy/V350040622_L03_222-612_1.fq.gz,s3://fastq/yyy/V350040622_L03_222-612_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,43,s3://fastq/yyy/V350040622_L03_222-613_1.fq.gz,s3://fastq/yyy/V350040622_L03_222-613_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,44,s3://fastq/yyy/V350040622_L03_222-614_1.fq.gz,s3://fastq/yyy/V350040622_L03_222-614_2.fq.gz
DNBSEQ,XX,0,DNBSEQ_normal,45,s3://fastq/yyy/V350040622_L03_222-615_1.fq.gz,s3://fastq/yyy/V350040622_L03_222-615_2.fq.gz
Novaseq,XX,1,Novaseq_tumour,1,s3://fastq/zzz/FCH3NJ5DSX3_L4_333-101_1.fq.gz,s3://fastq/zzz/FCH3NJ5DSX3_L4_333-101_2.fq.gz
Novaseq,XX,0,Novaseq_normal,1,s3://fastq/ttt/FCH3NJ5DSX3_L4_444-96_1.fq.gz,s3://fastq/ttt/FCH3NJ5DSX3_L4_444-96_2.fq.gz
```


**Output:**
```
-[nf-core/sarek] Pipeline completed successfully-
WARN: To render the execution DAG in the required format it is required to install Graphviz -- See http://www.graphviz.org for more info.
Completed at: 11-May-2022 17:26:01
Duration    : 8m 25s
CPU hours   : 0.1 (91% cached)
Succeeded   : 2
Cached      : 41
```


```
$ aws s3 ls s3://omeran/nextflow/sarek/results/
                           PRE build/
                           PRE multiqc/
                           PRE pipeline_info/
                           PRE preprocessing/
```

**Expected behaviour:**
`results/VariantCalling` folder should be created.


What am I doing wrong?


**Info:**

N E X T F L O W  ~  version 22.04.0
Launching `https://github.com/nf-core/sarek` [golden_torvalds] DSL2 - revision: 3bcf3bbcb6 [dev]
",bounlu,https://github.com/nf-core/sarek/issues/543
I_kwDOCvwIC85JuaLO,[BUG] Mutect2 fails,CLOSED,2022-05-16T09:51:58Z,2023-10-23T17:45:07Z,2023-03-21T13:13:53Z,"I run sarek as below:

```
nextflow run nf-core/sarek \
-profile docker \
-dsl1 \
--sentieon \
--step mapping \
--tools tnscope,mutect2,strelka,snpeff,vep \
--input '/Users/omeran/Desktop/aws/sarek/samplesheet.tsv' \
--outdir 's3://omeran/nextflow/sarek/results/' \
-bucket-dir 's3://omeran/nextflow/sarek/work/' \
-c '/Users/omeran/Desktop/aws/sarek/custom.config' \
-r master
```

It works fine for the steps till variant calling. However, I keep getting the below error on `Mutect2` step:
```
Execution cancelled -- Finishing pending tasks before exit
WARN: Got an interrupted exception while taking agent result | java.lang.InterruptedException
Error executing process > 'Mutect2Single (FFLC85_Novaseq_tumour-chr16_46380683-90228345)'

Caused by:
  Task failed to start - DockerTimeoutError: Could not transition to created; timed out after waiting 4m0s

Command executed:

  # Get raw calls
  gatk --java-options ""-Xmx7g""       Mutect2       -R Homo_sapiens_assembly38.fasta      -I FFLC85_Novaseq_tumour.recal.bam  -tumor FFLC85_Novaseq_tumour       -L chr16_46380683-90228345.bed              --germline-resource gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz              -O chr16_46380683-90228345_FFLC85_Novaseq_tumour.vcf

Command exit status:
  1

Command output:
  (empty)

Command wrapper:
  nxf-scratch-dir ip-172-31-45-123.ap-southeast-1.compute.internal:/tmp/nxf.8LordsTTl5
  
  An error occurred (AllAccessDisabled) when calling the ListObjectsV2 operation: All access to this object has been disabled
  fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden
  
  An error occurred (AllAccessDisabled) when calling the ListObjectsV2 operation: All access to this object has been disabled
  fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden
  download failed: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta to ./Homo_sapiens_assembly38.fasta [Errno 12] Cannot allocate memory
  main: line 266:   112 Killed                  /home/ec2-user/.awscliv2/binaries/aws s3 cp --only-show-errors ""$source"" ""$target""

Work dir:
  s3://omeran/nextflow/sarek/work/6b/0140014bdc844aebf4ce63b3cdb2b7

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line


Unexpected error [AbortedException]


Failed to sanitize XML document destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler


Failed to sanitize XML document destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler


Failed to sanitize XML document destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler


-[nf-core/sarek] Pipeline completed with errors-
WARN: Killing running tasks (271)
```

When I looked up for the error, I got 2 possibilities related to this error:

1. Wrong IAM permissions -> [here](https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-listobjects-sync/)

I verify this is not the case for access to my buckets as the relevant role (**ecsInstanceRole**) has `AmazonS3FullAccess`.

But I am not sure if this points to the permissions to iGenomes folder.

2. Non-existing folder path -> [here](https://stackoverflow.com/questions/51405858/aws-s3-clian-error-occurred-allaccessdisabled-when-calling-the-putobject-oper)

I am not sure how to rectify this, as I checked the path to reference fasta file and it exists.


```
$ nextflow info
  Version: 22.04.0 build 5697
  Created: 23-04-2022 18:00 UTC (24-04-2022 02:00 SGST)
  System: Mac OS X 11.3
  Runtime: Groovy 3.0.10 on OpenJDK 64-Bit Server VM 11.0.11+9
  Encoding: UTF-8 (UTF-8)
```",bounlu,https://github.com/nf-core/sarek/issues/548
I_kwDOCvwIC85J4mCP,[BUG] Test pipeline run,CLOSED,2022-05-18T07:53:50Z,2022-05-18T15:39:33Z,2022-05-18T15:39:33Z,"<!--
# nf-core/sarek bug report

Hi there!

I did run the command for the minimal pipline:

nextflow run nf-core/sarek -profile test,docker



and it finished with this message:

.......
snpEff DB         : WBcel235.86
species           : caenorhabditis_elegans
VEP cache version : 99
Publish dir mode  : copy
Config Profile    : test,docker
Config Description: Minimal test dataset to check pipeline function
Config Files      : /home/ubuntu/.nextflow/assets/nf-core/sarek/nextflow.config
----------------------------------------------------
No such variable: ch_software_versions_yaml

 -- Check script '/home/ubuntu/.nextflow/assets/nf-core/sarek/main.nf' at line: 450 or see '.nextflow.log' file for more details
[.nextflow.log](https://github.com/nf-core/sarek/files/8714259/default.nextflow.log)

",sheucke,https://github.com/nf-core/sarek/issues/554
I_kwDOCvwIC85KDbKE,[BUG] Sarek 2.7.1 doesn't reuse all cached `MapReads` jobs ,CLOSED,2022-05-19T23:01:31Z,2022-06-10T09:41:40Z,2022-06-10T09:41:40Z,"## Check Documentation

I have checked the following places for your error, but I didn't find anything related to caching. 

- [x] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [x] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

When I relaunch a Sarek workflow, a seemingly random subset of `MapReads` jobs (ranging from 5 to 50%) are re-run despite cached results being available. This causes all downstream jobs for the affected samples to also be re-run. For large datasets, especially those being processed in the cloud, this bug causes a significant cost to be incurred if the workflow doesn't complete successfully on the first try, especially if most of the workflow is complete before an error arises. 

I tried using previous versions of Sarek (specifically, `2.6.1`, `2.5.2`, and `2.5`), and they all seem to run into this bug. 

<img width=""720"" alt=""image"" src=""https://user-images.githubusercontent.com/740725/169415073-b682c815-051c-4538-8744-0f54f388de99.png"">

## Steps to reproduce

I adapted the input file from the `test` profile by artificially increasing the number of rows to 100. This makes it easier to spot this issue, which doesn't always occur with only a few samples. It's also easier to notice the issue if you monitor the workflow in Nextflow Tower. 

1. Run the following Nextflow command where `params.yml` contains the parameters copied below. It should complete successfully. On a local machine, this command took less than 2 minutes. 
    ```console
    nextflow run 'https://github.com/nf-core/sarek' -r '2.7.1' -dsl1 -profile 'test,docker' -params-file 'params.yml'
    ```
2. Re-run the same Nextflow command with `-resume`. It should also complete successfully, but a subset of `MapReads` jobs should have been re-run instead of being cached. 

**Parameters**

The parameters aside from `input` are included to skip steps that aren't relevant to the reproducible example. 

```yaml
input: https://gist.githubusercontent.com/BrunoGrandePhD/6869bf506e6acf7b920a22b666f4e443/raw/ff7ea0757ebbd324cd125b1d48f54e6ec9f969c9/sarek-test.tsv
skip_qc: ""bamqc,baserecalibrator,bcftools,documentation,fastqc,markduplicates,multiqc,samtools,sentieon,vcftools,versions""
no_intervals: true
skip_markduplicates: true
known_indels: null
```

## Expected behaviour

I expect that workflow re-runs will leverage cached results unless there's a reason to believe that the job needs to be updated. In the situation I describe above, nothing changes between the two runs, so there shouldn't be a reason for jobs to be re-run, especially not a seemingly random subset of jobs. 

## Log files

Have you provided the following extra information/files:

- [x] The command used to run the pipeline (see ""Steps to reproduce"" above)
- [x] The `.nextflow.log` files

The Nextflow logs below correspond to my walkthrough of the steps I listed in ""Steps to reproduce"". 

[firstpass-nextflow.log](https://github.com/nf-core/sarek/files/8734624/firstpass-nextflow.log)
[secondpass-nextflow.log](https://github.com/nf-core/sarek/files/8734625/secondpass-nextflow.log)

## System

- Hardware: AWS EC2 instances
- Executor: Tried on both local EC2 instances and on AWS Batch
- OS: Amazon Linux
- Version: 2 (Kernel: `Linux 4.14.232-176.381.amzn2.x86_64`)

## Nextflow Installation

- Version: `22.04.2`

## Container engine

- Engine: Docker
- Version: `20.10.13`
- Image tag: `nfcore/sarek:2.7.1`

## Additional context

<!-- Add any other context about the problem here. -->
",BrunoGrandePhD,https://github.com/nf-core/sarek/issues/555
I_kwDOCvwIC85KN5RO,[BUG] MarkDuplicatesSpark fails,CLOSED,2022-05-23T13:16:52Z,2022-05-26T10:42:36Z,2022-05-26T10:42:36Z,"## Check Documentation

I have checked the following places for your error:

- [ x ] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [ x ] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

I'm experiencing issues when enabling the use of spark (`--use_gatk_spark`).
I edited the files `/etc/security/limits.conf` and `/etc/sysctl.conf` on computing nodes as suggested in https://nf-co.re/sarek/usage#spark-related-issues, but not `/etc/sysconfig/docker` since I'm using singularity (should I edit a different file?).

## Steps to reproduce

Steps to reproduce the behavior (I added the SINGULARITYENV_* variables according to https://github.com/nf-core/sarek/pull/295#issuecomment-720760907):

1. Command line: 
```
export NXF_OPTS='-Xms1g -Xmx4g'
export SINGULARITYENV_SPARK_LOCAL_IP=127.0.0.1
export SINGULARITYENV_SPARK_PUBLIC_DNS=127.0.0.1
nextflow run nf-core/sarek -r 2.7.1 --cpus $SLURM_CPUS_PER_TASK --max_cpus 64 --single_cpu_mem '8 GB' --max_memory '500 GB' --input ../data/sample_data.tsv -profile singularity --tools FreeBayes,HaplotypeCaller,VEP --use_gatk_spark --outdir ../results --step mapping -resume
```
2. See error: 
```
  12:36:04.453 INFO  MarkDuplicatesSpark - ------------------------------------------------------------
  12:36:04.454 INFO  MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.1.7.0
  12:36:04.454 INFO  MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/
  12:36:04.458 INFO  MarkDuplicatesSpark - Initializing engine
  12:36:04.458 INFO  MarkDuplicatesSpark - Done initializing engine
  12:36:04.691 INFO  MarkDuplicatesSpark - Shutting down engine
  [May 23, 2022 12:36:04 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapse
d time: 0.01 minutes.
  Runtime.totalMemory()=4557111296
  Exception in thread ""main"" java.lang.ExceptionInInitializerError
        at org.apache.spark.SparkConf$.<init>(SparkConf.scala:716)
        at org.apache.spark.SparkConf$.<clinit>(SparkConf.scala)
        at org.apache.spark.SparkConf.set(SparkConf.scala:95)
        at org.apache.spark.SparkConf$$anonfun$loadFromSystemProperties$3.apply(SparkConf.scala:77)
        at org.apache.spark.SparkConf$$anonfun$loadFromSystemProperties$3.apply(SparkConf.scala:76)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
        at org.apache.spark.SparkConf.loadFromSystemProperties(SparkConf.scala:76)
        at org.apache.spark.SparkConf.<init>(SparkConf.scala:71)
        at org.apache.spark.SparkConf.<init>(SparkConf.scala:58)
        at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.setupSparkConf(SparkContextFactory.java:173)
        at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:183)
        at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:117)
        at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28)
        at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139)
        at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191)
        at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210)
        at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163)
        at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206)
        at org.broadinstitute.hellbender.Main.main(Main.java:292)
  Caused by: java.net.UnknownHostException: nodo01: nodo01: No address associated with hostname
        at java.net.InetAddress.getLocalHost(InetAddress.java:1506)
        at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:946)
        at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:939)
        at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:939)
        at org.apache.spark.util.Utils$$anonfun$localCanonicalHostName$1.apply(Utils.scala:996)
        at org.apache.spark.util.Utils$$anonfun$localCanonicalHostName$1.apply(Utils.scala:996)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.util.Utils$.localCanonicalHostName(Utils.scala:996)
        at org.apache.spark.internal.config.package$.<init>(package.scala:302)
        at org.apache.spark.internal.config.package$.<clinit>(package.scala)
        ... 23 more
  Caused by: java.net.UnknownHostException: nodo01: No address associated with hostname
        at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
        at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)
        at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)
        at java.net.InetAddress.getLocalHost(InetAddress.java:1501)
        ... 32 more
```


## Log files
[log.txt](https://github.com/nf-core/sarek/files/8754797/log.txt)
[nextflow.log.txt](https://github.com/nf-core/sarek/files/8754814/nextflow.log.txt)


Have you provided the following extra information/files:

- [ x ] The command used to run the pipeline
- [ x ] The `.nextflow.log` file

## System

- Hardware: HPC
- Executor: slurm
- OS: CentOS
- Version  7.9

## Nextflow Installation

- Version: 21.04.1

## Container engine

- Engine: Singularity
- version: 3.7
- Image tag: nfcore/sarek:2.7.1

Should I increase the java memory options or is it a bug?",jacorvar,https://github.com/nf-core/sarek/issues/556
I_kwDOCvwIC85KfXar,create  panel of normals with the pipeline,CLOSED,2022-05-26T15:42:56Z,2023-08-15T10:48:39Z,2023-08-15T10:48:39Z,"Hello, when I do somatic variants calling, the pineline suggested to have panel of normals for GATK Mutect2, but I am just wondering that it seems I couldn't produce PON by Sarek. Therefore, is that true that I should go back to GATK to create PON, or the Sarek would do that? Thanks!
",sabrina0701,https://github.com/nf-core/sarek/issues/565
I_kwDOCvwIC85Kq_Rr,[BUG] Error executing process but process finishes successfully,CLOSED,2022-05-30T14:00:08Z,2022-08-30T14:58:08Z,2022-08-30T14:58:08Z,"Hi, 

I have been running the pipeline on tumor/normal WGS data to do both mapping and now variant calling. Recently I haven been running into many of the same issues at different steps, with different callers and different samples. What I see happening is that I get an email that the pipeline has crashed on a particular step, usually with the information ""Command exit status:  -, Command output: (empty)"". But then I check the working directory indicated in the email and the particular process that is supposedly affected has continued to run for some time after this error has been raised and finishes with an exit code 0. I have also checked the slurm stats of such jobs and according to those the jobs are completed successfully. I am providing an example error message and the contents of the command files below. I'd appreciate any help with what could be causing these problems. 

Error message 17/5/2022, 15:00:

```
nf-core/sarek execution completed unsuccessfully!

The exit status of the task that caused the workflow execution to fail was: null.

The full error message was:

Error executing process > 'StrelkaBP (T-CCB070010_vs_N-CCB070010)'

Caused by:
  Process `StrelkaBP (T-CCB070010_vs_N-CCB070010)` terminated for an unknown reason -- Likely it has been terminated by the external system

Command executed:

  configureStrelkaSomaticWorkflow.py         --tumor T-CCB070010.recal.reheader.bam         --normal N-CCB070010.recal.reheader.bam         --referenceFasta cf4.b6.14.fa         --indelCandidates Manta_T-CCB070010_vs_N-CCB070010.candidateSmallIndels.vcf.gz                  --runDir Strelka
  
  python Strelka/runWorkflow.py -m local -j 4
  
  mv Strelka/results/variants/somatic.indels.vcf.gz         StrelkaBP_T-CCB070010_vs_N-CCB070010_somatic_indels.vcf.gz
  mv Strelka/results/variants/somatic.indels.vcf.gz.tbi         StrelkaBP_T-CCB070010_vs_N-CCB070010_somatic_indels.vcf.gz.tbi
  mv Strelka/results/variants/somatic.snvs.vcf.gz         StrelkaBP_T-CCB070010_vs_N-CCB070010_somatic_snvs.vcf.gz
  mv Strelka/results/variants/somatic.snvs.vcf.gz.tbi         StrelkaBP_T-CCB070010_vs_N-CCB070010_somatic_snvs.vcf.gz.tbi

Command exit status:
  -

Command output:
  (empty)

Command wrapper:
  [2022-05-17T12:36:09.193251Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2086_chrUn_S895H1059_0000_to_chromId_2090_chrUn_S899H1063_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.193546Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2086_chrUn_S895H1059_0000_to_chromId_2090_chrUn_S899H1063_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.194066Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2091_chrUn_S89H248_0000_to_chromId_2095_chrUn_S903H1067_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.194368Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2091_chrUn_S89H248_0000_to_chromId_2095_chrUn_S903H1067_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.194913Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2096_chrUn_S904H1068_0000_to_chromId_2101_chrUn_S909H1073_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.233351Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2096_chrUn_S904H1068_0000_to_chromId_2101_chrUn_S909H1073_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.281002Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2102_chrUn_S90H249_0000_to_chromId_2106_chrUn_S914H1079_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.347781Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2102_chrUn_S90H249_0000_to_chromId_2106_chrUn_S914H1079_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.424089Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2107_chrUn_S916H1081_0000_to_chromId_2111_chrUn_S91H250_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.424373Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2107_chrUn_S916H1081_0000_to_chromId_2111_chrUn_S91H250_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.424825Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2112_chrUn_S920H1085_0000_to_chromId_2115_chrUn_S923H1089_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.425054Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2112_chrUn_S920H1085_0000_to_chromId_2115_chrUn_S923H1089_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.425539Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2116_chrUn_S924H1090_0000_to_chromId_2121_chrUn_S929H1095_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.425777Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2116_chrUn_S924H1090_0000_to_chromId_2121_chrUn_S929H1095_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.426237Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2122_chrUn_S92H251_0000_to_chromId_2126_chrUn_S933H1099_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.426468Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2122_chrUn_S92H251_0000_to_chromId_2126_chrUn_S933H1099_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.427047Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2127_chrUn_S934H1100_0000_to_chromId_2132_chrUn_S939H1105_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.427286Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2127_chrUn_S934H1100_0000_to_chromId_2132_chrUn_S939H1105_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.427769Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2133_chrUn_S93H252_0000_to_chromId_2137_chrUn_S943H1109_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.428000Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2133_chrUn_S93H252_0000_to_chromId_2137_chrUn_S943H1109_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.428465Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2138_chrUn_S944H1110_0000_to_chromId_2143_chrUn_S949H1115_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.428696Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2138_chrUn_S944H1110_0000_to_chromId_2143_chrUn_S949H1115_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.429150Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2144_chrUn_S94H253_0000_to_chromId_2148_chrUn_S953H1119_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.429379Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2144_chrUn_S94H253_0000_to_chromId_2148_chrUn_S953H1119_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.429957Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2149_chrUn_S954H1120_0000_to_chromId_2153_chrUn_S95H254_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.430194Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2149_chrUn_S954H1120_0000_to_chromId_2153_chrUn_S95H254_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.430714Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2154_chrUn_S960H1126_0000_to_chromId_2159_chrUn_S965H1131_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.430949Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2154_chrUn_S960H1126_0000_to_chromId_2159_chrUn_S965H1131_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.431403Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2160_chrUn_S966H1132_0000_to_chromId_2164_chrUn_S96H255_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.431634Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2160_chrUn_S966H1132_0000_to_chromId_2164_chrUn_S96H255_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.432202Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2165_chrUn_S970H1136_0000_to_chromId_2170_chrUn_S975H1141_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.432502Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2165_chrUn_S970H1136_0000_to_chromId_2170_chrUn_S975H1141_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.433032Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2171_chrUn_S976H1142_0000_to_chromId_2175_chrUn_S97H256_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.433313Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2171_chrUn_S976H1142_0000_to_chromId_2175_chrUn_S97H256_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.433847Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2176_chrUn_S980H1146_0000_to_chromId_2181_chrUn_S985H1151_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.434145Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2176_chrUn_S980H1146_0000_to_chromId_2181_chrUn_S985H1151_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.434677Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2182_chrUn_S986H1152_0000_to_chromId_2186_chrUn_S98H257_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.434972Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2182_chrUn_S986H1152_0000_to_chromId_2186_chrUn_S98H257_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.435501Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2187_chrUn_S990H1157_0000_to_chromId_2192_chrUn_S995H1162_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.435809Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2187_chrUn_S990H1157_0000_to_chromId_2192_chrUn_S995H1162_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.436314Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+callGenomeSegment_chromId_2193_chrUn_S996H1163_0000_to_chromId_2197_chrUn_S99H258_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.437239Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+compressSegmentOutput_chromId_2193_chrUn_S996H1163_0000_to_chromId_2197_chrUn_S99H258_0000' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.437523Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+completedAllGenomeSegments' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.443677Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+SNV_concat_vcf' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.483453Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+SNV_index_vcf' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.485174Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+Indel_concat_vcf' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.485406Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+Indel_index_vcf' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.488626Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+mergeRunStats' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.489194Z] [s118.uppmax.uu.se] [60986_1] [WorkflowRunner] Adding command task 'CallGenome+removeTmpDir' to sub-workflow 'CallGenome'
  [2022-05-17T12:36:09.489458Z] [s118.uppmax.uu.se] [60986_1] [TaskRunner:CallGenome] Finished task specification for sub-workflow

Work dir:
  /crex/proj/uppstore2017228/KLT.05.GRUS/TK-2771_2/run/work/de/8376ebf4fc403489a62e5eaf0b2b90

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

```
.exitcode

0

.command.out

Successfully created workflow run script.
To execute the workflow, run the following script and set appropriate options:

/scratch/6216633/nxf.n1PHON6g9F/Strelka/runWorkflow.py

.command.log
[command.log.txt](https://github.com/nf-core/sarek/files/8798945/command.log.txt)

.command.err 
[command.err.txt](https://github.com/nf-core/sarek/files/8798950/command.err.txt)

",rpensch,https://github.com/nf-core/sarek/issues/568
I_kwDOCvwIC85KucGK,Use GATK4 module mergeVcfs ,CLOSED,2022-05-31T10:51:36Z,2022-06-06T18:02:34Z,2022-06-06T18:02:34Z,"Use GATK4's module mergeVcfs for merge vcf-files in Sarek.

Currently, Sarek uses the nextflow-module `modules/local/concat_vcf/main.nf` for merging vcf-file. That module wraps the bash-script `bin/concatenateVCFs.sh`.

This change was suggested by @FriederikeHanssen .",asp8200,https://github.com/nf-core/sarek/issues/569
I_kwDOCvwIC85K25p_,[BUG]: VEP version in output vcf and HTML summary report is not updated as per the vep_cache_version provided by the user. ,CLOSED,2022-06-01T12:44:23Z,2022-06-15T12:20:09Z,2022-06-15T12:20:09Z,"Hi there!
Am running nf-core sarek pipeline for germline variants and using VEP for variant annotation using cache file (version 106). I use the flags --annotation_cache with --vep_cache_version 106 --vep_cache <file path>. The pipeline is executed successfully. However in my output vcf file and HTML report, I observe that the version is mentioned as ""v99"", although the cache file path refers to the 106version file. 

## Run command 
nextflow -log germline_vep106.log -bg run nf-core/sarek -profile docker -with-report -with-trace -with-timeline --genome GRCh38 --input input.vcf.gz --outdir ./germline_vep106 --tools vep --vep_cache /vep_cache/ --annotation_cache --vep_cache_version \""106\"" --cpus 64

## Issue observed
In the output vcf file -
##**VEP=""v99""** time=""2022-05-18 07:23:04"" cache=""/homo_sapiens/106_GRCh38"" ensembl-funcgen=99.0832337 ensembl=99.d3e7d31 ensembl-variation=99.642e1cd ensembl-io=99.441b05b 1000genomes=""phase3"" COSMIC=""92"" ClinVar=""202109"" ESP=""V2-SSA137"" HGMD-PUBLIC=""20204"" assembly=""GRCh38.p13"" dbSNP=""154"" gencode=""GENCODE 40"" genebuild=""2014-07"" gnomAD=""r2.1.1"" polyphen=""2.2.2"" regbuild=""1.0"" sift=""sift5.2.2""

## Expected output
Right version to be shown on the outputs. 


",IndhuPS,https://github.com/nf-core/sarek/issues/573
I_kwDOCvwIC85K27Fo,[FEATURE]: MultiQC does not include VEP reports,CLOSED,2022-06-01T12:47:05Z,2022-06-09T15:00:51Z,2022-06-09T15:00:51Z,"Hi,

I am running nf-core Sarek (installed using docker) and am using VEP for variant annotations. The pipeline is successfully completed, but the multiQC report summary does not include the VEP results. 

The version of multiQC (from docker) is 1.8 and I see that VEP section is included only in higher versions. Is there anyway to upgrade the multiQC version in the docker to include the VEP results as well? ",IndhuPS,https://github.com/nf-core/sarek/issues/574
I_kwDOCvwIC85K28-3,[FEATURE]: VEP output configuration to get the results in tab format instead of VCF,CLOSED,2022-06-01T12:50:36Z,2022-06-17T07:12:52Z,2022-06-17T07:12:51Z,"Hi, 
Using the VEP tool for variant annotation and wondering if we can configure the output format to ""tab"" instead of ""vcf"". VEP tool allows 3 different output formats (vcf, tab and json) and giving this option to configure would help users to choose their desired format. ",IndhuPS,https://github.com/nf-core/sarek/issues/575
I_kwDOCvwIC85LSEsp,Create an option to have .pileup files be transient (not retained in the cache),CLOSED,2022-06-07T09:50:10Z,2022-06-17T14:16:48Z,2022-06-17T14:16:48Z,"As part of the Control-FREEC process, very large uncompressed pileup files are generated and these are by default exported to the final results directory, as well as retained in the cache, and no option seems to exist to prevent this. Given the size of these files (ca 300GB), a run of more than a handful of WGS samples will more than fill about 5-6 TB of cache space and most likely the same amount of space in the results directory (unless --publish_dir_mode is changed to move or symlink). In my case the amount of space taken up in the work directory for the whole run on 16 samples (half of which are pileup files) is enough to cause the pipeline to crash due to reaching the physical limits of a 12TB hard drive entirely dedicated as work directory. I suspect not all users will be interested in retaining these files, which are mostly a means to an end to make Control-FREEC run. Therefore, it would be good if either these were deleted within the relevant process (so that they are not retained in the work directory) or if there was an option to do so. They are relatively easy to recreate from the BAM files later on if needed.",jowkar,https://github.com/nf-core/sarek/issues/579
I_kwDOCvwIC85LudQG,[FEATURE]: Update example samplesheet in `assets`,CLOSED,2022-06-14T08:50:47Z,2022-06-14T08:52:32Z,2022-06-14T08:52:32Z,"Hi,

Would you see fit to add an example samplesheet in `assets` with all possible fields defined for each possible input type?
function `extract_csv` parses a bunch of fields for different inputs, but I don't see it documented anywhere.

Thanks
Matthias
",matthdsm,https://github.com/nf-core/sarek/issues/588
I_kwDOCvwIC85Luj6H,[BUG] Sarek-dev skips directly to multiqc after interval split,CLOSED,2022-06-14T09:10:51Z,2022-06-17T07:59:43Z,2022-06-17T07:59:43Z,"<!--
# nf-core/sarek bug report

Hi there!

Thanks for telling us about a problem with the pipeline.
Please delete this text and anything that's not relevant from the template below:
-->

## Check Documentation

I have checked the following places for your error:

- [x ] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [x ] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

When running the current sarek dev branch with a csv formatted like this 
```
HG0001,XX,0,HG0001,1,/home/ubuntu/U0a_CGATGT_L002/U0a_CGATGT_L002_R1_001.fastq.gz,/home/ubuntu/U0a_CGATGT_L002/U0a_CGATGT_L002_R2_001.fastq.gz 
```
 the process starts fine, but then after `NFCORE_SAREK:sarek:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT` it jumps straight to multiQC and makes no useful output. Further discussion in [this](https://nfcore.slack.com/archives/C02MDBZAYJK/p1655121583870879) thread and [this](https://nfcore.slack.com/archives/C02MDBZAYJK/p1655193790239759).

It's probably because the header line is missing

## Steps to reproduce


```
#Minimal reproducible example

#1 start aws c5.24xlarge 96-core machine using ECS-optimized AWS image (has docker etc)

#2 get input data
Download U0a_CGATGT_L002_R1_001.fastq.gz (and R2) from GiAB web-site


#3 get miniconda
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
sh Miniconda3-latest-Linux-x86_64.sh


#4 install nextflow (22.04.3) using curl instructions from www.nextflow.com
curl -fsSL get.nextflow.io | bash
mkdir bin
mv nextflow ~/bin/



#5 get Sarek 
git clone git@github.com:nf-core/sarek.git 
cd sarek
git checkout dev (hash 7b304c3f9a6e4257bb65176155740b66ea85ec81)
git checkout 4420a02d15be691f54cf6d5ef68dcf4acc0d6ba8

#6 make a samples.csv that looks like this
HG0001,XX,0,HG0001,1,/home/ubuntu/U0a_CGATGT_L002/U0a_CGATGT_L002_R1_001.fastq.gz,/home/ubuntu/U0a_CGATGT_L002/U0a_CGATGT_L002_R2_001.fastq.gz 


#7 run 
nextflow run main.nf \
-profile docker -work-dir ~/data/work/‘ \
-—tools strelka \
—vep_cache_version 104
```
## Expected behaviour

Don't silently skip all the alignment and calling steps.

## Log files

Have you provided the following extra information/files:

- [x] The command used to run the pipeline
- [ ] The `.nextflow.log` file <!-- this is a hidden file in the directory where you launched the pipeline -->

## System

- Hardware: c5.24xlarge 96-core
- Executor: tested with local and awsbatch
- OS: Ubuntu Linux AWS 
- Version: 22.04 LTS

## Nextflow Installation

- Version: (22.04.3)

## Container engine

- Engine: docker
- version:  20.10.12


## Additional context

It would be nice if a user-friendly error message explaining what to do.
",lassefolkersen,https://github.com/nf-core/sarek/issues/589
I_kwDOCvwIC85McZRF,Bonus feature: check that the tools actual fit for the data (i.e. HAplotypecaller should cause a failure if only status tumor in samplesheet),CLOSED,2022-06-23T14:35:54Z,2022-07-13T07:13:53Z,2022-07-13T07:13:18Z,,FriederikeHanssen,https://github.com/nf-core/sarek/issues/603
I_kwDOCvwIC85Mn_dn,Failed to pull singularity image,CLOSED,2022-06-27T09:56:04Z,2022-06-30T08:41:17Z,2022-06-30T08:41:17Z,"Hi there, 

Maybe I'm misconsidering something but I have some problems when running the last version (2.7.2) of _sarek_ (_Nextflow_ version 21.10.6).

The command,

`nextflow run nf-core/sarek -r 2.7.2 -params-file myparams.yaml -profile singularity -c mynextflow.config`

myparams.yaml,
```
## input/output options
input          : ""/home/jgarces/projects/GEMRNA.WES/nxf_tests_3/samples.tsv""

## main options
target_bed     : ""/home/jgarces/genomes/TWIST/Twist_Exome_RefSeq_targets_hg38.bed""

## preprocessing options
save_bam_mapped: false

## reference genome options
igenomes_base  : /home/jgarces/genomes/iGenomes/

## variant calling and annotation
tools          : ASCAT,Control-FREEC,Manta,MSIsensor,Mutect2, Strelka,snpEff,VEP
```

mynexflow.config, 
```
process {
        executor  = 'slurm'
        queue     = 'medium'
        clusterOptions = ""--exclude=nodo11""
        time = 21.h
}

executor.queueSize = 6 //array limit
```

and the error log:
```
nf-core/sarek execution completed unsuccessfully!
The exit status of the task that caused the workflow execution to fail was: null.

The full error message was:

Error executing process > 'VEP (377_SP - Strelka - Strelka_377_SP_variants.vcf.gz)'

Caused by:
  Failed to pull singularity image
  command: singularity pull  --name nfcore-sarekvep-2.7.2.GRCh38.img.pulling.1656111108337 docker://nfcore/sarekvep:2.7.2.GRCh38 > /dev/null
  status : 143
  message:
    Activating Modules:
      1) Java/13.0.2     2) Nextflow/21.10.6
    
     [34mINFO:    [0m Starting build...
    Getting image source signatures
    Copying blob sha256:68ced04f60ab5c7a5f1d0b0b4e7572c5a4c8cce44866513d30d9df1a15277d6b
    
     0 B / 25.84 MiB [-------------------------------------------------------------]
     1.00 MiB / 25.84 MiB [==>-----------------------------------------------------]
     1.50 MiB / 25.84 MiB [===>----------------------------------------------------]
     2.00 MiB / 25.84 MiB [====>---------------------------------------------------]
     4.19 MiB / 25.84 MiB [=========>----------------------------------------------]
     (...)
     25.84 MiB / 25.84 MiB [====================================================] 3s
    Copying blob sha256:9c388eb6d33c40662539172f0d9a357287bd1cd171692ca5c08db2886bc527c3
    
     0 B / 76.50 MiB [-------------------------------------------------------------]
     1.01 MiB / 76.50 MiB [>-------------------------------------------------------]
     2.54 MiB / 76.50 MiB [=>------------------------------------------------------]
     (...)
     76.50 MiB / 76.50 MiB [====================================================] 8s
    Copying blob sha256:96cf53b3a9dd496f4c91ab86eeadca2c8a31210c2e5c2a82badbb0dcf8c8f76b
    
     0 B / 48.03 MiB [-------------------------------------------------------------]
     1.00 MiB / 48.03 MiB [=>------------------------------------------------------]
     (...)
     48.03 MiB / 48.03 MiB [====================================================] 5s
    Copying blob sha256:3fbba84366ddeb8b6dece9fb1c547db6afa6e727c4bfe599a5ca2cfdff4c964e
    
     0 B / 761.52 KiB [------------------------------------------------------------]
     761.52 KiB / 761.52 KiB [=====================================================]
     761.52 KiB / 761.52 KiB [==================================================] 0s
    Copying blob sha256:4d6e4d839cd155e0308bf249833dcccb7cfae00466cc98516b015f623cc4f086
    
     0 B / 262 B [-----------------------------------------------------------------]
     262 B / 262 B [============================================================] 0s
    Copying blob sha256:d1d7141290df2a590d413f1cf436ac7089297f9f5df25673162b6a1773049c8c
    
     0 B / 349.36 MiB [------------------------------------------------------------]
     (...)
     349.36 MiB / 349.36 MiB [=================================================] 38s
    Copying blob sha256:bf8443906c36e2794392a902931ad474b8c490fd7a69a237312b67930befcec6
    
     0 B / 13.79 GiB [-------------------------------------------------------------]
     1.46 MiB / 13.79 GiB [>-------------------------------------------------------]
    (...)
     3.56 GiB / 13.79 GiB [==============>-----------------------------------------]
     (...)
     6.28 GiB / 13.79 GiB [=========================>------------------------------]
     (...)
     7.86 GiB / 13.79 GiB [===============================>------------------------]
```

Any idea about what I'm missing, please? Thanks in advance.",jgarces02,https://github.com/nf-core/sarek/issues/607
I_kwDOCvwIC85M1MOy,[BUG] UMI test profile stops at FASTQTOBAM,CLOSED,2022-06-29T17:04:35Z,2022-07-17T16:01:15Z,2022-07-17T16:01:15Z,"

## Check Documentation

I have checked the following places for your error:

- [X] [nf-core website: troubleshooting](https://nf-co.re/usage/troubleshooting)
- [X] [nf-core/sarek pipeline documentation](https://nf-co.re/sarek/usage)

## Description of the bug

This in regards to the development version (3.0). I have discussed this with the author's in the sarek slack channel, but figured I would also put in an issue here. 

The umi test profile runs the NFCORE_SAREK:SAREK:CREATE_UMI_CONSENSUS workflow, but currently only runs `FASTQTOBAM` and finishes. It doesn't proceed to `BAM2FASTQ`, or `MAPPING_UMI`, `SAMBLASTER`, `GROUPREADSBYUMI` or `CALLUMICONSENSUS`.



## Steps to reproduce

This happens when running the following command with nextflow v21.10.6

```bash
nextflow run nf-core/sarek  -r dev -profile test,umi,docker
```
The commit hash used was `ffd1100`


This is the output

```
[e7/dcc056] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX (genome.fasta)                       [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                      -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                  -
[83/329894] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY (genome.fasta)      [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                  -
[2f/dcbac4] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX (genome.fasta)                      [100%] 1 of 1 ✔
[8a/5a9418] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP (dbsnp_146.hg38.vcf)                   [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                            -
[58/01dbca] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS (mills_and_1000G.indels.vcf)    [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                                          -
[72/7f8f5f] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (genome.interval_list)     [100%] 1 of 1 ✔
[9a/59be0c] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED (genome)                [100%] 1 of 1 ✔
[05/8cc149] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (chr22_1-40001) [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP                    -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP                -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP                  -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP                  -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP                     -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_INPUT:COLLATE_FASTQ_UNMAP                      -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_INPUT:COLLATE_FASTQ_MAP                        -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_INPUT:CAT_FASTQ                                -
[4c/441391] process > NFCORE_SAREK:SAREK:RUN_FASTQC:FASTQC (test-test_L1)                                  [100%] 1 of 1 ✔
[29/46486b] process > NFCORE_SAREK:SAREK:CREATE_UMI_CONSENSUS:FASTQTOBAM (test-test_L1)                    [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:CREATE_UMI_CONSENSUS:BAM2FASTQ                                    -
[-        ] process > NFCORE_SAREK:SAREK:CREATE_UMI_CONSENSUS:MAPPING_UMI:BWAMEM1_MEM                      -
[-        ] process > NFCORE_SAREK:SAREK:CREATE_UMI_CONSENSUS:MAPPING_UMI:BWAMEM2_MEM                      -
[-        ] process > NFCORE_SAREK:SAREK:CREATE_UMI_CONSENSUS:MAPPING_UMI:DRAGMAP_ALIGN                    -
[-        ] process > NFCORE_SAREK:SAREK:CREATE_UMI_CONSENSUS:SAMBLASTER                                   -
[-        ] process > NFCORE_SAREK:SAREK:CREATE_UMI_CONSENSUS:GROUPREADSBYUMI                              -
[-        ] process > NFCORE_SAREK:SAREK:CREATE_UMI_CONSENSUS:CALLUMICONSENSUS                             -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_UMI:SAMTOOLS_VIEW_MAP_MAP                      -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_UMI:SAMTOOLS_VIEW_UNMAP_UNMAP                  -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_UMI:SAMTOOLS_VIEW_UNMAP_MAP                    -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_UMI:SAMTOOLS_VIEW_MAP_UNMAP                    -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_UMI:SAMTOOLS_MERGE_UNMAP                       -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_UMI:COLLATE_FASTQ_UNMAP                        -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_UMI:COLLATE_FASTQ_MAP                          -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMENT_TO_FASTQ_UMI:CAT_FASTQ                                  -
[-        ] process > NFCORE_SAREK:SAREK:GATK4_MAPPING:BWAMEM1_MEM                                         -
[-        ] process > NFCORE_SAREK:SAREK:GATK4_MAPPING:BWAMEM2_MEM                                         -
[-        ] process > NFCORE_SAREK:SAREK:GATK4_MAPPING:DRAGMAP_ALIGN                                       -
[-        ] process > NFCORE_SAREK:SAREK:MARKDUPLICATES:GATK4_MARKDUPLICATES                               -
[-        ] process > NFCORE_SAREK:SAREK:MARKDUPLICATES:BAM_TO_CRAM:SAMTOOLS_BAMTOCRAM                     -
[-        ] process > NFCORE_SAREK:SAREK:MARKDUPLICATES:BAM_TO_CRAM:SAMTOOLS_STATS_CRAM                    -
[-        ] process > NFCORE_SAREK:SAREK:MARKDUPLICATES:BAM_TO_CRAM:MOSDEPTH                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_RECALIBRATION:BASERECALIBRATOR                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_RECALIBRATION:GATHERBQSRREPORTS                           -
[-        ] process > NFCORE_SAREK:SAREK:RECALIBRATE:APPLYBQSR                                             -
[-        ] process > NFCORE_SAREK:SAREK:RECALIBRATE:MERGE_INDEX_CRAM:MERGE_CRAM                           -
[-        ] process > NFCORE_SAREK:SAREK:RECALIBRATE:MERGE_INDEX_CRAM:INDEX_CRAM                           -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_QC:SAMTOOLS_STATS                                            -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_QC:MOSDEPTH                                                  -
[-        ] process > NFCORE_SAREK:SAREK:SAMTOOLS_CRAMTOBAM_RECAL                                          -
[8e/d69cfd] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS (1)                                   [100%] 1 of 1 ✔
[e8/1eb686] process > NFCORE_SAREK:SAREK:MULTIQC                                                           [100%] 1 of 1 ✔
-[nf-core/sarek] Pipeline completed successfully-
```



## Log files

[dev_ffd1100.nextflow.log](https://github.com/nf-core/sarek/files/9012619/dev_ffd1100.nextflow.log)


",tijeco,https://github.com/nf-core/sarek/issues/611
I_kwDOCvwIC85M4L08,mcontainer creation failed,CLOSED,2022-06-30T08:40:55Z,2023-05-26T07:18:26Z,2023-05-26T07:18:26Z,"_Originally posted by @jgarces02 in https://github.com/nf-core/sarek/issues/607#issuecomment-1169721779_

> I'm having another error, this time it's different and can be caused by [_singularity_'s version](https://github.com/nf-core/eager/issues/95). I'm going to update it and I'll tell you ASAP, thanks for your patience.

 ```
Error executing process > 'PileupSummariesForMutect2 (350_SP-chr20_31051509-31107036)'

Caused by:
  Process `PileupSummariesForMutect2 (350_SP-chr20_31051509-31107036)` terminated with an error exit status (255)

Command executed:

  gatk --java-options ""-Xmx7g""         GetPileupSummaries         -I 350_SP.recal.bam         -V gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz         -L chr20_31051509-31107036.bed         -O chr20_31051509-31107036_350_SP_pileupsummaries.table

Command exit status:
  255

Command output:
  (empty)

Command error:
  FATAL:   mcontainer creation failed: failed to add  as session directory: path . is not an absolute path
```

> Nope, it's not because the singularity's version. Any idea about what's happening?",maxulysse,https://github.com/nf-core/sarek/issues/612
I_kwDOCvwIC85NJvbr,TIDDIT output folder named `null`,CLOSED,2022-07-05T14:42:55Z,2022-07-14T20:46:38Z,2022-07-14T20:46:38Z,"As described on SLack, the output folder of the current `dev` branch for TIDDIT when annotated is simply called `null`.",apeltzer,https://github.com/nf-core/sarek/issues/621
I_kwDOCvwIC85NNg-N,Cannot invoke method toInteger() on null object,CLOSED,2022-07-06T07:54:04Z,2022-07-06T10:57:20Z,2022-07-06T10:55:12Z,"I am getting this error:

Cannot invoke method toInteger() on null object

 -- Check script '/home/ubuntu/.nextflow/assets/nf-core/sarek/main.nf' at line: 4183 or see '.nextflow.log' file for more details


here the complet log:

(base) ubuntu@pipetest:/mnt/quobyte/nextflow_real_data_test$ cat .nextflow.log
Jul-06 07:39:30.936 [main] DEBUG nextflow.cli.Launcher - $> nextflow run nf-core/sarek -dsl1 -name sarek_7 -profile docker -params-file nf-params.json
Jul-06 07:39:31.081 [main] INFO  nextflow.cli.CmdRun - N E X T F L O W  ~  version 22.04.3
Jul-06 07:39:31.979 [main] DEBUG nextflow.scm.AssetManager - Git config: /home/ubuntu/.nextflow/assets/nf-core/sarek/.git/config; branch: master; remote: origin; url: https://github.com/nf-core/sarek.git
Jul-06 07:39:31.991 [main] DEBUG nextflow.scm.AssetManager - Git config: /home/ubuntu/.nextflow/assets/nf-core/sarek/.git/config; branch: master; remote: origin; url: https://github.com/nf-core/sarek.git
Jul-06 07:39:32.473 [main] INFO  nextflow.scm.AssetManager - NOTE: Your local project version looks outdated - a different revision is available in the remote repository [e8f56e5bbb]
Jul-06 07:39:32.484 [main] DEBUG nextflow.config.ConfigBuilder - Found config base: /home/ubuntu/.nextflow/assets/nf-core/sarek/nextflow.config
Jul-06 07:39:32.485 [main] DEBUG nextflow.config.ConfigBuilder - Parsing config file: /home/ubuntu/.nextflow/assets/nf-core/sarek/nextflow.config
Jul-06 07:39:32.503 [main] DEBUG nextflow.config.ConfigBuilder - Applying config profile: `docker`
Jul-06 07:39:32.849 [main] DEBUG nextflow.plugin.PluginsFacade - Using Default plugins manager
Jul-06 07:39:32.861 [main] INFO  org.pf4j.DefaultPluginStatusProvider - Enabled plugins: []
Jul-06 07:39:32.862 [main] INFO  org.pf4j.DefaultPluginStatusProvider - Disabled plugins: []
Jul-06 07:39:32.866 [main] INFO  org.pf4j.DefaultPluginManager - PF4J version 3.4.1 in 'deployment' mode
Jul-06 07:39:32.869 [main] DEBUG nextflow.plugin.PluginsFacade - Using Default plugins manager
Jul-06 07:39:33.065 [main] DEBUG nextflow.plugin.PluginsFacade - Using Default plugins manager
Jul-06 07:39:33.065 [main] DEBUG nextflow.plugin.PluginsFacade - Using Default plugins manager
Jul-06 07:39:33.251 [main] DEBUG nextflow.config.ConfigBuilder - Available config profiles: [cfc_dev, ifb_core, denbi_qbic, genotoul, alice, test_annotation, mjolnir_globe, uppmax, abims, test_tool, nihbiowulf, nu_genomics, oist, sahmri, mpcdf, leicester, lugh, vsc_ugent, sage, cambridge, unibe_ibu, vai, podman, test_umi_qiaseq, czbiohub_aws, jax, cheaha, ccga_med, test, google, computerome, seg_globe, sanger, pasteur, eddie, test_umi_tso, azurebatch, bi, bigpurple, test_trimming, cedars, docker, gis, eva, test_use_gatk_spark, utd_ganymede, charliecloud, fgcz, conda, singularity, icr_davros, munin, rosalind, prince, hasta, hebbe, cfc, utd_sysbio, uzh, debug, genouest, cbe, ebc, ccga_dx, crick, test_split_fastq, marvin, phoenix, biohpc_gen, shifter, awsbatch, uct_hpc, test_targeted, imperial, maestro, aws_tower, binac]
Jul-06 07:39:33.291 [main] DEBUG nextflow.cli.CmdRun - Applied DSL=null from config declaration
Jul-06 07:39:33.292 [main] INFO  nextflow.cli.CmdRun - Launching `https://github.com/nf-core/sarek` [sarek_7] DSL1 - revision: 68b9930a74 [master]
Jul-06 07:39:33.293 [main] DEBUG nextflow.plugin.PluginsFacade - Setting up plugin manager > mode=prod; plugins-dir=/home/ubuntu/.nextflow/plugins; core-plugins: nf-amazon@1.7.2,nf-azure@0.13.2,nf-console@1.0.3,nf-ga4gh@1.0.3,nf-google@1.1.4,nf-sqldb@0.4.0,nf-tower@1.4.0
Jul-06 07:39:33.294 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]
Jul-06 07:39:33.302 [main] INFO  org.pf4j.DefaultPluginStatusProvider - Enabled plugins: []
Jul-06 07:39:33.302 [main] INFO  org.pf4j.DefaultPluginStatusProvider - Disabled plugins: []
Jul-06 07:39:33.304 [main] INFO  org.pf4j.DefaultPluginManager - PF4J version 3.4.1 in 'deployment' mode
Jul-06 07:39:33.315 [main] INFO  org.pf4j.AbstractPluginManager - No plugins
Jul-06 07:39:33.365 [main] DEBUG nextflow.Session - Session uuid: 41535480-1a5e-4fe4-8d0c-9913a2777fad
Jul-06 07:39:33.366 [main] DEBUG nextflow.Session - Run name: sarek_7
Jul-06 07:39:33.366 [main] DEBUG nextflow.Session - Executor pool size: 28
Jul-06 07:39:33.398 [main] DEBUG nextflow.cli.CmdRun - 
  Version: 22.04.3 build 5703
  Created: 18-05-2022 19:22 UTC 
  System: Linux 5.4.0-121-generic
  Runtime: Groovy 3.0.10 on OpenJDK 64-Bit Server VM 11.0.15+10-Ubuntu-0ubuntu0.20.04.1
  Encoding: UTF-8 (UTF-8)
  Process: 267282@pipetest [193.196.29.8]
  CPUs: 28 - Mem: 251.7 GB (202 GB) - Swap: 0 (0)
Jul-06 07:39:33.615 [main] DEBUG nextflow.Session - Work-dir: /mnt/quobyte/nextflow_real_data_test/work [fuseblk]
Jul-06 07:39:33.628 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]
Jul-06 07:39:33.640 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory
Jul-06 07:39:33.725 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory
Jul-06 07:39:33.735 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 29; maxThreads: 1000
Jul-06 07:39:34.288 [main] DEBUG nextflow.Session - Session start invoked
Jul-06 07:39:34.294 [main] DEBUG nextflow.trace.TraceFileObserver - Flow starting -- trace file: /mnt/quobyte/nextflow_real_data_test/results/pipeline_info/pipeline_dag_2022-07-06_07-39-33.svg
Jul-06 07:39:34.306 [main] DEBUG nextflow.Session - Using default localLib path: /home/ubuntu/.nextflow/assets/nf-core/sarek/lib
Jul-06 07:39:34.309 [main] DEBUG nextflow.Session - Adding to the classpath library: /home/ubuntu/.nextflow/assets/nf-core/sarek/lib
Jul-06 07:39:34.310 [main] DEBUG nextflow.Session - Adding to the classpath library: /home/ubuntu/.nextflow/assets/nf-core/sarek/lib/nfcore_external_java_deps.jar
Jul-06 07:39:39.521 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution
Jul-06 07:39:39.536 [main] INFO  nextflow.Nextflow - 

------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .´ _  `.
   /  |\`-_ \      __        __   ___     
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /¯¯\ |  \ |___ |  \
    `|____\´

  nf-core/sarek v2.7.1
------------------------------------------------------

Jul-06 07:39:39.809 [main] DEBUG nextflow.plugin.PluginUpdater - Installing plugin nf-amazon version: 1.7.2
Jul-06 07:39:39.820 [Actor Thread 5] ERROR nextflow.extension.OperatorEx - @unknown
java.lang.NullPointerException: Cannot invoke method toInteger() on null object
        at org.codehaus.groovy.runtime.NullObject.invokeMethod(NullObject.java:91)
        at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:44)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
        at org.codehaus.groovy.runtime.callsite.NullCallSite.call(NullCallSite.java:34)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:130)
        at Script_c3d27a41$_extractFastq_closure8.doCall(Script_c3d27a41:4183)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)
        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)
        at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)
        at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:38)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:139)
        at nextflow.extension.MapOp$_apply_closure1.doCall(MapOp.groovy:57)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)
        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)
        at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)
        at groovy.lang.Closure.call(Closure.java:412)
        at groovyx.gpars.dataflow.operator.DataflowOperatorActor.startTask(DataflowOperatorActor.java:120)
        at groovyx.gpars.dataflow.operator.DataflowOperatorActor.onMessage(DataflowOperatorActor.java:108)
        at groovyx.gpars.actor.impl.SDAClosure$1.call(SDAClosure.java:43)
        at groovyx.gpars.actor.AbstractLoopingActor.runEnhancedWithoutRepliesOnMessages(AbstractLoopingActor.java:293)
        at groovyx.gpars.actor.AbstractLoopingActor.access$400(AbstractLoopingActor.java:30)
        at groovyx.gpars.actor.AbstractLoopingActor$1.handleMessage(AbstractLoopingActor.java:93)
        at groovyx.gpars.util.AsyncMessagingCore.run(AsyncMessagingCore.java:132)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Jul-06 07:39:39.895 [Actor Thread 5] DEBUG nextflow.Session - Session aborted -- Cause: Cannot invoke method toInteger() on null object
Jul-06 07:39:39.916 [Actor Thread 5] DEBUG nextflow.Session - The following nodes are still active:
  [operator] map
  [operator] map

Jul-06 07:39:39.979 [main] INFO  org.pf4j.AbstractPluginManager - Plugin 'nf-amazon@1.7.2' resolved
Jul-06 07:39:39.980 [main] INFO  org.pf4j.AbstractPluginManager - Start plugin 'nf-amazon@1.7.2'
Jul-06 07:39:39.990 [main] DEBUG nextflow.plugin.BasePlugin - Plugin started nf-amazon@1.7.2
Jul-06 07:39:40.002 [main] DEBUG nextflow.file.FileHelper - > Added 'S3FileSystemProvider' to list of installed providers [s3]
Jul-06 07:39:40.003 [main] DEBUG nextflow.file.FileHelper - Started plugin 'nf-amazon' required to handle file: s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz
Jul-06 07:39:40.009 [main] DEBUG nextflow.file.FileHelper - Creating a file system instance for provider: S3FileSystemProvider
Jul-06 07:39:40.013 [main] DEBUG nextflow.file.FileHelper - AWS S3 config details: {max_error_retry=5}
Jul-06 07:39:40.315 [main] DEBUG c.u.s3fs.S3FileSystemProvider - Using S3 multi-part downloader
Jul-06 07:39:40.317 [main] DEBUG c.u.s3fs.ng.S3ParallelDownload - Creating S3 download thread pool: workers=10; chunkSize=10 MB; queueSize=10000; max-mem=1 GB; maxAttempts=5; maxDelay=1m 30s; pool-capacity=103
Jul-06 07:39:45.335 [main] INFO  nextflow.Nextflow - Core Nextflow options
  revision              : master
  runName               : sarek_7
  containerEngine       : docker
  container             : nfcore/sarek:2.7.1
  launchDir             : /mnt/quobyte/nextflow_real_data_test
  workDir               : /mnt/quobyte/nextflow_real_data_test/work
  projectDir            : /home/ubuntu/.nextflow/assets/nf-core/sarek
  userName              : ubuntu
  profile               : docker
  configFiles           : /home/ubuntu/.nextflow/assets/nf-core/sarek/nextflow.config

Input/output options
  input                 : example_pair_fastq.tsv

Main options
  tools                 : mpileup, VEP
  skip_qc               : null

Trim/split FASTQ
  clip_r1               : 0
  clip_r2               : 0
  three_prime_clip_r1   : 0
  three_prime_clip_r2   : 0
  trim_nextseq          : 0

Preprocessing
  markdup_java_options  : ""-Xms4000m -Xmx7g""

Variant Calling
  ascat_ploidy          : null
  ascat_purity          : null
  cf_contamination      : null
  cf_ploidy             : 2
  read_structure1       : null
  read_structure2       : null

Annotation
  annotate_tools        : null
  annotation_cache      : true
  cadd_indels           : false
  cadd_indels_tbi       : false
  cadd_wg_snvs          : false
  cadd_wg_snvs_tbi      : false
  snpeff_cache          : null
  vep_cache             : /mnt/quobyte/GDC_VEP_Cache_file

Reference genome options
  genome                : GRCh38
  bwa                   : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Sequence/BWAIndex/Homo_sapiens_assembly38.fasta.64.{alt,amb,ann,bwt,pac,sa}
  dbsnp                 : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz
  dbsnp_index           : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz.tbi
  dict                  : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.dict
  fasta                 : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta
  fasta_fai             : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai
  intervals             : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/intervals/wgs_calling_regions.hg38.bed
  known_indels          : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz
  known_indels_index    : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz.tbi
  species               : homo_sapiens
  vep_cache_version     : 99
  igenomes_base         : s3://ngi-igenomes/igenomes/
  genomes_base          : null

Generic options
  max_multiqc_email_size: 25 MB
  sequencing_center     : null

Max job request options
  single_cpu_mem        : 7 GB
  max_memory            : 128 GB
  max_time              : 10d

------------------------------------------------------
 Only displaying parameters that differ from defaults.
------------------------------------------------------
Jul-06 07:39:45.338 [main] INFO  nextflow.Nextflow - Pipeline Release  : master
Run Name          : sarek_7
Max Resources     : 128 GB memory, 16 cpus, 10d time per job
Container         : docker - nfcore/sarek:2.7.1
Output dir        : ./results
Launch dir        : /mnt/quobyte/nextflow_real_data_test
Working dir       : /mnt/quobyte/nextflow_real_data_test/work
Script dir        : /home/ubuntu/.nextflow/assets/nf-core/sarek
User              : ubuntu
Input             : example_pair_fastq.tsv
Step              : mapping
Genome            : GRCh38
Nucleotides/s     : 1000
Tools             : mpileup, vep
MarkDuplicates    : Options
Java options      : ""-Xms4000m -Xmx7g""
GATK Spark        : No
Save BAMs mapped  : No
Skip MarkDuplicates: No
Annotation cache  : Enabled
VEP cache         : /mnt/quobyte/GDC_VEP_Cache_file
AWS iGenomes base : s3://ngi-igenomes/igenomes/
Save Reference    : No
BWA indexes       : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Sequence/BWAIndex/Homo_sapiens_assembly38.fasta.64.{alt,amb,ann,bwt,pac,sa}
dbsnp             : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz
dbsnpIndex        : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz.tbi
dict              : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.dict
fasta reference   : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta
fasta index       : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai
intervals         : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/intervals/wgs_calling_regions.hg38.bed
known indels      : s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz
known indels index: s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz.tbi
species           : homo_sapiens
VEP cache version : 99
Publish dir mode  : copy
Config Profile    : docker
Config Files      : /home/ubuntu/.nextflow/assets/nf-core/sarek/nextflow.config
Jul-06 07:39:45.339 [main] INFO  nextflow.Nextflow - ----------------------------------------------------
Jul-06 07:39:45.395 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:get_software_versions` matches process get_software_versions
Jul-06 07:39:45.397 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:45.397 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:45.401 [main] DEBUG nextflow.executor.Executor - [warm up] executor > local
Jul-06 07:39:45.480 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=28; memory=251.7 GB; capacity=28; pollInterval=100ms; dumpInterval=5m
Jul-06 07:39:45.567 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:45.568 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:45.652 [Actor Thread 13] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/bin/scrape_software_versions.py
Jul-06 07:39:48.029 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:48.063 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:48.071 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:48.071 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:48.077 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:48.077 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:48.083 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:48.083 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:48.091 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:48.091 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:48.095 [main] DEBUG nextflow.processor.TaskProcessor - Creating *combiner* operator for each param(s) at index(es): [0]
Jul-06 07:39:48.367 [Actor Thread 17] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=4; maxSize=4; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false
Jul-06 07:39:50.370 [Actor Thread 17] INFO  nextflow.file.FilePorter - Staging foreign file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta
Jul-06 07:39:53.510 [FileTransfer-2] DEBUG nextflow.file.FilePorter - Copying foreign file s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz to work dir: /mnt/quobyte/nextflow_real_data_test/work/stage/15/7d71a4cfdc302d7b97be13c0c7a0f0/dbsnp_146.hg38.vcf.gz
Jul-06 07:39:54.127 [FileTransfer-1] DEBUG nextflow.file.FilePorter - Invalid cached stage path - deleting: /mnt/quobyte/nextflow_real_data_test/work/stage/f2/9157f24a272a74da64432f5b427f83/Homo_sapiens_assembly38.fasta
Jul-06 07:39:54.212 [FileTransfer-1] DEBUG nextflow.file.FilePorter - Copying foreign file s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta to work dir: /mnt/quobyte/nextflow_real_data_test/work/stage/f2/9157f24a272a74da64432f5b427f83/Homo_sapiens_assembly38.fasta
Jul-06 07:39:55.412 [FileTransfer-2] DEBUG c.u.s3fs.S3FileSystemProvider - S3 parallel download with direct buffer: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz
Jul-06 07:39:55.509 [Actor Thread 29] INFO  nextflow.file.FilePorter - Staging foreign file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz
Jul-06 07:39:57.106 [FileTransfer-3] DEBUG nextflow.file.FilePorter - Copying foreign file s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/beta/Homo_sapiens_assembly38.known_indels.vcf.gz to work dir: /mnt/quobyte/nextflow_real_data_test/work/stage/d2/8e67e19e2089a6d417b95c4c41c6f4/Homo_sapiens_assembly38.known_indels.vcf.gz
Jul-06 07:39:57.642 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.642 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.649 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.649 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.655 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.655 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.685 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_2` matches labels `FastQC,cpus_2` for process with name FastQCFQ
Jul-06 07:39:57.686 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:FastQC` matches labels `FastQC,cpus_2` for process with name FastQCFQ
Jul-06 07:39:57.686 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.686 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.691 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_2` matches labels `FastQC,cpus_2` for process with name FastQCBAM
Jul-06 07:39:57.691 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:FastQC` matches labels `FastQC,cpus_2` for process with name FastQCBAM
Jul-06 07:39:57.692 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.692 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.706 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.706 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.715 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.715 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.720 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.720 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.726 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.726 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.732 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.732 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.738 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max` for process with name MapReads
Jul-06 07:39:57.738 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MapReads` matches process MapReads
Jul-06 07:39:57.739 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.739 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.747 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_MapReads
Jul-06 07:39:57.747 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_MapReads
Jul-06 07:39:57.748 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.748 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.753 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_8` matches labels `cpus_8` for process with name MergeBamMapped
Jul-06 07:39:57.753 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.754 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.757 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_8` matches labels `cpus_8` for process with name IndexBamMergedForSentieon
Jul-06 07:39:57.758 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.758 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.762 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_8` matches labels `cpus_8` for process with name IndexBamFile
Jul-06 07:39:57.762 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.763 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.780 [Actor Thread 14] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:57.780 [Actor Thread 24] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:57.784 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_16` matches labels `cpus_16` for process with name MarkDuplicates
Jul-06 07:39:57.784 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.785 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.797 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_Dedup
Jul-06 07:39:57.798 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_Dedup
Jul-06 07:39:57.798 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.798 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.803 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name BaseRecalibrator
Jul-06 07:39:57.803 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.804 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.808 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_2` matches labels `memory_singleCPU_2_task,cpus_2` for process with name GatherBQSRReports
Jul-06 07:39:57.808 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_singleCPU_2_task` matches labels `memory_singleCPU_2_task,cpus_2` for process with name GatherBQSRReports
Jul-06 07:39:57.809 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.809 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.819 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_2` matches labels `memory_singleCPU_2_task,cpus_2` for process with name ApplyBQSR
Jul-06 07:39:57.819 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_singleCPU_2_task` matches labels `memory_singleCPU_2_task,cpus_2` for process with name ApplyBQSR
Jul-06 07:39:57.819 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.820 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.824 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_BQSR
Jul-06 07:39:57.825 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_BQSR
Jul-06 07:39:57.825 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.825 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.828 [Actor Thread 14] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:57.829 [Actor Thread 14] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:57.829 [Actor Thread 14] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:57.830 [Actor Thread 14] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:57.832 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_8` matches labels `cpus_8` for process with name MergeBamRecal
Jul-06 07:39:57.833 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.833 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.837 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_8` matches labels `cpus_8` for process with name IndexBamRecal
Jul-06 07:39:57.838 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.838 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.844 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_2` matches labels `cpus_2` for process with name SamtoolsStats
Jul-06 07:39:57.845 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.845 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.848 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_16` matches labels `memory_max,cpus_16` for process with name BamQC
Jul-06 07:39:57.849 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `memory_max,cpus_16` for process with name BamQC
Jul-06 07:39:57.849 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.849 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.854 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_2` matches labels `memory_singleCPU_task_sq,cpus_2` for process with name HaplotypeCaller
Jul-06 07:39:57.854 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_singleCPU_task_sq` matches labels `memory_singleCPU_task_sq,cpus_2` for process with name HaplotypeCaller
Jul-06 07:39:57.855 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.855 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.859 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.859 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.862 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_DNAseq
Jul-06 07:39:57.863 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_DNAseq
Jul-06 07:39:57.863 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.864 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.867 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_DNAscope
Jul-06 07:39:57.867 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_DNAscope
Jul-06 07:39:57.868 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.868 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.871 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max` for process with name StrelkaSingle
Jul-06 07:39:57.871 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max` for process with name StrelkaSingle
Jul-06 07:39:57.872 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.872 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.875 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max` for process with name MantaSingle
Jul-06 07:39:57.875 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max` for process with name MantaSingle
Jul-06 07:39:57.876 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.876 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.880 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.880 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.883 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name FreebayesSingle
Jul-06 07:39:57.884 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.884 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.895 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name FreeBayes
Jul-06 07:39:57.895 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.895 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.902 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name Mutect2
Jul-06 07:39:57.902 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.903 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.908 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name Mutect2Single
Jul-06 07:39:57.908 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.909 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.913 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.913 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.917 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_8` matches labels `concat_vcf,cpus_8` for process with name ConcatVCF
Jul-06 07:39:57.917 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:concat_vcf` matches labels `concat_vcf,cpus_8` for process with name ConcatVCF
Jul-06 07:39:57.918 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.918 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.919 [FileTransfer-1] DEBUG c.u.s3fs.S3FileSystemProvider - S3 parallel download with direct buffer: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta
Jul-06 07:39:57.922 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_8` matches labels `concat_vcf,cpus_8` for process with name ConcatVCF_Mutect2
Jul-06 07:39:57.922 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:concat_vcf` matches labels `concat_vcf,cpus_8` for process with name ConcatVCF_Mutect2
Jul-06 07:39:57.922 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.922 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.927 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name PileupSummariesForMutect2
Jul-06 07:39:57.927 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.927 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.931 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name MergePileupSummaries
Jul-06 07:39:57.931 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.931 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.935 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name CalculateContamination
Jul-06 07:39:57.935 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.935 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.940 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name FilterMutect2Calls
Jul-06 07:39:57.940 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.940 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.944 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_TNscope
Jul-06 07:39:57.944 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max,sentieon` for process with name Sentieon_TNscope
Jul-06 07:39:57.945 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.945 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.948 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.949 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.953 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max` for process with name Strelka
Jul-06 07:39:57.953 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max` for process with name Strelka
Jul-06 07:39:57.953 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.953 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.957 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max` for process with name Manta
Jul-06 07:39:57.958 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max` for process with name Manta
Jul-06 07:39:57.958 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.958 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.963 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_max` matches labels `cpus_max,memory_max` for process with name StrelkaBP
Jul-06 07:39:57.963 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_max,memory_max` for process with name StrelkaBP
Jul-06 07:39:57.964 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.964 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.968 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.968 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.970 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1,memory_max,msisensor` for process with name MSIsensor_scan
Jul-06 07:39:57.971 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_1,memory_max,msisensor` for process with name MSIsensor_scan
Jul-06 07:39:57.971 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:msisensor` matches labels `cpus_1,memory_max,msisensor` for process with name MSIsensor_scan
Jul-06 07:39:57.971 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.971 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.976 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_4` matches labels `cpus_4,memory_max,msisensor` for process with name MSIsensor_msi
Jul-06 07:39:57.976 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_4,memory_max,msisensor` for process with name MSIsensor_msi
Jul-06 07:39:57.977 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:msisensor` matches labels `cpus_4,memory_max,msisensor` for process with name MSIsensor_msi
Jul-06 07:39:57.977 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.977 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.981 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_4` matches labels `cpus_4,memory_max,msisensor` for process with name MSIsensor_msiSingle
Jul-06 07:39:57.981 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_max` matches labels `cpus_4,memory_max,msisensor` for process with name MSIsensor_msiSingle
Jul-06 07:39:57.981 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:msisensor` matches labels `cpus_4,memory_max,msisensor` for process with name MSIsensor_msiSingle
Jul-06 07:39:57.982 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.982 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.985 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_singleCPU_2_task` matches labels `memory_singleCPU_2_task` for process with name AlleleCounter
Jul-06 07:39:57.985 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.985 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.990 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_singleCPU_2_task` matches labels `memory_singleCPU_2_task` for process with name ConvertAlleleCounts
Jul-06 07:39:57.991 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.991 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.994 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_singleCPU_2_task` matches labels `memory_singleCPU_2_task` for process with name Ascat
Jul-06 07:39:57.995 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.995 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:57.998 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1,memory_singleCPU_2_task` for process with name Mpileup
Jul-06 07:39:57.998 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_singleCPU_2_task` matches labels `cpus_1,memory_singleCPU_2_task` for process with name Mpileup
Jul-06 07:39:57.999 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:57.999 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.006 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name MergeMpileup
Jul-06 07:39:58.007 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.007 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.013 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_8` matches labels `cpus_8` for process with name ControlFREEC
Jul-06 07:39:58.014 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.014 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.018 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_8` matches labels `cpus_8` for process with name ControlFREECSingle
Jul-06 07:39:58.019 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.019 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.022 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_singleCPU_2_task` matches labels `memory_singleCPU_2_task` for process with name ControlFreecViz
Jul-06 07:39:58.023 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.023 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.026 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:memory_singleCPU_2_task` matches labels `memory_singleCPU_2_task` for process with name ControlFreecVizSingle
Jul-06 07:39:58.026 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.026 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.034 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name BcftoolsStats
Jul-06 07:39:58.035 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.035 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.038 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_1` matches labels `cpus_1` for process with name Vcftools
Jul-06 07:39:58.038 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.039 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.043 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:Snpeff` matches process Snpeff
Jul-06 07:39:58.044 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.044 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.046 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.047 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.050 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_4` matches labels `VEP,cpus_4` for process with name VEP
Jul-06 07:39:58.051 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:VEP` matches labels `VEP,cpus_4` for process with name VEP
Jul-06 07:39:58.051 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.051 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.055 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:cpus_4` matches labels `VEP,cpus_4` for process with name VEPmerge
Jul-06 07:39:58.055 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withLabel:VEP` matches labels `VEP,cpus_4` for process with name VEPmerge
Jul-06 07:39:58.056 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.056 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.059 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.059 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.074 [main] DEBUG nextflow.script.ProcessConfig - Config settings `withName:MultiQC` matches process MultiQC
Jul-06 07:39:58.074 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.074 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.077 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null
Jul-06 07:39:58.077 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'local'
Jul-06 07:39:58.080 [main] DEBUG nextflow.Session - Workflow process names [dsl1]: FreebayesSingle, VEP, Mpileup, MapReads, MergeMutect2Stats, UMIMapBamFile, ApplyBQSR, IndexBamRecal, Ascat, AlleleCounter, Vcftools, Sentieon_BQSR, BuildGermlineResourceIndex, TrimGalore, Snpeff, BuildKnownIndelsIndex, GroupReadsByUmi, StrelkaSingle, Sentieon_TNscope, HaplotypeCaller, Sentieon_DNAseq, StrelkaBP, Output_documentation, ControlFREEC, BuildIntervals, PileupSummariesForMutect2, ControlFREECSingle, MantaSingle, BuildPonIndex, ControlFreecViz, CompressVCFvep, BuildFastaFai, CalculateContamination, VEPmerge, IndexBamFile, Mutect2Single, BcftoolsStats, Strelka, MergePileupSummaries, UMIFastqToBAM, IndexBamMergedForSentieon, MergeBamMapped, BuildDict, FastQCFQ, Manta, BamQC, GenotypeGVCFs, CompressVCFsnpEff, CreateIntervalBeds, MergeMpileup, GatherBQSRReports, BuildDbsnpIndex, get_software_versions, TIDDIT, FastQCBAM, MarkDuplicates, MSIsensor_msi, BaseRecalibrator, CNVkit, MultiQC, MergeBamRecal, SamtoolsStats, BuildBWAindexes, FreeBayes, CompressSentieonVCF, ConvertAlleleCounts, ControlFreecVizSingle, CallMolecularConsensusReads, Sentieon_MapReads, Mutect2, ConcatVCF, Sentieon_DNAscope, FilterMutect2Calls, MSIsensor_scan, MSIsensor_msiSingle, Sentieon_Dedup, ConcatVCF_Mutect2
Jul-06 07:39:58.081 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/output.md
Jul-06 07:39:58.081 [main] DEBUG nextflow.script.ScriptRunner - > Await termination 
Jul-06 07:39:58.082 [main] DEBUG nextflow.Session - Session await
Jul-06 07:39:58.082 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/NGI_logo.svg
Jul-06 07:39:58.082 [main] DEBUG nextflow.Session - Session await > all process finished
Jul-06 07:39:58.082 [main] DEBUG nextflow.Session - Session await > all barriers passed
Jul-06 07:39:58.082 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/nf-core-sarek-germline_logo.svg
Jul-06 07:39:58.083 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/BTB_logo.png
Jul-06 07:39:58.083 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/NBIS_logo.svg
Jul-06 07:39:58.083 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/nf-core_logo.png
Jul-06 07:39:58.084 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/BTB_logo.svg
Jul-06 07:39:58.084 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/nf-core_logo.svg
Jul-06 07:39:58.084 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/NBIS_logo.png
Jul-06 07:39:58.085 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/QBiC_logo.png
Jul-06 07:39:58.085 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/nf-core-sarek-somatic_logo.svg
Jul-06 07:39:58.086 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/sarek_logo.svg
Jul-06 07:39:58.086 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/sarek/sarek_dark_grey.svg
Jul-06 07:39:58.087 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/sarek/sarek_dark_color.svg
Jul-06 07:39:58.087 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/sarek/sarek_mono.svg
Jul-06 07:39:58.087 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/sarek/sarek_dark_mono.svg
Jul-06 07:39:58.088 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/sarek/sarek_color.svg
Jul-06 07:39:58.088 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/sarek/sarek_grey.svg
Jul-06 07:39:58.089 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/sarek-germline/sarek-germline.svg
Jul-06 07:39:58.089 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/nf-core_sarek/nf-core_sarek_dark_color.svg
Jul-06 07:39:58.090 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/nf-core_sarek/nf-core_sarek_grey.svg
Jul-06 07:39:58.090 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/nf-core_sarek/nf-core_sarek_mono.svg
Jul-06 07:39:58.090 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/nf-core_sarek/nf-core_sarek_dark_mono.svg
Jul-06 07:39:58.091 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/nf-core_sarek/nf-core_sarek_dark_grey.svg
Jul-06 07:39:58.092 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/nf-core_sarek/nf-core_sarek_color.svg
Jul-06 07:39:58.092 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/logos/sarek-somatic/sarek-somatic_logo.svg
Jul-06 07:39:58.093 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/QBiC_logo.svg
Jul-06 07:39:58.093 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/SciLifeLab_logo.svg
Jul-06 07:39:58.094 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/NGI_logo.png
Jul-06 07:39:58.094 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/nf-core-sarek_logo.svg
Jul-06 07:39:58.095 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/sarek_workflow.png
Jul-06 07:39:58.096 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/sarek_icon.svg
Jul-06 07:39:58.096 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/SciLifeLab_logo.png
Jul-06 07:39:58.096 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/nf-core-sarek_logo.png
Jul-06 07:39:58.097 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/docs/images/sarek_workflow.svg
Jul-06 07:39:58.098 [Actor Thread 10] DEBUG nextflow.util.CacheHelper - Hash asset file sha-256: /home/ubuntu/.nextflow/assets/nf-core/sarek/bin/markdown_to_html.py
Jul-06 07:39:58.148 [Actor Thread 14] DEBUG nextflow.sort.BigSort - Sort completed -- entries: 1; slices: 1; internal sort time: 0.001 s; external sort time: 0.01 s; total time: 0.011 s
Jul-06 07:39:58.186 [Actor Thread 14] DEBUG nextflow.file.FileCollector - Saved collect-files list to: /tmp/049830c6d373f8b36c08b4c1c5e6f4b3.collect-file
Jul-06 07:39:58.193 [Actor Thread 14] DEBUG nextflow.file.FileCollector - Deleting file collector temp dir: /tmp/nxf-9398729317390444387
Jul-06 07:39:58.297 [main] INFO  nextflow.Nextflow - -[nf-core/sarek] Pipeline completed with errors-
Jul-06 07:39:58.302 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=0; ignoredCount=0; cachedCount=0; pendingCount=2; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=0ms; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=0; peakCpus=0; peakMemory=0; ]
Jul-06 07:39:58.302 [main] DEBUG nextflow.trace.TraceFileObserver - Flow completing -- flushing trace file
Jul-06 07:39:58.304 [main] DEBUG nextflow.trace.ReportObserver - Flow completing -- rendering html report
Jul-06 07:39:58.306 [main] DEBUG nextflow.trace.ReportObserver - Execution report summary data:
  []
Jul-06 07:39:58.914 [main] DEBUG nextflow.trace.TimelineObserver - Flow completing -- rendering html timeline
Jul-06 07:39:59.096 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:59.096 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:59.096 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:59.096 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:59.097 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:59.097 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:59.097 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:59.097 [main] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Jul-06 07:39:59.105 [Actor Thread 11] INFO  nextflow.file.FilePorter - Staging foreign file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/beta/Homo_sapiens_assembly38.known_indels.vcf.gz
Jul-06 07:39:59.715 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done
Jul-06 07:39:59.715 [main] INFO  org.pf4j.AbstractPluginManager - Stop plugin 'nf-amazon@1.7.2'
Jul-06 07:39:59.716 [main] DEBUG nextflow.plugin.BasePlugin - Plugin stopped nf-amazon
Jul-06 07:39:59.716 [main] DEBUG c.u.s3fs.ng.S3ParallelDownload - Shutdown S3 downloader
Jul-06 07:40:02.945 [main] DEBUG c.u.s3fs.ng.S3ParallelDownload - Shutdown S3 downloader - done
Jul-06 07:40:03.039 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye
(base) ubuntu@pipetest:/mnt/quobyte/nextflow_real_data_test$ ",sheucke,https://github.com/nf-core/sarek/issues/622
I_kwDOCvwIC85NV66f,Error executing process > 'Output_documentation',CLOSED,2022-07-07T14:57:10Z,2022-07-08T15:10:45Z,2022-07-08T15:10:45Z,"## Description of the bug

Checking the workflow using test command

## Steps to reproduce

```
(base) [ash022@login-4.SAGA /cluster/projects/nn9036k/precFDA]$ ./nextflow run nf-core/sarek -profile test,singularity
N E X T F L O W  ~  version 22.04.4
Pulling nf-core/sarek ...
 downloaded from https://github.com/nf-core/sarek.git
Launching `https://github.com/nf-core/sarek` [magical_nightingale] DSL1 - revision: e8f56e5bbb [master]


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .´ _  `.
   /  |\`-_ \      __        __   ___
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /¯¯\ |  \ |___ |  \
    `|____\´

  nf-core/sarek v2.7.2
------------------------------------------------------

Core Nextflow options
  revision                  : master
  runName                   : magical_nightingale
  containerEngine           : singularity
  container                 : nfcore/sarek:2.7.2
  launchDir                 : /cluster/projects/nn9036k/precFDA
  workDir                   : /cluster/projects/nn9036k/precFDA/work
  projectDir                : /cluster/home/ash022/.nextflow/assets/nf-core/sarek
  userName                  : ash022
  profile                   : test,singularity
  configFiles               : /cluster/home/ash022/.nextflow/assets/nf-core/sarek/nextflow.config

Input/output options
  input                     : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/testdata/tsv/tiny-manta-https.tsv

Main options
  tools                     : null
  skip_qc                   : null

Trim/split FASTQ
  clip_r1                   : 0
  clip_r2                   : 0
  three_prime_clip_r1       : 0
  three_prime_clip_r2       : 0
  trim_nextseq              : 0

Preprocessing
  markdup_java_options      : ""-Xms4000m -Xmx7g""

Variant Calling
  ascat_ploidy              : null
  ascat_purity              : null
  cf_contamination          : null
  cf_ploidy                 : 2
  read_structure1           : null
  read_structure2           : null

Annotation
  annotate_tools            : null
  cadd_indels               : false
  cadd_indels_tbi           : false
  cadd_wg_snvs              : false
  cadd_wg_snvs_tbi          : false
  snpeff_cache              : null
  vep_cache                 : null

Reference genome options
  genome                    : smallGRCh37
  dbsnp                     : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/reference/dbsnp_138.b37.small.vcf.gz
  fasta                     : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/reference/human_g1k_v37_decoy.small.fasta
  intervals                 : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/reference/small.intervals
  known_indels              : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/reference/Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.smal                          l.vcf.gz
  snpeff_db                 : WBcel235.86
  species                   : caenorhabditis_elegans
  vep_cache_version         : 99
  igenomes_base             : s3://ngi-igenomes/igenomes/
  genomes_base              : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/reference
  igenomes_ignore           : true

Generic options
  max_multiqc_email_size    : 25 MB
  sequencing_center         : null

Max job request options
  single_cpu_mem            : 7 GB
  max_cpus                  : 2
  max_memory                : 6 GB
  max_time                  : 2d

Institutional config options
  config_profile_name       : Test profile
  config_profile_description: Minimal test dataset to check pipeline function

------------------------------------------------------
 Only displaying parameters that differ from defaults.
------------------------------------------------------
Pipeline Release  : master
Run Name          : magical_nightingale
Max Resources     : 6 GB memory, 2 cpus, 2d time per job
Container         : singularity - nfcore/sarek:2.7.2
Output dir        : ./results
Launch dir        : /cluster/projects/nn9036k/precFDA
Working dir       : /cluster/projects/nn9036k/precFDA/work
Script dir        : /cluster/home/ash022/.nextflow/assets/nf-core/sarek
User              : ash022
Input             : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/testdata/tsv/tiny-manta-https.tsv
Step              : mapping
Genome            : smallGRCh37
Nucleotides/s     : 1000
MarkDuplicates    : Options
Java options      : ""-Xms4000m -Xmx7g""
GATK Spark        : No
Save BAMs mapped  : No
Skip MarkDuplicates: No
AWS iGenomes      : Do not use
Save Reference    : No
dbsnp             : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/reference/dbsnp_138.b37.small.vcf.gz
fasta reference   : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/reference/human_g1k_v37_decoy.small.fasta
intervals         : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/reference/small.intervals
known indels      : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/reference/Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz
snpEff DB         : WBcel235.86
species           : caenorhabditis_elegans
VEP cache version : 99
Publish dir mode  : copy
Config Profile    : test,singularity
Config Description: Minimal test dataset to check pipeline function
Config Files      : /cluster/home/ash022/.nextflow/assets/nf-core/sarek/nextflow.config
----------------------------------------------------
[-        ] process > get_software_versions      -
[-        ] process > get_software_versions       -
[-        ] process > get_software_versions       -
[-        ] process > BuildBWAindexes             -
[-        ] process > BuildDict                   -
[-        ] process > BuildFastaFai               -
[-        ] process > BuildDbsnpIndex             -
[-        ] process > get_software_versions       -
[-        ] process > BuildBWAindexes             -
[-        ] process > BuildDict                   -
[-        ] process > BuildFastaFai               -
[-        ] process > BuildDbsnpIndex             -
[-        ] process > BuildGermlineResourceIndex  -
[-        ] process > BuildKnownIndelsIndex       -
[-        ] process > BuildPonIndex               -
[-        ] process > BuildIntervals              -
[-        ] process > CreateIntervalBeds          -
[-        ] process > FastQCFQ                    -
[-        ] process > FastQCBAM                   -
[-        ] process > TrimGalore                  -
[-        ] process > UMIFastqToBAM               -
[-        ] process > UMIMapBamFile               -
[-        ] process > GroupReadsByUmi             -
[-        ] process > CallMolecularConsensusReads -
[-        ] process > MapReads                    -
[-        ] process > Sentieon_MapReads           -
[-        ] process > MergeBamMapped              -
[-        ] process > IndexBamMergedForSentieon   -
[-        ] process > IndexBamFile                -
[-        ] process > MarkDuplicates              -
[-        ] process > Sentieon_Dedup              -
[-        ] process > BaseRecalibrator            -
[-        ] process > GatherBQSRReports           -
[-        ] process > ApplyBQSR                   -
[-        ] process > Sentieon_BQSR               -
[-        ] process > MergeBamRecal               -
[-        ] process > IndexBamRecal               -
[-        ] process > get_software_versions       -
[-        ] process > BuildBWAindexes             -
[-        ] process > BuildDict                   -
[-        ] process > BuildFastaFai               -
[-        ] process > BuildDbsnpIndex             -
[-        ] process > BuildGermlineResourceIndex  -
[-        ] process > BuildKnownIndelsIndex       -
[-        ] process > BuildPonIndex               -
[-        ] process > BuildIntervals              -
[-        ] process > CreateIntervalBeds          -
[-        ] process > FastQCFQ                    -
[-        ] process > FastQCBAM                   -
[-        ] process > TrimGalore                  -
[-        ] process > UMIFastqToBAM               -
[-        ] process > UMIMapBamFile               -
[-        ] process > GroupReadsByUmi             -
[-        ] process > CallMolecularConsensusReads -
[-        ] process > MapReads                    -
[-        ] process > Sentieon_MapReads           -
[-        ] process > MergeBamMapped              -
[-        ] process > IndexBamMergedForSentieon   -
[-        ] process > IndexBamFile                -
[-        ] process > MarkDuplicates              -
[-        ] process > Sentieon_Dedup              -
[-        ] process > BaseRecalibrator            -
[-        ] process > GatherBQSRReports           -
[-        ] process > ApplyBQSR                   -
[-        ] process > Sentieon_BQSR               -
[-        ] process > MergeBamRecal               -
[-        ] process > IndexBamRecal               -
[-        ] process > SamtoolsStats               -
[-        ] process > BamQC                       -
[-        ] process > get_software_versions       -
[-        ] process > BuildBWAindexes             -
[-        ] process > BuildDict                   -
[-        ] process > BuildFastaFai               -
[-        ] process > BuildDbsnpIndex             -
[-        ] process > BuildGermlineResourceIndex  -
[-        ] process > BuildKnownIndelsIndex       -
[-        ] process > BuildPonIndex               -
[-        ] process > BuildIntervals              -
[-        ] process > CreateIntervalBeds          -
[-        ] process > FastQCFQ                    -
[-        ] process > FastQCBAM                   -
[-        ] process > TrimGalore                  -
[-        ] process > UMIFastqToBAM               -
[-        ] process > UMIMapBamFile               -
[-        ] process > GroupReadsByUmi             -
[-        ] process > CallMolecularConsensusReads -
[-        ] process > MapReads                    -
[-        ] process > Sentieon_MapReads           -
[-        ] process > MergeBamMapped              -
[-        ] process > IndexBamMergedForSentieon   -
[-        ] process > IndexBamFile                -
[-        ] process > MarkDuplicates              -
[-        ] process > Sentieon_Dedup              -
[-        ] process > BaseRecalibrator            -
[-        ] process > GatherBQSRReports           -
[-        ] process > ApplyBQSR                   -
[-        ] process > Sentieon_BQSR               -
[-        ] process > MergeBamRecal               -
[-        ] process > IndexBamRecal               -
[-        ] process > SamtoolsStats               -
[-        ] process > BamQC                       -
[-        ] process > get_software_versions       -
[-        ] process > BuildBWAindexes             -
[-        ] process > BuildDict                   -
[-        ] process > BuildFastaFai               -
[-        ] process > BuildDbsnpIndex             -
[-        ] process > BuildGermlineResourceIndex  -
[-        ] process > BuildKnownIndelsIndex       -
[-        ] process > BuildPonIndex               -
[-        ] process > BuildIntervals              -
[-        ] process > CreateIntervalBeds          -
[-        ] process > FastQCFQ                    -
[-        ] process > FastQCBAM                   -
[-        ] process > TrimGalore                  -
[-        ] process > UMIFastqToBAM               -
[-        ] process > UMIMapBamFile               -
[-        ] process > GroupReadsByUmi             -
[-        ] process > CallMolecularConsensusReads -
[-        ] process > MapReads                    -
[-        ] process > Sentieon_MapReads           -
[-        ] process > MergeBamMapped              -
[-        ] process > IndexBamMergedForSentieon   -
[-        ] process > IndexBamFile                -
[-        ] process > MarkDuplicates              -
[-        ] process > Sentieon_Dedup              -
[-        ] process > BaseRecalibrator            -
[-        ] process > GatherBQSRReports           -
[-        ] process > ApplyBQSR                   -
[-        ] process > Sentieon_BQSR               -
[-        ] process > MergeBamRecal               -
[-        ] process > IndexBamRecal               -
[-        ] process > SamtoolsStats               -
[-        ] process > BamQC                       -
[-        ] process > get_software_versions       [  0%] 0 of 1
[-        ] process > BuildBWAindexes             [  0%] 0 of 1
[-        ] process > BuildDict                   [  0%] 0 of 1
[-        ] process > BuildFastaFai               [  0%] 0 of 1
[-        ] process > BuildDbsnpIndex             [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex  -
[-        ] process > BuildKnownIndelsIndex       -
[-        ] process > BuildPonIndex               -
[-        ] process > BuildIntervals              -
[-        ] process > CreateIntervalBeds          [  0%] 0 of 1
[-        ] process > FastQCFQ                    [  0%] 0 of 8
[-        ] process > FastQCBAM                   -
[-        ] process > TrimGalore                  -
[-        ] process > UMIFastqToBAM               -
[-        ] process > UMIMapBamFile               -
[-        ] process > GroupReadsByUmi             -
[-        ] process > CallMolecularConsensusReads -
[-        ] process > MapReads                    -
[-        ] process > Sentieon_MapReads           -
[-        ] process > MergeBamMapped              -
[-        ] process > IndexBamMergedForSentieon   -
[-        ] process > IndexBamFile                -
[-        ] process > MarkDuplicates              -
[-        ] process > Sentieon_Dedup              -
[-        ] process > BaseRecalibrator            -
[-        ] process > GatherBQSRReports           -
[-        ] process > ApplyBQSR                   -
[-        ] process > Sentieon_BQSR               -
[-        ] process > MergeBamRecal               -
[-        ] process > IndexBamRecal               -
[-        ] process > SamtoolsStats               -
[-        ] process > get_software_versions       [  0%] 0 of 1
[-        ] process > BuildBWAindexes             [  0%] 0 of 1
[-        ] process > BuildDict                   [  0%] 0 of 1
[-        ] process > BuildFastaFai               [  0%] 0 of 1
[-        ] process > BuildDbsnpIndex             [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex  -
[-        ] process > BuildKnownIndelsIndex       [  0%] 0 of 1
[-        ] process > BuildPonIndex               -
[-        ] process > BuildIntervals              -
[-        ] process > CreateIntervalBeds          [  0%] 0 of 1
[-        ] process > FastQCFQ                    [  0%] 0 of 11
[-        ] process > FastQCBAM                   -
[-        ] process > TrimGalore                  -
[-        ] process > UMIFastqToBAM               -
[-        ] process > UMIMapBamFile               -
[-        ] process > GroupReadsByUmi             -
[-        ] process > CallMolecularConsensusReads -
[-        ] process > MapReads                    -
[-        ] process > Sentieon_MapReads           -
[-        ] process > MergeBamMapped              -
[-        ] process > IndexBamMergedForSentieon   -
[-        ] process > IndexBamFile                -
[-        ] process > MarkDuplicates              -
[-        ] process > Sentieon_Dedup              -
[-        ] process > BaseRecalibrator            -
[-        ] process > GatherBQSRReports           -
[-        ] process > ApplyBQSR                   -
[-        ] process > Sentieon_BQSR               -
[-        ] process > MergeBamRecal               -
[-        ] process > IndexBamRecal               -
[-        ] process > SamtoolsStats               -
executor >  local (3)
[-        ] process > get_software_versions       [  0%] 0 of 1
[-        ] process > BuildBWAindexes             [  0%] 0 of 1
[-        ] process > BuildDict                   [  0%] 0 of 1
[-        ] process > BuildFastaFai               [  0%] 0 of 1
[-        ] process > BuildDbsnpIndex             [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex  -
[-        ] process > BuildKnownIndelsIndex       [  0%] 0 of 1
[-        ] process > BuildPonIndex               -
[-        ] process > BuildIntervals              -
[-        ] process > CreateIntervalBeds          [  0%] 0 of 1
[a1/53d746] process > FastQCFQ (1234-1234N_M7)    [  0%] 0 of 11
[-        ] process > FastQCBAM                   -
[-        ] process > TrimGalore                  -
[-        ] process > UMIFastqToBAM               -
[-        ] process > UMIMapBamFile               -
[-        ] process > GroupReadsByUmi             -
[-        ] process > CallMolecularConsensusReads -
[-        ] process > MapReads                    -
[-        ] process > Sentieon_MapReads           -
[-        ] process > MergeBamMapped              -
[-        ] process > IndexBamMergedForSentieon   -
[-        ] process > IndexBamFile                -
[-        ] process > MarkDuplicates              -
[-        ] process > Sentieon_Dedup              -
[-        ] process > BaseRecalibrator            -
[-        ] process > GatherBQSRReports           -
[-        ] process > ApplyBQSR                   -
[-        ] process > Sentieon_BQSR               -
[-        ] process > MergeBamRecal               -
executor >  local (8)
[-        ] process > get_software_versions                       [  0%] 0 of 1
[-        ] process > BuildBWAindexes                             [  0%] 0 of 1
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta) [  0%] 0 of 1
[-        ] process > BuildFastaFai                               [  0%] 0 of 1
[-        ] process > BuildDbsnpIndex                             [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex                  -
[-        ] process > BuildKnownIndelsIndex                       [  0%] 0 of 1
[-        ] process > BuildPonIndex                               -
[-        ] process > BuildIntervals                              -
[-        ] process > CreateIntervalBeds                          [  0%] 0 of 1
[84/e197b3] process > FastQCFQ (1234-1234N_M5)                    [  0%] 0 of 11
[-        ] process > FastQCBAM                                   -
[-        ] process > TrimGalore                                  -
[-        ] process > UMIFastqToBAM                               -
[-        ] process > UMIMapBamFile                               -
[-        ] process > GroupReadsByUmi                             -
[-        ] process > CallMolecularConsensusReads                 -
[-        ] process > MapReads                                    -
[-        ] process > Sentieon_MapReads                           -
[-        ] process > MergeBamMapped                              -
[-        ] process > IndexBamMergedForSentieon                   -
[-        ] process > IndexBamFile                                -
[-        ] process > MarkDuplicates                              -
[-        ] process > Sentieon_Dedup                              -
[-        ] process > BaseRecalibrator                            -
[-        ] process > GatherBQSRReports                           -
[-        ] process > ApplyBQSR                                   -
[-        ] process > Sentieon_BQSR                               -
[-        ] process > MergeBamRecal                               -
executor >  local (11)
[-        ] process > get_software_versions                             [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta) [  0%] 0 of 1
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)       [  0%] 0 of 1
[-        ] process > BuildFastaFai                                     [  0%] 0 of 1
[-        ] process > BuildDbsnpIndex                                   [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex                        -
[-        ] process > BuildKnownIndelsIndex                             [  0%] 0 of 1
[-        ] process > BuildPonIndex                                     -
[-        ] process > BuildIntervals                                    -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)              [  0%] 0 of 1
[05/d6c9c5] process > FastQCFQ (1234-9876T_M5)                          [  0%] 0 of 11
[-        ] process > FastQCBAM                                         -
[-        ] process > TrimGalore                                        -
[-        ] process > UMIFastqToBAM                                     -
[-        ] process > UMIMapBamFile                                     -
[-        ] process > GroupReadsByUmi                                   -
[-        ] process > CallMolecularConsensusReads                       -
[-        ] process > MapReads                                          -
[-        ] process > Sentieon_MapReads                                 -
[-        ] process > MergeBamMapped                                    -
[-        ] process > IndexBamMergedForSentieon                         -
[-        ] process > IndexBamFile                                      -
[-        ] process > MarkDuplicates                                    -
[-        ] process > Sentieon_Dedup                                    -
[-        ] process > BaseRecalibrator                                  -
[-        ] process > GatherBQSRReports                                 -
[-        ] process > ApplyBQSR                                         -
[-        ] process > Sentieon_BQSR                                     -
[-        ] process > MergeBamRecal                                     -
executor >  local (12)
[96/9b32b5] process > get_software_versions                             [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta) [  0%] 0 of 1
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)       [  0%] 0 of 1
[-        ] process > BuildFastaFai                                     [  0%] 0 of 1
[-        ] process > BuildDbsnpIndex                                   [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex                        -
[-        ] process > BuildKnownIndelsIndex                             [  0%] 0 of 1
[-        ] process > BuildPonIndex                                     -
[-        ] process > BuildIntervals                                    -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)              [  0%] 0 of 1
[05/d6c9c5] process > FastQCFQ (1234-9876T_M5)                          [  0%] 0 of 11
[-        ] process > FastQCBAM                                         -
[-        ] process > TrimGalore                                        -
[-        ] process > UMIFastqToBAM                                     -
[-        ] process > UMIMapBamFile                                     -
[-        ] process > GroupReadsByUmi                                   -
[-        ] process > CallMolecularConsensusReads                       -
[-        ] process > MapReads                                          -
[-        ] process > Sentieon_MapReads                                 -
[-        ] process > MergeBamMapped                                    -
[-        ] process > IndexBamMergedForSentieon                         -
[-        ] process > IndexBamFile                                      -
[-        ] process > MarkDuplicates                                    -
[-        ] process > Sentieon_Dedup                                    -
[-        ] process > BaseRecalibrator                                  -
[-        ] process > GatherBQSRReports                                 -
[-        ] process > ApplyBQSR                                         -
[-        ] process > Sentieon_BQSR                                     -
[-        ] process > MergeBamRecal                                     -
executor >  local (13)
[96/9b32b5] process > get_software_versions                             [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta) [  0%] 0 of 1
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)       [  0%] 0 of 1
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)   [  0%] 0 of 1
[-        ] process > BuildDbsnpIndex                                   [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex                        -
[-        ] process > BuildKnownIndelsIndex                             [  0%] 0 of 1
[-        ] process > BuildPonIndex                                     -
[-        ] process > BuildIntervals                                    -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)              [  0%] 0 of 1
[05/d6c9c5] process > FastQCFQ (1234-9876T_M5)                          [  0%] 0 of 11
[-        ] process > FastQCBAM                                         -
[-        ] process > TrimGalore                                        -
[-        ] process > UMIFastqToBAM                                     -
[-        ] process > UMIMapBamFile                                     -
[-        ] process > GroupReadsByUmi                                   -
[-        ] process > CallMolecularConsensusReads                       -
[-        ] process > MapReads                                          -
[-        ] process > Sentieon_MapReads                                 -
[-        ] process > MergeBamMapped                                    -
[-        ] process > IndexBamMergedForSentieon                         -
[-        ] process > IndexBamFile                                      -
[-        ] process > MarkDuplicates                                    -
[-        ] process > Sentieon_Dedup                                    -
[-        ] process > BaseRecalibrator                                  -
[-        ] process > GatherBQSRReports                                 -
[-        ] process > ApplyBQSR                                         -
[-        ] process > Sentieon_BQSR                                     -
[-        ] process > MergeBamRecal                                     -
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [  0%] 0 of 1
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [  0%] 0 of 1
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [  0%] 0 of 1
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [  0%] 0 of 1
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [  0%] 0 of 1
[78/da8cb3] process > FastQCFQ (1234-9876T_M3)                                                                   [  0%] 0 of 11
[-        ] process > FastQCBAM                                                                                  -
[-        ] process > TrimGalore                                                                                 -
[-        ] process > UMIFastqToBAM                                                                              -
[-        ] process > UMIMapBamFile                                                                              -
[-        ] process > GroupReadsByUmi                                                                            -
[-        ] process > CallMolecularConsensusReads                                                                -
[-        ] process > MapReads                                                                                   -
[-        ] process > Sentieon_MapReads                                                                          -
[-        ] process > MergeBamMapped                                                                             -
[-        ] process > IndexBamMergedForSentieon                                                                  -
[-        ] process > IndexBamFile                                                                               -
[-        ] process > MarkDuplicates                                                                             -
[-        ] process > Sentieon_Dedup                                                                             -
[-        ] process > BaseRecalibrator                                                                           -
[-        ] process > GatherBQSRReports                                                                          -
[-        ] process > ApplyBQSR                                                                                  -
[-        ] process > Sentieon_BQSR                                                                              -
[-        ] process > MergeBamRecal                                                                              -
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [  0%] 0 of 1
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [  0%] 0 of 1
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [  0%] 0 of 1
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [  0%] 0 of 1
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [  0%] 0 of 1
[78/da8cb3] process > FastQCFQ (1234-9876T_M3)                                                                   [  0%] 0 of 11

...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [  0%] 0 of 1
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [  0%] 0 of 1
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [  0%] 0 of 1
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [  0%] 0 of 1
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [  0%] 0 of 1
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [  0%] 0 of 1
[78/da8cb3] process > FastQCFQ (1234-9876T_M3)                                                                   [  0%] 0 of 11

...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [100%] 1 of 1 ✔
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [  0%] 0 of 1
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [100%] 1 of 1 ✔
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [100%] 1 of 1 ✔
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [100%] 1 of 1 ✔
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [100%] 1 of 1 ✔
[78/da8cb3] process > FastQCFQ (1234-9876T_M3)                                                                   [  0%] 0 of 11

...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [100%] 1 of 1 ✔
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [100%] 1 of 1 ✔
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [100%] 1 of 1 ✔
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [100%] 1 of 1 ✔
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [100%] 1 of 1 ✔
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [100%] 1 of 1 ✔
[78/da8cb3] process > FastQCFQ (1234-9876T_M3)                                                                   [  0%] 0 of 11

...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [100%] 1 of 1 ✔
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [100%] 1 of 1 ✔
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [100%] 1 of 1 ✔
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [100%] 1 of 1 ✔
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [100%] 1 of 1 ✔
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [100%] 1 of 1 ✔
[94/6300a4] process > FastQCFQ (1234-1234N_M6)                                                                   [ 45%] 5 of 11
...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [100%] 1 of 1 ✔
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [100%] 1 of 1 ✔
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [100%] 1 of 1 ✔
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [100%] 1 of 1 ✔
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [100%] 1 of 1 ✔
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [100%] 1 of 1 ✔
[fe/52791d] process > FastQCFQ (1234-1234N_M4)                                                                   [ 54%] 6 of 11
...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [100%] 1 of 1 ✔
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [100%] 1 of 1 ✔
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [100%] 1 of 1 ✔
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [100%] 1 of 1 ✔
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [100%] 1 of 1 ✔
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [100%] 1 of 1 ✔
[0c/5070e9] process > FastQCFQ (1234-9876T_M1)                                                                   [ 81%] 9 of 11
...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [  0%] 0 of 1
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [100%] 1 of 1 ✔
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [100%] 1 of 1 ✔
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [100%] 1 of 1 ✔
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [100%] 1 of 1 ✔
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [100%] 1 of 1 ✔
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [100%] 1 of 1 ✔
[3b/313b53] process > FastQCFQ (1234-9876T_M2)                                                                   [100%] 11 of 11 ✔
...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [100%] 1 of 1, failed: 1 ✘
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [100%] 1 of 1 ✔
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [100%] 1 of 1 ✔
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [100%] 1 of 1 ✔
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [100%] 1 of 1 ✔
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [100%] 1 of 1 ✔
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [100%] 1 of 1 ✔
[3b/313b53] process > FastQCFQ (1234-9876T_M2)                                                                   [100%] 11 of 11 ✔
...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [100%] 1 of 1, failed: 1 ✘
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [100%] 1 of 1 ✔
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [100%] 1 of 1 ✔
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [100%] 1 of 1 ✔
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [100%] 1 of 1 ✔
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [100%] 1 of 1 ✔
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [100%] 1 of 1 ✔
[3b/313b53] process > FastQCFQ (1234-9876T_M2)                                                                   [100%] 11 of 11 ✔
...
executor >  local (19)
[96/9b32b5] process > get_software_versions                                                                      [100%] 1 of 1, failed: 1 ✘
[8d/b0f454] process > BuildBWAindexes (human_g1k_v37_decoy.small.fasta)                                          [100%] 1 of 1 ✔
[f4/1ba4bf] process > BuildDict (human_g1k_v37_decoy.small.fasta)                                                [100%] 1 of 1 ✔
[e9/a76cc8] process > BuildFastaFai (human_g1k_v37_decoy.small.fasta)                                            [100%] 1 of 1 ✔
[a9/2450aa] process > BuildDbsnpIndex (dbsnp_138.b37.small.vcf.gz)                                               [100%] 1 of 1 ✔
[-        ] process > BuildGermlineResourceIndex                                                                 -
[9a/8dfbd0] process > BuildKnownIndelsIndex (Mills_1000G_gold_standard_and_1000G_phase1.indels.b37.small.vcf.gz) [100%] 1 of 1 ✔
[-        ] process > BuildPonIndex                                                                              -
[-        ] process > BuildIntervals                                                                             -
[4a/29ce35] process > CreateIntervalBeds (small.intervals)                                                       [100%] 1 of 1 ✔
[3b/313b53] process > FastQCFQ (1234-9876T_M2)                                                                   [100%] 11 of 11 ✔
...
[7d/6054e0] process > Output_documentation                                                                       [100%] 1 of 1, failed: 1 ✘
Execution cancelled -- Finishing pending tasks before exit
-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'Output_documentation'

Caused by:
  Process `Output_documentation` terminated with an error exit status (127)

Command executed:

  markdown_to_html.py output.md -o results_description.html

Command exit status:
  127

Command output:
  (empty)

Command error:
  /usr/bin/env: ‘python\r’: No such file or directory

Work dir:
  /cluster/projects/nn9036k/precFDA/work/7d/6054e04beeca8c1083c627bc7068df

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line





```
",animesh,https://github.com/nf-core/sarek/issues/625
I_kwDOCvwIC85Ng9Vy,missing container tag 2.7.2.GRCm38 for sareksnpeff,CLOSED,2022-07-11T10:04:11Z,2022-07-21T08:29:02Z,2022-07-13T09:27:48Z,"<!--
# nf-core/sarek bug report

Hi there!

## Description of the bug

I run germ-line variant call of mouse dataset. The error showed that pulling from nf-core is not success.  
please help.

## Steps to reproduce

Steps to reproduce the behaviour:

1. Command line: <!-- [`nextflow run nf-core/sarek -r 2.7.2 --input infiles.tsv --genome GRCm38 -profile singularity --max_memory 120GB --max_cpus 30 -resume --tools Mutect2,Strelka,Manta,TIDDIT,ASCAT,ControlFREEC,snpEff,VEP ""
`] -->

2. See error: <!-- [Pulling Singularity image docker://nfcore/sareksnpeff:2.7.2.GRCm38 [cache /fullpath/NBres/work/singularity/nfcore-sareksnpeff-2.7.2.GRCm38.img]
Pulling Singularity image docker://nfcore/sarekvep:2.7.2.GRCm38 [cache /fullpath/Jhx_data/NBres/work/singularity/nfcore-sarekvep-2.7.2.GRCm38.img]
WARN: Singularity cache directory has not been defined -- Remote image will be stored in the path: /storage/Intawat/Jhx_data/NBres/work/singularity -- Use env variable NXF_SINGULARITY_CACHEDIR to specify a different location
Error executing process > 'VEP (NB - TIDDIT - TIDDIT_NB.vcf.gz)'

Caused by:
  Failed to pull singularity image
  command: singularity pull  --name nfcore-sarekvep-2.7.2.GRCm38.img.pulling.1657483696643 docker://nfcore/sarekvep:2.7.2.GRCm38 > /dev/null
  status : 255
  message:
    time=""2022-07-10T15:08:16-05:00"" level=warning msg=""\""/run/user/1692700009\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/1692700009: no such file or directory: Trying to pull image in the event that it is a public image.""
    FATAL:   While making image from oci registry: failed to get checksum for docker://nfcore/sarekvep:2.7.2.GRCm38: Error reading manifest 2.7.2.GRCm38 in docker.io/nfcore/sarekvep: manifest unknown: manifest unknown

-[nf-core/sarek] Pipeline completed with errors-
WARN: Killing running tasks (1)] -->
",intawat,https://github.com/nf-core/sarek/issues/627
I_kwDOCvwIC85Nw8l_,[BUG] VEP and SnpEff cache cannot be defined in nextflow.config,CLOSED,2022-07-14T11:51:58Z,2022-07-14T14:10:26Z,2022-07-14T14:10:25Z,"## Description of the bug

For Sarek 3.0, dev-branch, wWhen including the following in a config file
```
params{
   vep_cache = 's3://somewhere'
}
```
The expected behaviour is to _not_ pull the  VEP-container, but instead use a cache. However it does pull the (quite big) container. So the bug may show as a `CannotPullContainerError: write /var/lib/docker/tmp/GetImageBlob370078793: no space left on device` error, especially when running with awsbatch. The choice is [made on this line](https://github.com/nf-core/sarek/blob/42b1860da9e0e4229b434c27aa6c1427510d0b99/conf/modules.config#L1160), where `params.vep_cache` is always empty. This can be bypassed by instead providing `--vep_cache` as command line switch, which is why the bug is marked as low-priority and postponed for backlog.

## Additional context

In [this slack thread](https://nfcore.slack.com/archives/C02MDBZAYJK/p1657633991763159), the sarek3.0 channel of nf-core slack. Possibly related to [this issue.](https://github.com/nextflow-io/nextflow/issues/2662)",lassefolkersen,https://github.com/nf-core/sarek/issues/634
I_kwDOCvwIC85N0vKu,how to get VEP working,CLOSED,2022-07-15T07:13:59Z,2022-07-20T06:19:12Z,2022-07-20T06:16:31Z,"I cant get VEP to work.

I downloaded:

```bash
# Download the cache for VEP GRCh38(VEP version 104)
mkdir -p local_vep_cache_dir_tmp
wget -P local_vep_cache_dir_tmp ftp://ftp.ensembl.org/pub/release-104/variation/indexed_vep_cache/homo_sapiens_vep_104_GRCh38.tar.gz
```

I have the following nf-params:

```bash
{
    ""input"": ""example_pair_fastq.tsv"",
    ""tools"": ""FreeBayes, Manta, snpEff, VEP"",
    ""annotation_cache"": true,
    ""vep_cache"": ""\/mnt\/quobyte\/local_vep_cache_dir_tmp\/homo_sapiens\/104_GRCh38"",
    ""max_multiqc_email_size"": ""100.MB"",
    ""max_cpus"": 28,
    ""max_memory"": ""245.GB""
}
```
and my start command:

`nextflow run nf-core/sarek -name sarek_VEPcache -profile docker -params-file nf-params.json -dsl1 -bg`



VEP is the only thing that always fails.

",sheucke,https://github.com/nf-core/sarek/issues/636
I_kwDOCvwIC85OIozO,[BUG] ControlFreec issues with targeted sequencing data,CLOSED,2022-07-20T11:22:33Z,2024-09-16T11:29:10Z,2022-09-07T15:04:10Z,"## Description of the bug

Running Sarek with `-r dev` and using a capture / targeted BED runs into trouble with ControlFreec. 

https://github.com/BoevaLab/FREEC/issues/106 for more context, @FriederikeHanssen knows about it ;-) 


## Expected behaviour

ControlFreec just running through

## Log files

```console
Error: chromosome HLA-DRB1*16:02:01 present in your /foo/bar/baz/Homo_sapiens_assembly38.fasta.fai file was not detected in your file with capture regions wgs_calling_regions.hg38.bed
  Please solve this issue and rerun Control-FREEC
```

@[chernomor](https://github.com/OlgaChern) reported it to me at BI, now reporting here so that @FriederikeHanssen has an issue ",apeltzer,https://github.com/nf-core/sarek/issues/670
I_kwDOCvwIC85OSJ8k,Azure errors at Map reads step with segementation fault,CLOSED,2022-07-21T14:48:45Z,2023-03-21T13:13:41Z,2023-03-21T13:13:41Z,"### Description of the bug

In Azure we see an error with igenomes using Map reads. We get a segementation fault, which I beleive is due to some issue with S3 pulling the indexes down.

### Command used and terminal output

```console
nextflow run nf-core/sarek -c /home/zvasconcelos/data/sarek/azure --input /home/zvasconcelos/data/sarek/sarek_azure.tsv --outdir 'az://genomas-raros/results' --tools HaplotypeCaller -work-dir 'az://genomas-raros/work'
```


### Relevant files

_No response_

### System information

Nextflow version : 22.04.04
Hardware: Cloud 
Executor: Azurebatch
Container engine: Docker
OS: Centos
Version of nf-core/sarek: 2.7.2",vsmalladi,https://github.com/nf-core/sarek/issues/678
I_kwDOCvwIC85OXDc2,Freebays intermediate files are published,CLOSED,2022-07-22T08:40:01Z,2022-07-25T07:28:14Z,2022-07-25T07:28:14Z,"### Description of the bug

cf https://nf-co.re/sarek/results#sarek/results-5bb160ddb2cee50c811585e450b16cba13c95a02/germline_test/freebayes/

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/680
I_kwDOCvwIC85OX51f,Deepvariant requires additional cli option for recalibrated BAM files,OPEN,2022-07-22T11:29:46Z,2024-10-29T08:34:16Z,,"### Description of the bug

Currently, Sarek produces BAM files for variant calling through the GATK best-practices, including recalibration (default).

For DeepVariant, this is discouraged, as stated by the developers:

https://github.com/google/deepvariant/blob/r1.4/docs/trio-merge-case-study.md

```
It is recommended to use BAM files with original quality scores. In the case that BAM files went through recalibration, optional DV flags can be used in order to use original scores: --parse_sam_aux_fields, --use_original_quality_scores.
```

While recalibration can be disabled globally within Sarek, this would potentially have a negative impact on e.g. the GATK subworkflow, where recalibration is still considered to be useful (not sure it really is...). 

Solution:
1) Easy: Have an optional parameter in the Deepvariant module that sets the above flags if recalibration was performed
2) Slighty less easy: Emit both deduped and deduped+recalibrated BAM/CRAM files after the GATK BP alignment/deduping and pass them to the appropriate subworkflows, i.e. GATK gets the recal BAM file, all other subworkflows get the non-recalibrated BAM. This might be the better choice in the long-term, depending on what the effect of recalibration is on other callers/tools (largely unexplored, I think).


### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",marchoeppner,https://github.com/nf-core/sarek/issues/682
I_kwDOCvwIC85Od8x1,Update the nf-core-modules used in Sarek 3.1,CLOSED,2022-07-25T08:40:44Z,2022-07-28T10:05:21Z,2022-07-28T10:05:21Z,"### Description of feature

Update the nf-core-modules used in Sarek 3.1

Requested by @maxulysse.

",asp8200,https://github.com/nf-core/sarek/issues/684
I_kwDOCvwIC85OhbEa,vep plugins,CLOSED,2022-07-25T21:14:55Z,2022-07-26T16:26:26Z,2022-07-26T16:26:26Z,"### Description of the bug

Came across this when trying to figure out why a VEP summary html file was missing the summary plots, which I was not able to replicate...

I think that the `LoF`, `SpliceAI` and `dbNSFP` plugins might not be working properly for VEP.

Please find attached a `.zip` directory containing the workflow logs, and a script I used to re-generate the warning messages. Below is the main body of the `.command.sh` script for annotating using VEP:

```console
vep \
  --assembly GRCh38 \
  --cache \
  --cache_version 106 \
  --dir_cache /.vep \
  --everything \
  --filter_common \
  --fork 6 \
  --format vcf \
  --input_file HG00138N.haplotypecaller.filtered.vcf.gz \
  --offline \
  --output_file HG00138N.haplotypecaller.filtered_VEP.ann.vcf \
  --per_gene \
  --plugin dbNSFP,ALL,dbNSFP4.2a_grch38.gz,rs_dbSNP,HGVSc_VEP,HGVSp_VEP,1000Gp3_EAS_AF,1000Gp3_AMR_AF,LRT_score,GERP++_RS,gnomAD_exomes_AF \
  --plugin LoF,loftee_path:/opt/conda/envs/nf-core-vep-106.1/share/ensembl-vep-106.1-0 \
  --plugin SpliceAI,snv=spliceai_scores.raw.snv.hg38.vcf.gz,spliceai_scores.raw.indel.hg38.vcf.gz \
  --plugin SpliceRegion \
  --species homo_sapiens \
  --stats_file HG00138N.haplotypecaller.filtered_VEP.summary.html \
  --total_length \
  --vcf
```

### 1. dbNSFP consequence

Currently, `params.dbnsfp_consequence` is pasted without appending `consequence=` to the parameter value.

```console
--plugin dbNSFP,ALL,dbNSFP4.2a_grch38.gz,rs_dbSNP,HGVSc_VEP,HGVSp_VEP,1000Gp3_EAS_AF,1000Gp3_AMR_AF,LRT_score,GERP++_RS,gnomAD_exomes_AF
```

Gives the warning:

```console
WARNING: Failed to instantiate plugin dbNSFP: ERROR: Could not retrieve dbNSFP version from filename ALL
```

i.e the `dbNSFP` plugin thinks that the consequence string (e.g `'ALL'`) is the database file.

### Solution

I think the lines below:

https://github.com/nf-core/sarek/blob/5bb160ddb2cee50c811585e450b16cba13c95a02/conf/modules.config#L1211

should be:

```groovy
(params.vep_dbnsfp && params.dbnsfp && params.dbnsfp_consequence)     ? ""--plugin dbNSFP,'consequence=${params.dbnsfp_consequence}',${params.dbnsfp.split(""/"")[-1]},${params.dbnsfp_fields}""               : '',
```


### 2. LoF 

Nothing to fix here, the `LoF` plugin will not run because the `Bio::perl` module is not installed:

```console
WARNING: Failed to compile plugin LoF: Can't locate Bio/Perl.pm in @INC (you may need to install the Bio::Perl module) (@INC contains: /home/bdigby/.vep/Plugins /opt/conda/envs/nf-core-vep-106.1/share/ensembl-vep-106.1-0/modules /opt/conda/envs/nf-core-vep-106.1/share/ensembl-vep-106.1-0 /opt/conda/envs/nf-core-vep-106.1/lib/perl5/5.32/site_perl /opt/conda/envs/nf-core-vep-106.1/lib/perl5/site_perl /opt/conda/envs/nf-core-vep-106.1/lib/perl5/5.32/vendor_perl /opt/conda/envs/nf-core-vep-106.1/lib/perl5/vendor_perl /opt/conda/envs/nf-core-vep-106.1/lib/perl5/5.32/core_perl /opt/conda/envs/nf-core-vep-106.1/lib/perl5/core_perl .) at /opt/conda/envs/nf-core-vep-106.1/share/ensembl-vep-106.1-0/LoF.pm line 46.
BEGIN failed--compilation aborted at /opt/conda/envs/nf-core-vep-106.1/share/ensembl-vep-106.1-0/LoF.pm line 46.
Compilation failed in require at (eval 42) line 2.
BEGIN failed--compilation aborted at (eval 42) line 2.
```

I don't know enough about `LoF` vs. [Loftool](https://github.com/Ensembl/VEP_plugins/blob/release/107/LoFtool.pm), but the `Loftool` plugin works (with [LoFtool_scores.txt](https://github.com/Ensembl/VEP_plugins/blob/release/107/LoFtool_scores.txt)) in the current VEP container provided with the workflow. 

### 3. SpliceAI

```console
--plugin SpliceAI,snv=spliceai_scores.raw.snv.hg38.vcf.gz,spliceai_scores.raw.indel.hg38.vcf.gz 
```

Gives the warning:

```console
WARNING: Failed to instantiate plugin SpliceAI: ERROR: Failed to parse parameter spliceai_scores.raw.indel.hg38.vcf.gz
```

### Solution

I think it could be as simple as changing:

https://github.com/nf-core/sarek/blob/5bb160ddb2cee50c811585e450b16cba13c95a02/conf/modules.config#L1213

to 

```groovy
(params.vep_spliceai && params.spliceai_snv && params.spliceai_indel) ? ""--plugin SpliceAI,snv=${params.spliceai_snv.split(""/"")[-1]},indel=${params.spliceai_indel.split(""/"")[-1]}""                  : '',
```

***


### Command used and terminal output

_No response_

### Relevant files

[veptest.zip](https://github.com/nf-core/sarek/files/9184768/veptest.zip)


### System information

* nextflow 22.04.5
* AWS 
* awsbatch?
* Docker
* ?
* nf-core/sarek 3.0",BarryDigby,https://github.com/nf-core/sarek/issues/687
I_kwDOCvwIC85OqK_5,We're missing a proper intervals file for GRCh37,CLOSED,2022-07-27T14:47:33Z,2022-09-07T15:01:20Z,2022-09-07T15:01:20Z,"### Description of the bug

we have a `.list`, we should either have a `.bed` or a `.interval_list`

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/693
I_kwDOCvwIC85OtVq5,errors using Launch on website,CLOSED,2022-07-28T07:24:16Z,2022-11-08T12:48:32Z,2022-11-08T12:48:31Z,"### Description of the bug

Hi Sarek Team,

I wanted to try the new 3.0 version but run into a lot of errors doing so.

First the documentations is wrong here:

First, go to the [nf-core/sarek releases page](https://github.com/nf-core/sarek/releases) and find the latest version number - numeric only (eg. 3.0.0). Then specify this when running the pipeline with -r (one hyphen) - eg. -r 3.0.0.

It only works like with -r 3.0 otherwise I get error that 3.0.0 does not exist.

then I get errors with the launch tool:

for the --input field with: example_pair_fastq.csv

Must match pattern \.csv$
Path to comma-separated file containing information about the samples in the experiment.

I did transform the tsv into a csv file, that can not be the problem.

then later for --tools:  FreeBayes, HaplotypeCaller, Manta, snpEff, VEP

I get the error:

Must match pattern ^((ascat|cnvkit|controlfreec|deepvariant|freebayes|haplotypecaller|manta|merge|mpileup|msisensorpro|mutect2|snpeff|strelka|tiddit|vep)*,?)*$

sry i cant translate that, in the help its still written comma separated, which I did.

best regards
sebastian





### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",sheucke,https://github.com/nf-core/sarek/issues/694
I_kwDOCvwIC85OuEtG,Add md5-sums to CI-tests,CLOSED,2022-07-28T10:08:23Z,2022-08-26T09:37:21Z,2022-08-26T09:37:20Z,"### Description of feature

Add md5-sums to CI-tests.  Okayed by @maxulysse ",asp8200,https://github.com/nf-core/sarek/issues/695
I_kwDOCvwIC85Oyjo8,cnv erro,CLOSED,2022-07-29T07:37:18Z,2023-03-21T13:13:05Z,2023-03-21T13:13:05Z,"### Description of the bug

Hello dear software developers:
I used sarek to process our sequencing data, and it showed very good performance in the process of generating bam files, and then recently we encountered some problems when running cnv files. As I uploaded the file, I am running snp, inde and sv without any problem, but at present it reported such an error, I hope you can help me find out what is the problem?
Best day!

### Command used and terminal output

```console
nextflow run /home/lidong/nextflow/sarek-3.0-pig/ --genome susScr3 --input CNV.csv --step variant_calling  --tools cnvkit --outdir ./ -profile docker

-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX (GCF_000003025.6_Sscrofa11.1_genomic.fna)'

Caused by:
  Process `NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX (GCF_000003025.6_Sscrofa11.1_genomic.fna)` terminated with an error exit status (127)

Command executed:

  samtools \
      faidx \
      GCF_000003025.6_Sscrofa11.1_genomic.fna
 
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX"":
      samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
  END_VERSIONS

Command exit status:
  127

Command output:
  (empty)

Command error:
  docker: Error response from daemon: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:75: mounting ""/etc/passwd"" to rootfs at ""/etc/passwd"" caused: mount through procfd: no such file or directory: unknown.
  time=""2022-07-29T10:37:43+08:00"" level=error msg=""error waiting for container: context canceled""

Work dir:
  /home/lidong/nextflow/PRJEB1683/csv/work/e4/645ccc15404fbdcbaa5fb92a08dbf2

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line
```


### Relevant files

[CNV.csv](https://github.com/nf-core/sarek/files/9217618/CNV.csv)


### System information

_No response_",ld9866,https://github.com/nf-core/sarek/issues/697
I_kwDOCvwIC85O4IMJ,fail to run test data,CLOSED,2022-07-31T01:30:13Z,2023-03-21T13:12:49Z,2023-03-21T13:12:49Z,"### Description of the bug

fail to run test data

### Command used and terminal output

```console
$ nextflow run nf-core/sarek -profile test,docker --outdir .
```


### Relevant files

java.nio.file.NoSuchFileException: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_2.fastq.gz
        at nextflow.file.FileHelper.checkIfExists(FileHelper.groovy:1036)
        at nextflow.file.FileHelper$checkIfExists$1.call(Unknown Source)
        at nextflow.Nextflow.file(Nextflow.groovy:151)
        at nextflow.Nextflow$file.callStatic(Unknown Source)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:55)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:217)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:240)
        at Script_0b202ed2$_extract_csv_closure5.doCall(Script_0b202ed2:1191)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)
        at org.codehaus.groovy.runtime.metaclass.TransformMetaMethod.invoke(TransformMetaMethod.java:55)
        at groovy.lang.MetaClassImpl$2.invoke(MetaClassImpl.java:1308)
        at org.codehaus.groovy.runtime.metaclass.TransformMetaMethod.doMethodInvoke(TransformMetaMethod.java:62)
        at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)
        at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:38)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
        at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:53)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:139)
        at nextflow.extension.MapOp$_apply_closure1.doCall(MapOp.groovy:57)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)
        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)
        at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)
        at groovy.lang.Closure.call(Closure.java:412)
        at groovyx.gpars.dataflow.operator.DataflowOperatorActor.startTask(DataflowOperatorActor.java:120)
        at groovyx.gpars.dataflow.operator.DataflowOperatorActor.onMessage(DataflowOperatorActor.java:108)
        at groovyx.gpars.actor.impl.SDAClosure$1.call(SDAClosure.java:43)
        at groovyx.gpars.actor.AbstractLoopingActor.runEnhancedWithoutRepliesOnMessages(AbstractLoopingActor.java:293)
        at groovyx.gpars.actor.AbstractLoopingActor.access$400(AbstractLoopingActor.java:30)
        at groovyx.gpars.actor.AbstractLoopingActor$1.handleMessage(AbstractLoopingActor.java:93)
        at groovyx.gpars.util.AsyncMessagingCore.run(AsyncMessagingCore.java:132)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Jul-31 03:09:27.215 [Actor Thread 8] DEBUG nextflow.Session - Session aborted -- Cause: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_2.fastq.gz

### System information

Nextflow version：22.04.0
Hardware：WSL2
Executor：local
Container engine：docker
OS：WSL2
Version of nf-core/sarek：3.0",bioPG,https://github.com/nf-core/sarek/issues/698
I_kwDOCvwIC85O7BCX,Pass target-bed-file to bcftools/stats?,OPEN,2022-08-01T09:13:51Z,2024-08-19T13:12:28Z,,"### Description of feature

Do we want to pass a target-bed-file to the module `BCFTOOLS_STATS` in `subworkflows/nf-core/vcf_qc.nf`? By supplying a target-bed-file to `BCFTOOLS_STATS`, we (supposedly) get some nice depth-plot.

With the target-bed-file but without the tbi-file for the vcf-input-file, `BCFTOOLS_STATS` throws the following error:
```
nextflow run main.nf -profile test,tools_tumoronly,singularity --tools manta
```
resulted in the following error:
```
  Process `NFCORE_SAREK:SAREK:VCF_QC:BCFTOOLS_STATS (sample2)` terminated with an error exit status (255)

Command executed:

  bcftools stats \
       \
      --regions-file multi_intervals.bed \
      sample2.manta.tumor_sv.vcf.gz > sample2.manta.tumor_sv.bcftools_stats.txt

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:VCF_QC:BCFTOOLS_STATS"":
      bcftools: $(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*$//')
  END_VERSIONS

Command exit status:
  255

Command output:
  (empty)

Command error:
  [E::idx_find_and_load] Could not retrieve index file for 'sample2.manta.tumor_sv.vcf.gz'
  Failed to read from sample2.manta.tumor_sv.vcf.gz: could not load index

Work dir:
  /home/aspe/dev/fork_sarek/work/9b/0c3268ad4e17ee489871c6849292ff
  ```
    
The advantage of passing the target-bed-file to bcftools-stats, would be that then we get a depth-plot for wes-samples.

@SusiJo wrote the following on Slack explaining the problem in detail:  ""My problem was that I do not have any real WES data to test this properly, only the testdata itself. If the depth plot is really not displayed correctly in the multiqc report for real data without a target-bed file, then i would say it’s worth the effort. An empty plot in a multiqc is really weird and confusing. There are some modules in multiqc that state explicitly when there’s nothing to plot (like a warning). It would be nice to have sth like that for bcftools as well rather than showing an empty plot.
For some testdata cases there were these empty plots, but i assumed there would be depths to plot for real data.
So before changing the module upstream and a lot of stuff in sarek with these tbiand vcf  files i’d like to know if anyone who ran with real WES data has seen empty depth plots.. or if that’s really just a problem with the testdata.. If it’s just a testdata issue, it might even make sense to remove the target_bed from the bcftools module again. ""

",asp8200,https://github.com/nf-core/sarek/issues/699
I_kwDOCvwIC85O_N8C,BAM index (bai) should be provided,CLOSED,2022-08-02T02:19:00Z,2023-09-29T17:56:21Z,2023-05-25T19:25:20Z,"### Description of the bug

When I started with (u)BAM, I prepare a CSV contained patient, status, sample, lane, bam columns, I got an error: BAM index (bai) should be provided.

Why does this happen? The tutorial doesn't say Samplesheet Columns must contain bai. Must I provide Bam index bai when I start with (u)BAM? 

### Command used and terminal output

```console
command: nextflow run nf-core/sarek \
--input /SAN/colcc/CPI_raw/Jansen_CRC/Jansen_CRC.csv \
-profile singularity \
--step mapping

output:
BAM index (bai) should be provided.
Argument of `file` function cannot be null
```


### Relevant files

_No response_

### System information

Nextflow: 22.04.5.5708
Hardware: HPC
Container: Singularity
nf-core/sarek v3.0",sabrina0701,https://github.com/nf-core/sarek/issues/700
I_kwDOCvwIC85PiC0Z,Improve the process requests for tools in the pipeline,CLOSED,2022-08-10T09:16:18Z,2022-09-07T15:13:37Z,2022-09-07T15:13:36Z,"### Description of feature

Samtools pileup has the config process label of `process_medium`, but is not parallelised at all. 
It requests 6 cores and 36GB of RAM by default, but doesn't need the cores.
If the RAM requirement could also be decreased by default that would also be a good idea.

These are the modules that use `process_medium`:
https://github.com/nf-core/sarek/search?q=process_medium",SPPearce,https://github.com/nf-core/sarek/issues/701
I_kwDOCvwIC85PoCks,Output from ascat not stable,CLOSED,2022-08-11T12:33:16Z,2023-04-05T13:13:50Z,2023-04-05T13:13:50Z,"### Description of the bug

When running the following test, which is also also part of the CI-test `tests/test_tools_manually.yml`,

```nextflow run main.nf -profile test,tools_somatic_ascat```

@SusiJo found that the content (not just timestamps or something like that, but proper values!) of the output-files 
```
results/variant_calling/ascat/sample4_vs_sample3/sample4_vs_sample3.tumour_tumourBAF.txt
results/variant_calling/ascat/sample4_vs_sample3/sample4_vs_sample3.tumour_tumourLogR.txt
```
change on reruns:

![image](https://user-images.githubusercontent.com/37172585/184126552-c929a530-8bbb-4a1f-9591-fac45f8f600a.png)



### Command used and terminal output

```console
nextflow run main.nf -profile test,tools_somatic_ascat


""unstable"" output-files
results/variant_calling/ascat/sample4_vs_sample3/sample4_vs_sample3.tumour_tumourBAF.txt
results/variant_calling/ascat/sample4_vs_sample3/sample4_vs_sample3.tumour_tumourLogR.txt
```


### Relevant files

`sample4_vs_sample3.tumour_tumourBAF.txt` from consecutive runs of the test: 

[ascat2.zip](https://github.com/nf-core/sarek/files/9308388/ascat2.zip)
[ascat1.zip](https://github.com/nf-core/sarek/files/9308389/ascat1.zip)



### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/702
I_kwDOCvwIC85PwEiI,markduplicate change memory option is not work !,CLOSED,2022-08-13T16:22:16Z,2022-09-26T20:03:01Z,2022-09-26T20:02:54Z,"### Description of the bug

I try to change memory for mark duplicate but it not working. I used the command below. Please suggest what I should do.

sbatch --nodes=1 --ntasks=128 --time=100:00:00 --mem=120gb --partition=phi --job-name=sarekHB --output=sarekHB_vcall.log --wrap ""conda activate nextflow; module load singularity; nextflow run nf-core/sarek -r 2.7.2 --input infiles.tsv --genome GRCm38 -profile singularity --max_memory 180GB --max_cpus 30 -resume --tools HaplotypeCaller,Strelka,Manta,TIDDIT,snpEff,VEP --markdup_java_options \""-Xms4000m -Xmx200g\"" ""

However it use the default set up

Command executed:

  gatk --java-options ""-Xms3g -Xmx6g""         MarkDuplicates         --INPUT SAMPLE_ID.bam         --METRICS_FILE SAMPLE_ID.bam.metrics         --TMP_DIR .         --ASSUME_SORT_ORDER coordinate         --CREATE_INDEX true         --OUTPUT SAMPLE_ID.md.bam


Command exit status:
  1

Command output:
  (empty)

Command error:
  INFO  2022-08-13 03:06:39     MarkDuplicates  Tracking 16042768 as yet unmatched pairs. 10821298 records in RAM.
  INFO  2022-08-13 03:09:57     MarkDuplicates  Read   980,000,000 records.  Elapsed time: 12:30:19s.  Time for last 1,000,000:  198s.  Last read position: 2:98,667,040
  INFO  2022-08-13 03:09:57     MarkDuplicates  Tracking 16372570 as yet unmatched pairs. 11133555 records in RAM.
  INFO  2022-08-13 03:13:41     MarkDuplicates  Read   981,000,000 records.  Elapsed time: 12:34:03s.  Time for last 1,000,000:  223s.  Last read position: 2:98,667,042
  INFO  2022-08-13 03:13:41     MarkDuplicates  Tracking 16534416 as yet unmatched pairs. 11275291 records in RAM.
  INFO  2022-08-13 03:14:14     MarkDuplicates  Read   982,000,000 records.  Elapsed time: 12:34:36s.  Time for last 1,000,000:   33s.  Last read position: 2:98,667,044
  INFO  2022-08-13 03:14:14     MarkDuplicates  Tracking 16964439 as yet unmatched pairs. 11681597 records in RAM.
  INFO  2022-08-13 03:14:50     MarkDuplicates  Read   983,000,000 records.  Elapsed time: 12:35:12s.  Time for last 1,000,000:   35s.  Last read position: 2:98,667,046
  INFO  2022-08-13 03:14:50     MarkDuplicates  Tracking 17317945 as yet unmatched pairs. 12008545 records in RAM.
  INFO  2022-08-13 03:15:30     MarkDuplicates  Read   984,000,000 records.  Elapsed time: 12:35:51s.  Time for last 1,000,000:   39s.  Last read position: 2:98,667,048
  INFO  2022-08-13 03:15:30     MarkDuplicates  Tracking 17470328 as yet unmatched pairs. 12127653 records in RAM.
  INFO  2022-08-13 03:16:09     MarkDuplicates  Read   985,000,000 records.  Elapsed time: 12:36:30s.  Time for last 1,000,000:   38s.  Last read position: 2:98,667,050
  INFO  2022-08-13 03:16:09     MarkDuplicates  Tracking 17829708 as yet unmatched pairs. 12455928 records in RAM.
  INFO  2022-08-13 03:16:51     MarkDuplicates  Read   986,000,000 records.  Elapsed time: 12:37:13s.  Time for last 1,000,000:   42s.  Last read position: 2:98,667,053
  INFO  2022-08-13 03:16:51     MarkDuplicates  Tracking 18240443 as yet unmatched pairs. 12835077 records in RAM.
  INFO  2022-08-13 03:17:29     MarkDuplicates  Read   987,000,000 records.  Elapsed time: 12:37:51s.  Time for last 1,000,000:   37s.  Last read position: 2:98,667,055
  INFO  2022-08-13 03:17:29     MarkDuplicates  Tracking 18530286 as yet unmatched pairs. 13091879 records in RAM.
  INFO  2022-08-13 03:19:56     MarkDuplicates  Read   988,000,000 records.  Elapsed time: 12:40:17s.  Time for last 1,000,000:  146s.  Last read position: 2:98,667,057
  INFO  2022-08-13 03:19:56     MarkDuplicates  Tracking 18672057 as yet unmatched pairs. 13198180 records in RAM.
  INFO  2022-08-13 03:22:52     MarkDuplicates  Read   989,000,000 records.  Elapsed time: 12:43:14s.  Time for last 1,000,000:  176s.  Last read position: 2:98,667,058
  INFO  2022-08-13 03:22:52     MarkDuplicates  Tracking 18747010 as yet unmatched pairs. 13235826 records in RAM.
  INFO  2022-08-13 03:27:34     MarkDuplicates  Read   990,000,000 records.  Elapsed time: 12:47:56s.  Time for last 1,000,000:  282s.  Last read position: 2:98,667,061
  INFO  2022-08-13 03:27:34     MarkDuplicates  Tracking 19174881 as yet unmatched pairs. 13628199 records in RAM.
  INFO  2022-08-13 03:36:55     MarkDuplicates  Read   991,000,000 records.  Elapsed time: 12:57:17s.  Time for last 1,000,000:  560s.  Last read position: 2:98,667,063
  INFO  2022-08-13 03:36:55     MarkDuplicates  Tracking 19466422 as yet unmatched pairs. 13881969 records in RAM.
  [Sat Aug 13 04:11:05 UTC 2022] picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 811.50 minutes.
  Runtime.totalMemory()=5726797824
  To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp
  Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.lang.String.<init>(String.java:325)
        at htsjdk.samtools.util.StringUtil.bytesToString(StringUtil.java:301)
        at htsjdk.samtools.util.StringUtil.bytesToString(StringUtil.java:288)
        at htsjdk.samtools.BinaryTagCodec.readNullTerminatedString(BinaryTagCodec.java:423)
        at htsjdk.samtools.BinaryTagCodec.readSingleValue(BinaryTagCodec.java:318)
        at htsjdk.samtools.BinaryTagCodec.readTags(BinaryTagCodec.java:282)
        at htsjdk.samtools.BAMRecord.decodeAttributes(BAMRecord.java:412)
        at htsjdk.samtools.BAMRecord.getAttribute(BAMRecord.java:391)
        at htsjdk.samtools.SAMRecord.isValid(SAMRecord.java:2034)
        at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:848)
        at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:834)
        at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:802)
        at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:574)
        at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:553)
        at picard.sam.markduplicates.MarkDuplicates.buildSortedReadEndLists(MarkDuplicates.java:524)
        at picard.sam.markduplicates.MarkDuplicates.doWork(MarkDuplicates.java:257)
        at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305)
        at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25)
        at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163)
        at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206)
        at org.broadinstitute.hellbender.Main.main(Main.java:292)

Work dir:
  /storage/Intawat/Jhx_data/HBres/work/a1/f0a4cb854b4cfc9abd964fecc74c40

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`


### Command used and terminal output

```console
sbatch --nodes=1 --ntasks=128 --time=100:00:00 --mem=120gb --partition=phi --job-name=sarekHB --output=sarekHB_vcall.log --wrap ""conda activate nextflow; module load singularity; nextflow run nf-core/sarek -r 2.7.2 --input infiles.tsv --genome GRCm38 -profile singularity --max_memory 180GB --max_cpus 30 -resume --tools HaplotypeCaller,Strelka,Manta,TIDDIT,snpEff,VEP --markdup_java_options \""-Xms4000m -Xmx200g\"" ""
```


### Relevant files

_No response_

### System information

_No response_",intawat,https://github.com/nf-core/sarek/issues/703
I_kwDOCvwIC85QKCj3,Process exceeded running time limit even if I changed the config. file,CLOSED,2022-08-19T19:09:20Z,2022-08-25T08:31:58Z,2022-08-25T08:31:58Z,"### Description of the bug

I am running the WES for my tumor samples and I am getting the 16h time limit for manta. Then I changed the config file according to the suggestions on this webpage https://nf-co.re/sarek/3.0.1/usage, but the problem persisted.

### Command used and terminal output

```console
1. The slurm file I am running
#!/bin/bash
#SBATCH -o PDO106.out
#SBATCH -e PDO106.err
#SBATCH -J PDO106.job
#SBATCH -n 1
#SBATCH -N 1
#SBATCH -c 16
#SBATCH -t 200:00:00
#SBATCH --mem=64G
#SBATCH --get-user-env
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=ew152@duke.edu

nextflow run nf-core/sarek --max_memory 64.GB -profile singularity --wes --intervals /work/ew152/WES/hg38_bed3_GATK.bed --input PDO106.csv --outdir . --tools manta,snpeff -c /work/ew152/WES/01.RawData/WES.conf

2. The customized config files WES.conf
process {
      withName:NFCORE_SAREK:SAREK:TUMOR_ONLY_VARIANT_CALLING:RUN_MANTA_TUMORONLY:MANTA_TUMORONLY {
          memory = 64.GB
          time = 500.h
    }
      withName:NFCORE_SAREK:SAREK:TUMOR_ONLY_VARIANT_CALLING:RUN_STRELKA_SINGLE:STRELKA_SINGLE {
          memory = 64.GB
          time = 500.h
    }
      withName:NFCORE_SAREK:SAREK:FASTP {
          memory = 64.GB
          time = 500.h
    }
}

3. the error I am still getting
executor >  local (236)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[1f/bd0c04] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[b3/e1eb24] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 1 of 1 ✔
[49/95183c] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 65 of 65 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[6a/042598] process > NFCORE_SAREK:SAREK:RUN_FAST... [100%] 2 of 2 ✔
[45/7e82c7] process > NFCORE_SAREK:SAREK:FASTP (P... [100%] 2 of 2 ✔
[c8/e3793d] process > NFCORE_SAREK:SAREK:GATK4_MA... [100%] 24 of 24 ✔
[-        ] process > NFCORE_SAREK:SAREK:GATK4_MA... -
[-        ] process > NFCORE_SAREK:SAREK:GATK4_MA... -
[22/5a95fd] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 1 of 1 ✔
[79/474c72] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 1 of 1 ✔
[51/51b113] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 1 of 1 ✔
[cc/fd019f] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 1 of 1 ✔
[dc/856539] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 65 of 65 ✔
[72/e05db9] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 1 of 1 ✔
[30/dce7e6] process > NFCORE_SAREK:SAREK:RECALIBR... [100%] 65 of 65 ✔
[c5/4fbe6d] process > NFCORE_SAREK:SAREK:RECALIBR... [100%] 1 of 1 ✔
[80/15783c] process > NFCORE_SAREK:SAREK:RECALIBR... [100%] 1 of 1 ✔
[7c/78f8c8] process > NFCORE_SAREK:SAREK:CRAM_QC:... [100%] 1 of 1 ✔
[35/23b3fb] process > NFCORE_SAREK:SAREK:CRAM_QC:... [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:SAMTOOLS... -
[-        ] process > NFCORE_SAREK:SAREK:GERMLINE... -
[-        ] process > NFCORE_SAREK:SAREK:GERMLINE... -
[-        ] process > NFCORE_SAREK:SAREK:GERMLINE... -
[-        ] process > NFCORE_SAREK:SAREK:GERMLINE... -
[5a/71125b] process > NFCORE_SAREK:SAREK:TUMOR_ON... [  1%] 1 of 65, failed: 1
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:B... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:V... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:V... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:V... -
[-        ] process > NFCORE_SAREK:SAREK:ANNOTATE... -
[-        ] process > NFCORE_SAREK:SAREK:ANNOTATE... -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_D... -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC     -
Execution cancelled -- Finishing pending tasks before exit
Error executing process > 'NFCORE_SAREK:SAREK:TUMOR_ONLY_VARIANT_CALLING:RUN_MANTA_TUMORONLY:MANTA_TUMORONLY (PDO)'

Caused by:
  Process exceeded running time limit (16h)

Command executed:

  configManta.py         --tumorBam PDO.recal.cram         --reference Homo_sapiens_assembly38.fasta         --exome --callRegions chr17_17137915-17138077.bed.gz         --runDir manta

  python manta/runWorkflow.py -m local -j 12

  mv manta/results/variants/candidateSmallIndels.vcf.gz         PDO.manta.chr17_17137915-17138077.candidate_small_indels.vcf.gz
  mv manta/results/variants/candidateSmallIndels.vcf.gz.tbi         PDO.manta.chr17_17137915-17138077.candidate_small_indels.vcf.gz.tbi
  mv manta/results/variants/candidateSV.vcf.gz         PDO.manta.chr17_17137915-17138077.candidate_sv.vcf.gz
  mv manta/results/variants/candidateSV.vcf.gz.tbi         PDO.manta.chr17_17137915-17138077.candidate_sv.vcf.gz.tbi
  mv manta/results/variants/tumorSV.vcf.gz         PDO.manta.chr17_17137915-17138077.tumor_sv.vcf.gz
  mv manta/results/variants/tumorSV.vcf.gz.tbi         PDO.manta.chr17_17137915-17138077.tumor_sv.vcf.gz.tbi

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:TUMOR_ONLY_VARIANT_CALLING:RUN_MANTA_TUMORONLY:MANTA_TUMORONLY"":
      manta: $( configManta.py --version )
  END_VERSIONS

Command exit status:
  -

Command output:

  Successfully created workflow run script.
  To execute the workflow, run the following script and set appropriate options:

  manta/runWorkflow.py

Command error:
  INFO:    Converting SIF file to temporary sandbox...
  .command.run: line 31: /dev/fd/62: No such file or directory

Work dir:
  /work/ew152/WES/01.RawData/106_out/work/5a/71125b027320b6fc97a79fbbdfeb9c

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line
```


### Relevant files

_No response_

### System information

nextflow                  22.04.0              h4a94de4_0    bioconda
nf-core                   2.4.1              pyh5e36f6f_1    bioconda

Running slurm on the HPC, with the Sigularity container.",andrewucla,https://github.com/nf-core/sarek/issues/711
I_kwDOCvwIC85QSELF,BWA index isn't coppied over with using save-reference option,OPEN,2022-08-22T20:00:19Z,2024-02-08T15:12:34Z,,"### Description of the bug

BWA index isn't copied over with using save-reference option on Azure. This is because it needs files in the path to copy over.

Can be fixed with updating the index modules: https://github.com/nf-core/modules/pull/1964

### Command used and terminal output

```console
./nextflow run .nextflow/assets/nf-core/sarek/main.nf -c azure.config -w ""az://genomas-raros/work"" --outdir ""az://genomas-raros/results""  --tools haplotypecaller --input sarek.csv --igenomes_ignore --save_reference --genome 'custom'
```


### Relevant files

_No response_

### System information

* nextflow version 22.04.5.5708
*  Azure VM 
* Executor Azure
* Docker
* OS Ubuntu
* 3.0.1",vsmalladi,https://github.com/nf-core/sarek/issues/712
I_kwDOCvwIC85QX3Hf,The executor freezes for two days without any error message,OPEN,2022-08-23T19:17:53Z,2023-06-16T09:20:34Z,,"### Description of the bug

I am using Sarek for Whole exome sequencing, running on slurm. I noticed this pipeline is stuck in one process without outputing any error messages. It hangs in the step [ 0%] 0 of 65.

### Command used and terminal output

```console
1. script
#!/bin/bash
#SBATCH -o POSC187.out
#SBATCH -e POSC187.err
#SBATCH -J POSC187.job
#SBATCH -n 1
#SBATCH -N 1
#SBATCH -c 16
#SBATCH -p scavenger
#SBATCH -t 500:00:00
#SBATCH --mem=128G
#SBATCH --get-user-env
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=ew152@duke.edu

nextflow run nf-core/sarek --max_memory 128.GB -profile singularity --wes --intervals /work/ew152/WES/hg38_bed3_GATK.bed --input POSC187.csv --outdir . --tools manta,snpeff -c /work/ew152/WES/01.RawData/WES_nolim.conf

2. terminal output

executor >  local (235)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[a9/1f64ae] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[45/82b624] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 1 of 1 ✔
[b0/76dc9e] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 65 of 65 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[2b/de0021] process > NFCORE_SAREK:SAREK:RUN_FAST... [100%] 2 of 2 ✔
[bf/335305] process > NFCORE_SAREK:SAREK:FASTP (P... [100%] 2 of 2 ✔
[09/74febf] process > NFCORE_SAREK:SAREK:GATK4_MA... [100%] 24 of 24 ✔
[-        ] process > NFCORE_SAREK:SAREK:GATK4_MA... -
[-        ] process > NFCORE_SAREK:SAREK:GATK4_MA... -
[dd/ae261e] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 1 of 1 ✔
[52/687d90] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 1 of 1 ✔
[26/96df17] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 1 of 1 ✔
[22/1be9cd] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 1 of 1 ✔
[a9/805102] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 65 of 65 ✔
[08/fd290a] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 1 of 1 ✔
[89/5a2afb] process > NFCORE_SAREK:SAREK:RECALIBR... [100%] 65 of 65 ✔
[90/0de739] process > NFCORE_SAREK:SAREK:RECALIBR... [100%] 1 of 1 ✔
[5e/1f7a97] process > NFCORE_SAREK:SAREK:RECALIBR... [100%] 1 of 1 ✔
[72/471faa] process > NFCORE_SAREK:SAREK:CRAM_QC:... [100%] 1 of 1 ✔
[69/d752cd] process > NFCORE_SAREK:SAREK:CRAM_QC:... [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:SAMTOOLS... -
[-        ] process > NFCORE_SAREK:SAREK:GERMLINE... -
[-        ] process > NFCORE_SAREK:SAREK:GERMLINE... -
[-        ] process > NFCORE_SAREK:SAREK:GERMLINE... -
[-        ] process > NFCORE_SAREK:SAREK:GERMLINE... -
[b4/ce31c2] process > NFCORE_SAREK:SAREK:TUMOR_ON... [  0%] 0 of 65
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:B... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:V... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:V... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:V... -
[-        ] process > NFCORE_SAREK:SAREK:ANNOTATE... -
[-        ] process > NFCORE_SAREK:SAREK:ANNOTATE... -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_D... -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC     -
Staging foreign file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai
Staging foreign file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta
```


### Relevant files

_No response_

### System information

_No response_",andrewucla,https://github.com/nf-core/sarek/issues/713
I_kwDOCvwIC85QciHZ,Add documentation for Azure config specs,CLOSED,2022-08-24T15:43:31Z,2022-09-07T14:35:23Z,2022-09-07T14:35:23Z,"### Description of feature

Add Azure specifc Sarek pipeline VM_sizes documentation

```
azure {
    batch {
        pools {
            auto {
               autoScale = true

               sku = ""batch.node.centos 7""

               offer = ""centos-container""

               publisher = ""microsoft-azure-batch""

               vmType = 'Standard_E64_v3'

               vmCount = 1

            }
        }
    }
}
```",vsmalladi,https://github.com/nf-core/sarek/issues/714
I_kwDOCvwIC85QliRx,More input checks,CLOSED,2022-08-26T09:36:41Z,2022-09-09T08:02:56Z,2022-09-09T08:02:56Z,"### Description of feature

- [x] `--joint_germline` should fail if no `--tools haplotypecaller` is provided
- [ ] ~~samples' uniqueness' tests should be tested on multiple `samplesheet.csv` files~~

",maxulysse,https://github.com/nf-core/sarek/issues/717
I_kwDOCvwIC85RAsSx,params tools should fail with a space (finishing coma),CLOSED,2022-09-01T16:49:50Z,2022-09-06T09:05:24Z,2022-09-06T09:05:24Z,"### Description of the bug

```
nextflow run . -profile test,docker --tools strelka,manta, mutect2
```
Doesn't fail and it should.
Need to check the regex to figure out that the `,` should be the last character.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/722
I_kwDOCvwIC85RUaN1,A USER ERROR has occurred: Bad input: Sample $name is not in BAM header: [...],CLOSED,2022-09-07T08:39:25Z,2022-11-02T22:02:19Z,2022-11-02T22:02:19Z,"# Description of the bug
When running sarek 3.0.1 with Mutect2 on tumor-normal paired WES data in uBAM format, the pipeline will crash with an error message saying: 
""""""
A USER ERROR has occurred: Bad input: Sample $name is not in BAM header: [...]
""""""

When running the same command on tumor-normal paired WES in fastq format, the pipeline will run just fine.

# Command used
```
ref_dir=/path/to/refs
vep_dir=${ref_dir}/vep106.1

NXF_SINGULARITY_CACHEDIR=/path/to/singularity_cache
export NXF_SINGULARITY_CACHEDIR=$NXF_SINGULARITY_CACHEDIR
nextflow run /my/local/sarek-3.0.1/workflow/ \
	-with-trace trace_complete.txt \
	-with-timeline timeline_complete.html \
	-with-dag dag_complete.pdf \
	--tools 'mutect2' \
	--input 't-n_wes_ubams.csv' \
	--wes \
	--genome GATK.GRCh38 \
	--igenomes_base ${ref_dir} \
	--vep_cache ${vep_dir} \
	--max_time '240.h' \
	--max_memory '32.GB' \
	--max_cpus 16 \
	-profile singularity > complete.log 2>&1
```

# Findings
We checked the code and it indeed looks for {meta.patient}\_{meta.normal_id} in the CRAM/BAM header, but our CRAM header only matched the {meta.normal_id}.
https://github.com/nf-core/sarek/blob/ad2b34f39fead34d7a09051e67506229e827e892/conf/modules.config#L1111
When we modified the code so that it would only look for {meta.normal_id}, the code would run until the end without any issues. We did a rerun starting from fastq files and there the original pipeline did not crash. We proceeded to look into the read_group generation and there we found the issue:
Starting from fastq assigns {row.patient}\_{row.sample} to ""SM"" in the read group,
https://github.com/nf-core/sarek/blob/5bb160ddb2cee50c811585e450b16cba13c95a02/workflows/sarek.nf#L1196
while Starting from bam assigns {row.sample} to ""SM"".
https://github.com/nf-core/sarek/blob/5bb160ddb2cee50c811585e450b16cba13c95a02/workflows/sarek.nf#L1219
We think that this is a bug, since Mutect2 variant calling would always crash when looking for the {meta.patient}\_{meta.normal_id}  in the {row.sample} BAM header of uBAM-derived sequencing data.

We think the solution would be to update the ""\\tSM:${row.sample}"" to ""\\tSM:${row.patient}_${row.sample}"" in line **1219** of **sarek/workflows/sarek.nf**  (However, we did not test this yet).

For now people are able to circumvent this bug in tumor-normal mutect2 runs, by starting from fastq reads.",petanska,https://github.com/nf-core/sarek/issues/732
I_kwDOCvwIC85RV-ax,Incorrect meta used for bcftools sort step of gatk joint_variant_calling ,CLOSED,2022-09-07T14:01:29Z,2022-09-08T19:44:50Z,2022-09-08T19:44:50Z,"### Description of the bug

I am currently trying to use Sarek to perform joint gatk4 variant calling for a WES cohort. However, I am running into an error in the NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_HAPLOTYPECALLER:JOINT_GERMLINE:BCFTOOLS_SORT step where the meta freebayes is used for bcftools sorting. 

I think this is due to incorrect meta used for the bcftools sort step in the joint_germline subworkflow. 

### Command used and terminal output

```console
$ nextflow run ~/nf-core-sarek-3.0.1/workflow -profile singularity
	--input 20220824_isgs_sarek.csv  \
	--wes --intervals ../xgen-exome-hyb-panel-v2-targets-hg38.bed \
	-resume -c $HOME/niagara.config \
	--igenomes_base {pathtogenome} \
	--tools haplotypecaller,manta,cnvkit,snpeff \
	--joint_germline --use_gatk_spark markduplicates \
	 --nucleotides_per_second 50


Error executing process > 'NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_HAPLOTYPECALLER:JOINT_GERMLINE:BCFTOOLS_SORT (joint_variant_calling)'

Caused by:
  Process `NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_HAPLOTYPECALLER:JOINT_GERMLINE:BCFTOOLS_SORT (joint_variant_calling)` terminated with an error exit status (255)

Command executed:

  bcftools \
      sort \
      --output joint_variant_calling.freebayes.vcf.gz \
       \
      joint_germline.vcf.gz
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_HAPLOTYPECALLER:JOINT_GERMLINE:BCFTOOLS_SORT"":
      bcftools: $(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*$//')
  END_VERSIONS

Command exit status:
  255

Command output:
  (empty)
```


### Relevant files

_No response_

### System information

Nextflow version 21.10.3, hardware: HPC, executor: slurm (although this nextflow job was submitted as local since on my cluster I can only use whole nodes (i.e 40-cores)), container: singularity, sarek version:3.0.1",peter-yufan-zeng,https://github.com/nf-core/sarek/issues/737
I_kwDOCvwIC85RZklf,"Combine all VCFs across different variants caller, such as mutect2, strelka2",CLOSED,2022-09-08T07:08:59Z,2022-12-07T08:42:17Z,2022-12-07T08:42:17Z,"### Description of feature

We usually combine multiple software results in tumor variants calling, so I think it is necessary to use appropriate methods to combine them, such as union and consistency. I am looking forward to this feature",Githubguanxudong,https://github.com/nf-core/sarek/issues/738
I_kwDOCvwIC85RZrX1,Control-FREEC crashes under custom BED file (`--intervals`),CLOSED,2022-09-08T07:29:54Z,2022-09-08T08:00:39Z,2022-09-08T07:58:17Z,"### Description of the bug

Hi, I've processes my samples with a commercial capture kit (TWIST) and, consequently, I'm using the capture BED file in the pipeline... but under _Control-FREEC_ execution, it prompts the following error with _HLA-DRB1_ gene:

```
Error: chromosome HLA-DRB1*11:01:02 present in your Homo_sapiens_assembly38.fasta.fai file was not detected in your file with capture regions Twist_Exome_RefSeq_targets_hg38.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome HLA-DRB1*11:01:02 from your Homo_sapiens_assembly38.fasta.fai
```

### Command used and terminal output

```console
### command ---
nextflow run nf-core/sarek -r 3.0.1 -profile singularity -params-file myparams.yaml -c mynextflow.config --input samples.csv


### myparams.yaml ---
## main
intervals : ""/beegfs/home/jgarces/genomes/WES_baits/TWIST/Twist_Exome_RefSeq_targets_hg38.bed""
wes : true
tools          : snpeff,ascat,manta,mutect2,strelka,controlfreec,vep,tiddit
skip_tools : mosdepth,baserecalibrator_report,markduplicates_report

## preprocessing
save_bam_mapped: false

## variant calling
only_paired_variant_calling : true

## reference genome
igenomes_base  : ""/beegfs/home/jgarces/genomes/iGenomes/""
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/9524665/nextflow.log)


### System information

- Nextflow version: 22.04.0
- Hardware: HPC
- Executor: slurm
- Container engine: singularity
- OS: CentOS Linux
- Version of nf-core/sarek: 3.0.1",jgarces02,https://github.com/nf-core/sarek/issues/739
I_kwDOCvwIC85Rggpm,No index file found,CLOSED,2022-09-09T08:29:09Z,2022-09-09T09:40:52Z,2022-09-09T09:40:52Z,"### Description of the bug

Hi,

I am running Sarek-Mutect (tumor only) on some samples. I am getting an error that there was no index provided for the germline resource. But I am providing the --germline_resource_index...
I checked in the work directory, but I cannot see the index file that I am providing.
![image](https://user-images.githubusercontent.com/56801139/189306966-642d6be2-73a7-4c3c-994d-53e8fcfc9fb2.png)

Someone knows what could happen?

Thanks!

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.0.1 -profile singularity -c /workspace/datasets/sjd_seq/code/sarek/sarek_3.0.1.conf --input input_vc.csv --step variant_calling --tools 'mutect2' --fasta /workspace/datasets/genomes/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fa --fasta_fai /workspace/datasets/genomes/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fa.fai --dict /workspace/datasets/genomes/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.dict --germline_resource /workspace/datasets/genomes/iGenomes/Homo_sapiens/GATK/GRCh38/Annotation/GermlineResource/gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz --germline_resource_index /workspace/datasets/genomes/iGenomes/Homo_sapiens/GATK/GRCh38/Annotation/GermlineResource/gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz.tbi -resume



Error executing process > 'NFCORE_SAREK:SAREK:TUMOR_ONLY_VARIANT_CALLING:GATK_TUMOR_ONLY_SOMATIC_VARIANT_CALLING:GETPILEUPSUMMARIES (AQ5174)'

Caused by:
  Process `NFCORE_SAREK:SAREK:TUMOR_ONLY_VARIANT_CALLING:GATK_TUMOR_ONLY_SOMATIC_VARIANT_CALLING:GETPILEUPSUMMARIES (AQ5174)` terminated with an error exit status (2)

Command executed:

  gatk --java-options ""-Xmx12g"" GetPileupSummaries \
      --input AQ5174.recal.cram \
      --variant gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz \
      --output AQ5174.mutect2.chr1_143184588-223558935.pileups.table \
      --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fa \
      --intervals chr1_143184588-223558935.bed \
      --tmp-dir . \
  
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:TUMOR_ONLY_VARIANT_CALLING:GATK_TUMOR_ONLY_SOMATIC_VARIANT_CALLING:GETPILEUPSUMMARIES"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  2

Command output:
  (empty)

Command error:
  Using GATK jar /usr/local/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx12g -jar /usr/local/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar GetPileupSummaries --input AQ5174.recal.cram --variant gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz --output AQ5174.mutect2.chr1_143184588-223558935.pileups.table --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fa --intervals chr1_143184588-223558935.bed --tmp-dir .
  07:59:46.319 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so
  07:59:46.499 INFO  GetPileupSummaries - ------------------------------------------------------------
  07:59:46.499 INFO  GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.2.6.1
  07:59:46.499 INFO  GetPileupSummaries - For support and documentation go to https://software.broadinstitute.org/gatk/
  07:59:46.499 INFO  GetPileupSummaries - Executing as msanchezg@bbgn021 on Linux v4.18.0-348.el8.0.2.x86_64 amd64
  07:59:46.499 INFO  GetPileupSummaries - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src
  07:59:46.500 INFO  GetPileupSummaries - Start Date/Time: September 9, 2022 at 7:59:46 AM GMT
  07:59:46.500 INFO  GetPileupSummaries - ------------------------------------------------------------
  07:59:46.500 INFO  GetPileupSummaries - ------------------------------------------------------------
  07:59:46.500 INFO  GetPileupSummaries - HTSJDK Version: 2.24.1
  07:59:46.500 INFO  GetPileupSummaries - Picard Version: 2.27.1
  07:59:46.500 INFO  GetPileupSummaries - Built for Spark Version: 2.4.5
  07:59:46.500 INFO  GetPileupSummaries - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  07:59:46.500 INFO  GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  07:59:46.500 INFO  GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  07:59:46.500 INFO  GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  07:59:46.500 INFO  GetPileupSummaries - Deflater: IntelDeflater
  07:59:46.501 INFO  GetPileupSummaries - Inflater: IntelInflater
  07:59:46.501 INFO  GetPileupSummaries - GCS max retries/reopens: 20
  07:59:46.501 INFO  GetPileupSummaries - Requester pays: disabled
  07:59:46.501 INFO  GetPileupSummaries - Initializing engine
  07:59:48.291 INFO  FeatureManager - Using codec VCFCodec to read file file://gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz
  07:59:48.348 INFO  GetPileupSummaries - Shutting down engine
  [September 9, 2022 at 7:59:48 AM GMT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 0.03 minutes.
  Runtime.totalMemory()=2147483648
  ***********************************************************************
  
  A USER ERROR has occurred: An index is required but was not found for file gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz. Support for unindexed block-compressed files has been temporarily disabled. Try running IndexFeatureFile on the input.
  
  ***********************************************************************
  Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.

Work dir:
  /workspace/nobackup/sjd_seq/platinum_results/20220809/pt1-t1-allsamples-t1/mutect_results/work/e1/d809a2c83b1125451adf202217b809

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line


java.nio.channels.ClosedByInterruptException


Unexpected error [NullPointerException]


A DataflowVariable can only be assigned once. Use bind() to allow for equal values to be passed into already-bound variables.
```


### Relevant files

[command_files.zip](https://github.com/nf-core/sarek/files/9533630/command_files.zip)


### System information

Nextflow version: 22.04.5 build 5708
Hardware: HPC
Executor: slurm
Container engine: Singularity
OS: CentOS Linux
Version of nf-core/sarek: 3.0.1",msguixe,https://github.com/nf-core/sarek/issues/745
I_kwDOCvwIC85Rkxgg,UMI in fastq read names,CLOSED,2022-09-10T11:32:47Z,2022-11-28T16:21:47Z,2022-09-26T19:58:52Z,"###  UMI in fastq read names or additional fastq

Can the UMI be present in the fastq read names (e.g. `@A00154:125:HGKTMDMXX:1:1101:10420:1000 3:N:0:AACTGAGG+ATGCGTC_myUMI`)? Or in an additional fastq file, containing only the UMIs?

Looks like the only way to deal with UMIs is having them in the reads and extracting directly using the UMI Read Structure?  https://nf-co.re/sarek/usage#how-to-handle-umis

Thanks!
",migrau,https://github.com/nf-core/sarek/issues/746
I_kwDOCvwIC85R0enh,Singularity: container creation failed possible causes are that your kernel doesn't support the compression algorithm or the image is corrupted,CLOSED,2022-09-14T10:05:46Z,2022-10-08T05:32:29Z,2022-10-08T05:32:29Z,"### Description of the bug

Hi,

Thanks for your time.

I installed nextflow and singularity referred the manual of 'https://nf-co.re/sarek'

To run it offline, iGenome files (GRCh38) were downloaded with 'bash aws-igenomes.sh -s GATK'.

And the fastq files were 
SRR7890918_WES_HCC1395-EA_tumor_1.fastq.gz
SRR7890918_WES_HCC1395-EA_tumor_2.fastq.gz
SRR7890919_WES_HCC1395BL-EA_normal_1.fastq.gz
SRR7890919_WES_HCC1395BL-EA_normal_2.fastq.gz.


The sarek pipeline always crash at 'BWAMEM1_MEM', could you help me solving this. Thanks a lot.

By the way, I have run the nf-core-rnaseq-3.7 successfully , but sarek always failed.


### Command used and terminal output

```console
nextflow run /home/cjh/jiehui/WES/nf-core-sarek-3.0.1/workflow -profile singularity \
	--custom_config_base /home/cjh/jiehui/WES/nf-core-sarek-3.0.1/configs \
	--input samplesheet.csv \
	--igenomes_base /home/cjh/download/references \
	--outdir /home/cjh/jiehui/WES/nf-core-awsmegatests/output
```


### Relevant files

[Sarek_jiehui.zip](https://github.com/nf-core/sarek/files/9565171/Sarek_jiehui.zip)


### System information

Nextflow version:version 22.04.5, build 5708 (15-07-2022 16:09 UTC)
Hardware:HPC
Container engine: Singularity
OS:Linux
Version of nf-core/sarek:3.0.1",jiehuichen,https://github.com/nf-core/sarek/issues/747
I_kwDOCvwIC85THg2u,containers missing in nf-core download,CLOSED,2022-09-14T10:35:34Z,2023-07-05T22:27:50Z,2023-03-21T13:12:15Z,"### Description of the bug

Hi,

I have downloaded sarek latest version using nf-core tools, with all singularity containers (in theory). When I move the pipeline to the cluster (no internet access) and execute it selecting deepvariant as a variant caller it fails locating the deepvariant container and therefore the job fails (no internet).

I can download it manually with the command line:
```bash
singularity pull  --name google-deepvariant-1.3.0.img.pulling.1663095569629 docker://google/deepvariant:1.3.0 > /dev/null
```

but perhaps this is happening for other containers? It might be worth taking a look into the list.

Best,
JL

### Command used and terminal output

```console
Error executing process > 'NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_DEEPVARIANT:DEEPVARIANT (WTCHG_722292)'

Caused by:
  Failed to pull singularity image
  command: singularity pull  --name google-deepvariant-1.3.0.img.pulling.1663095569629 docker://google/deepvariant:1.3.0 > /dev/null
  status : 255
  message:
    FATAL:   While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 54.242.59.189:443: i/o timeout
```",egenomics,https://github.com/nf-core/sarek/issues/769
I_kwDOCvwIC85SAw4W,Add an option in '--tools' parameter for QC task only,CLOSED,2022-09-16T13:11:33Z,2022-09-21T16:55:24Z,2022-09-21T16:55:23Z,"### Description of feature

Dear Serak developer, 

I have tumor-normal pair bam files, and would like to only do a BAM file QC analysis (without any variant calling process). Is that possible for add such an option in --tools parameter, and ask the pipeline to run QC tasks only (e.g. fastqc, samtools, BamQC, bcftools, multiQC....). 

Thanks in advance

Sen
",senzhaocode,https://github.com/nf-core/sarek/issues/753
I_kwDOCvwIC85SG7u4,Tools load into a cluster job,CLOSED,2022-09-19T07:50:46Z,2023-06-07T06:11:15Z,2023-06-07T06:11:14Z,"### Description of the bug

Dear,
I am working in a cluster without connection to Internet. I donwload the pipeline by nf-core with all singularity images. In addition, I download the snpeff and vep cache and the reference genome GATK.GRCh38.
When I run the pipeline on my local PC, there isn't any problem. But, when I upload the pipeline on the cluster and I run the same command, my pipeline is stopped after finishing the recalibration step. I try to do variant calling with strelka and/or mutect2 but 
the pipeline is stopped before this step and my output said ""Pipeline completed successfully"".

Any idea?

### Command used and terminal output

```console
#!/bin/bash
#SBATCH --qos=standard
#SBATCH --job-name=s_301
#SBATCH --nodes=1
#SBATCH --tasks=2
#SBATCH --cpus-per-task=20                  
#SBATCH --tasks-per-node=2
#SBATCH --output=/slgpfs/projects/cli20/w_directory/sarek_301_%j.out
#SBATCH --chdir=/slgpfs/projects/cli20/w_directory
#SBATCH --error=/slgpfs/projects/cli20/w_directory/sarek_300_%j.err

module load java singularity/3.6.3 nextflow/21.10.6 python

export SINGULARITY_TMPDIR=/slgpfs/projects/soft_support/nf-core-sarek-3.0/singularity-images
export SINGULARITY_LOCALCACHEDIR=/slgpfs/soft_support/nf-core-sarek-3.0/singularity-images
export NXF_SINGULARITY_CACHEDIR=/slgpfs/projects/soft_support/nf-core-sarek-3.0/singularity-images

nextflow run /slgpfs/projects/soft_support/nf-core-sarek-3.0.1/workflow/main.nf 
--tools mutect2,vep -profile singularity --input './1.csv' --genome GATK.GRCh38 
--igenomes_base '/slgpfs/projects/nf-core-sarek-3.0.1/references' --outdir ./prova_array 
--max_cpus 16 --snpeff_cache '/slgpfs/projects/snpEff/data' 
--vep_cache '/slgpfs/projects/vep' --vep_cache_version 107


My final output is:

executor >  local (1282)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[43/38d92c] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[93/e74d1d] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[9b/af679b] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 1 of 1 ✔
[4d/3de30d] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 133 of 133 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[-        ] process > NFCORE_SAREK:SAREK:ALIGNMEN... -
[91/a2fdd7] process > NFCORE_SAREK:SAREK:RUN_FAST... [100%] 4 of 4 ✔
[94/c65d24] process > NFCORE_SAREK:SAREK:FASTP (""... [100%] 4 of 4 ✔
[5d/d3b5ff] process > NFCORE_SAREK:SAREK:GATK4_MA... [100%] 48 of 48 ✔
[-        ] process > NFCORE_SAREK:SAREK:GATK4_MA... -
[-        ] process > NFCORE_SAREK:SAREK:GATK4_MA... -
[10/97abf0] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 4 of 4 ✔
[81/c8f129] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 4 of 4 ✔
[7c/6094cb] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 4 of 4 ✔
[af/dcc7d5] process > NFCORE_SAREK:SAREK:MARKDUPL... [100%] 4 of 4 ✔
[c3/633b52] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 532 of 532 ✔
[b1/e23c11] process > NFCORE_SAREK:SAREK:PREPARE_... [100%] 4 of 4 ✔
[9c/0b5556] process > NFCORE_SAREK:SAREK:RECALIBR... [100%] 532 of 532 ✔
[5d/1d9a75] process > NFCORE_SAREK:SAREK:RECALIBR... [100%] 4 of 4 ✔
[-        ] process > NFCORE_SAREK:SAREK:RECALIBR... -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_QC:... -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_QC:... -
[-        ] process > NFCORE_SAREK:SAREK:SAMTOOLS... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:TUMOR_ON... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:PAIR_VAR... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:B... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:V... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:V... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC:V... -
[-        ] process > NFCORE_SAREK:SAREK:ANNOTATE... -
[-        ] process > NFCORE_SAREK:SAREK:ANNOTATE... -
[e4/0507f6] process > NFCORE_SAREK:SAREK:CUSTOM_D... [100%] 1 of 1 ✔
[68/df8064] process > NFCORE_SAREK:SAREK:MULTIQC     [100%] 1 of 1 ✔
-[nf-core/sarek] Pipeline completed successfully-
Completed at: 17-sept.-2022 02:07:28
Duration    : 9h 15m 14s
CPU hours   : 159.0
Succeeded   : 1'282
```


### Relevant files

_No response_

### System information

load JDK/12.0.2 (PATH, C_INCLUDE_PATH, CPLUS_INCLUDE_PATH, JAVA_HOME,
JAVA_ROOT, JAVA_BINDIR, SDK_HOME)
load SINGULARITY/3.6.3 (PATH, SINGULARITYENV_TZ)
load NEXTFLOW/21.10.6 (PATH)
load PYTHON/3.6.5 (PATH, MANPATH, LD_LIBRARY_PATH, LIBRARY_PATH,
PKG_CONFIG_PATH, C_INCLUDE_PATH, CPLUS_INCLUDE_PATH, PYTHONHOME)

Hardware: HPC
Version sarek 3.0.1",jbague,https://github.com/nf-core/sarek/issues/754
I_kwDOCvwIC85SMW_W,start pipeline from --joint_germline step,OPEN,2022-09-20T07:47:14Z,2024-08-19T13:12:28Z,,"### Description of feature

In order to deal with continuously growing number of gvcf files from haplotypecaller, it would be helpful to start the pipeline from the --joint_germline step. This would enable to tackle the n+1 problem without having to rerun the haplotypecaller for all samples.",jrhaas,https://github.com/nf-core/sarek/issues/755
I_kwDOCvwIC85SRk_Q,cnvkit Process exceeded running time limit (4h),CLOSED,2022-09-21T05:29:10Z,2024-09-25T10:08:12Z,2022-09-21T16:45:02Z,"I do get the following error, when using the following run command with json.params file:

comand:

```bash
nextflow run nf-core/sarek -r 3.0 -profile docker -params-file nf-params.json -bg
```

params.json:

```json
{
    ""input"": ""example_pair_fastq.csv"",
    ""tools"": ""freebayes,msisensorpro,controlfreec,snpeff,vep,cnvkit"",
    ""wes"": true,
    ""vep_cache_version"": 104,
    ""vep_cache"": ""\/mnt\/quobyte\/local_vep_cache_dir_tmp"",
    ""max_cpus"": 28,
    ""max_memory"": ""245.GB"",
    ""max_time"": ""10.h""
}
```

execution_report error:

Workflow execution completed unsuccessfully!
The exit status of the task that caused the workflow execution to fail was: null.

The full error message was:

Error executing process > 'NFCORE_SAREK:SAREK:PAIR_VARIANT_CALLING:RUN_CNVKIT:CNVKIT_BATCH (SAMPLE_ID_SRR7890844_vs_SAMPLE_ID_SRR7890845)'

Caused by:
  Process exceeded running time limit (4h)

Command executed:

  samtools view -T Homo_sapiens_assembly38.fasta --fai-reference Homo_sapiens_assembly38.fasta.fai SAMPLE_ID_SRR7890844.recal.cram -@ 2 -o SAMPLE_ID_SRR7890844.recal.bam
  samtools view -T Homo_sapiens_assembly38.fasta --fai-reference Homo_sapiens_assembly38.fasta.fai SAMPLE_ID_SRR7890845.recal.cram -@ 2 -o SAMPLE_ID_SRR7890845.recal.bam
  
  cnvkit.py \
      batch \
      SAMPLE_ID_SRR7890844.recal.bam \
      --normal SAMPLE_ID_SRR7890845.recal.bam \
      --fasta Homo_sapiens_assembly38.fasta \
       \
      --targets wgs_calling_regions.hg38.bed \
      --processes 2 \
      --method hybrid --diagram --scatter
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:PAIR_VARIANT_CALLING:RUN_CNVKIT:CNVKIT_BATCH"":
      samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
      cnvkit: $(cnvkit.py version | sed -e ""s/cnvkit v//g"")
  END_VERSIONS

Command exit status:
  -

Command output:
  (empty)

Command error:
  Extracting sequences from chromosome chr2
  Extracting sequences from chromosome chr3
  Extracting sequences from chromosome chr4
  Extracting sequences from chromosome chr5
  Extracting sequences from chromosome chr6
  Extracting sequences from chromosome chr7
  Extracting sequences from chromosome chr8
  Extracting sequences from chromosome chr9
  Extracting sequences from chromosome chr10
  Extracting sequences from chromosome chr11
  Extracting sequences from chromosome chr12
  Extracting sequences from chromosome chr13
  Extracting sequences from chromosome chr14
  Extracting sequences from chromosome chr15
  Extracting sequences from chromosome chr16
  Extracting sequences from chromosome chr17
  Extracting sequences from chromosome chr18
  Extracting sequences from chromosome chr19
  Extracting sequences from chromosome chr20
  Extracting sequences from chromosome chr21
  Extracting sequences from chromosome chr22
  Extracting sequences from chromosome chrX
  Extracting sequences from chromosome chrY
  Correcting for GC bias...
  Correcting for RepeatMasker bias...
  /usr/local/lib/python3.9/site-packages/cnvlib/reference.py:149: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
    ref_df = ref_df.append(anti_ref_df, ignore_index=True, sort=False)
  Calculating average bin coverages
  Calculating bin spreads
  Targets: 5287260 (48.2236%) bins failed filters (log2 < -5.0, log2 > 5.0, spread > 1.0)
  Antitargets: 1187 (100.0000%) bins failed filters
  Wrote ./reference.cnn with 10965235 regions
  Running 1 samples in 2 processes (that's 2 processes per bam)
  Running the CNVkit pipeline on SAMPLE_ID_SRR7890844.recal.bam ...
  [E::idx_find_and_load] Could not retrieve index file for 'SAMPLE_ID_SRR7890844.recal.bam'
  Indexing BAM file SAMPLE_ID_SRR7890844.recal.bam
  Processing reads in SAMPLE_ID_SRR7890844.recal.bam
  Time: 3422.543 seconds (29624 reads/sec, 3203 bins/sec)
  Summary: #bins=10964048, #reads=101389421, mean=9.2474, min=0.0, max=7112.3650793650795 
  Percent reads in regions: 95.430 (of 106244758 mapped)
  Wrote ./SAMPLE_ID_SRR7890844.targetcoverage.cnn with 10964048 regions
  Processing reads in SAMPLE_ID_SRR7890844.recal.bam
  Time: 2.356 seconds (0 reads/sec, 504 bins/sec)
  Summary: #bins=1187, #reads=0, mean=0.0000, min=0.0, max=0.0 
  Percent reads in regions: 0.000 (of 106244758 mapped)
  Wrote ./SAMPLE_ID_SRR7890844.antitargetcoverage.cnn with 1187 regions
  Processing target: SAMPLE_ID_SRR7890844
  Keeping 5676800 of 10964048 bins
  Correcting for GC bias...
  Correcting for density bias...

Work dir:
  /mnt/quobyte/sarek_3_complete/work/34/4b16903a28c29a3f2a021605318444

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`
",sheucke,https://github.com/nf-core/sarek/issues/756
I_kwDOCvwIC85Sy7h2,Pipeline does not complete variant calling when staring from the '--step prepare_recalibration',CLOSED,2022-09-28T10:02:43Z,2024-07-23T11:59:01Z,2022-11-11T14:53:42Z,"### Description of the bug

Hello Sarek developer, 

I am using Sarek pipeline (v3.0.1) to do somatic variant calling, and I started an analysis from '--step prepare_recalibration' using bam file after mark duplicates (e.g. *.md.bam); - see the command line in attached figure. 

Pipeline was running normally and no errors were reported when it done, but it did not finish all steps (including variant calling) and there were no recalibrated bam files generated in output (e.g. *.recal.bam) as well. It looks the pipeline was terminated automatically after creating *.recal.table; - see attached log file. 

I could not figure it out, and could you help me a troubleshooting. 

<img width=""1272"" alt=""Screenshot 2022-09-28 at 11 21 35"" src=""https://user-images.githubusercontent.com/26706345/192743860-e5d9ca3b-5f3e-46a2-99d9-4637b6a3bcba.png"">

<img width=""1251"" alt=""Screenshot 2022-09-28 at 11 32 58"" src=""https://user-images.githubusercontent.com/26706345/192744563-a4a34e37-ce1f-4d37-8a93-da7a999806d1.png"">

Best

Sen

### Command used and terminal output

_No response_

### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/9663804/nextflow.log)


### System information

Nextflow version - 21.11.0
Hardware - HPC
Executor - slurm jobscript
Container engine - Singularity
OS - CentOS Linux 
Version of nf-core/sarek - v3.0.1
",senzhaocode,https://github.com/nf-core/sarek/issues/763
I_kwDOCvwIC85S2ttY,[FEATURE] Option to output gvcf files from haplotypecaller,CLOSED,2022-09-29T00:05:22Z,2023-08-08T21:16:06Z,2023-08-08T21:16:06Z,"### Description of feature

In previous versions of Sarek, there was an option to save gvcf files as output from the GATK haplotypecaller. With the addition of joint-calling to the pipeline, the option to save gvcfs is no longer present in Sarek 3. I suggest that this feature should be re-introduced for a few reasons:

- Analysts often use gvcf files for QC and preliminary analysis
- QC should be performed before joint calling so that poor-quality samples can be flagged and removed before joint calling. Without the option to save gvcfs, the user is locked into doing the joint calling blindly with all samples
- If a user wants to add additional samples or cohorts at a later time, they will be unable to perform joint calling without access to previously generated gvcf outputs. 

I realize that this issue is similar to [#453](https://github.com/nf-core/sarek/issues/453), so if you would like me to remove it and expand the conversation on that issue just let me know!

Best,
Will
",wpoehlm,https://github.com/nf-core/sarek/issues/764
I_kwDOCvwIC85S4Jc5,fastp step makes machine unresponsive,CLOSED,2022-09-29T08:14:08Z,2022-11-02T10:30:22Z,2022-11-02T10:30:22Z,"I am running the pipeline on multiple files, and the fastp step performed concurrently on all files (which is what I believe is happening) slows down and makes the machine unresponsive.

Now, I can see one cannot skip this step. I wonder whether there is any suggestion on how to queue this step to run sequentially, or perhaps another way to skip it all together or other suggestions are very much appreciated.",stefcardinale,https://github.com/nf-core/sarek/issues/765
I_kwDOCvwIC85TFdBw,GenomicsDBImport running slowly on cluster,CLOSED,2022-10-02T23:47:47Z,2022-11-11T10:06:37Z,2022-11-11T10:06:37Z,"### Description of the bug

NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_HAPLOTYPECALLER:JOINT_GERMLINE:GATK4_GENOMICSDBIMPORT processes consistently run slowly on my cluster. My initial test runs (joint germline calling with five WGS samples) crashed after hitting walltime on these processes, after the standard retry with doubled resources. Quadrupling the default walltime got them through, but even then the occasional job timed out and needed a retry.

I found some relevant info here: https://gatk.broadinstitute.org/hc/en-us/articles/360056138571-GenomicsDBImport-usage-and-performance-guidelines

I edited my local copy of `modules/nf-core/modules/gatk4/genomicsdbimport/main.nf` to add the following options in the gatk command:
```
--genomicsdb-shared-posixfs-optimizations true
--bypass-feature-reader
```

This resulted in a ~60-fold speed-up of the GATK_GENOMICSDBIMPORT jobs, with the mean duration reduced from 3h21m to 3m08s.

I'm not sure whether these options should be on by default, or whether that might cause issues on other systems.

I'm running a locally-installed Sarek 3.0, with PBS Pro as scheduler. The cluster is a mix of nodes with 28 Intel CPUs and 128GB RAM, and nodes with 128 AMD CPUs and 1TB RAM, all connected to scratch storage by 100Ggbs ethernet or infiniband.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",DrMcStrange,https://github.com/nf-core/sarek/issues/767
I_kwDOCvwIC85TnIu4,save_bam_mapped  + gatk_spark fails,OPEN,2022-10-10T08:12:04Z,2024-08-19T13:13:18Z,,"### Description of the bug

Merging and saveing the mapped bams fails when using GATK spark. This is due to name-sorting the reads in this case however, for merging they need to be coordinate sorted. 

We should decide if 

a) no save_bam_mapped with GATK spark
b) no merging and save sharded bam files
c) two different sort orders ( no imho)

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/778
I_kwDOCvwIC85TnJoL,MultiQC doesn't show BCFTools/VCFTools plots for WGS,CLOSED,2022-10-10T08:14:34Z,2022-10-11T13:39:03Z,2022-10-11T13:39:03Z,"### Description of the bug

as stated in the title. I believe we might need to use the interval bed files afterall, or see if it is a mqc issue.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/779
I_kwDOCvwIC85TnMeB,Fix readgroup when using dragmap,CLOSED,2022-10-10T08:23:51Z,2022-11-08T12:49:28Z,2022-11-08T12:49:27Z,"### Description of the bug

Readgroups are not correctly assigned when using dragmap in combination with Freebayes & mutect2.

Possibe related issue: https://github.com/Illumina/DRAGMAP/issues/9

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/780
I_kwDOCvwIC85TnqUz,Allow for processing of single-end data,CLOSED,2022-10-10T09:59:30Z,2022-10-10T10:07:22Z,2022-10-10T10:07:21Z,"### Description of feature

For a pilot analysis we wanted to use sarek to call somatic variants from single-end data. However, testing and having a look at the pipeline code revealed that the newest version strictly requires paired-end data. This was not the case for previous versions of the pipeline as far as I know. Is there a possibility to allow for the usage of single-end data for variant calling too?",dmalzl,https://github.com/nf-core/sarek/issues/781
I_kwDOCvwIC85TsCCJ,vcf to maf conversion,OPEN,2022-10-11T06:02:55Z,2022-10-11T09:21:21Z,,"### Description of feature

Hi,

I was wondering whether it is possible to integrate vcf2maf as well into sarek to directly convert annotated vcf files to maf.

https://github.com/mskcc/vcf2maf

Best,
 Axel",kunstner,https://github.com/nf-core/sarek/issues/785
I_kwDOCvwIC85TsfEQ,Mutect2 tumor vs normal does not perform FILTERMUTECTCALLS step,CLOSED,2022-10-11T07:53:54Z,2022-12-13T16:14:21Z,2022-12-13T16:13:36Z,"### Description of the bug

Hi,

I just realized that when running Mutect2, in tumor vs normal mode, it does not perform the FILTERMUTECTCALLS step. When I run it in single tumor mode, it does perform the filtering step. How can I add the filtering step with the sarek command?
Thanks,

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.0.1 -profile singularity -c /workspace/datasets/sjd_seq/code/sarek/sarek_3.0.1.conf --input input_clones_vs_blood.csv --step variant_calling --tools 'strelka,ascat,manta,mutect2' --fasta /workspace/datasets/genomes/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fa --fasta_fai /workspace/datasets/genomes/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fa.fai --dict /workspace/datasets/genomes/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.dict --germline_resource /workspace/datasets/genomes/iGenomes/Homo_sapiens/GATK/GRCh38/Annotation/GermlineResource/gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz --germline_resource_tbi /workspace/datasets/genomes/iGenomes/Homo_sapiens/GATK/GRCh38/Annotation/GermlineResource/gnomAD.r2.1.1.GRCh38.PASS.AC.AF.only.vcf.gz.tbi
```


### Relevant files

patient,sex,status,sample,cram,crai
pt1,XX,1,AX4958,/workspace/datasets/sjd_seq/sarek_results/pt1/results/preprocessing/recalibrated/AX4958/AX4958.recal.cram,/workspace/datasets/sjd_seq/sarek_results/pt1/results/preprocessing/recalibrated/AX4958/AX4958.recal.cram.crai
pt1,XX,1,AX4961,/workspace/datasets/sjd_seq/sarek_results/pt1/results/preprocessing/recalibrated/AX4961/AX4961.recal.cram,/workspace/datasets/sjd_seq/sarek_results/pt1/results/preprocessing/recalibrated/AX4961/AX4961.recal.cram.crai
pt1,XX,0,AQ5174,/workspace/datasets/sjd_seq/platinum_results/20220809/pt1-t1-allsamples-t1/sarek_results/results/preprocessing/recalibrated/AQ5174/AQ5174.recal.cram,/workspace/datasets/sjd_seq/platinum_results/20220809/pt1-t1-allsamples-t1/sarek_results/results/preprocessing/recalibrated/AQ5174/AQ5174.recal.cram.crai


### System information

_No response_",msguixe,https://github.com/nf-core/sarek/issues/786
I_kwDOCvwIC85Tsk3b,"How to prepare the basic referece, GATK bundle and annotation database with bad network connection?",CLOSED,2022-10-11T08:08:39Z,2023-03-21T13:09:57Z,2023-03-21T13:09:57Z,"### Description of the bug

Dear sarek developers,
I wonder implement sarek to my WGS project. But the network is bad. So, how can I build this pipeline step by step with local download reference and other basic database for variation detection and annotation?
BTW, for the bad network of my platform, the basic database for this pipeline could not download directly with wget commend, any suggestion for this situation.

Thanks,



### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",xflicsu,https://github.com/nf-core/sarek/issues/787
I_kwDOCvwIC85TtBUx,Mutect2 somatic test not working,CLOSED,2022-10-11T09:24:35Z,2022-10-11T15:36:29Z,2022-10-11T15:36:29Z,,SusiJo,https://github.com/nf-core/sarek/issues/789
I_kwDOCvwIC85T7j1r,Community feedback needed: (pre)-validation at the Sarek level,OPEN,2022-10-13T16:42:27Z,2024-08-19T13:12:29Z,,"### Description of feature

Could be good to do whatever can be done when we release to help prepare for validation.
I'm thinking benchmarking, GiAB, checking results...
Could be interesting to talk with multiple collaborators to figure out what can be done and how we can help the community.",maxulysse,https://github.com/nf-core/sarek/issues/798
I_kwDOCvwIC85UBjJT,Unexpected error [UnsupportedOperationException] when running the test case,CLOSED,2022-10-14T18:11:54Z,2022-10-14T21:25:29Z,2022-10-14T21:25:29Z,"### Description of the bug

Unexpected error [UnsupportedOperationException] when running the test case

### Command used and terminal output

```console
$ nextflow run nf-core/sarek -profile test_full,docker --outdir out
```


### Relevant files

Oct-14 12:07:56.647 [main] DEBUG nextflow.cli.Launcher - $> nextflow run nf-core/sarek -profile test_full,docker --outdir out
Oct-14 12:07:56.699 [main] INFO  nextflow.cli.CmdRun - N E X T F L O W  ~  version 21.10.6
Oct-14 12:07:57.456 [main] DEBUG nextflow.scm.AssetManager - Git config: /users/syu86/.nextflow/assets/nf-core/sarek/.git/config; branch: master; remote: origin; url: https://github.com/nf-core/sarek.git
Oct-14 12:07:57.466 [main] DEBUG nextflow.scm.AssetManager - Git config: /users/syu86/.nextflow/assets/nf-core/sarek/.git/config; branch: master; remote: origin; url: https://github.com/nf-core/sarek.git
Oct-14 12:07:57.529 [main] INFO  nextflow.cli.CmdRun - Launching `nf-core/sarek` [festering_volta] - revision: bcd7bf9cb9 [master]
Oct-14 12:07:57.873 [main] DEBUG nextflow.config.ConfigBuilder - Found config base: /users/syu86/.nextflow/assets/nf-core/sarek/nextflow.config
Oct-14 12:07:57.873 [main] DEBUG nextflow.config.ConfigBuilder - Found config local: /mydata/nf_gwas/nextflow.config
Oct-14 12:07:57.873 [main] DEBUG nextflow.config.ConfigBuilder - Parsing config file: /users/syu86/.nextflow/assets/nf-core/sarek/nextflow.config
Oct-14 12:07:57.873 [main] DEBUG nextflow.config.ConfigBuilder - Parsing config file: /mydata/nf_gwas/nextflow.config
Oct-14 12:07:57.880 [main] DEBUG nextflow.config.ConfigBuilder - Applying config profile: `test_full,docker`
Oct-14 12:07:58.133 [main] DEBUG nextflow.plugin.PluginsFacade - Using Default plugins manager
Oct-14 12:07:58.141 [main] INFO  org.pf4j.DefaultPluginStatusProvider - Enabled plugins: []
Oct-14 12:07:58.142 [main] INFO  org.pf4j.DefaultPluginStatusProvider - Disabled plugins: []
Oct-14 12:07:58.144 [main] INFO  org.pf4j.DefaultPluginManager - PF4J version 3.4.1 in 'deployment' mode
Oct-14 12:07:58.147 [main] DEBUG nextflow.plugin.PluginsFacade - Using Default plugins manager
Oct-14 12:07:58.307 [main] DEBUG nextflow.plugin.PluginsFacade - Using Default plugins manager
Oct-14 12:07:58.307 [main] DEBUG nextflow.plugin.PluginsFacade - Using Default plugins manager
Oct-14 12:07:59.148 [main] DEBUG nextflow.config.ConfigBuilder - Applying config profile: `test_full,docker`
Oct-14 12:07:59.163 [main] DEBUG nextflow.config.ConfigBuilder - Available config profiles: [cfc_dev, ifb_core, denbi_qbic, alice, mjolnir_globe, uppmax, lugh, test_full_germline, unibe_ibu, vai, czbiohub_aws, jax, ccga_med, tigem, google, use_gatk_spark, eddie, medair, alignment_to_fastq, bi, bigpurple, sbc_sharc, adcra, cedars, tools, utd_ganymede, charliecloud, targeted, icr_davros, save_bam_mapped, munin, rosalind, hasta, annotation, cfc, uzh, split_fastq, ebc, ku_sund_dangpu, ccga_dx, variantcalling_channels, crick, recalibrate_bam, marvin, biohpc_gen, shifter, mana, mamba, awsbatch, imperial, maestro, skip_bqsr, genotoul, skip_markduplicates, abims, nihbiowulf, nu_genomics, oist, sahmri, mpcdf, leicester, vsc_ugent, sage, trimming, cambridge, tools_tumoronly, podman, cheaha, test, prepare_recalibration_bam, computerome, tools_germline, seg_globe, sanger, tools_somatic_ascat, pasteur, test_full, azurebatch, hki, crukmi, docker, gis, umi, eva, markduplicates_bam, fgcz, conda, singularity, prince, hebbe, utd_sysbio, debug, genouest, tools_somatic, cbe, markduplicates_cram, recalibrate_cram, no_intervals, prepare_recalibration_cram, pair, phoenix, gitpod, uct_hpc, aws_tower, binac]
Oct-14 12:07:59.196 [main] DEBUG nextflow.plugin.PluginsFacade - Setting up plugin manager > mode=prod; plugins-dir=/users/syu86/.nextflow/plugins
Oct-14 12:07:59.197 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]
Oct-14 12:07:59.200 [main] INFO  org.pf4j.DefaultPluginStatusProvider - Enabled plugins: []
Oct-14 12:07:59.200 [main] INFO  org.pf4j.DefaultPluginStatusProvider - Disabled plugins: []
Oct-14 12:07:59.201 [main] INFO  org.pf4j.DefaultPluginManager - PF4J version 3.4.1 in 'deployment' mode
Oct-14 12:07:59.208 [main] INFO  org.pf4j.AbstractPluginManager - No plugins
Oct-14 12:07:59.248 [main] DEBUG nextflow.Session - Session uuid: b179f581-bd07-4059-b472-25858bdfa43e
Oct-14 12:07:59.248 [main] DEBUG nextflow.Session - Run name: festering_volta
Oct-14 12:07:59.249 [main] DEBUG nextflow.Session - Executor pool size: 144
Oct-14 12:07:59.274 [main] DEBUG nextflow.cli.CmdRun - 
  Version: 21.10.6 build 5660
  Created: 21-12-2021 16:55 UTC (09:55 MDT)
  System: Linux 5.4.0-100-generic
  Runtime: Groovy 3.0.9 on OpenJDK 64-Bit Server VM 11.0.13+7-b1751.21
  Encoding: UTF-8 (UTF-8)
  Process: 125630@node0.syu86-135964.latencymodel-pg0.clemson.cloudlab.us [130.127.134.38]
  CPUs: 144 - Mem: 251.2 GB (209.9 GB) - Swap: 3 GB (3 GB)
Oct-14 12:07:59.284 [main] DEBUG nextflow.Session - Work-dir: /mydata/nf_gwas/work [ext2/ext3]
Oct-14 12:07:59.284 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /users/syu86/.nextflow/assets/nf-core/sarek/bin
Oct-14 12:07:59.291 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]
Oct-14 12:07:59.299 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory
Oct-14 12:07:59.398 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 145; maxThreads: 1000
Oct-14 12:07:59.456 [main] DEBUG nextflow.Session - Session start invoked
Oct-14 12:07:59.461 [main] DEBUG nextflow.trace.TraceFileObserver - Flow starting -- trace file: /mydata/nf_gwas/out/pipeline_info/execution_trace_2022-10-14_12-07-58.txt
Oct-14 12:07:59.469 [main] DEBUG nextflow.Session - Using default localLib path: /users/syu86/.nextflow/assets/nf-core/sarek/lib
Oct-14 12:07:59.472 [main] DEBUG nextflow.Session - Adding to the classpath library: /users/syu86/.nextflow/assets/nf-core/sarek/lib
Oct-14 12:07:59.473 [main] DEBUG nextflow.Session - Adding to the classpath library: /users/syu86/.nextflow/assets/nf-core/sarek/lib/nfcore_external_java_deps.jar
Oct-14 12:08:00.178 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution
Oct-14 12:08:00.376 [main] INFO  nextflow.Nextflow - 

-[2m----------------------------------------------------[0m-
                                        [0;32m,--.[0;30m/[0;32m,-.[0m
[0;34m        ___     __   __   __   ___     [0;32m/,-._.--~'[0m
[0;34m  |\ | |__  __ /  ` /  \ |__) |__         [0;33m}  {[0m
[0;34m  | \| |       \__, \__/ |  \ |___     [0;32m\`-._,-`-,[0m
                                        [0;32m`._,._,'[0m
[0;37m      ____[0m
[0;37m    .´ _  `.[0m
[0;37m   /  [0;32m|\[0m`-_ \[0m     [0;34m __        __   ___     [0m
[0;37m  |   [0;32m| \[0m  `-|[0m    [0;34m|__`  /\  |__) |__  |__/[0m
[0;37m   \ [0;32m|   \[0m  /[0m     [0;34m.__| /¯¯\ |  \ |___ |  \[0m
[0;37m    `[0;32m|[0m____[0;32m\[0m´[0m

[0;35m  nf-core/sarek v3.0.2[0m
-[2m----------------------------------------------------[0m-
[1mCore Nextflow options[0m
  [0;34mrevision                  : [0;32mmaster[0m
  [0;34mrunName                   : [0;32mfestering_volta[0m
  [0;34mcontainerEngine           : [0;32mdocker[0m
  [0;34mlaunchDir                 : [0;32m/mydata/nf_gwas[0m
  [0;34mworkDir                   : [0;32m/mydata/nf_gwas/work[0m
  [0;34mprojectDir                : [0;32m/users/syu86/.nextflow/assets/nf-core/sarek[0m
  [0;34muserName                  : [0;32msyu86[0m
  [0;34mprofile                   : [0;32mtest_full,docker[0m
  [0;34mconfigFiles               : [0;32m/users/syu86/.nextflow/assets/nf-core/sarek/nextflow.config, /mydata/nf_gwas/nextflow.config[0m

[1mInput/output options[0m
  [0;34minput                     : [0;32mhttps://raw.githubusercontent.com/nf-core/test-datasets/sarek/testdata/csv/HCC1395_WXS_somatic_full_test.csv[0m
  [0;34moutdir                    : [0;32mout[0m

[1mMain options[0m
  [0;34msplit_fastq               : [0;32m20000000[0m
  [0;34mwes                       : [0;32mtrue[0m
  [0;34mintervals                 : [0;32ms3://nf-core-awsmegatests/sarek/input/S07604624_Padded_Agilent_SureSelectXT_allexons_V6_UTR.bed[0m
  [0;34mtools                     : [0;32mstrelka,mutect2,freebayes,ascat,manta,cnvkit,tiddit,controlfreec,vep[0m

[1mVariant Calling[0m
  [0;34mpon                       : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz[0m
  [0;34mpon_tbi                   : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz.tbi[0m

[1mReference genome options[0m
  [0;34mascat_genome              : [0;32mhg38[0m
  [0;34mascat_alleles             : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_alleles_hg38.zip[0m
  [0;34mascat_loci                : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_loci_hg38.zip[0m
  [0;34mascat_loci_gc             : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/GC_G1000_hg38.zip[0m
  [0;34mascat_loci_rt             : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/RT_G1000_hg38.zip[0m
  [0;34mbwa                       : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAIndex/[0m
  [0;34mbwamem2                   : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAmem2Index/[0m
  [0;34mchr_dir                   : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/Chromosomes[0m
  [0;34mdbsnp                     : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz[0m
  [0;34mdbsnp_tbi                 : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz.tbi[0m
  [0;34mdbsnp_vqsr                : [0;32m--resource:dbsnp,known=false,training=true,truth=false,prior=2.0 dbsnp_146.hg38.vcf.gz[0m
  [0;34mdict                      : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.dict[0m
  [0;34mdragmap                   : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/dragmap/[0m
  [0;34mfasta                     : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta[0m
  [0;34mfasta_fai                 : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai[0m
  [0;34mgermline_resource         : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz[0m
  [0;34mgermline_resource_tbi     : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz.tbi[0m
  [0;34mknown_indels              : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz[0m
  [0;34mknown_indels_tbi          : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz.tbi[0m
  [0;34mknown_indels_vqsr         : [0;32m--resource:gatk,known=false,training=true,truth=true,prior=10.0 Homo_sapiens_assembly38.known_indels.vcf.gz --resource:mills,known=false,training=true,truth=true,prior=10.0 Mills_and_1000G_gold_standard.indels.hg38.vcf.gz[0m
  [0;34mknown_snps                : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000G_omni2.5.hg38.vcf.gz[0m
  [0;34mknown_snps_tbi            : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000G_omni2.5.hg38.vcf.gz.tbi[0m
  [0;34mknown_snps_vqsr           : [0;32m--resource:1000G,known=false,training=true,truth=true,prior=10.0 1000G_omni2.5.hg38.vcf.gz[0m
  [0;34mmappability               : [0;32ms3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Control-FREEC/out100m2_hg38.gem[0m
  [0;34msnpeff_db                 : [0;32mGRCh38.105[0m
  [0;34msnpeff_genome             : [0;32mGRCh38[0m
  [0;34msnpeff_version            : [0;32m5.1[0m
  [0;34mvep_genome                : [0;32mGRCh38[0m
  [0;34mvep_species               : [0;32mhomo_sapiens[0m
  [0;34mvep_cache_version         : [0;32m106[0m
  [0;34mvep_version               : [0;32m106.1[0m
  [0;34migenomes_base             : [0;32ms3://ngi-igenomes/igenomes[0m

[1mInstitutional config options[0m
  [0;34mconfig_profile_name       : [0;32mFull test profile[0m
  [0;34mconfig_profile_description: [0;32mFull test dataset to check pipeline function[0m

!! Only displaying parameters that differ from the pipeline defaults !!
-[2m----------------------------------------------------[0m-
If you use nf-core/sarek for your analysis please cite:

* The pipeline
  https://doi.org/10.12688/f1000research.16665.2
  https://doi.org/10.5281/zenodo.4468605

* The nf-core framework
  https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md
-[2m----------------------------------------------------[0m-
Oct-14 12:08:01.365 [main] DEBUG nextflow.plugin.PluginUpdater - Installing plugin nf-amazon version: 1.3.4
Oct-14 12:08:01.372 [main] INFO  org.pf4j.AbstractPluginManager - Plugin 'nf-amazon@1.3.4' resolved
Oct-14 12:08:01.372 [main] INFO  org.pf4j.AbstractPluginManager - Start plugin 'nf-amazon@1.3.4'
Oct-14 12:08:01.382 [main] DEBUG nextflow.plugin.BasePlugin - Plugin started nf-amazon@1.3.4
Oct-14 12:08:01.393 [main] DEBUG nextflow.file.FileHelper - > Added 'S3FileSystemProvider' to list of installed providers [s3]
Oct-14 12:08:01.394 [main] DEBUG nextflow.file.FileHelper - Started plugin 'nf-amazon' required to handle file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_alleles_hg38.zip
Oct-14 12:08:01.399 [main] DEBUG nextflow.file.FileHelper - Creating a file system instance for provider: S3FileSystemProvider
Oct-14 12:08:01.403 [main] DEBUG nextflow.Global - Using AWS credential defined in `default` section in file: /users/syu86/.aws/credentials
Oct-14 12:08:01.405 [main] DEBUG nextflow.file.FileHelper - AWS S3 config details: {secret_key=6bGt7Y.., max_error_retry=5, access_key=AKIAXL..}
Oct-14 12:08:05.710 [main] DEBUG nextflow.file.FileHelper - Path matcher not defined by 'S3FileSystem' file system -- using default default strategy
Oct-14 12:08:05.715 [main] DEBUG nextflow.Session - Session aborted -- Cause: java.lang.UnsupportedOperationException
Oct-14 12:08:05.726 [main] ERROR nextflow.cli.Launcher - @unknown
java.lang.UnsupportedOperationException: null
	at com.upplication.s3fs.S3FileSystemProvider.getFileAttributeView(S3FileSystemProvider.java:666)
	at java.base/java.nio.file.Files.getFileAttributeView(Files.java:1776)
	at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:221)
	at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:282)
	at java.base/java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:328)
	at java.base/java.nio.file.Files.walkFileTree(Files.java:2792)
	at nextflow.file.FileHelper.visitFiles(FileHelper.groovy:797)
	at nextflow.file.FileHelper$visitFiles$4.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:166)
	at nextflow.Nextflow.fileNamePattern(Nextflow.groovy:121)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)
	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite$StaticMetaMethodSiteNoUnwrapNoCoerce.invoke(StaticMetaMethodSite.java:149)
	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.callStatic(StaticMetaMethodSite.java:100)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:55)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:217)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:249)
	at nextflow.Nextflow.file(Nextflow.groovy:167)
	at nextflow.Nextflow$file.callStatic(Unknown Source)
	at Script_28ace5fe.runScript(Script_28ace5fe:55)
	at nextflow.script.BaseScript.runDsl2(BaseScript.groovy:169)
	at nextflow.script.BaseScript.run(BaseScript.groovy:200)
	at nextflow.script.ScriptParser.runScript(ScriptParser.groovy:221)
	at nextflow.script.ScriptParser.runScript(ScriptParser.groovy:207)
	at nextflow.script.ScriptParser.runScript(ScriptParser.groovy:201)
	at nextflow.script.IncludeDef.memoizedMethodPriv$loadModule0PathMapSession(IncludeDef.groovy:142)
	at nextflow.script.IncludeDef.access$0(IncludeDef.groovy)
	at nextflow.script.IncludeDef$__clinit__closure1.doCall(IncludeDef.groovy)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)
	at groovy.lang.Closure.call(Closure.java:412)
	at org.codehaus.groovy.runtime.memoize.Memoize$MemoizeFunction.lambda$call$0(Memoize.java:137)
	at org.codehaus.groovy.runtime.memoize.ConcurrentCommonCache.getAndPut(ConcurrentCommonCache.java:137)
	at org.codehaus.groovy.runtime.memoize.ConcurrentCommonCache.getAndPut(ConcurrentCommonCache.java:113)
	at org.codehaus.groovy.runtime.memoize.Memoize$MemoizeFunction.call(Memoize.java:136)
	at nextflow.script.IncludeDef.loadModule0(IncludeDef.groovy)
	at nextflow.script.IncludeDef.load0(IncludeDef.groovy:114)
	at nextflow.script.IncludeDef$load0$1.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:139)
	at Script_c05744b2.runScript(Script_c05744b2:79)
	at nextflow.script.BaseScript.runDsl2(BaseScript.groovy:169)
	at nextflow.script.BaseScript.run(BaseScript.groovy:200)
	at nextflow.script.ScriptParser.runScript(ScriptParser.groovy:221)
	at nextflow.script.ScriptRunner.run(ScriptRunner.groovy:212)
	at nextflow.script.ScriptRunner.execute(ScriptRunner.groovy:120)
	at nextflow.cli.CmdRun.run(CmdRun.groovy:308)
	at nextflow.cli.Launcher.run(Launcher.groovy:480)
	at nextflow.cli.Launcher.main(Launcher.groovy:639)


### System information

- Nextflow version : 21.10.6.5660
- Hardware : HPC
- Executor : local
- Container engine: Docker
- OS : Ubuntu 20.04
- Version of nf-core/sarek : 3.0.2",shaojunyu,https://github.com/nf-core/sarek/issues/799
I_kwDOCvwIC85UHiYs,segmentation fles are empty,CLOSED,2022-10-17T09:46:30Z,2023-10-26T22:39:36Z,2022-10-18T08:05:19Z,"### Description of the bug

 I having the following error running sarek, at the beginning i thought it was due to some problem with samplefile but I don't see what is wrong there
Error executing process > 'NFCORE_SAREK:sarek:TUMOR_ONLY_VARIANT_CALLING:GATK_TUMOR_ONLY_SOMATIC_VARIANT_CALLING:CALCULATECONTAMINATION (L21-6580)'

Caused by:
  Process `NFCORE_SAREK:sarek:TUMOR_ONLY_VARIANT_CALLING:GATK_TUMOR_ONLY_SOMATIC_VARIANT_CALLING:CALCULATECONTAMINATION (L21-6580)` terminated with an error exit status (3)

Command executed:

  gatk --java-options ""-Xmx48g"" CalculateContamination \
      --input L21-6580.mutect2.pileupsummaries.table \
      --output L21-6580.mutect2.contamination.table \
       \
      --tmp-dir . \
      -tumor-segmentation L21-6580.mutect2.segmentation.table
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:sarek:TUMOR_ONLY_VARIANT_CALLING:GATK_TUMOR_ONLY_SOMATIC_VARIANT_CALLING:CALCULATECONTAMINATION"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  3

Command output:
  (empty)

Command error:
  Using GATK jar /usr/local/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx48g -jar /usr/local/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar CalculateContamination --input L21-6580.mutect2.pileupsummaries.table --output L21-6580.mutect2.contamination.table --tmp-dir . -tumor-segmentation L21-6580.mutect2.segmentation.table
  14:15:35.997 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl_compression.so
  14:15:36.071 INFO  CalculateContamination - ------------------------------------------------------------
  14:15:36.072 INFO  CalculateContamination - The Genome Analysis Toolkit (GATK) v4.2.6.1
  14:15:36.072 INFO  CalculateContamination - For support and documentation go to https://software.broadinstitute.org/gatk/
  14:15:36.072 INFO  CalculateContamination - Executing as lab@7572d1f7f25e on Linux v5.15.0-50-generic amd64
  14:15:36.072 INFO  CalculateContamination - Java runtime: OpenJDK 64-Bit Server VM v11.0.9.1-internal+0-adhoc..src
  14:15:36.072 INFO  CalculateContamination - Start Date/Time: October 14, 2022 at 2:15:35 PM GMT
  14:15:36.072 INFO  CalculateContamination - ------------------------------------------------------------
  14:15:36.072 INFO  CalculateContamination - ------------------------------------------------------------
  14:15:36.072 INFO  CalculateContamination - HTSJDK Version: 2.24.1
  14:15:36.072 INFO  CalculateContamination - Picard Version: 2.27.1
  14:15:36.073 INFO  CalculateContamination - Built for Spark Version: 2.4.5
  14:15:36.073 INFO  CalculateContamination - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  14:15:36.073 INFO  CalculateContamination - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  14:15:36.073 INFO  CalculateContamination - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  14:15:36.073 INFO  CalculateContamination - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  14:15:36.073 INFO  CalculateContamination - Deflater: IntelDeflater
  14:15:36.073 INFO  CalculateContamination - Inflater: IntelInflater
  14:15:36.073 INFO  CalculateContamination - GCS max retries/reopens: 20
  14:15:36.073 INFO  CalculateContamination - Requester pays: disabled
  14:15:36.073 INFO  CalculateContamination - Initializing engine
  14:15:36.073 INFO  CalculateContamination - Done initializing engine
  14:15:36.097 INFO  CalculateContamination - Shutting down engine
  [October 14, 2022 at 2:15:36 PM GMT] org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination done. Elapsed time: 0.00 minutes.
  Runtime.totalMemory()=1056964608
  java.lang.IllegalArgumentException: Null object is not allowed here.
  	at org.broadinstitute.hellbender.utils.Utils.nonNull(Utils.java:643)
  	at org.broadinstitute.hellbender.utils.Utils.nonNull(Utils.java:631)
  	at org.broadinstitute.hellbender.utils.tsv.TableWriter.writeMetadata(TableWriter.java:183)
  	at org.broadinstitute.hellbender.tools.walkers.contamination.MinorAlleleFractionRecord.writeToFile(MinorAlleleFractionRecord.java:48)
  	at org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination.doWork(CalculateContamination.java:128)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211)
  	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
  	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
  	at org.broadinstitute.hellbender.Main.main(Main.java:289)

Work dir:
  /home/lab/Documentos/20220808_MCrespo_22LLC/work/38/7f5f81b38ffe6a89cf9e91265d1a37

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`
the sample file looks like


patient,sample,status,lane,fastq_1,fastq_2
L21-6583,L21-6583,1,1,./data/L21-6583_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6583_TTTTTT_l000_R2_001.fastq.gz
L21-6569,L21-6569,1,1,./data/L21-6569_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6569_TTTTTT_l000_R2_001.fastq.gz
L21-6581,L21-6581,1,1,./data/L21-6581_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6581_TTTTTT_l000_R2_001.fastq.gz
L21-6563,L21-6563,1,1,./data/L21-6563_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6563_TTTTTT_l000_R2_001.fastq.gz
L21-6567,L21-6567,1,1,./data/L21-6567_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6567_TTTTTT_l000_R2_001.fastq.gz
L21-6565,L21-6565,1,1,./data/L21-6565_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6565_TTTTTT_l000_R2_001.fastq.gz
L21-6579,L21-6579,1,1,./data/L21-6579_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6579_TTTTTT_l000_R2_001.fastq.gz
L21-6575,L21-6575,1,1,./data/L21-6575_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6575_TTTTTT_l000_R2_001.fastq.gz
L21-6577,L21-6577,1,1,./data/L21-6577_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6577_TTTTTT_l000_R2_001.fastq.gz
L21-6571,L21-6571,1,1,./data/L21-6571_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6571_TTTTTT_l000_R2_001.fastq.gz
L21-6573,L21-6573,1,1,./data/L21-6573_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6573_TTTTTT_l000_R2_001.fastq.gz
L21-6568,L21-6568,1,1,./data/L21-6568_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6568_TTTTTT_l000_R2_001.fastq.gz
L21-6582,L21-6582,1,1,./data/L21-6582_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6582_TTTTTT_l000_R2_001.fastq.gz
L21-6580,L21-6580,1,1,./data/L21-6580_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6580_TTTTTT_l000_R2_001.fastq.gz
L21-6566,L21-6566,1,1,./data/L21-6566_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6566_TTTTTT_l000_R2_001.fastq.gz
L21-6564,L21-6564,1,1,./data/L21-6564_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6564_TTTTTT_l000_R2_001.fastq.gz
L21-6562,L21-6562,1,1,./data/L21-6562_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6562_TTTTTT_l000_R2_001.fastq.gz
L21-6578,L21-6578,1,1,./data/L21-6578_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6578_TTTTTT_l000_R2_001.fastq.gz
L21-6570,L21-6570,1,1,./data/L21-6570_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6570_TTTTTT_l000_R2_001.fastq.gz
L21-6572,L21-6572,1,1,./data/L21-6572_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6572_TTTTTT_l000_R2_001.fastq.gz
L21-6574,L21-6574,1,1,./data/L21-6574_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6574_TTTTTT_l000_R2_001.fastq.gz
L21-6576,L21-6576,1,1,./data/L21-6576_TTTTTT_l000_R1_001.fastq.gz,./data/L21-6576_TTTTTT_l000_R2_001.fastq.gz
SampleSheet.csv (END)
and files  L21-6580.mutect2.pileupsummaries.table and L21-6580.mutect2.segmentation.table are empties

### Command used and terminal output

```console
nextflow run nf-core/sarek -profile docker --input SampleSheet.csv --genome hg19 -r 3.0.2 -c sarek.conf --intervals projectInfo/Target_regions_genes_chr.bed  --known_indels Homo_sapiens_assembly19.known_indels.vcf.gz --pon somatic-b37_Mutect2-WGS-panel-b37_chr.vcf.gz --germline_resource Mills_and_1000G_gold_standard.indels.b37.vcf.gz""

def check_max(obj) {
   try {
        if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
            return params.max_memory as nextflow.util.MemoryUnit
        else
            return obj
    } catch (all) {
        println ""   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj""
        return obj
    }
}

params {
    max_cpus = 20 
    max_time = 12.h
    max_memory=62.Gb 
    trim_fastq = true
    tools = 'mutect2,snpeff'
    intervals = 'projectInfo/Target_regions_genes_chr.bed'
    config_profile_description = 'bioinfo config'
    config_profile_contact = 'paumarc paumarc@vhio.net'
    config_profile_url = ""tobecopiedingithub""


}

process {
    errorStrategy = 'retry'
    maxErrors = '-1'
    maxRetries = 3
}
```


### Relevant files


[files.tar.gz](https://github.com/nf-core/sarek/files/9799133/files.tar.gz)



### System information

nextflow version 21.10.6.5660

Linux adsystems-System-Product-Name 5.15.0-50-generic #56-Ubuntu SMP Tue Sep 20 13:23:26 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

Executor: bash

Container Docker

sarek version 3.0.2",paumarc,https://github.com/nf-core/sarek/issues/800
I_kwDOCvwIC85UbQYf,MarkDuplicates step fails with UMIs from read names and 4 lanes,OPEN,2022-10-20T11:14:03Z,2024-07-25T02:03:18Z,,"### Description of the bug

I am using Sarek with a configuration that reads UMIs from the FASTQ read names. 
The workflow fails at the ""MarkDuplicates"" step with the error message: 
```htsjdk.samtools.SAMException: Value was put into PairInfoMap more than once.  1: RGHD789:806050 ```
According to the documentation of this error, it can result from reads having the same read name in the BAM file. 
I have four lanes for one sample and realized that UMI consensus calling is performed for each lane separately. During that process, the read names are changed  to a scheme containing the sample name HD789 and a continuous number. When BAMs for all four lanes are merged, this results into different reads having the same name (4x readpairs since I have 4 lanes) that results into this error in the MarkDuplicates step.

In general. I was wondering if it is intended to call UMI consensus for each lane separately or whether it should be called on a merged file, as the same UMI could be distributed across multiple lanes.

### Command used and terminal output

```console
nextflow run nf-core-sarek-3.0.2/workflow/ --input run1_samplesheet.csv --outdir run1_full --genome GATK.GRCh38 --igenomes_base igenomes/references -profile singularity -c TSO500.config --tools freebayes,mpileup,mutect2,strelka,manta,tiddit,merge -resume --umi_read_structure NA --group_by_umi_strategy paired --snpeff_cache snpeff/ --vep_cache vep/

Error executing process > 'NFCORE_SAREK:SAREK:MARKDUPLICATES:GATK4_MARKDUPLICATES (HD789)'

Caused by:
  Process `NFCORE_SAREK:SAREK:MARKDUPLICATES:GATK4_MARKDUPLICATES (HD789)` terminated with an error exit status (3)

Command executed:

  gatk --java-options ""-Xmx30g"" MarkDuplicates \
      --INPUT HD789-L002.0006.bam --INPUT HD789-L002.0002.bam --INPUT HD789-L002.0003.bam --INPUT HD789-L002.0001.bam --INPUT HD789-L002.0005.bam --INPUT HD789-L002.0004.bam --INPUT HD789-L004.0001.bam --INPUT HD789-L004.0003.bam --INPUT HD789-L004.0006.bam --INPUT HD789-L004.0004.bam --INPUT HD789-L004.0002.bam --INPUT HD789-L004.0005.bam --INPUT HD789-L001.0006.bam --INPUT HD789-L001.0003.bam --INPUT HD789-L001.0002.bam --INPUT HD789-L001.0005.bam --INPUT HD789-L001.0004.bam --INPUT HD789-L001.0001.bam --INPUT HD789-L003.0001.bam --INPUT HD789-L003.0002.bam --INPUT HD789-L003.0004.bam --INPUT HD789-L003.0003.bam --INPUT HD789-L003.0006.bam --INPUT HD789-L003.0005.bam \
      --OUTPUT HD789.md.bam \
      --METRICS_FILE HD789.md.metrics \
      --TMP_DIR . \
      -REMOVE_DUPLICATES false -VALIDATION_STRINGENCY LENIENT --CREATE_INDEX true
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:MARKDUPLICATES:GATK4_MARKDUPLICATES"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  3

Command output:
  (empty)

Command error:
  INFO	2022-10-18 10:53:55	MarkDuplicates	Tracking 9051 as yet unmatched pairs. 546 records in RAM.
  INFO	2022-10-18 10:53:59	MarkDuplicates	Read     3,000,000 records.  Elapsed time: 00:00:12s.  Time for last 1,000,000:    3s.  Last read position: chr1:113,104,534
  INFO	2022-10-18 10:53:59	MarkDuplicates	Tracking 16635 as yet unmatched pairs. 708 records in RAM.
  INFO	2022-10-18 10:54:02	MarkDuplicates	Read     4,000,000 records.  Elapsed time: 00:00:16s.  Time for last 1,000,000:    3s.  Last read position: chr1:156,880,088
  INFO	2022-10-18 10:54:02	MarkDuplicates	Tracking 20851 as yet unmatched pairs. 895 records in RAM.
  INFO	2022-10-18 10:54:07	MarkDuplicates	Read     5,000,000 records.  Elapsed time: 00:00:21s.  Time for last 1,000,000:    4s.  Last read position: chr1:193,212,181
  INFO	2022-10-18 10:54:07	MarkDup Elapsed time: 00:00:16s.  Time for last 1,000,000:    3s.  Last read position: chr1:156,880,088
  INFO	2022-10-18 10:54:02	MarkDuplicates	Tracking 20851 as yet unmatched pairs. 895 records in RAM.
  INFO	2022-10-18 10:54:07	MarkDuplicates	Read     5,000,000 records.  Elapsed time: 00:00:21s.  Time for last 1,000,000:    4s.  Last read position: chr1:193,212,181
  INFO	2022-10-18 10:54:07	MarkDuplicates	Tracking 25749 as yet unmatched pairs. 558 records in RAM.
  INFO	2022-10-18 10:54:11	MarkDuplicates	Read     6,000,000 records.  Elapsed time: 00:00:25s.  Time for last 1,000,000:    4s.  Last read position: chr1:231,436,969
  INFO	2022-10-18 10:54:11	MarkDuplicates	Tracking 30831 as yet unmatched pairs. 193 records in RAM.
  INFO	2022-10-18 10:54:17	MarkDuplicates	Read     7,000,000 records.  Elapsed time: 00:00:31s.  Time for last 1,000,000:    5s.  Last read position: chr2:15,946,332
  INFO	2022-10-18 10:54:17	MarkDuplicates	Tracking 39249 as yet unmatched pairs. 6622 records in RAM.
  INFO	2022-10-18 10:54:23	MarkDuplicates	Read     8,000,000 records.  Elapsed time: 00:00:37s.  Time for last 1,000,000:    5s.  Last read position: chr2:29,511,003
  INFO	2022-10-18 10:54:23	MarkDuplicates	Tracking 43958 as yet unmatched pairs. 8035 records in RAM.
  INFO	2022-10-18 10:54:28	MarkDuplicates	Read     9,000,000 records.  Elapsed time: 00:00:42s.  Time for last 1,000,000:    5s.  Last read position: chr2:74,090,879
  INFO	2022-10-18 10:54:28	MarkDuplicates	Tracking 49514 as yet unmatched pairs. 2513 records in RAM.
  INFO	2022-10-18 10:54:32	MarkDuplicates	Read    10,000,000 records.  Elapsed time: 00:00:46s.  Time for last 1,000,000:    3s.  Last read position: chr2:113,223,748
  INFO	2022-10-18 10:54:32	MarkDuplicates	Tracking 54431 as yet unmatched pairs. 3001 records in RAM.
  INFO	2022-10-18 10:54:37	MarkDuplicates	Read    11,000,000 records.  Elapsed time: 00:00:51s.  Time for last 1,000,000:    4s.  Last read position: chr2:141,169,200
  INFO	2022-10-18 10:54:37	MarkDuplicates	Tracking 57016 as yet unmatched pairs. 1670 records in RAM.
  INFO	2022-10-18 10:54:40	MarkDuplicates	Read    12,000,000 records.  Elapsed time: 00:00:54s.  Time for last 1,000,000:    3s.  Last read position: chr2:211,630,484
  INFO	2022-10-18 10:54:40	MarkDuplicates	Tracking 63059 as yet unmatched pairs. 1454 records in RAM.
  INFO	2022-10-18 10:54:45	MarkDuplicates	Read    13,000,000 records.  Elapsed time: 00:00:58s.  Time for last 1,000,000:    4s.  Last read position: chr3:10,039,325
  INFO	2022-10-18 10:54:45	MarkDuplicates	Tracking 66844 as yet unmatched pairs. 5403 records in RAM.
  INFO	2022-10-18 10:54:49	MarkDuplicates	Read    14,000,000 records.  Elapsed time: 00:01:03s.  Time for last 1,000,000:    4s.  Last read position: chr3:24,097,025
  INFO	2022-10-18 10:54:49	MarkDuplicates	Tracking 69255 as yet unmatched pairs. 4605 records in RAM.
  INFO	2022-10-18 10:54:54	MarkDuplicates	Read    15,000,000 records.  Elapsed time: 00:01:07s.  Time for last 1,000,000:    4s.  Last read position: chr3:52,561,677
  INFO	2022-10-18 10:54:54	MarkDuplicates	Tracking 72825 as yet unmatched pairs. 4569 records in RAM.
  INFO	2022-10-18 10:54:58	MarkDuplicates	Read    16,000,000 records.  Elapsed time: 00:01:12s.  Time for last 1,000,000:    4s.  Last read position: chr3:122,076,905
  INFO	2022-10-18 10:54:58	MarkDuplicates	Tracking 76914 as yet unmatched pairs. 2482 records in RAM.
  INFO	2022-10-18 10:55:03	MarkDuplicates	Read    17,000,000 records.  Elapsed time: 00:01:16s.  Time for last 1,000,000:    4s.  Last read position: chr3:169,764,978
  INFO	2022-10-18 10:55:03	MarkDuplicates	Tracking 80808 as yet unmatched pairs. 1357 records in RAM.
  INFO	2022-10-18 10:55:09	MarkDuplicates	Read    18,000,000 records.  Elapsed time: 00:01:23s.  Time for last 1,000,000:    6s.  Last read position: chr3:195,674,026
  INFO	2022-10-18 10:55:09	MarkDuplicates	Tracking 83573 as yet unmatched pairs. 336 records in RAM.
  [Tue Oct 18 10:55:11 GMT 2022] picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 1.44 minutes.
  Runtime.totalMemory()=5460983808
  To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp
  htsjdk.samtools.SAMException: Value was put into PairInfoMap more than once.  3: RGHD789:1546605/A
  	at htsjdk.samtools.CoordinateSortedPairInfoMap.ensureSequenceLoaded(CoordinateSortedPairInfoMap.java:133)
  	at htsjdk.samtools.CoordinateSortedPairInfoMap.remove(CoordinateSortedPairInfoMap.java:86)
  	at picard.sam.markduplicates.util.DiskBasedReadEndsForMarkDuplicatesMap.remove(DiskBasedReadEndsForMarkDuplicatesMap.java:61)
  	at picard.sam.markduplicates.MarkDuplicates.buildSortedReadEndLists(MarkDuplicates.java:558)
  	at picard.sam.markduplicates.MarkDuplicates.doWork(MarkDuplicates.java:258)
  	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308)
  	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37)
  	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
  	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
  	at org.broadinstitute.hellbender.Main.main(Main.java:289)

Work dir:
  /home/kai/ukd/projects/TSO500/project-dev-TSO500-extwf-test/nxf_sarek/run1_full/work/0a/63585815a3434e8db1c0ed8675d2d8
```


### Relevant files

My config file looks like this:
```
params {
  max_memory = 44.GB
  max_cpus = 6
  max_time = 72.h
  // usage of targeted panel
  wes = true
  intervals = ""TSO500_manifest_GRCh38.bed""
  //keep some intermediate files for testing
  save_bam_mapped = true
  save_trimmed = true
  save_split = true
  save_reference = true
}

singularity {
  enabled = true
  cacheDir = ""/home/kai/software/nextflow_singularity_cache""  
}

// extract UMIS from read names, also see https://github.com/nf-core/sarek/issues/746
process {
	withName: 'FASTQTOBAM' {
    	ext.args         = '--extract-umis-from-read-names'
    }
}
```
And my samplesheet for this one sample X like this:
```
patient,sample,lane,status,fastq_1,fastq_2
HD789,HD789,L001,1, HD789-DNA_S2_L001_R1_001.fastq.gz,HD789-DNA_S2_L001_R2_001.fastq.gz
HD789,HD789,L002,1, HD789-DNA_S2_L002_R1_001.fastq.gz,HD789-DNA_S2_L002_R2_001.fastq.gz
HD789,HD789,L003,1,HD789-DNA_S2_L003_R1_001.fastq.gz,HD789-DNA_S2_L003_R2_001.fastq.gz
HD789,HD789,L004,1,HD789-DNA_S2_L004_R1_001.fastq.gz,HD789-DNA_S2_L004_R2_001.fastq.gz
```

### System information

Nextflow: 22.04.5
Hardware: Desktop
Executor: local
Container engine: Singularity
OS: Ubuntu 22.04
Sarek: 3.0.2
",sci-kai,https://github.com/nf-core/sarek/issues/802
I_kwDOCvwIC85UcoW5,Can be tumor-only somatic variant calling performed using Strelka?,CLOSED,2022-10-20T15:03:19Z,2024-10-29T11:58:06Z,2024-10-29T11:58:05Z,"### Description of the bug

As you mention in your Nextflow pipeline: @https://nf-co.re/sarek/3.0.2/parameters it would be possible to obtain the results of tumor-only somatic variant calling using strelka tool (among other methods). 

I've run the pipeline using strelka with tumor-only samples and I've ended up having results for germline variants (variants.vcf.gz, genome.S${N}.vcf.gz) instead of somatic ones (somatic.snvs.vcf.gz, somatic.indels.vcf.gz) (@https://github.com/Illumina/strelka/blob/v2.9.x/docs/userGuide/README.md ).

I read that strelka can perform somatic variant calling when you have tumor-normal samples pairs, however it is not possible to do so when you have tumor-only samples.  @https://academic.oup.com/bioinformatics/article/28/14/1811/218573. Did I miss any extra parameter?

Sorry in advance if i misunderstood something.





### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",ariadnaaterrades,https://github.com/nf-core/sarek/issues/803
I_kwDOCvwIC85Ugy3V,resolved path doesn't match with opened path singularity,CLOSED,2022-10-21T07:25:53Z,2022-11-10T19:41:59Z,2022-11-10T19:41:59Z,"### Description of the bug

I am encountering a bug previously described on the archived version of this pipeline here: https://github.com/SciLifeLab/Sarek/issues/780

basically the issue is that when I run a pipeline with the sarek container, I get the following:
```
Error executing process > 'trimSampleReads (18)'

Caused by:
  Process `trimSampleReads (18)` terminated with an error exit status (255)

Command executed [/mnt/lustre/users/jmorrice/projects/fifty-genomes/templates/pre-processing/trim-sample-reads.sh]:

  #!/bin/bash
  
  trim_galore     --basename SCD-369_trimmed     --paired     --fastqc     --gzip     HCFVTCCXX-7_S0_L007_R1_001.fastq.gz HCFVTCCXX-7_S0_L007_R2_001.fastq.gz

Command exit status:
  255

Command output:
  (empty)

Command error:
  FATAL:   resolved path /mnt/lustre/users/jmorrice/projects/fifty-genomes/software/sarek_latest.sif doesn't match with opened path /mnt/lustre/users/jmorrice/projects/fifty-genomes/software/sarek_latest.sif (deleted)
```

Did anyone find a resolution to this issue? The thread ended when the pipeline was moved to nf core and I could not find the thread continued on this new repo. 

Warm regards,
Jack


### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",jackmo375,https://github.com/nf-core/sarek/issues/804
I_kwDOCvwIC85VAaYI,Avoid use of exit-statements,CLOSED,2022-10-27T19:33:01Z,2023-03-31T11:50:03Z,2023-03-31T11:50:03Z,"### Description of feature

(Not sure if this should be a feature-issue or bug-issue, anyways here goes...)

Version 3.0.2 of Sarek contains a bunch of `exit`-statement. Here is an example:

https://github.com/nf-core/sarek/blob/bcd7bf9cb98cddec27bb54fb47ee122c09388c02/workflows/sarek.nf#L74

I recently learned from Marcel Ribeiro-Dantas (@mribeirodantas) that those `exit`-statements should probably be replaced by exceptions like, say,

`throw new Exception(""Bad stuff happend"")`

since ""_if Nextflow is running in a subprocess of something else, it [an exit-statement] kills the entire thing_"".

What do you say we replace the `exit`-statements with exceptions?",asp8200,https://github.com/nf-core/sarek/issues/805
I_kwDOCvwIC85ViZoO,controlfreec with multiple ploidy values results in plotting error,OPEN,2022-11-03T18:14:20Z,2024-02-08T15:12:37Z,,"### Description of the bug

When running `nf-core/sarek 3.0.2` with `--cf_ploidy ""2,3,4""` `controlfreec` runs and produces output but the `makeGraph.R` script to plot the results crashes, as it expects a single number for the `ploidy` parameter.  

- [Relevant code in `nf-core/sarek` module](https://github.com/nf-core/sarek/blob/bcd7bf9cb98cddec27bb54fb47ee122c09388c02/modules/nf-core/modules/controlfreec/makegraph/main.nf#L28)

- [Relevant code in `controlfreec` script](https://github.com/BoevaLab/FREEC/blob/800ae6d4577eab6faebde1cff40200df312ca278/scripts/makeGraph.R#L8)

Suggested solution: if `--cf_ploidy` is used with more than one ploidy option, extract the `Output_Ploidy` from the `tumour_vs_normal.tumor.mpileup.gz_info.txt` file (attached) that is produced and pass this to the `makeGraph.R` script. 

### Command used and terminal output

```console
run nf-core/sarek -r 3.0.2 -profile singularity  \
 -c nextflow.config \
 --step variant_calling \
 --outdir ./results_3.0.2 \
 -w /scratch/wsspaces/sarek_3.0.2_cfascat-0/work/ \
 --genome GATK.GRCh38 \
 --wes \
 --intervals /data/exome_targets/truseq-dna-exome-targeted-regions-manifest-v1-2_GRCh38_sorted_unique_controlFREEC.bed  \
 --cf_chrom_len /data/reference_genomes/custom_references/custom_Homo_sapiens_assembly38.fasta.fai \
 --tools controlfreec,ascat \
 --cf_contamination_adjustment true \
 --cf_ploidy ""2,3,4"" \
 --schema_ignore_params cf_ploidy,genomes \
 -resume admiring_noether
```


### Relevant files

[command.err.txt](https://github.com/nf-core/sarek/files/9931875/command.err.txt)
[tumour_vs_normal_p30.tumor.mpileup.gz_info.txt](https://github.com/nf-core/sarek/files/9931935/tumour_vs_normal_p30.tumor.mpileup.gz_info.txt)


### System information

- Nextflow version 21.10.5
- HPC
- PBS torque
- Singularity
- Scientific Linux 7.5 (Nitrogen)
- nf-core/sarek 3.0.2",mjakobs,https://github.com/nf-core/sarek/issues/808
I_kwDOCvwIC85VncUp,Intersection of VCF-files,OPEN,2022-11-04T17:20:25Z,2022-11-08T07:31:02Z,,"### Description of feature

Some users requested that Sarek produce a VCF-file which is the ""intersection"" of the VCF-files from the different variant-callers. Additional description can be found here:

https://nfcore.slack.com/archives/CGFUX04HZ/p1667579492003829

@adamrtalbot and @drpatelh suggest using `bcftools isec` for this.

This is somewhat similar to the following issue which we plan to solve using `bcftools concat`: https://github.com/nf-core/sarek/issues/738",asp8200,https://github.com/nf-core/sarek/issues/809
I_kwDOCvwIC85VvRJX,pipeline launch from nf-core website - input csv bug,CLOSED,2022-11-07T14:24:56Z,2022-11-07T17:02:41Z,2022-11-07T17:02:41Z,"### Description of the bug

When inputing the csv path in the graphical interface (website) of nf-core (e.g. https://nf-co.re/launch) the input csv shows an error even when the path is correct. The error is related to pattern matching.  For an example how this looks like, see: 
![image](https://user-images.githubusercontent.com/106694299/200316592-56c8bb4b-0d25-46a3-bf6a-459236f10609.png) 



### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information


-     Hardware: nf-core website
-     Version of nf-core/sarek: 3.0.2

",aheritas,https://github.com/nf-core/sarek/issues/811
I_kwDOCvwIC85V2-A_,GatherPileupSummaries in wgs mode failing because of sample mix-up,CLOSED,2022-11-08T16:00:20Z,2023-02-01T10:59:59Z,2023-02-01T10:59:59Z,"### Description of the bug

I was running sarek with mutect2 on a group of control samples using a custom config file as first step of PON generation. The same command was successful on a selected list of genes using a small bed file and the --wes parameter: 

`nextflow run nf-core/sarek -r 3.0.2 -profile ibiss -c mutect2.pon.config --igenomes_base $refdir --input $ponssf --wes --intervals $tbed --step variant_calling --tools mutect2 --outdir ${pondir}/poncalls_nomnp --pon false `

I then ran it using the command below and the pipeline exited with error because in the GatherPileupSummaries  step of sample p28 there was one file from another sample:
`p92.mutect2.chr2_94140558-94293015.pileups.table`
I verified that in the process folder /lustre/home/mmutarelli/project_prostate/sammy/pon/work/60/297638c50dc710ce33f8b9a4c64b29 there was a soft link of this file among the correct ones 


### Command used and terminal output

```console
$nextflow run nf-core/sarek -r 3.0.2 -profile ibiss -c mutect2.pon.config --igenomes_base /lustre/home/mmutarelli/references --input /lustre/home/mmutarelli/project_prostate/sammy/pon/normals_for_pon.csv --step variant_calling --tools mutect2 --outdir /lustre/home/mmutarelli/project_prostate/sammy/pon/prostate_ctrl_pon_calls --pon false


-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:TUMOR_ONLY_VARIANT_CALLING:GATK_TUMOR_ONLY_SOMATIC_VARIANT_CALLING:GATHERPILEUPSUMMARIES (p28)'

Caused by:
  Process `NFCORE_SAREK:SAREK:TUMOR_ONLY_VARIANT_CALLING:GATK_TUMOR_ONLY_SOMATIC_VARIANT_CALLING:GATHERPILEUPSUMMARIES (p28)` terminated with an error exit status (2)

Command executed:

  gatk --java-options ""-Xmx12g"" GatherPileupSummaries \
      --I p28.mutect2.chr2_97489619-238903659.pileups.table --I p28.mutect2.chr4_58921382-190123121.pileups.table --I p28.mutect2.chr3_93705575-198235559.pileups.table --I p28.mutect2.chr12
_37257056-37379851.pileups.table --I p28.mutect2.chr1_29553836-121976459.pileups.table --I p28.mutect2.chr3_10001-90565295.pileups.table --I p28.mutect2.chr14_19611714-106883718.pileups.tab
le --I p28.mutect2.chr5_50109808-139452659.pileups.table --I p28.mutect2.chr7_61967064-61976104.pileups.table --I p28.mutect2.chr1_143184588-223558935.pileups.table --I p28.mutect2.chr10_47
870369-124121200.pileups.table --I p28.mutect2.chr6_95070791-167591393.pileups.table --I p28.mutect2.chr13_18408107-86202979.pileups.table --I p28.mutect2.chr9_68220553-134183092.pileups.ta
ble --I p28.mutect2.chr15_23276875-84270066.pileups.table --I p28.mutect2.chr6_60001-58453888.pileups.table --I p28.mutect2.chr7_240243-58119653.pileups.table --I p28.mutect2.chr8_85714223-
145078636.pileups.table --I p28.mutect2.chr2_32917626-89330679.pileups.table --I p28.mutect2.chr17_26820267-26859724.pileups.table --I p28.mutect2.chrX_62462543-114281198.pileups.table --I
p28.mutect2.chr11_60001-50821348.pileups.table --I p28.mutect2.chr11_88002897-135076622.pileups.table --I p28.mutect2.chr16_46380683-90228345.pileups.table --I p28.mutect2.chr9_10001-412259
86.pileups.table --I p28.mutect2.chr8_45927266-85664222.pileups.table --I p28.mutect2.chr10_10001-38529907.pileups.table --I p28.mutect2.chrX_2132995-37099262.pileups.table --I p28.mutect2.
chr6_61371373-95020790.pileups.table --I p28.mutect2.chr22_18339130-18433513.pileups.table --I p28.mutect2.chr8_12284346-43983744.pileups.table --I p28.mutect2.chr19_27240875-58607616.pileu
ps.table --I p28.mutect2.chr21_10269869-10274327.pileups.table --I p28.mutect2.chr5_17580549-46435900.pileups.table --I p28.mutect2.chr20_36314720-64334167.pileups.table --I p28.mutect2.chr
12_7084651-34719407.pileups.table --I p28.mutect2.chr20_63841-26364240.pileups.table --I p28.mutect2.chr18_20571467-20830724.pileups.table --I p28.mutect2.chr18_54537529-80263285.pileups.ta
ble --I p28.mutect2.chr5_155761325-181478259.pileups.table --I p28.mutect2.chr13_86252980-111703855.pileups.table --I p28.mutect2.chr19_60001-24448980.pileups.table --I p28.mutect2.chrX_120
929382-144425606.pileups.table --I p28.mutect2.chr4_8816478-9272916.pileups.table --I p28.mutect2.chr17_491112-21795850.pileups.table --I p28.mutect2.chr1_228608365-248946422.pileups.table
--I p28.mutect2.chr16_10001-18436486.pileups.table --I p28.mutect2.chr15_84320067-101981189.pileups.table --I p28.mutect2.chr5_10001-17530548.pileups.table --I p28.mutect2.chr11_71055697-87
978202.pileups.table --I p28.mutect2.chr2_16146120-32867130.pileups.table --I p28.mutect2.chr4_32839017-49336924.pileups.table --I p28.mutect2.chr11_54525075-70955696.pileups.table --I p28.
mutect2.chr5_139453660-155760324.pileups.table --I p28.mutect2.chr2_10001-16145119.pileups.table --I p28.mutect2.chr18_10001-15410899.pileups.table --I p28.mutect2.chr7_143700805-159335973.
pileups.table --I p28.mutect2.chr16_18486487-33214595.pileups.table --I p28.mutect2.chr1_16849164-29552233.pileups.table --I p28.mutect2.chrX_37285838-49348394.pileups.table --I p28.mutect2
.chrX_144475607-156030895.pileups.table --I p28.mutect2.chr1_2746291-12954384.pileups.table --I p28.mutect2.chr10_124121503-133690466.pileups.table --I p28.mutect2.chrY_11642903-11647442.pi
leups.table --I p28.mutect2.chrX_50278965-58555579.pileups.table --I p28.mutect2.chr8_60001-7617127.pileups.table --I p28.mutect2.chr18_47019913-54536574.pileups.table --I p28.mutect2.chr4_
1441553-8797477.pileups.table --I p28.mutect2.chr4_51793952-58878793.pileups.table --I p28.mutect2.chr12_10001-7083650.pileups.table --I p28.mutect2.chrY_2781480-9046914.pileups.table --I p
28.mutect2.chr10_41693522-41916265.pileups.table --I p28.mutect2.chr20_30811899-31001508.pileups.table --I p28.mutect2.chrY_21805282-26673214.pileups.table --I p28.mutect2.chr1_223608936-22
8558364.pileups.table --I p28.mutect2.chr18_15791048-20564714.pileups.table --I p28.mutect2.chr8_7667128-12234345.pileups.table --I p28.mutect2.chrX_116595567-120879381.pileups.table --I p2
8.mutect2.chr9_134185537-138334717.pileups.table --I p28.mutect2.chr17_22813680-26627010.pileups.table --I p28.mutect2.chrX_58605580-62412542.pileups.table --I p28.mutect2.chr1_13004385-167
99163.pileups.table --I p28.mutect2.chr21_43262463-46699983.pileups.table --I p28.mutect2.chr11_51078349-54425074.pileups.table --I p28.mutect2.chr2_94140558-94293015.pileups.table --I p92.
mutect2.chr2_94140558-94293015.pileups.table --I p28.mutect2.chr1_10001-207666.pileups.table --I p28.mutect2.chr12_34829238-37185252.pileups.table --I p28.mutect2.chr9_43282957-43332174.pil
eups.table --I p28.mutect2.chr4_49708101-51743951.pileups.table --I p28.mutect2.chr16_36260629-36261158.pileups.table --I p28.mutect2.chr22_16302844-16304296.pileups.table --I p28.mutect2.c
hr10_39686683-41593521.pileups.table --I p28.mutect2.chr20_26596364-28499358.pileups.table --I p28.mutect2.chr2_92188146-94090557.pileups.table --I p28.mutect2.chr16_34339330-34521510.pileu
ps.table --I p28.mutect2.chr8_44033745-45877265.pileups.table --I p28.mutect2.chr13_111843442-113673020.pileups.table --I p28.mutect2.chrX_222347-1949345.pileups.table --I p28.mutect2.chrY_
20257794-21739542.pileups.table --I p28.mutect2.chr22_12488691-12641730.pileups.table --I p28.mutect2.chr9_66591388-67920552.pileups.table --I p28.mutect2.chr14_16054460-16061677.pileups.ta
ble --I p28.mutect2.chr9_65048125-65080082.pileups.table --I p28.mutect2.chrY_10316945-10544039.pileups.table --I p28.mutect2.chr21_8756716-8886604.pileups.table --I p28.mutect2.chr9_615188
09-61735368.pileups.table --I p28.mutect2.chr15_21242091-21778502.pileups.table --I p28.mutect2.chr4_31832570-32833016.pileups.table --I p28.mutect2.chr7_60878235-61327788.pileups.table --I
 p28.mutect2.chr9_62798833-62958371.pileups.table --I p28.mutect2.chr9_63968448-64135013.pileups.table --I p28.mutect2.chr15_22358243-23226874.pileups.table --I p28.mutect2.chr16_33442412-3
4289329.pileups.table --I p28.mutect2.chrY_9453714-10266944.pileups.table --I p28.mutect2.chr20_29271827-29315342.pileups.table --I p28.mutect2.chr21_7743701-7865746.pileups.table --I p28.mutect2.chr22_11428057-11497337.pileups.table --I p28.mutect2.chr9_60518559-60688432.pileups.table --I p28.mutect2.chr20_28504765-28751119.pileups.table --I p28.mutect2.chr21_6789086-6934219.pileups.table --I p28.mutect2.chrX_49528395-50228964.pileups.table --I p28.mutect2.chr13_113723021-114354328.pileups.table --I p28.mutect2.chr20_30088349-30425128.pileups.table \
      --O p28.mutect2.pileupsummaries.table \
      --sequence-dictionary Homo_sapiens_assembly38.dict \
      --tmp-dir . \


  Runtime.totalMemory()=9401532416
  ***********************************************************************

  A USER ERROR has occurred: Bad input: Combining PileupSummaryTables from different samples is not supported. Got samples p28_p28 and p92_p92

  ***********************************************************************
  Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.

Work dir:
  /lustre/home/mmutarelli/project_prostate/sammy/pon/work/60/297638c50dc710ce33f8b9a4c64b29

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`
```


### Relevant files

[.nextflow.log](https://github.com/nf-core/sarek/files/9962869/i.nextflow.log)


### System information

- Nextflow version 21.10.6 build 5660
- Hardware HPC
- Executor slurm
- Container engine: Singularity
- OS Linux CentosOS 7
- Version of nf-core/sarek 3.0.2",daisymut,https://github.com/nf-core/sarek/issues/818
I_kwDOCvwIC85V6nSz,DAG file already exists,CLOSED,2022-11-09T05:56:21Z,2023-02-01T11:00:42Z,2023-02-01T11:00:42Z,"### Description of the bug

When running the test example with Sarek `2.7.1 | 2.7.2`,  I got an error. When using Sarek 2.7 or 3.0.2, it works.

### Command used and terminal output

```console

$ ls -la
total 32
drwxrwx---  2 mgrau mm  4096 Nov  9 06:59 ./
drwxrwx--- 30 mgrau mm  4096 Nov  9 05:48 ../
-rw-rw----  1 mgrau mm  3959 Nov  9 05:53 input.tsv
-rwx--x---  1 mgrau mm 14721 Nov  9 05:49 nextflow

$ ./nextflow run nf-core/sarek -r 2.7.2 -profile test,singularity

N E X T F L O W  ~  version 22.10.1
Launching `https://github.com/nf-core/sarek` [happy_chandrasekhar] DSL1 - revision: 68b9930a74 [2.7.1]
DAG file already exists: /home/mgrau/out/pipeline_info/pipeline_dag_2022-11-09_06-50-14.svg
```


### System information

N E X T F L O W  ~  version 22.10.1
Sarek  2.7.1 | 2.7.2 
Singularity version 3.7.3
",migrau,https://github.com/nf-core/sarek/issues/819
I_kwDOCvwIC85WC2Sf,Cannot parse container string for deepvariant,CLOSED,2022-11-10T09:53:50Z,2022-11-10T10:09:40Z,2022-11-10T10:09:40Z,"### Description of the bug

I am downloading the sarek pipeline for offline use, including the referenced Singularity images. Everything works well, except for the following Error:
```
ERROR    Cannot parse container string in '/cluster/customapps/biomed/grlab/users/akahles/nf-core/workflows/sarek-3.0.2/workflow/modules/nf-core/modules/deepvariant/main.nf':  download.py:483
                                                                                                                                                                                               
             ${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?                                                                                     
                     'google/deepvariant:1.3.0' :                                                                                                                                              
                     'google/deepvariant:1.3.0' }                                                                                                                                              
                                                                                                                                                                                               
         ⚠ Skipping this singularity image.. 
```
 Is this possibly related to #769 ?

### Command used and terminal output

```console
nf-core download sarek --container singularity --singularity-cache-only --revision 3.0.2 --outdir /path/on/system/nf-core/workflows/sarek-3.0.2 --compress none
```


### Relevant files

_No response_

### System information

Nextflow version: 22.04.3 build 5704
nf-core version: version 2.6
singularity-ce version 3.8.3
Hardware: HPC
OS: Gentoo Base System release 2.6
version nf-core/sarek: 3.0.2",akahles,https://github.com/nf-core/sarek/issues/823
I_kwDOCvwIC85WEd3o,External CNVkit reference file for tumor-only WES,CLOSED,2022-11-10T14:31:18Z,2022-11-11T14:17:27Z,2022-11-11T14:17:27Z,"### Description of feature

Hi,
I realised that CNVkit can be used in tumor-only mode but it uses the `reference.cnn` file created based on the Fasta and target regions. This might be good enough for WGS samples, but WES samples would need a reference file reflecting the coverage profile of the specific WES capture kit. It would be great if we could provide a previously generated `reference.cnn` file using the pipeline parameters. How complicated would it be to implement such a feature?",berguner,https://github.com/nf-core/sarek/issues/824
I_kwDOCvwIC85WGLST,BCFTools use region/target file,OPEN,2022-11-10T19:39:16Z,2024-08-19T13:12:29Z,,"### Description of feature

Pretty much title, we should discuss this and add it to the code. Probably have to make distinctions between WES and WGS",FriederikeHanssen,https://github.com/nf-core/sarek/issues/826
I_kwDOCvwIC85WGzK-,Add T2T reference,OPEN,2022-11-10T21:59:59Z,2022-11-10T21:59:59Z,,"### Description of feature

For now we pretty much only have the fasta, but gotta start somewhere :) ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/829
I_kwDOCvwIC85WJB4n,Customize Preprocessing based on each tool,OPEN,2022-11-11T09:38:02Z,2024-08-19T13:12:29Z,,"### Description of feature

Hi,
It seems like the CNVkit workflow uses `cram_recalibrated` files as input here: https://github.com/nf-core/sarek/blob/bcd7bf9cb98cddec27bb54fb47ee122c09388c02/subworkflows/nf-core/variantcalling/cnvkit/main.nf#L8-L12. As far as I remember,  recalibrated files of WES or panel samples don't contain off-target reads because base recalibration is applied over the intervals only. It would be better using CRAM files containing all the reads (`cram_markduplicates` ?) for CNVkit analysis for utilizing off-target reads. This is especially important for custom panels where there are fewer target regions compared to WES.",berguner,https://github.com/nf-core/sarek/issues/830
I_kwDOCvwIC85WTuXW,Improve subway-map of variant-callers,OPEN,2022-11-14T12:56:13Z,2024-08-19T13:12:30Z,,"### Description of feature

As mentioned here,

https://github.com/nf-core/sarek/pull/831#issuecomment-1313457016

the subway-map of the variant-callers seem to indicate that the variant-callers are being called sequentially while, in fact, they are called in parallel.  Try to make a subway-map which indicate that the variant-callers are being called in parallel.",asp8200,https://github.com/nf-core/sarek/issues/834
I_kwDOCvwIC85WUwvd,vcf-file from mpileup is not getting annotated,CLOSED,2022-11-14T15:31:19Z,2023-06-15T10:05:28Z,2023-06-15T10:05:28Z,"### Description of the bug

The vcf-file from mpileup isn't getting annotated along with the other germline-vcfs. 

https://github.com/nf-core/sarek/blob/bcd7bf9cb98cddec27bb54fb47ee122c09388c02/workflows/sarek.nf#L984-L989

The command below resulted in vcf-files both from mpileup and strelka, but only an annotated vcf-file from strelka.

### Command used and terminal output

```console
nextflow run main.nf -profile test,singularity  --input tests/csv/3.0/mapped_joint_bam.csv --step variant_calling --tools mpileup,strelka,vep --skip_tools multiqc
```


### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/835
I_kwDOCvwIC85WZQTV,Add samtools markdup as duplicate marking alternative,OPEN,2022-11-15T09:33:00Z,2024-08-19T13:12:30Z,,"### Description of feature

With the new template sort Samtools markdup is supposedly much faster than GATK Markduplicates while having similar results. 

I think this would be worth to add a faster alternative, that is documented & maintained.",FriederikeHanssen,https://github.com/nf-core/sarek/issues/838
I_kwDOCvwIC85WZ9Sm,error downloading singularity images using sarek 3.0+ version,CLOSED,2022-11-15T11:42:33Z,2022-11-28T13:05:56Z,2022-11-28T13:05:56Z,"### Description of the bug

I tried to download singularity images for offline uppmax. it failed to download 'google/deepvariant:1.3.0'

### Command used and terminal output

```console
command:
nf-core download nf-core/sarek -r 3.0/3.0.1/3.0.2 -c singularity --force

error:
ERROR    Cannot parse container string in                                                               download.py:483
         'nf-core-sarek-3.0.1/workflow/modules/nf-core/modules/deepvariant/main.nf':

             ${ workflow.containerEngine == 'singularity' &&
         !task.ext.singularity_pull_docker_container ?
                     'google/deepvariant:1.3.0' :
                     'google/deepvariant:1.3.0' }

         ⚠ Skipping this singularity image..
```


### Relevant files

_No response_

### System information

_No response_",yqkiuo,https://github.com/nf-core/sarek/issues/840
I_kwDOCvwIC85WfvOL,Use -cancer for snpeff samples,OPEN,2022-11-16T08:53:40Z,2024-08-19T13:12:30Z,,"### Description of feature

SnpEff for cancer samples should be run with `-cancer` flag:

https://pcingola.github.io/SnpEff/se_cansersamples/

For this we need to add meta data back in as well as tracking the status when starting from the annotation step.",FriederikeHanssen,https://github.com/nf-core/sarek/issues/845
I_kwDOCvwIC85WgUfp,Launch parameter validation for --input failing on valid input,CLOSED,2022-11-16T10:26:26Z,2022-11-16T11:20:59Z,2022-11-16T11:20:58Z,"### Description of the bug

https://nf-co.re/launch for Sarek fails when e.g. ""samplesheet.csv"" is provided in for ``--input``. This should pass the regular expression ``.csv$``.

![Screenshot 2022-11-16 at 10 24 50](https://user-images.githubusercontent.com/6746627/202155535-6cbb4a3b-890c-4563-8176-aeae304d16dc.png)

Possibly only needs one ``\`` in nextflow_schema.json:

```
                ""input"": {
                    ""type"": ""string"",
                    ""format"": ""file-path"",
                    ""mimetype"": ""text/csv"",
                    ""pattern"": ""\\.csv$"",
                    ""schema"": ""assets/schema_input.json"",
                    ""description"": ""Path to comma-separated file containing information about the samples in the experiment."",
                    ""help_text"": ""A design file with information about the samples in your experiment. Use this parameter to specify the location of the input files. It has to be a comma-separated file with a header row. See [usage docs](https://nf-co.re/sarek/usage#input).\n\nIf no input file is specified, sarek will attempt to locate one in the `{outdir}` directory."",
                    ""fa_icon"": ""fas fa-file-csv""
                },
```

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",ameynert,https://github.com/nf-core/sarek/issues/847
I_kwDOCvwIC85WgWIV,Provide typical usage of --tools,OPEN,2022-11-16T10:30:32Z,2024-08-19T13:12:31Z,,"### Description of feature

The regex for describing how to specify ``--tools`` is not easy to understand for non-command line users. Adding some tool combination examples for typical usage would be helpful. Also suggest specifying the exact capitalization for each tool with the tool listing, not just in the regex.",ameynert,https://github.com/nf-core/sarek/issues/848
I_kwDOCvwIC85WopgJ,add more info about which process are affected by --wes,OPEN,2022-11-17T15:00:17Z,2024-08-19T13:12:31Z,,"### Description of feature

Add more info in the docs",maxulysse,https://github.com/nf-core/sarek/issues/852
I_kwDOCvwIC85WrD76,GroupKey Logic for multiple lanes with different sizes based on line splitting,CLOSED,2022-11-17T22:59:17Z,2024-01-17T10:36:44Z,2024-01-17T10:36:43Z,"Will this logic work for samples that have been run across multiple lanes and split_fastq turned on?
If BAM_MARKDUPLICATES_SPARK is the next step and 1 lane of fastqs was split into 2 and the other was split into 3 then BAM_MARKDUPLICATES_SPARK  won't run at all or it will only run with 4 out of the 5 aligned bams.

Am I missing something?

https://github.com/nf-core/sarek/blob/0b7f0e2b6f0be7053838ba1884b2eb03e9f83f9e/workflows/sarek.nf#L500-L522",achristofferson-bbi,https://github.com/nf-core/sarek/issues/853
I_kwDOCvwIC85Wz3Xf,Ensembl VEP annotation is skipped with yeast genome ,CLOSED,2022-11-19T12:17:33Z,2022-11-21T16:45:00Z,2022-11-21T16:36:44Z,"Hello,

I ran the following:

```
nextflow run nf-core/sarek -r 3.1 --input yeast-test-sample-sheet.csv --outdir yeast-sarek-local-native-R64-1-1-DV-FB-ST-snpeff -profile docker --genome R64-1-1 --vep_out_format tab --skip_tools baserecalibrator --tools deepvariant,freebayes,manta,cnvkit,tiddit,vep,snpeff
```

and noticed that neither VEP, nor SNPeff, ran on my samples, using the latest Ensembl S. Cerevisiae reference genome from Igenomes.

Is this to be expected? I assumed that VEP should run fine for that genome, given that it's from Ensembl and there's definitely a VEP cache available for it.
",amizeranschi,https://github.com/nf-core/sarek/issues/854
I_kwDOCvwIC85XG6Cm,MultiQC error when running via HTCondor: Path 'multiqc_config.yml' does not exist,CLOSED,2022-11-23T10:01:40Z,2022-11-24T08:58:00Z,2022-11-24T08:58:00Z,"### Description of the bug

The command below works fine when running locally, but fails when setting `process.executor = 'condor'` in nextflow.config.


### Command used and terminal output

```console
$ nextflow run nf-core/sarek -r 3.1.1 --input yeast-test-sample-sheet.csv --outdir yeast-sarek-NFS-HTCondor-R64-1-1-DV-FB-ST -profile docker --genome R64-1-1 --vep_out_format tab --skip_tools baserecalibrator --tools deepvariant,freebayes,manta,cnvkit,tiddit,vep,snpeff

-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:MULTIQC'

Caused by:
  Process `NFCORE_SAREK:SAREK:MULTIQC` terminated with an error exit status (2)

Command executed:

  multiqc \
      --force \
       \
      --config multiqc_config.yml \
       \
      .
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:MULTIQC"":
      multiqc: $( multiqc --version | sed -e ""s/multiqc, version //g"" )
  END_VERSIONS

Command exit status:
  2

Command output:
  Usage: multiqc [OPTIONS] [ANALYSIS DIRECTORY]
  This is MultiQC v1.13
  For more help, run 'multiqc --help' or visit http://multiqc.info
  ╭─ Error ──────────────────────────────────────────────────────────────────────╮
  │ Invalid value for '-c' / '--config': Path 'multiqc_config.yml' does not      │
  │ exist.                                                                       │
  ╰──────────────────────────────────────────────────────────────────────────────╯

Command error:
  Unable to find image 'quay.io/biocontainers/multiqc:1.13--pyhdfd78af_0' locally
  1.13--pyhdfd78af_0: Pulling from biocontainers/multiqc
  aa44502a478a: Pulling fs layer
  bef3901422b5: Pulling fs layer
  b6572cc046f2: Pulling fs layer
  6dbbfe98ccd4: Pulling fs layer
  4ca545ee6d5d: Pulling fs layer
  00c3f0867075: Pulling fs layer
  6dbbfe98ccd4: Waiting
  4ca545ee6d5d: Waiting
  00c3f0867075: Waiting
  b6572cc046f2: Verifying Checksum
  b6572cc046f2: Download complete
  bef3901422b5: Verifying Checksum
  bef3901422b5: Download complete
  aa44502a478a: Download complete
  aa44502a478a: Pull complete
  bef3901422b5: Pull complete
  b6572cc046f2: Pull complete
  6dbbfe98ccd4: Verifying Checksum
  6dbbfe98ccd4: Download complete
  4ca545ee6d5d: Verifying Checksum
  4ca545ee6d5d: Download complete
  6dbbfe98ccd4: Pull complete
  4ca545ee6d5d: Pull complete
  00c3f0867075: Verifying Checksum
  00c3f0867075: Download complete
  00c3f0867075: Pull complete
  Digest: sha256:db913f47894a386040d76ae5f46b19149a71377f5107327d15ec89d3a1ec3f5f
  Status: Downloaded newer image for quay.io/biocontainers/multiqc:1.13--pyhdfd78af_0

Work dir:
  /data/share/nf-core/work/7c/f4b341ef330bbd16869c65c4da7717

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/10074347/nextflow.log)


### System information

_No response_",amizeranschi,https://github.com/nf-core/sarek/issues/861
I_kwDOCvwIC85XI0Mb,Don't publish pileup files when only ControlFreec is run (and not mpileup as separate tool),CLOSED,2022-11-23T15:06:51Z,2023-06-16T09:17:20Z,2023-06-16T09:17:20Z,"### Description of the bug

Currently pileup files are published when controlfreec is run, but not mpileup as a separate tool. Since these files are quite big I think we should not publish them by default. 

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/862
I_kwDOCvwIC85XI8Uc,Improve CNVKit,OPEN,2022-11-23T15:26:37Z,2024-08-19T13:12:31Z,,"### Description of feature

CNVKit can take sex information: https://github.com/etal/cnvkit/blob/b218280edb266788ea8326596d4283183ce425ea/doc/sex.rst as well as SNP information to to infer b-allel frequency: https://cnvkit.readthedocs.io/en/stable/pipeline.html#snp-allele-frequencies.

Adding sex should be simple we track that already for the other CNA, the vcfs we could discuss how to best pass them on. 

In addition, there are some recommendations for tumor-only analysis, such as removal of low coverage areas: https://cnvkit.readthedocs.io/en/stable/tumor.html
",FriederikeHanssen,https://github.com/nf-core/sarek/issues/863
I_kwDOCvwIC85XOwFk,Is it possible to change Mutect2 variant read threshold for samples with lower tumour cellularity ,CLOSED,2022-11-24T15:08:20Z,2022-12-17T20:33:59Z,2022-12-17T20:33:59Z,"### Description of feature

The type of tumour samples I work with tend to have low tumour cellularity by nature, when running Mutect2 (nf-core/sarek) in Wes mode, I have noticed that some of the variants are filleted out by Mutect2 , so for example a sample with 10% tumour cellularity that has a variant read below 13% is excluded although this variant is present and a true variant, which is important to be aware of in case of rare cancers. could you please guide me to how to solve this issue and get Mutect2 to have a lower filter threshold. 

To run the pipeline I use : 

 nextflow run nf-core/sarek -r 3.0.2 -profile singularity \
-c nextflow.config \
 --input Samplesheet.csv \
 -w /scratch/designated work space \
 --genome GATK.GRCh38 \
 --wes \
 --intervals /specified target_sorted.unique.bed \
 --tools cnvkit,freebayes,manta,mpileup,mutect2,strelka,tiddit,vep,snpeff
-- step annotate ^((cnvkit|freebayes|manta|merge|mpileup|mutect2|snpeff|strelka|tiddit|vep)?,?)*[^,]+$

Any guidance is appreciated. ",Ahas4,https://github.com/nf-core/sarek/issues/866
I_kwDOCvwIC85XSbxl,Population calling | joint calling | ensemble calling,OPEN,2022-11-25T12:00:51Z,2024-08-19T13:12:31Z,,"### Description of feature


Hi!

I was trying out the `joint_germline` option, which produces a multi-sample VCF file from haplotypecaller. It would be helpful to have a similar feature for other callers like deepvariant, freebayes and strelka2. 

It could also nice to have support for ensemble calling (from multiple callers), for some use cases where the number of samples is manageable.

One way to achieve this would be to use [bcbio.variation.recall](https://github.com/bcbio/bcbio.variation.recall), which was created by Brad Chapman and [used in bcbio-nextgen](https://bcbio-nextgen.readthedocs.io/en/latest/contents/germline_variants.html?highlight=bcbio.variation.recall#parameters).

Relevant documention from bcbio-nextgen can be found here:

* [population calling](https://bcbio-nextgen.readthedocs.io/en/latest/contents/germline_variants.html?highlight=bcbio.variation.recall#parameters)
* [ensemble calling](https://bcbio-nextgen.readthedocs.io/en/latest/contents/somatic_variants.html?highlight=ensemble#ensemble-variant-calling)

For joint calling, another solution could be to have the variant callers output gVCF files, and then use GATK's GenotypeGVCFs or [something similar](https://github.com/dnanexus-rnd/GLnexus) on those. There was a discussion about this on the deepvariant GitHub page a few years ago: https://github.com/google/deepvariant/issues/142#issuecomment-459440356.
",amizeranschi,https://github.com/nf-core/sarek/issues/868
I_kwDOCvwIC85XTZWX,flowcell in readgroup is not resolved,CLOSED,2022-11-25T15:43:07Z,2024-10-14T12:32:54Z,2024-10-14T12:32:53Z,"### Description of the bug

The ID field should have `${flowcell}.${row.sample}.${row.lane}`
`flowcell` gets resolved to `null`, so I'm guessing something with the `flowcellLaneFromFastq()` function.

cf AWS megatests in tower:

```
INDEX=`find -L ./ -name ""*.amb"" | sed 's/.amb//'`

bwa mem \
    -K 100000000 -Y -R ""@RG\tID:null.HCC1395N.1\tPU:1\tSM:HCC1395_HCC1395N\tLB:HCC1395N\tDS:s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta\tPL:ILLUMINA"" \
    -t 16 \
    $INDEX \
    0001.HCC1395N-1_1.fastp.fastq.gz 0001.HCC1395N-1_2.fastp.fastq.gz \
    | samtools sort  --threads 16 -o HCC1395N-1.0001.bam -

cat <<-END_VERSIONS > versions.yml
""NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP:BWAMEM1_MEM"":
    bwa: $(echo $(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*$//')
    samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
END_VERSIONS
```

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/869
I_kwDOCvwIC85XbFS5,Support for UMI FASTQ,OPEN,2022-11-28T16:30:43Z,2024-08-19T13:12:32Z,,"### Description of feature

UMI data is sometimes stored in a third FASTQ file, typically because the UMI is embedded in the index and bcl2fastq2 cannot combine it into forward/reverse. This produces three files:
R1: Forward
R2: UMI
R3: Reverse

We can support UMI processing using this method. Key points:

- Allow the samplesheet to include optional 3rd FASTQ
- Ignore/handle UMI FASTQ when sending to modules that expect 2 FASTQ files to ensure compatibility
- Allow/enforce `--umi_read_structure` to support `>2` masks.

See https://github.com/nf-core/fastquorum/pull/11 for an example implementation. Could be implemented as part of NF-Core subworkflow.",adamrtalbot,https://github.com/nf-core/sarek/issues/871
I_kwDOCvwIC85Xbcup,Fix the test-sample-sheet tests/csv/3.0/mapped_joint_bam.csv,CLOSED,2022-11-28T17:31:16Z,2023-01-18T10:17:38Z,2023-01-18T10:17:38Z,"### Description of the bug

https://nfcore.slack.com/archives/C02MDBZAYJK/p1669641933059249

Change the content of the patient- and sample-fields in `tests/csv/3.0/mapped_joint_bam.csv`so that they correspond to the ids in the bam-files, that is, `test` -> `testN` and `test1` -> `testT`:
```
patient,status,sample,bam,bai
testN,0,testN,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/bam/test.paired_end.sorted.bam,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/bam/test.paired_end.sorted.bam.bai
testT,0,testT,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/bam/test2.paired_end.sorted.bam,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/bam/test2.paired_end.sorted.bam.bai
```

This change will probably also require that some md5sums in `tests/test_haplotypecaller.yml` are updated.  

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/872
I_kwDOCvwIC85XnBUn,Add IGV visualization for alignment files,OPEN,2022-11-30T15:44:36Z,2024-08-19T13:12:32Z,,"### Description of feature

Basically add this as a module. I am unclear on the details and what we woud need, but this or something similar would be really great to have:

https://github.com/igvteam/igv.js",FriederikeHanssen,https://github.com/nf-core/sarek/issues/875
I_kwDOCvwIC85Xoji7,Make the filtering step in the haplotype_single_sample track skippable,CLOSED,2022-11-30T21:39:37Z,2023-01-18T10:55:03Z,2023-01-18T10:55:03Z,"### Description of feature

For some use cases, it would be useful to skip the filtering following Haplotypecaller in the single sample mode:

https://nfcore.slack.com/archives/CGFUX04HZ/p1669759507612189?thread_ts=1665054217.962299&cid=CGFUX04HZ
https://nfcore.slack.com/archives/CGFUX04HZ/p1668784847234849

",FriederikeHanssen,https://github.com/nf-core/sarek/issues/876
I_kwDOCvwIC85Xrkoa,"Put annotation in same output structure, regardless of -step entry point ",OPEN,2022-12-01T10:11:24Z,2022-12-01T10:34:15Z,,"### Description of feature

[This line](https://github.com/nf-core/sarek/blob/0b7f0e2b6f0be7053838ba1884b2eb03e9f83f9e/conf/modules/annotate.config#L58) and [this line](https://github.com/nf-core/sarek/blob/0b7f0e2b6f0be7053838ba1884b2eb03e9f83f9e/conf/modules/annotate.config#L79) (and possibly more places) specifies where annotated vcf files should be put after VEP and all annotation-tools, like this:
```
path: { ""${params.outdir}/annotation/${meta.variantcaller}/${meta.id}/"" },
```

 However, if Sarek was started using `-step annotate`, the `${meta.variantcaller}` part resolves to `NULL` and the output files are instead put in 

```
${params.outdir}/annotation/${meta.id}/
```

While it's a small thing, I think it would be more elegant to put them in a subfolder e.g. `vcf_input` or `no_mapping` or such. It seems a little forgetful that they just appear one step ""up"" in the file structure, depending on the `-step` input not being `mapping`.

(as discussed last Tuesday meeting @maxulysse )",lassefolkersen,https://github.com/nf-core/sarek/issues/877
I_kwDOCvwIC85XrwHy,annotation enhancement,OPEN,2022-12-01T10:45:36Z,2022-12-01T10:45:36Z,,"### Description of feature

following #792:

- [ ] Keep track of which variant caller were used before concatenation
- [ ] Create a csv file to allow restart from annotation
- [ ] #809 
- [ ] Annotate concatenated and/or intersected and/or regular vcfs",maxulysse,https://github.com/nf-core/sarek/issues/878
I_kwDOCvwIC85X1LC0,Variant annotation for non-coding genes,OPEN,2022-12-02T23:03:07Z,2022-12-02T23:03:07Z,,"### Description of feature

The addition to the sarek pipeline is suggested here : https://nfcore.slack.com/archives/CE6SDEDAA/p1668636097662079",jaybee84,https://github.com/nf-core/sarek/issues/879
I_kwDOCvwIC85YAgt3,Automatic identification of callable regions to parallelize BQSR and variant calling,OPEN,2022-12-05T13:06:52Z,2022-12-05T13:06:52Z,,"### Description of feature

Hi! It would be helpful to have the option of an automatic means of generating the [intervals file for BQSR and variant calling](https://nf-co.re/sarek/usage#intervals-for-base-quality-score-recalibration-and-variantcalling), by identifying callable regions in mapped reads themselves.

An example approach would be [the one described in bcbio-nextgen here](https://bcbio-nextgen.readthedocs.io/en/latest/contents/internals.html#parallel), and in a bit more depth [here](http://bcb.io/2013/05/22/scaling-variant-detection-pipelines-for-whole-genome-sequencing-analysis/). This is the relevant bit:

> Previously, we'd split analyses by chromosome but this has the downside of tying analysis times to chromosome 1, the largest chromosome.
> 
> The pipeline now identifies chromosome blocks without callable reads. These blocks group by either genomic features like repetitive hard to align sequence or analysis requirements like defined target regions. Using the globally shared callable regions across samples, we fraction the genome into more uniform sections for processing. As a result we can work on smaller chunks of reads during time critical parts of the process: applying base recalibration, de-duplication, realignment and variant calling. 

This could help quite a bit with speeding up the WGS data analysis for nonhuman species.
",amizeranschi,https://github.com/nf-core/sarek/issues/880
I_kwDOCvwIC85YJ3kD,"--genome flag, allow for AWS iGenomes GATK.b37",OPEN,2022-12-06T12:26:38Z,2022-12-06T12:26:38Z,,"### Description of feature

Currently the available genomes include:

```
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  Genome 'GATK.b37' not found in any config files provided to the pipeline.
  Currently, the available genome keys are:
  GATK.GRCh37, GATK.GRCh38, Ensembl.GRCh37, NCBI.GRCh38, GRCm38, TAIR10, EB2, UMD3.1, WBcel235, CanFam3.1, GRCz10, BDGP6, EquCab2, EB1, Galgal4, Gm01, Mmul_1, IRGSP-1.0, CHIMP2.1.4, Rnor_5.0, Rnor_6.0, R64-1-1, EF2, Sbi1, Sscrofa10.2, AGPv3, hg38, hg19, mm10, bosTau8, ce10, canFam3, danRer10, dm6, equCab2, galGal4, panTro4, rn6, sacCer3, susScr3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```
According to some online [discussion](http://lh3.github.io/2017/11/13/which-human-reference-genome-to-use), when selecting a human genome, the best possibility to choose is hs37-1kg, which, according to GATK[ is pretty much equivalent to b37](https://gatk.broadinstitute.org/hc/en-us/articles/360035890711-GRCh37-hg19-b37-humanG1Kv37-Human-Reference-Discrepancies). Since b37 is available in AWS iGenomes (which is used for other genomes in the sarek pipeline), I would like to request if it is possible to add GATK.b37 (or humanG1Kv37 from another source) as an available genome key. Thank you!",aheritas,https://github.com/nf-core/sarek/issues/881
I_kwDOCvwIC85YS8DW,Mention the option --concatenate_vcfs in usage.md and output.md,CLOSED,2022-12-07T08:47:31Z,2023-08-17T11:23:03Z,2023-08-17T11:23:03Z,"### Description of feature

Document the new option `--concatenate_vcfs` (for concatenating GERMLINE vcfs) in `docs/usage.md` and `docs/output.md`.",asp8200,https://github.com/nf-core/sarek/issues/883
I_kwDOCvwIC85YVIae,Replace the locale module add_info_to_vcf with bcftools,OPEN,2022-12-07T13:28:45Z,2024-08-19T13:12:33Z,,"### Description of feature

If possible, use bcftools for adding the INFO-field `SOURCE` to germline-vcf-files instead of the local module `add_info_to_vcf`.

https://nfcore.slack.com/archives/C02MDBZAYJK/p1670415397942539

https://samtools.github.io/bcftools/howtos/plugin.fill-tags.html",asp8200,https://github.com/nf-core/sarek/issues/885
I_kwDOCvwIC85Yjtzd,Sample metadata out of sync when running BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2,CLOSED,2022-12-09T01:33:32Z,2023-02-01T10:32:10Z,2023-02-01T10:32:09Z,"### Description of the bug

Error messages for several samples from the processes MERGE_MUTECT2 and GATHERPILEUPSUMMARIES.

MERGE_MUTECT2 .command.sh:
```bash
#!/bin/bash -euo pipefail
gatk --java-options ""-Xmx4g"" MergeVcfs \
    --INPUT AA0180-01-04f.mutect2.chr17_26820267-26859724.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_97489619-238903659.vcf.gz --INPUT AA0229-01-04a.mutect2.chr3_93705575-198235559.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_58921382-190123121.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_37257056-37379851.vcf.gz --INPUT AA0229-01-04a.mutect2.chr14_19611714-106883718.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_29553836-121976459.vcf.gz --INPUT AA0229-01-04a.mutect2.chr3_10001-90565295.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_50109808-139452659.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_47870369-124121200.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_61967064-61976104.vcf.gz --INPUT AA0229-01-04a.mutect2.chr6_95070791-167591393.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_143184588-223558935.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_68220553-134183092.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_23276875-84270066.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_18408107-86202979.vcf.gz --INPUT AA0229-01-04a.mutect2.chr6_60001-58453888.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_240243-58119653.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_32917626-89330679.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_85714223-145078636.vcf.gz --INPUT AA0229-01-04a.mutect2.chr17_26820267-26859724.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_60001-50821348.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_62462543-114281198.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_46380683-90228345.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_88002897-135076622.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_10001-41225986.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_45927266-85664222.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_2132995-37099262.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_10001-38529907.vcf.gz --INPUT AA0229-01-04a.mutect2.chr6_61371373-95020790.vcf.gz --INPUT AA0229-01-04a.mutect2.chr19_27240875-58607616.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_10269869-10274327.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_18339130-18433513.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_12284346-43983744.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_17580549-46435900.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_36314720-64334167.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_7084651-34719407.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_63841-26364240.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_20571467-20830724.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_54537529-80263285.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_155761325-181478259.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_86252980-111703855.vcf.gz --INPUT AA0229-01-04a.mutect2.chr19_60001-24448980.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_120929382-144425606.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_8816478-9272916.vcf.gz --INPUT AA0229-01-04a.mutect2.chr17_491112-21795850.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_228608365-248946422.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_10001-18436486.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_84320067-101981189.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_10001-17530548.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_71055697-87978202.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_16146120-32867130.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_32839017-49336924.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_54525075-70955696.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_139453660-155760324.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_10001-16145119.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_143700805-159335973.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_10001-15410899.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_18486487-33214595.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_16849164-29552233.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_37285838-49348394.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_144475607-156030895.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_2746291-12954384.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_124121503-133690466.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_50278965-58555579.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_11642903-11647442.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_60001-7617127.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_47019913-54536574.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_1441553-8797477.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_51793952-58878793.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_10001-7083650.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_2781480-9046914.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_41693522-41916265.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_30811899-31001508.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_223608936-228558364.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_15791048-20564714.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_7667128-12234345.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_21805282-26673214.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_116595567-120879381.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_134185537-138334717.vcf.gz --INPUT AA0229-01-04a.mutect2.chr17_22813680-26627010.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_58605580-62412542.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_13004385-16799163.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_43262463-46699983.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_51078349-54425074.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_10001-207666.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_94140558-94293015.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_34829238-37185252.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_43282957-43332174.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_49708101-51743951.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_36260629-36261158.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_16302844-16304296.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_39686683-41593521.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_92188146-94090557.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_44033745-45877265.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_26596364-28499358.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_34339330-34521510.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_111843442-113673020.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_222347-1949345.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_20257794-21739542.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_66591388-67920552.vcf.gz --INPUT AA0229-01-04a.mutect2.chr14_16054460-16061677.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_12488691-12641730.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_65048125-65080082.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_10316945-10544039.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_8756716-8886604.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_61518809-61735368.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_21242091-21778502.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_31832570-32833016.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_60878235-61327788.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_62798833-62958371.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_63968448-64135013.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_22358243-23226874.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_33442412-34289329.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_9453714-10266944.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_29271827-29315342.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_60518559-60688432.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_11428057-11497337.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_7743701-7865746.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_28504765-28751119.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_6789086-6934219.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_49528395-50228964.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_113723021-114354328.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_30088349-30425128.vcf.gz \
    --OUTPUT AA0229-01-04a.mutect2.vcf.gz \
    --SEQUENCE_DICTIONARY Homo_sapiens_assembly38.dict \
    --TMP_DIR . \


cat <<-END_VERSIONS > versions.yml
""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:MERGE_MUTECT2"":
    gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
END_VERSIONS
```

MERGE_MUTECT2 .command.err:
```
WARNING: While bind mounting '/vast:/vast': destination is already in the mount point list
Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar
Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar MergeVcfs --INPUT AA0180-01-04f.mutect2.chr17_26820267-26859724.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_97489619-238903659.vcf.gz --INPUT AA0229-01-04a.mutect2.chr3_93705575-198235559.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_58921382-190123121.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_37257056-37379851.vcf.gz --INPUT AA0229-01-04a.mutect2.chr14_19611714-106883718.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_29553836-121976459.vcf.gz --INPUT AA0229-01-04a.mutect2.chr3_10001-90565295.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_50109808-139452659.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_47870369-124121200.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_61967064-61976104.vcf.gz --INPUT AA0229-01-04a.mutect2.chr6_95070791-167591393.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_143184588-223558935.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_68220553-134183092.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_23276875-84270066.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_18408107-86202979.vcf.gz --INPUT AA0229-01-04a.mutect2.chr6_60001-58453888.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_240243-58119653.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_32917626-89330679.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_85714223-145078636.vcf.gz --INPUT AA0229-01-04a.mutect2.chr17_26820267-26859724.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_60001-50821348.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_62462543-114281198.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_46380683-90228345.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_88002897-135076622.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_10001-41225986.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_45927266-85664222.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_2132995-37099262.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_10001-38529907.vcf.gz --INPUT AA0229-01-04a.mutect2.chr6_61371373-95020790.vcf.gz --INPUT AA0229-01-04a.mutect2.chr19_27240875-58607616.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_10269869-10274327.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_18339130-18433513.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_12284346-43983744.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_17580549-46435900.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_36314720-64334167.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_7084651-34719407.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_63841-26364240.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_20571467-20830724.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_54537529-80263285.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_155761325-181478259.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_86252980-111703855.vcf.gz --INPUT AA0229-01-04a.mutect2.chr19_60001-24448980.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_120929382-144425606.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_8816478-9272916.vcf.gz --INPUT AA0229-01-04a.mutect2.chr17_491112-21795850.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_228608365-248946422.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_10001-18436486.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_84320067-101981189.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_10001-17530548.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_71055697-87978202.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_16146120-32867130.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_32839017-49336924.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_54525075-70955696.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_139453660-155760324.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_10001-16145119.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_143700805-159335973.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_10001-15410899.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_18486487-33214595.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_16849164-29552233.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_37285838-49348394.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_144475607-156030895.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_2746291-12954384.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_124121503-133690466.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_50278965-58555579.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_11642903-11647442.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_60001-7617127.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_47019913-54536574.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_1441553-8797477.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_51793952-58878793.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_10001-7083650.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_2781480-9046914.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_41693522-41916265.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_30811899-31001508.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_223608936-228558364.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_15791048-20564714.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_7667128-12234345.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_21805282-26673214.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_116595567-120879381.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_134185537-138334717.vcf.gz --INPUT AA0229-01-04a.mutect2.chr17_22813680-26627010.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_58605580-62412542.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_13004385-16799163.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_43262463-46699983.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_51078349-54425074.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_10001-207666.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_94140558-94293015.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_34829238-37185252.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_43282957-43332174.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_49708101-51743951.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_36260629-36261158.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_16302844-16304296.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_39686683-41593521.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_92188146-94090557.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_44033745-45877265.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_26596364-28499358.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_34339330-34521510.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_111843442-113673020.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_222347-1949345.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_20257794-21739542.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_66591388-67920552.vcf.gz --INPUT AA0229-01-04a.mutect2.chr14_16054460-16061677.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_12488691-12641730.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_65048125-65080082.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_10316945-10544039.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_8756716-8886604.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_61518809-61735368.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_21242091-21778502.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_31832570-32833016.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_60878235-61327788.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_62798833-62958371.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_63968448-64135013.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_22358243-23226874.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_33442412-34289329.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_9453714-10266944.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_29271827-29315342.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_60518559-60688432.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_11428057-11497337.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_7743701-7865746.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_28504765-28751119.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_6789086-6934219.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_49528395-50228964.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_113723021-114354328.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_30088349-30425128.vcf.gz --OUTPUT AA0229-01-04a.mutect2.vcf.gz --SEQUENCE_DICTIONARY Homo_sapiens_assembly38.dict --TMP_DIR .
00:42:32.483 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
[Fri Dec 09 00:42:32 GMT 2022] MergeVcfs --INPUT AA0180-01-04f.mutect2.chr17_26820267-26859724.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_97489619-238903659.vcf.gz --INPUT AA0229-01-04a.mutect2.chr3_93705575-198235559.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_58921382-190123121.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_37257056-37379851.vcf.gz --INPUT AA0229-01-04a.mutect2.chr14_19611714-106883718.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_29553836-121976459.vcf.gz --INPUT AA0229-01-04a.mutect2.chr3_10001-90565295.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_50109808-139452659.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_47870369-124121200.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_61967064-61976104.vcf.gz --INPUT AA0229-01-04a.mutect2.chr6_95070791-167591393.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_143184588-223558935.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_68220553-134183092.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_23276875-84270066.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_18408107-86202979.vcf.gz --INPUT AA0229-01-04a.mutect2.chr6_60001-58453888.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_240243-58119653.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_32917626-89330679.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_85714223-145078636.vcf.gz --INPUT AA0229-01-04a.mutect2.chr17_26820267-26859724.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_60001-50821348.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_62462543-114281198.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_46380683-90228345.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_88002897-135076622.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_10001-41225986.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_45927266-85664222.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_2132995-37099262.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_10001-38529907.vcf.gz --INPUT AA0229-01-04a.mutect2.chr6_61371373-95020790.vcf.gz --INPUT AA0229-01-04a.mutect2.chr19_27240875-58607616.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_10269869-10274327.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_18339130-18433513.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_12284346-43983744.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_17580549-46435900.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_36314720-64334167.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_7084651-34719407.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_63841-26364240.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_20571467-20830724.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_54537529-80263285.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_155761325-181478259.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_86252980-111703855.vcf.gz --INPUT AA0229-01-04a.mutect2.chr19_60001-24448980.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_120929382-144425606.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_8816478-9272916.vcf.gz --INPUT AA0229-01-04a.mutect2.chr17_491112-21795850.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_228608365-248946422.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_10001-18436486.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_84320067-101981189.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_10001-17530548.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_71055697-87978202.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_16146120-32867130.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_32839017-49336924.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_54525075-70955696.vcf.gz --INPUT AA0229-01-04a.mutect2.chr5_139453660-155760324.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_10001-16145119.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_143700805-159335973.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_10001-15410899.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_18486487-33214595.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_16849164-29552233.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_37285838-49348394.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_144475607-156030895.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_2746291-12954384.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_124121503-133690466.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_50278965-58555579.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_11642903-11647442.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_60001-7617127.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_47019913-54536574.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_1441553-8797477.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_51793952-58878793.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_10001-7083650.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_2781480-9046914.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_41693522-41916265.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_30811899-31001508.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_223608936-228558364.vcf.gz --INPUT AA0229-01-04a.mutect2.chr18_15791048-20564714.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_7667128-12234345.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_21805282-26673214.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_116595567-120879381.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_134185537-138334717.vcf.gz --INPUT AA0229-01-04a.mutect2.chr17_22813680-26627010.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_58605580-62412542.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_13004385-16799163.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_43262463-46699983.vcf.gz --INPUT AA0229-01-04a.mutect2.chr11_51078349-54425074.vcf.gz --INPUT AA0229-01-04a.mutect2.chr1_10001-207666.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_94140558-94293015.vcf.gz --INPUT AA0229-01-04a.mutect2.chr12_34829238-37185252.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_43282957-43332174.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_49708101-51743951.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_36260629-36261158.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_16302844-16304296.vcf.gz --INPUT AA0229-01-04a.mutect2.chr10_39686683-41593521.vcf.gz --INPUT AA0229-01-04a.mutect2.chr2_92188146-94090557.vcf.gz --INPUT AA0229-01-04a.mutect2.chr8_44033745-45877265.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_26596364-28499358.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_34339330-34521510.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_111843442-113673020.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_222347-1949345.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_20257794-21739542.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_66591388-67920552.vcf.gz --INPUT AA0229-01-04a.mutect2.chr14_16054460-16061677.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_12488691-12641730.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_65048125-65080082.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_10316945-10544039.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_8756716-8886604.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_61518809-61735368.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_21242091-21778502.vcf.gz --INPUT AA0229-01-04a.mutect2.chr4_31832570-32833016.vcf.gz --INPUT AA0229-01-04a.mutect2.chr7_60878235-61327788.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_62798833-62958371.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_63968448-64135013.vcf.gz --INPUT AA0229-01-04a.mutect2.chr15_22358243-23226874.vcf.gz --INPUT AA0229-01-04a.mutect2.chr16_33442412-34289329.vcf.gz --INPUT AA0229-01-04a.mutect2.chrY_9453714-10266944.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_29271827-29315342.vcf.gz --INPUT AA0229-01-04a.mutect2.chr9_60518559-60688432.vcf.gz --INPUT AA0229-01-04a.mutect2.chr22_11428057-11497337.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_7743701-7865746.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_28504765-28751119.vcf.gz --INPUT AA0229-01-04a.mutect2.chr21_6789086-6934219.vcf.gz --INPUT AA0229-01-04a.mutect2.chrX_49528395-50228964.vcf.gz --INPUT AA0229-01-04a.mutect2.chr13_113723021-114354328.vcf.gz --INPUT AA0229-01-04a.mutect2.chr20_30088349-30425128.vcf.gz --OUTPUT AA0229-01-04a.mutect2.vcf.gz --SEQUENCE_DICTIONARY Homo_sapiens_assembly38.dict --TMP_DIR . --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false
[Fri Dec 09 00:42:32 GMT 2022] Executing as munro.j@sml-n03.hpc.wehi.edu.au on Linux 3.10.0-1160.42.2.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 11.0.15-internal+0-adhoc..src; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.3.0.0
[Fri Dec 09 00:42:33 GMT 2022] picard.vcf.MergeVcfs done. Elapsed time: 0.01 minutes.
Runtime.totalMemory()=2109734912
To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp
java.lang.IllegalArgumentException: Input path /vast/scratch/users/munro.j/runs/sarek_wgs_22.12.0/work/a6/89321e488babf2bf8ff7ddbe598a1d/AA0229-01-04a.mutect2.chr2_97489619-238903659.vcf.gz has sample entries that don't match the other files.
        at picard.vcf.MergeVcfs.doWork(MergeVcfs.java:207)
        at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:309)
        at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37)
        at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
        at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
        at org.broadinstitute.hellbender.Main.main(Main.java:289)
```

This issue seems to be that first input VCF is from the wrong sample. This error occurs for multiple samples across multiple runs.

Testing has revealed the issue seems to arise at: https://github.com/nf-core/sarek/blob/0b7f0e2b6f0be7053838ba1884b2eb03e9f83f9e/subworkflows/local/bam_variant_calling_tumor_only_mutect2/main.nf#L64-L78
Testing code:
```nextflow
MERGE_MUTECT2(
        mutect2_vcf_branch.intervals
        .map{ meta, vcf ->
            assert vcf.name.contains(meta.id)     // #1 pass
            assert vcf.name.contains(meta.sample) // #2 pass
            new_meta = [
                        id:             meta.sample,
                        num_intervals:  meta.num_intervals,
                        patient:        meta.patient,
                        sample:         meta.sample,
                        sex:            meta.sex,
                        status:         meta.status,
                    ]
            assert vcf.name.contains(new_meta.id) // #3 fail
            [groupKey(new_meta, meta.num_intervals), vcf]
        }.groupTuple()
        dict)
```
where the third assertion fails for some samples.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

Nextflow version 22.10.1, HPC, SLURM, Singularity, CentOS, nf-core/sarek v3.1.1",jemunro,https://github.com/nf-core/sarek/issues/887
I_kwDOCvwIC85Y1pDi,json schema does not validate --tools and --skip_tools,CLOSED,2022-12-11T21:21:48Z,2023-02-16T16:05:08Z,2023-02-16T16:05:08Z,"### Description of the bug

```
nextflow run nf-core/sarek --profile test,docker --tools strelka2
```
should fail, but isn't

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/892
I_kwDOCvwIC85Y8hD1,add even more samplesheet checks,CLOSED,2022-12-12T16:20:23Z,2023-05-29T11:47:21Z,2023-05-29T11:47:20Z,"### Description of feature

Check the other possibility, but we should at least test for:

- [ ] bam field does countain bam file(s)
- [ ] cram field does countain cram file(s)",maxulysse,https://github.com/nf-core/sarek/issues/895
I_kwDOCvwIC85ZogEW,CalculateContamination table needs to be sorted,OPEN,2022-12-19T23:53:06Z,2024-08-19T13:12:33Z,,"Hi,

When I run following pipeline:

```
nextflow run /public/slst/home/wutao2/nf-core-sarek-3.1.1/workflow/ --input /public/slst/home/wutao2/sun_data/nf_pipeline/KAPA/Non_PDX/non_pdx_kapa.csv --outdir /public/slst/home/wutao2/sun_data/nf_pipeline/KAPA/Non_PDX/output --genome GATK.GRCh38 -profile singularity --wes --tools mutect2,merge --only_paired_variant_calling --max_cpus 36 --max_memory '128.GB' --save_mapped --save_output_as_bam --igenomes_base /public/slst/home/wutao2/aws-igenomes/igenomes --intervals /public/slst/home/wutao2/sun_data/nf_pipeline/KAPA/KAPA_target.bed 
```

It shows `CalculateContamination Error` :

```
-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:CALCULATECONTAMINATION (H208323_vs_H208344)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:CALCULATECONTAMINATION (H208323_vs_H208344)` terminated with an error exit status (3)

Command executed:

  gatk --java-options ""-Xmx12g"" CalculateContamination \
      --input H208323.mutect2.pileupsummaries.table \
      --output H208323_vs_H208344.mutect2.contamination.table \
      --matched-normal H208344.mutect2.pileupsummaries.table \
      --tmp-dir . \
      -tumor-segmentation H208323_vs_H208344.mutect2.segmentation.table
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:CALCULATECONTAMINATION"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  3

Command output:
  (empty)

Command error:
  14:45:39.575 INFO  CalculateContamination - Shutting down engine
  [December 19, 2022 at 2:45:39 PM GMT] org.broadinstitute.hellbender.tools.walkers.cr of points needed to calculate local changepoint costs (2 * window size = 100) exceeds number of data points (11).  Local changepoint costs will not be calculated for this window size.
  14:45:38.169 WARN  KernelSegmenter - No changepoint candidates were found.  The specified window sizes may be inappropriate, or there may be insufficient data points.
  14:45:38.170 INFO  KernelSegmenter - Found 0 changepoints after applying the changepoint penalty.
  14:45:38.533 INFO  KernelSegmenter - Found 0 changepoints after applying the changepoint penalty.
  14:45:38.659 INFO  KernelSegmenter - Found 0 changepoints after applying the changepoint penalty.
  14:45:38.811 INFO  KernelSegmenter - Found 2 changepoints after applying the changepoint penalty.
  14:45:38.840 WARN  KernelSegmenter - Specified dimension of the kernel approximation (100) exceeds the number of data points (91) to segment; using all data points to calculate kernel matrix.
  14:45:39.008 WARN  KernelSegmenter - Number of points needed to calculate local changepoint costs (2 * window size = 100) exceeds number of data points (91).  Local changepoint costs will not be calculated for this window size.
  14:45:39.009 WARN  KernelSegmenter - No changepoint candidates were found.  The specified window sizes may be inappropriate, or there may be insufficient data points.
  14:45:39.010 INFO  KernelSegmenter - Found 0 changepoints after applying the changepoint penalty.
  14:45:39.117 INFO  KernelSegmenter - Found 2 changepoints after applying the changepoint penalty.
  14:45:39.239 INFO  KernelSegmenter - Found 0 changepoints after applying the changepoint penalty.
  14:45:39.367 INFO  KernelSegmenter - Found 0 changepoints after applying the changepoint penalty.
  14:45:39.479 INFO  KernelSegmenter - Found 0 changepoints after applying the changepoint penalty.
  14:45:39.574 INFO  KernelSegmenter - Found 0 changepoints after applying the changepoint penalty.
  14:45:39.575 INFO  CalculateContamination - Shutting down engine
  [December 19, 2022 at 2:45:39 PM GMT] org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination done. Elapsed time: 0.07 minutes.
  Runtime.totalMemory()=2147483648
  java.lang.IllegalArgumentException: Invalid interval. Contig:chr22 start:41093206 end:40819179
  	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:804)
  	at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59)
  	at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:35)
  	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationSegmenter.lambda$findContigSegments$4(ContaminationSegmenter.java:71)
  	at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180)
  	at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104)
  	at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699)
  	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
  	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
  	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
  	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
  	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)
  	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationSegmenter.findContigSegments(ContaminationSegmenter.java:72)
  	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationSegmenter.lambda$findSegments$1(ContaminationSegmenter.java:43)
  	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:271)
  	at java.base/java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1693)
  	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
  	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
  	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
  	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
  	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)
  	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationSegmenter.findSegments(ContaminationSegmenter.java:45)
  	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.<init>(ContaminationModel.java:61)
  	at org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination.doWork(CalculateContamination.java:124)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211)
  	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
  	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
  	at org.broadinstitute.hellbender.Main.main(Main.java:289)

Work dir:
  /slst/home/wutao2/work/e8/82be9a8b520189393b3406eb9dc6f3

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`
```

The whole output log can be found in here : [run_netflow_log.zip](https://github.com/nf-core/sarek/files/10263532/run_netflow_log.zip).

But when I checked the dir `/slst/home/wutao2/work/e8/82be9a8b520189393b3406eb9dc6f3`:

```
(base) -bash-4.2$ ls -a
.   .command.begin  .command.log  .command.run  .command.trace  H208323.mutect2.pileupsummaries.table
..  .command.err    .command.out  .command.sh   .exitcode       H208344.mutect2.pileupsummaries.table
```

The `.command.err` file shows some warnings but no errors, the files in this dir can be found in here :  [command.zip](https://github.com/nf-core/sarek/files/10263550/command.zip). 

How could I fix this problem ? Thank you.

Tao Wu
",wt12318,https://github.com/nf-core/sarek/issues/899
I_kwDOCvwIC85Z4nCx,tabix fails for large chromosome sizes,OPEN,2022-12-22T14:43:29Z,2023-03-21T13:51:04Z,,"### Description of the bug

When running sarek using a large genome with chromosome sizes larger than 2^29, the pipeline fails on the `NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT` step

```
  bgzip  --threads 1 -c  Lcu.2RBY.Chr1_1-538362633.bed > Lcu.2RBY.Chr1_1-538362633.bed.gz
  tabix  Lcu.2RBY.Chr1_1-538362633.bed.gz
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT"":
      tabix: $(echo $(tabix -h 2>&1) | sed 's/^.*Version: //; s/ .*$//')
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  [E::hts_idx_check_range] Region 0..538362633 cannot be stored in a tbi index. Try using a csi index
  tbx_index_build failed: Lcu.2RBY.Chr1_1-538362633.bed.gz
```

It turns out the current `tabix` command doesn't deal with chromosomes larger than 2^29 bp. A possible fix is to run `tabic` with the ` -C` argument producing a `csi` instead of a `tbi` index file.
However the dependency on tbi files seems [hardcoded](https://github.com/nf-core/sarek/blob/96749f742197e828f4632cc2a7481190e7f642ac/subworkflows/local/prepare_intervals/main.nf#L95), so the pipeline would need adapting to use csi instead of tbi files I suppose.


### Command used and terminal output

```console
nf-core/sarek v3.1.1
```


### Relevant files

_No response_

### System information

nextflow version 22.10.4.5836
nf-core/sarek v3.1.1
container: docker
executor: local
",seb-mueller,https://github.com/nf-core/sarek/issues/900
I_kwDOCvwIC85aECEn,configManta.py ignores `crai` index file path,CLOSED,2022-12-26T14:37:31Z,2023-03-21T13:50:22Z,2023-03-21T13:50:22Z,"### Description of the bug

Hi, I just hit a problem that the `crai` index file path does not get passed to the `configManta.sh` script:
```bash
# cat .command.sh 
#!/bin/bash -euo pipefail
configManta.py         --bam GTEX-11EM3-0004-SM-6WBUK.cram         --reference Homo_sapiens_assembly38.fasta         --runDir manta         --callRegions chr18_20571467-20830724.bed.gz         
```

My files are named:
- `GTEX-11EM3-0004-SM-6WBUK.cram`
- `GTEX-11EM3-0004-SM-6WBUK.crai`
and I denoted this in the sample annotation.

 However, this information is not passed on to configManta.sh and therefore this step fails without properly named `.cram.crai` files.

### Command used and terminal output

```console
Setup any sample annotation with a non-standard index file name.
```


### Relevant files

_No response_

### System information

- nextflow version 22.04.5.5708
- Slurm cluster with Conda engine
- RockyLinux 8
- Sarek 3.1.1",Hoeze,https://github.com/nf-core/sarek/issues/901
I_kwDOCvwIC85aNZOr,Reference Allele Error on dbSNP on step HaplotypeCaller CNN1,OPEN,2022-12-29T07:24:41Z,2024-02-09T23:42:24Z,,"### Description of the bug

Hi,
This is about the GATK resource bundle but I am opening a bug report here because I encounter this while using this pipeline. 
dbSNP138 has the wrong reference allele at position chr6:31236715.


![image](https://user-images.githubusercontent.com/43095157/209917559-fb0d1060-8a48-4f48-8ff9-2ba5b984d831.png)


### Command used and terminal output

```console
Command was:
nextflow run nf-core/sarek -profile docker --input WES_samplesheet.csv --outdir WES_output/ --genome GATK.GRCh37 --intervals GRCh37_exome_all_.bed --wes --save_reference --tools deepvariant,haplotypecaller --max_cpus 3 --max_memory 20.GB -with-trace --skip_tools baserecalibrator -resume
Terminal output looks like this:
-[nf-core/sarek] Pipeline completed with errors-
             Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES (mysample)'
 Caused by:
      Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES (mysample)` terminated with an error exit status (2)
      Command executed:
      gatk --java-options ""-Xmx12g"" FilterVariantTranches \
      --variant mysample.cnn.vcf.gz \
      --resource dbsnp_138.b37.vcf.gz --resource 1000G_phase1.indels.b37.vcf.gz --resource Mills_and_1000G_gold_standard.indels.b37.vcf.gz --resource 1000G_phase1.snps.high_confidence.b37.vcf.gz \
      --output mysample.haplotypecaller.filtered.vcf.gz \
      --tmp-dir . \
          --info-key CNN_1D
      cat <<-END_VERSIONS > versions.yml
        ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES"":
  gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
     END_VERSIONS
 
.
.
.
o retrieve a sequence dictionary from the associated index file
        [95/1923]  16:07:11.173 WARN  IntelInflater - Zero Bytes Written : 0
    16:07:11.184 INFO  FilterVariantTranches - Done initializing engine
             16:07:11.294 INFO  ProgressMeter - Starting traversal
        16:07:11.295 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute
       16:07:11.296 INFO  FilterVariantTranches - Starting pass 0 through the variants
 16:07:24.653 INFO  ProgressMeter -          1:150597890              0.2                  3000          13476.1
       16:07:39.146 INFO  ProgressMeter -           2:68402012              0.5                  6000          12926.4
       16:07:54.029 INFO  ProgressMeter -           3:44283525              0.7                  9000          12636.6
       16:08:04.120 INFO  ProgressMeter -             4:762963              0.9                 11000          12494.3
       16:08:15.045 INFO  ProgressMeter -            5:1216775              1.1                 13000          12235.5
       16:08:25.728 INFO  ProgressMeter -          5:180235722              1.2                 15000          12091.4
       16:08:28.560 INFO  FilterVariantTranches - Filtered 0 SNPs out of 14542 and filtered 0 indels out of 1233 with INFO score: CNN_1D.
       16:08:28.563 INFO  FilterVariantTranches - Shutting down engine
                 [December 20, 2022 at 4:08:28 PM GMT] org.broadinstitute.hellbender.tools.walkers.vqsr.FilterVariantTranches done. Elapsed time: 1.32 minutes.   Runtime.totalMemory()=2260729856
          ***********************************************************************
             A USER ERROR has occurred: Bad input: The provided variant file(s) have inconsistent references for the same position(s) at 6:31236715, GC* in input vs. GA* in resource
               ***********************************************************************
         Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.
               Work dir:
                /home/owiepoc/work/37/f047461c745cbc3a799066d76ae620
    Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`

```


### Relevant files

_No response_

### System information

nextflow version 22.10.3.5834
Container engine docker
Local executor
OS: Ubuntu 20.04.5 LTS x86_64
Host: VMware Virtual Platform None
Kernel: 5.4.0-135-generic
Uptime: 22 days, 15 mins
Packages: 775 (dpkg), 6 (snap)
Shell: bash 5.0.17
Resolution: preferred
Terminal: /dev/pts/0
CPU: Intel Xeon Platinum 8360Y (20) @ 2.394GHz
GPU: 00:0f.0 VMware SVGA II Adapter
Memory: 3377MiB / 140829MiB",barslmn,https://github.com/nf-core/sarek/issues/902
I_kwDOCvwIC85air0L,VEP tab output does not retain FILTER info,CLOSED,2023-01-04T14:08:03Z,2023-03-21T12:55:08Z,2023-03-21T12:55:08Z,"### Description of the bug

When I export the variants as table with the option ` --vep_out_format 'tab'` I have no trace of the variants marked as ""PASS"" in the final table.
I think it would make sense to annotate only variants that passed the filter or retain the ""PASS"" status in the final table",ftucos,https://github.com/nf-core/sarek/issues/903
I_kwDOCvwIC85aiuIQ,Filter out variants in  homopolymeric regions,CLOSED,2023-01-04T14:13:47Z,2024-09-05T14:13:46Z,2024-09-05T14:13:46Z,"### Description of feature

In my experience, filtering out homopolymeric variants significantly contributed to removing artifactual variants.
I think that implementing VCFPolyX from [jvarkit](http://lindenb.github.io/jvarkit/) would be a nice feature.",ftucos,https://github.com/nf-core/sarek/issues/904
I_kwDOCvwIC85bIYTr,Support for Illumina ORA format,OPEN,2023-01-11T12:08:25Z,2024-08-19T13:12:33Z,,"### Description of feature

Hi,

Illumina has introduced a new read compression format, ORA: https://www.illumina.com/science/genomics-research/articles/design-ora-lossless-genomic-compression.html

ORA compresses human read data by 80% compared to traditional fastq.gz - I suspect it will become a commonly used option for data rolling off the upcoming NovaSeq X and NextSeq 1500 instruments (on-board support for ORA compression).

ORA is lossless and can be converted, or better yet streamed, into fastq.gz - which requires a reference and small command line utility - see: https://emea.support.illumina.com/sequencing/sequencing_software/DRAGENORA.html

For example, to stream ORA-compressed paired-end read data to bwa, you could do:

```
bwa mem humanref.fasta <(orad file.fastq.ora -c --raw --ora-reference /path/to/ora-reference ) > resu.sam
```

Would be nice to see support for this make it into Sarek.
",marchoeppner,https://github.com/nf-core/sarek/issues/907
I_kwDOCvwIC85bIcNI,Add option for outputting gvcfs instead of vcfs,OPEN,2023-01-11T12:19:42Z,2023-10-10T13:40:27Z,,"### Description of feature

It might be useful to add a CLI-option for outputting gvcf-files instead of vcf-files. It seems that with the current version that is only possible when running `--joint_germline`. (However, I could be wrong).",asp8200,https://github.com/nf-core/sarek/issues/908
I_kwDOCvwIC85bZnBh,Can not start with annotate,CLOSED,2023-01-14T19:22:24Z,2023-01-17T13:35:44Z,2023-01-17T13:35:44Z,"### Description of the bug

I can not start with 'annote' when I inputing vcf files, it only runs MultiQC, but vep can not start.....

### Command used and terminal output

```console
$nextflow run /sw/bioinfo/nf-core-pipelines/latest/bianca/sarek/3.1.2/workflow \
--input ""/home/kangwang/nf-core/nf-core-sarek-3.1.2/Sarek-data/variantcalled.csv"" \
-profile uppmax \
--wes true \
--project sens2022005 \
--genome GRCh38 \
--fasta ""/sw/data/uppnex/ToolBox/hg38bundle/Homo_sapiens_assembly38.fasta"" \
--fasta_fai ""/sw/data/uppnex/ToolBox/hg38bundle/Homo_sapiens_assembly38.fasta.fai"" \
--dict ""/sw/data/uppnex/ToolBox/hg38bundle/Homo_sapiens_assembly38.dict"" \
--step 'annotate' \
--tools 'vep' \
--vep_species ""homo_sapiens""
```


### Relevant files

_No response_

### System information

_No response_",WangKang-Leo,https://github.com/nf-core/sarek/issues/910
I_kwDOCvwIC85bdqxX,Add `GATK4_CALIBRATEDRAGSTRMODEL`,OPEN,2023-01-16T08:28:26Z,2023-01-16T08:28:26Z,,"### Description of feature

The `GATK4_HAPLOTYPECALLER` module can take Dragstr models as input. Is it possible to also add this module to the `bam_variant_calling_haplotypecaller` subworkflow as an optional process?
Thanks!",nvnieuwk,https://github.com/nf-core/sarek/issues/911
I_kwDOCvwIC85bfQKu,Unknown config attribute `params.test_data_base` -- check config file,CLOSED,2023-01-16T13:12:40Z,2023-01-16T13:38:27Z,2023-01-16T13:37:53Z,"### Description of the bug

Hi, 
I test the pipeline with 
nextflow run nf-core/sarek -profile test,docker --outdir outdir_test

got error:
N E X T F L O W  ~  version 22.10.4
Unknown config attribute `params.test_data_base` -- check config file: /home/xxx/.nextflow/assets/nf-core/sarek/nextflow.config

### Command used and terminal output

```console
nextflow run nf-core/sarek -profile test,docker --outdir outdir_test


N E X T F L O W  ~  version 22.10.4
Unknown config attribute `params.test_data_base` -- check config file: /home/xxx/.nextflow/assets/nf-core/sarek/nextflow.config
```


### Relevant files

_No response_

### System information

Ubuntu 22.04.1 LTS

N E X T F L O W  ~  version 22.10.4

Docker version 20.10.17",aihualin,https://github.com/nf-core/sarek/issues/912
I_kwDOCvwIC85bpgh6,cpu requirement,CLOSED,2023-01-18T08:41:27Z,2023-01-18T09:33:23Z,2023-01-18T09:33:23Z,"### Description of the bug

Hi, I run a sample test on my laptop. Is number of cpu a hard requirement?  

I got error:

Execution cancelled -- Finishing pending tasks before exit
-[nf-core/sarek] Pipeline completed with errors-
WARN: There's no process matching config selector: NFCORE_SAREK:SAREK:CRAM_QC_NO_MD:SAMTOOLS_STATS -- Did you mean: NFCORE_SAREK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS?
Error executing process > 'NFCORE_SAREK:SAREK:FASTP (tumor_sample-lane_1)'

Caused by:
  Process requirement exceeds available CPUs -- req: 12; avail: 8


### Command used and terminal output

```console
nextflow run nf-core/sarek --input ${dir_p}/sarek/samplesheet.csv --outdir ${dir_p}/sarek/output/test --genome GATK.GRCh37  -profile docker
```


### Relevant files

_No response_

### System information

_No response_",aihualin,https://github.com/nf-core/sarek/issues/913
I_kwDOCvwIC85bpx90,"targeted sequencing data, point mutations",CLOSED,2023-01-18T09:34:15Z,2023-03-22T08:20:27Z,2023-03-22T08:17:22Z,"### Description of feature

Hi, we have targeted sequencing data and only interested in two genes, ABL1 and BCR.

I wonder how to set up the running of the pipeline, for example, what correction should i change in the following command:

```
nextflow run nf-core/sarek --input ${dir_p}/sarek/samplesheet.csv --outdir ${dir_p}/sarek/output/test  --genome GATK.GRCh37 -profile docker --max_cpus 6
```

Thank you!

EDIT by @maxulysse: markdown console",aihualin,https://github.com/nf-core/sarek/issues/914
I_kwDOCvwIC85bqHfQ,running  does not continue,CLOSED,2023-01-18T10:36:02Z,2023-01-18T13:22:09Z,2023-01-18T13:22:09Z,"### Description of the bug

Hi, I set max_cpus to 6, and run the following command,

the running just hang forever by end of the following step and no update in the  output folder. 
what might be the reason?




### Command used and terminal output

```console
nextflow run nf-core/sarek --input ${dir_p}/sarek/samplesheet.csv --outdir ${dir_p}/sarek/output/test --genome GATK.GRCh37  -profile docker --max_cpus 6

[8d/0dc69b] process > NFCORE_SAREK:SAREK:FASTQC (tumor_sample-lane_1)                                            [100%] 1 of 1 ✔
[ca/bf013c] process > NFCORE_SAREK:SAREK:FASTP (tumor_sample-lane_1)                                             [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP:BWAMEM1_MEM                             -
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP:BWAMEM2_MEM                             -
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP:DRAGMAP_ALIGN                           -
[-        ] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES                                 -
[-        ] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:INDEX_MARKDUPLICATES                                 -
[-        ] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS             -
[-        ] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH                   -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM                                                             -
[-        ] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR                             -
[-        ] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS                            -
[-        ] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR                                           -
[-        ] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:MERGE_CRAM                      -
[-        ] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM                      -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS                                            -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:MOSDEPTH                                                  -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM_RECAL                                                       -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                                             -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC                                                                 -
```


### Relevant files

_No response_

### System information

_No response_",aihualin,https://github.com/nf-core/sarek/issues/915
I_kwDOCvwIC85caecz,Mutect2 multi-sample mode,CLOSED,2023-01-20T07:57:41Z,2023-07-05T11:21:36Z,2023-07-05T11:21:36Z,"### Description of feature

Hi,
It would be great if we could run Mutect2 in multi-sample mode for increased sensitivity and better concordance among samples. ~~For reference, there is a WDL implementation here:
https://github.com/broadinstitute/gatk/tree/master/scripts/mutect2_wdl#mutect2_multi_sample~~

It turns out that the linked WDL above was performing tumor/normal somatic variant calling for multiple pairs separately. What I mean is better described in discussions linked below:
https://gatk.broadinstitute.org/hc/en-us/community/posts/360071839192-Mutect2-multi-sample-pipeline
https://gatk.broadinstitute.org/hc/en-us/articles/360035894731-Somatic-short-variant-discovery-SNVs-Indels-

This comment summarises the steps:
https://gatk.broadinstitute.org/hc/en-us/community/posts/360071839192/comments/360012234432

It seems like GATK team hasn't implemented/provided a Mutect2 pipeline performing multi-sample variant calling yet. Here is a quick and dirty implementation that I did for a project back in the day:
https://github.com/berguner/variant_calling_pipeline/blob/master/mutect2_multiple_tumor.wdl",berguner,https://github.com/nf-core/sarek/issues/916
I_kwDOCvwIC85casBu,Pipeline completed with errors,CLOSED,2023-01-20T08:50:45Z,2023-02-01T11:03:38Z,2023-02-01T11:03:38Z,"I did analysis for target sequencing, with a .bed file. 

In the output folder, there is no variant_calling, What does this mean?
only folders: csv  multiqc  pipeline_info  preprocessing  reports
This was done without using the parameter --tools

then i tried --tools 'strelka,mutect2'. the anslysis stopped with error:
**The sample-sheet only contains normal-samples, but the following tools, which were requested with ""--tools"", expect at least one tumor-sample : mutect2**

My samplesheet.csv
patient,sample,lane,fastq_1,fastq_2
MP1708655B_S15,tumor_sample,lane_1,/dir/MP1708655B_S15_L001_R1_001.fastq.gz,/dir/MP1708655B_S15_L001_R2_001.fastq.gz


All the best
Aihua
",aihualin,https://github.com/nf-core/sarek/issues/917
I_kwDOCvwIC85cg5Hg,joint germline vcftools error,OPEN,2023-01-22T13:51:37Z,2024-06-11T09:55:48Z,,"### Description of the bug

Joint germline genotyping completes actual genotyping, but fails at TSV count vcftools step

### Command used and terminal output


Command:

nextflow run nf-core/sarek --skip_tools baserecalibrator --genome null --igenomes_ignore --joint_germline --intervals Ssal_v3.1_genomic.chroms.bed  --fasta Ssal_v3.1_genomic.chroms.fna --input salmo5samp.csv -profile docker --tools haplotypecaller,manta -resume

Error:

Error executing process > 'NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT (joint_variant_calling)'

Caused by:
  Process `NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT (joint_variant_calling)` terminated with an error exit status (139)

Command executed:

  vcftools \
      --gzvcf joint_germline.vcf.gz \
      --out joint_germline \
      --TsTv-by-count \
       \
  
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT"":
      vcftools: $(echo $(vcftools --version 2>&1) | sed 's/^.*VCFtools (//;s/).*//')
  END_VERSIONS

Command exit status:
  139

Command output:
  (empty)

Command error:
  
  VCFtools - 0.1.16
  (C) Adam Auton and Anthony Marcketta 2009
  
  Parameters as interpreted:
        --gzvcf joint_germline.vcf.gz
        --out joint_germline
        --TsTv-by-count
  
  Using zlib version: 1.2.11
  Warning: Expected at least 2 parts in FORMAT entry: ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, 
describing how the alternate alleles are phased in relation to one another; will always be heterozygous and is not intended to describe called alleles"">
  Warning: Expected at least 2 parts in FORMAT entry: ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">
  Warning: Expected at least 2 parts in FORMAT entry: ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">
  Warning: Expected at least 2 parts in FORMAT entry: ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">
  Warning: Expected at least 2 parts in INFO entry: ID=AC,Number=A,Type=Integer,Description=""Allele count in genotypes, for each ALT allele, in the same order as listed"">
  Warning: Expected at least 2 parts in INFO entry: ID=AC,Number=A,Type=Integer,Description=""Allele count in genotypes, for each ALT allele, in the same order as listed"">
  Warning: Expected at least 2 parts in INFO entry: ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">
  Warning: Expected at least 2 parts in INFO entry: ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">
  Warning: Expected at least 2 parts in INFO entry: ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">
  Warning: Expected at least 2 parts in INFO entry: ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">
  Warning: Expected at least 2 parts in INFO entry: ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for 
the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">
  Warning: Expected at least 2 parts in INFO entry: ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for 
the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">
  After filtering, kept 5 out of 5 Individuals
  Outputting Ts/Tv by Alternative Allele Count
  After filtering, kept 9896941 out of a possible 9896941 Sites
  Run Time = 49.00 seconds
  .command.sh: line 7:    27 Segmentation fault      (core dumped) vcftools --gzvcf joint_germline.vcf.gz --out joint_germline --TsTv-by-count

Work dir:
  /genomics/Tony/Atlantic_Salmon/work/8d/0c7967703f8969b8e8948f2bf3fd38

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line
```
```
",TonyKess,https://github.com/nf-core/sarek/issues/918
I_kwDOCvwIC85ci_-A,Pipeline completed with errors,CLOSED,2023-01-23T08:14:28Z,2023-02-01T10:31:01Z,2023-02-01T10:31:01Z,"### Description of the bug

Pipeline completed with errors

### Command used and terminal output

```console
nextflow run nf-core/sarek --input ${dir_p}/sarek/samplesheet.csv --intervals ${dir_p}/sarek/test.bed --tools 'strelka,mutect2,manta,vep' --outdir ${dir_p}/sarek/output/test --genome GATK.GRCh37  -profile docker --max_cpus 6 --max_memory '10.GB'

Pipeline completed with errors-
WARN: Unable to get file attributes file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/af-only-gnomad.raw.sites.vcf.gz -- Cause: com.amazonaws.SdkClientException: Unable to execute HTTP request: Network is unreachable
WARN: Unable to get file attributes file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/af-only-gnomad.raw.sites.vcf.gz.tbi -- Cause: com.amazonaws.SdkClientException: Unable to execute HTTP request: Network is unreachable
WARN: Unable to get file attributes file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Sequence/WholeGenomeFasta/human_g1k_v37_decoy.dict -- Cause: com.amazonaws.SdkClientException: Unable to execute HTTP request: Network is unreachable
WARN: Unable to get file attributes file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/af-only-gnomad.raw.sites.vcf.gz -- Cause: com.amazonaws.SdkClientException: Unable to execute HTTP request: Network is unreachable
WARN: Unable to get file attributes file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/af-only-gnomad.raw.sites.vcf.gz.tbi -- Cause: com.amazonaws.SdkClientException: Unable to execute HTTP request: Network is unreachable
WARN: Unable to get file attributes file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/af-only-gnomad.raw.sites.vcf.gz -- Cause: com.amazonaws.SdkClientException: Unable to execute HTTP request: Network is unreachable
WARN: Unable to get file attributes file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/af-only-gnomad.raw.sites.vcf.gz.tbi -- Cause: com.amazonaws.SdkClientException: Unable to execute HTTP request: Network is unreachable
```


### Relevant files

_No response_

### System information

_No response_",aihualin,https://github.com/nf-core/sarek/issues/919
I_kwDOCvwIC85cmKAK,Workflow execution completed unsuccessfully! due to CNVkit error ,CLOSED,2023-01-23T17:46:19Z,2023-06-05T07:11:23Z,2023-06-05T07:11:23Z,"### Description of the bug

Workflow execution completed unsuccessfully!
The exit status of the task that caused the workflow execution to fail was: 1.

The full error message was:

```
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_CNVKIT:CNVKIT_BATCH (Tumour_17_vs_Normal_17)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_CNVKIT:CNVKIT_BATCH (Tumour_17_vs_Normal_17)` terminated with an error exit status (1)

Command executed:

  samtools view -T Homo_sapiens_assembly38.fasta --fai-reference Homo_sapiens_assembly38.fasta.fai Tumour_17.recal.cram -@ 2 -o Tumour_17.recal.bam
  samtools view -T Homo_sapiens_assembly38.fasta --fai-reference Homo_sapiens_assembly38.fasta.fai Normal_17.recal.cram -@ 2 -o Normal_17.recal.bam
  
  cnvkit.py \
      batch \
      Tumour_17.recal.bam \
      --normal Normal_17.recal.bam \
      --fasta Homo_sapiens_assembly38.fasta \
       \
      --targets target_sorted.unique.bed \
      --processes 2 \
      --method hybrid --diagram --scatter
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_CNVKIT:CNVKIT_BATCH"":
      samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
      cnvkit: $(cnvkit.py version | sed -e ""s/cnvkit v//g"")
  END_VERSIONS

Command exit status:
  1

Command output:
  
  ++ stat mem=29517 3 516464 350016 800812 631652 276395 358
  ++ stat mem=31450 0 5492 1608 5496 1608 4 1
  ++ stat SUM=0 3 521956 351624 806308 633260 276399 359
  ++ stat PEAK=0 4 680268 502784 830252 652956 911544 3077
  
  ++ stat mem=29517 2 474128 307576 800812 631652 315706 395
  ++ stat mem=31450 0 5492 1608 5496 1608 4 1
  ++ stat SUM=0 2 479620 309184 806308 633260 315710 396
  ++ stat PEAK=0 4 680268 502784 830252 652956 911544 3077
  
  ++ stat mem=29517 2 456656 287816 800812 631652 356130 456
  ++ stat mem=31450 0 5492 1608 5496 1608 4 1
  ++ stat SUM=0 2 462148 289424 806308 633260 356134 457
  ++ stat PEAK=0 4 680268 502784 830252 652956 911544 3077
  
  ++ stat mem=29517 2 455188 289764 800812 631652 396869 495
  ++ stat mem=31450 0 5492 1608 5496 1608 4 1
  ++ stat SUM=0 2 460680 291372 806308 633260 396873 496
  ++ stat PEAK=0 4 680268 502784 830252 652956 911544 3077
  
  ++ stat mem=29517 2 381552 216128 800812 631652 437476 530
  ++ stat mem=31450 0 5492 1608 5496 1608 4 1
  ++ stat SUM=0 2 387044 217736 806308 633260 437480 531
  ++ stat PEAK=0 4 680268 502784 830252 652956 911544 3077
  
  ++ stat mem=29517 1 354908 197508 800812 631652 477966 571
  ++ stat mem=31450 0 5492 1608 5496 1608 4 1
  ++ stat SUM=0 1 360400 199116 806308 633260 477970 572
  ++ stat PEAK=0 4 680268 502784 830252 652956 911544 3077
  
  ++ stat mem=4143 2 2366912 260944 2376880 270528 1798 42
  ++ stat mem=31450 0 5492 1608 5496 1608 5 1
  ++ stat SUM=0 2 2372404 262552 2382376 272136 1803 43
  ++ stat PEAK=0 4 2372404 502784 2382376 652956 911544 3077
  
  ++ stat mem=4143 2 2496980 260012 2562516 270528 1830 46
  ++ stat mem=4312 2 2365908 234812 2365908 234812 9 154
  ++ stat mem=4313 2 2365908 234812 2365908 234812 19 122
  ++ stat mem=31450 0 5492 1608 5496 1608 5 1
  ++ stat SUM=0 6 7234288 731244 7299828 741760 1863 323
  ++ stat PEAK=0 6 7234288 731244 7299828 741760 911544 3077
  
  ++ stat mem=4143 2 2496980 260012 2562516 270528 1830 46
  ++ stat mem=4312 2 2365908 234812 2365908 234812 11 299
  ++ stat mem=4313 2 2365908 234812 2365908 234812 21 259
  ++ stat mem=31450 0 5492 1608 5496 1608 5 1
  ++ stat SUM=0 6 7234288 731244 7299828 741760 1867 605
  ++ stat PEAK=0 6 7234288 731244 7299828 741760 911544 3077

Command error:
  WARNING: While bind mounting '/scratch/wsspaces/ahasan-VC_Sarek-0/db/a3e8338d2f0eef52ec3b1fab859404:/scratch/wsspaces/ahasan-VC_Sarek-0/db/a3e8338d2f0eef52ec3b1fab859404': destination is already in the mount point list
  /usr/local/lib/python3.10/site-packages/skgenome/intersect.py:11: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.
    from pandas import Int64Index
  CNVkit 0.9.9
  Detected file format: bed
  Splitting large targets
  Wrote ./target_sorted.unique.target.bed with 226032 regions
  /usr/local/lib/python3.10/site-packages/skgenome/intersect.py:39: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.
    other_chroms = {c: o for c, o in other.groupby(['chromosome'], sort=False)}
  /usr/local/lib/python3.10/site-packages/skgenome/intersect.py:40: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.
    for chrom, ctable in table.groupby(['chromosome'], sort=False):
  Wrote ./target_sorted.unique.antitarget.bed with 39266 regions
  Building a copy number reference from normal samples...
  [E::idx_find_and_load] [E::idx_find_and_load] Could not retrieve index file for 'Normal_17.recal.bam'Could not retrieve index file for 'Normal_17.recal.bam'
  
  Indexing BAM file Normal_17.recal.bam
  Indexing BAM file Normal_17.recal.bam
  Processing reads in Normal_17.recal.bam
  corrupted size vs. prev_size in fastbins
  Processing reads in Normal_17.recal.bam
  Traceback (most recent call last):
    File ""/usr/local/bin/cnvkit.py"", line 9, in 
      args.func(args)
    File ""/usr/local/lib/python3.10/site-packages/cnvlib/commands.py"", line 110, in _cmd_batch
      args.reference, args.targets, args.antitargets = batch.batch_make_reference(
    File ""/usr/local/lib/python3.10/site-packages/cnvlib/batch.py"", line 139, in batch_make_reference
      target_fnames = [tf.result() for tf in tgt_futures]
    File ""/usr/local/lib/python3.10/site-packages/cnvlib/batch.py"", line 139, in 
      target_fnames = [tf.result() for tf in tgt_futures]
    File ""/usr/local/lib/python3.10/concurrent/futures/_base.py"", line 451, in result
      return self.__get_result()
    File ""/usr/local/lib/python3.10/concurrent/futures/_base.py"", line 403, in __get_result
      raise self._exception
  concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.

Work dir:
  /scratch/wsspaces/ahasan-VC_Sarek-0/db/a3e8338d2f0eef52ec3b1fab859404

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`
```

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.1.2 -profile singularity -c nextflow.config --input Samplesheet1.csv -w /scratch/wsspaces/ahasan-VC_Sarek-0 --genome GATK.GRCh38 --wes --intervals /data/cpo/ahasan/VC_Sarek/target_sorted.unique.bed --tools cnvkit,freebayes,manta,mpileup,mutect2,tiddit,deepvariant,haplotypecaller,strelka,ascat,msisensorpro,vep,snpeff
```


### Relevant files
```
// Perform work directory cleanup after a successful run
cleanup = true

singularity {
    enabled = true
    autoMounts = true
    cacheDir = ""/data/cpo/ahasan/VC_Sarek/nf_core_singularity_cache/""
    NFX_SINGULARITY_CACHEDIR = ""/data/cpo/ahasan/VC_Sarek/singularity_cache/""
}

process {
    beforeScript =  """"""
        ml apps/singularity/3.8.0
        ml compilers/java/11.0.1
        ml apps/nextflow/21.10.5
        ml apps/python/3.8.6
        ml compilers/gcc/7.4.0
        """"""
    executor = 'pbs'
    queueStatInterval = '2min'
    pollInterval = '2min'

    // common SGE error statuses
    errorStrategy = {task.exitStatus in [143,137,104,134,139,140] ? 'retry' : 'finish'}
    maxErrors = '-1'
    maxRetries = 3
}

env {
  NXF_OPTS=""Xms1g -Xmx1g""
  NXF_DEBUG=""1""
}
```
### System information

nextflow/21.10.5

singularity/3.8.0",Ahas4,https://github.com/nf-core/sarek/issues/920
I_kwDOCvwIC85cx9pG,Control-FREEC *.png output files don't show text,CLOSED,2023-01-25T12:53:51Z,2024-08-19T13:16:10Z,2024-08-19T13:16:10Z,"### Description of the bug

CONTROLFREEC_MAKEGRAPH generates tree *.png files as expected but those plots don't contain text instead there are some white little boxes (see image attached).

Biocontainer used: quay.io/biocontainers/control-freec:11.6--h1b792b2_1

<img width=""727"" alt=""Screenshot 2023-01-25 at 13 13 26"" src=""https://user-images.githubusercontent.com/64437596/214567909-98ee9b29-62f4-426d-a418-98c2962355a0.png"">


### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

Sarek version 3.1.2
Biocontainer used: quay.io/biocontainers/control-freec:11.6--h1b792b2_1",mauro-saporita,https://github.com/nf-core/sarek/issues/921
I_kwDOCvwIC85cybyH,Port local sarek subworkflows to modules,OPEN,2023-01-25T14:11:33Z,2024-08-19T13:12:34Z,,"### Description of feature

- [ ] annotation_cache_initialisation
- [ ] bam_applybqsr
- [ ] bam_applybqsr_spark
- [ ] bam_baserecalibrator
- [ ] bam_baserecalibrator_spark
- [ ] bam_convert_samtools
- [ ] bam_joint_calling_germline_gatk
- [ ] bam_joint_calling_germline_sentieon
- [ ] bam_markduplicates
- [ ] bam_markduplicates_spark
- [ ] bam_merge_index_samtools
- [ ] bam_sentieon_dedup
- [ ] bam_variant_calling_cnvkit
- [ ] bam_variant_calling_deepvariant
- [ ] bam_variant_calling_freebayes
- [ ] bam_variant_calling_germline_manta
- [ ] bam_variant_calling_haplotypecaller
- [ ] bam_variant_calling_mpileup
- [ ] bam_variant_calling_sentieon_dnascope
- [ ] bam_variant_calling_sentieon_haplotyper
- [ ] bam_variant_calling_single_strelka
- [ ] bam_variant_calling_single_tiddit
- [ ] bam_variant_calling_somatic_ascat
- [ ] bam_variant_calling_somatic_controlfreec
- [ ] bam_variant_calling_somatic_manta
- [ ] bam_variant_calling_somatic_mutect2
- [ ] bam_variant_calling_somatic_strelka
- [ ] bam_variant_calling_somatic_tiddit
- [ ] bam_variant_calling_tumor_only_controlfreec
- [ ] bam_variant_calling_tumor_only_manta
- [ ] bam_variant_calling_tumor_only_mutect2
- [ ] cram_merge_index_samtools
- [ ] cram_qc_mosdepth_samtools
- [ ] cram_sampleqc
- [ ] download_cache_snpeff_vep
- [ ] fastq_align_bwamem_mem2_dragmap_sentieon
- [ ] fastq_create_umi_consensus_fgbio
- [ ] vcf_annotate_bcftools
- [ ] vcf_concatenate_germline
- [ ] vcf_qc_bcftools_vcftools
- [ ] vcf_variant_filtering_gatk
",maxulysse,https://github.com/nf-core/sarek/issues/922
I_kwDOCvwIC85cz22e,joint genotyping optimal strategy,OPEN,2023-01-25T18:28:05Z,2024-08-19T13:12:34Z,,"Working with a large-ish dataset (30TB raw reads),  and was wondering what a good strategy would be for working with the joint_germline option to get cohort SNP calls. Would it make more sense to try to genotype all individuals together, or parallelize the joint genotyping runs across smaller groups of samples and then filter and combine with bcftools across jointgermline outputs? ",TonyKess,https://github.com/nf-core/sarek/issues/923
I_kwDOCvwIC85c8Zud,CNVkit reference issue for WGS tumor-only samples,OPEN,2023-01-27T08:06:39Z,2024-08-29T08:36:00Z,,"### Description of the bug

Hi,
I ran the pipeline (`3.1.2`) for a bunch of tumor-only WGS samples. CNVkit reference bed files have both target and antitarget with improper bin counts:
```
wc -l *.bed
 1187 cnvkit.reference.antitarget-tmp.bed
  356 cnvkit.reference.target-tmp.bed
```
There should be many more (~1 million) target bins and no antitarget bins since these aren't WES samples and `--intervals` wasn't set. As a result, the copy number estimates are completely inaccurate. I suspect the reason is that CNVkit was run without `--method wgs`. 

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",berguner,https://github.com/nf-core/sarek/issues/924
I_kwDOCvwIC85c9HZc,AD value of Strelka vcf,CLOSED,2023-01-27T10:33:28Z,2023-03-22T08:18:54Z,2023-03-22T08:18:54Z,"### Description of the bug


Hi, it turns out that the AD values in strelka vcf is incorrect for my analysis. 
where, ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed

because in my output, Allelic depths for the ref  is always 0. 


### Command used and terminal output

```console
nextflow run nf-core/sarek --input ${dir_sarek}/samplesheet.csv --intervals ${dir_sarek}/gene_${gene}.bed --tools 'strelka,mutect2' --outdir ${dir_out_sarek} --genome GATK.GRCh37  -profile docker --igenomes_base ${dir_sarek}/igenomes --max_cpus 6 --max_memory '10.GB'
```


### Relevant files

_No response_

### System information

_No response_

EDIT by @maxulysse Markdown syntax",aihualin,https://github.com/nf-core/sarek/issues/925
I_kwDOCvwIC85c9TyA,conda create issue,OPEN,2023-01-27T11:11:58Z,2023-02-21T12:44:57Z,,"### Description of feature

I am trying to test the pipeline and I have executed nextflow run nf-core/sarek -profile test,conda --outdir test_directory
It starts running but returns the following error 

> Execution cancelled -- Finishing pending tasks before exit
> -[nf-core/sarek] Pipeline completed with errors-
> WARN: There's no process matching config selector: .:FREEC_SOMATIC -- Did you mean: FREEC_SOMATIC?
> WARN: There's no process matching config selector: .:FILTERVARIANTTRANCHES -- Did you mean: FILTERVARIANTTRANCHES?
> WARN: There's no process matching config selector: NFCORE_SAREK:SAREK:CRAM_QC_NO_MD:SAMTOOLS_STATS -- Did you mean: NFCORE_SAREK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS?
> Error executing process > 'NFCORE_SAREK:SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED (genome)'
> 
> Caused by:
> Process NFCORE_SAREK:SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED (genome) terminated with an error exit status (1)
> 
> Command executed:
> 
> gatk --java-options ""-Xmx6g"" IntervalListToBed
> --INPUT genome.interval_list
> --OUTPUT genome.bed
> --TMP_DIR . \
> 
> cat <<-END_VERSIONS > versions.yml
> ""NFCORE_SAREK:SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED"":
> gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.(GATK) v//; s/ .$//')
> END_VERSIONS
> 
> Command exit status:
> 1
> 
> Command output:
> (empty)
> 
> Command error:
> Error: Invalid or corrupt jarfile /home/bigan/miniconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar
> Using GATK jar /home/bigan/miniconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar
> Running:
> java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6g -jar /home/bigan/miniconda3/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar IntervalListToBed --INPUT genome.interval_list --OUTPUT genome.bed --TMP_DIR .
> 
> Work dir:
>   /home/bigan/work/32/d693bac63a16e84df3985d6f464104
> 
> Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`

The environment I am working in is a conda environment. I have checked the version of GATK and it is the latest 4.3.0 and the version of Java is also up to date. Can you please help me resolve this issue? Thank you.",acebollada80,https://github.com/nf-core/sarek/issues/926
I_kwDOCvwIC85dcxK1,ControlFreec Plots broken,CLOSED,2023-02-02T11:01:18Z,2023-02-02T11:17:00Z,2023-02-02T11:16:43Z,"### Description of the bug

https://nf-co.re/sarek/results#sarek/results-c87f4eb694a7183e4f99c70fca0f1d4e91750b33/somatic_test/variant_calling/controlfreec/HCC1395T_vs_HCC1395N/HCC1395T_vs_HCC1395N_BAF.png

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",apeltzer,https://github.com/nf-core/sarek/issues/931
I_kwDOCvwIC85dc2Qt,Error executing process caused by exceeding runtime limit,CLOSED,2023-02-02T11:15:15Z,2023-03-01T10:21:21Z,2023-03-01T10:21:20Z,"### Description of the bug

After having extended particular processes runtimes in #756, my run is now failing as I think several variant calling methods aren't working (I confirmed this by looking at ```htop``` as well, as for the past 24h there has been no cpu activity).
<details open>
<summary>Sarek runtime descriptions</summary>

```
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                                                                                      -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_ALLELES                                                                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_LOCI                                                                                     -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_GC                                                                                       -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_RT                                                                                       -
[d3/1dd191] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (wgs_calling_regions_noseconds.hg38.bed)                               [100%] 1 of 1, cached: 1 ✔
[02/2f0ce1] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (chr13_113723021-114354328)                                 [100%] 124 of 124, cached: 124 ✔
[aa/cdd4be] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_ANTITARGET (igenomes)                                                         [100%] 1 of 1, cached: 1 ✔
[eb/572bad] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_REFERENCE (Homo_sapiens_assembly38.fasta)                                     [100%] 1 of 1, cached: 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP                                                                     -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP                                                                 -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP                                                                   -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP                                                                   -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP                                                                      -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_UNMAP                                                                       -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_MAP                                                                         -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:CAT_FASTQ                                                                                 -
[7d/5bced3] process > NFCORE_SAREK:SAREK:FASTQC (NK452-1)                                                                                              [100%] 8 of 8, cached: 8 ✔
[b5/5f2c24] process > NFCORE_SAREK:SAREK:FASTP (NK452-1)                                                                                               [100%] 8 of 8, cached: 8 ✔
[d0/e8778f] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP:BWAMEM1_MEM (NK452-1)                                                         [100%] 96 of 96, cached: 96 ✔
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP:BWAMEM2_MEM                                                                   -
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP:DRAGMAP_ALIGN                                                                 -
[6d/bb4c52] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (NK260)                                                               [100%] 8 of 8, cached: 8 ✔
[be/e7eb5b] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:INDEX_MARKDUPLICATES (NK260)                                                               [100%] 8 of 8, cached: 8 ✔
[af/0eda84] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS (NK260)                                           [100%] 8 of 8, cached: 8 ✔
[5f/4ff5f3] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH (NK260)                                                 [100%] 8 of 8, cached: 8 ✔
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM                                                                                                   -
[74/d7f2f8] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (NK260)                                                           [100%] 992 of 992, cached: 992 ✔
[19/71d132] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS (NK260)                                                          [100%] 8 of 8, cached: 8 ✔
[40/e8a6c2] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR (NK260)                                                                         [100%] 992 of 992, cached: 959 ✔
[c7/4bf581] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:MERGE_CRAM (NK260)                                                    [100%] 8 of 8, cached: 7 ✔
[f7/0cba6b] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM (NK260)                                                    [100%] 8 of 8, cached: 6 ✔
[e6/ac1c30] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS (NK260)                                                                          [100%] 8 of 8, cached: 6 ✔
[fb/b34d6b] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:MOSDEPTH (CTRL115)                                                                              [ 37%] 3 of 8, cached: 3
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM_RECAL                                                                                             -
[17/67418e] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_CNVKIT:CNVKIT_BATCH (CTRL452)                            [100%] 4 of 4, cached: 4 ✔
[46/8851b5] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE (CTRL76)                   [  0%] 3 of 496, failed: 3
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA                             -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME                      -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_CNVKIT:CNVKIT_BATCH                                    -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:MUTECT2                             -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:MERGE_MUTECT2                       -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:MERGEMUTECTSTATS                    -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:LEARNREADORIENTATIONMODEL           -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:GETPILEUPSUMMARIES                  -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:GATHERPILEUPSUMMARIES               -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:CALCULATECONTAMINATION              -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:FILTERMUTECTCALLS                   -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE                          -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA                           -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME                    -
[3b/4d3238] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_CNVKIT:CNVKIT_BATCH (NK260_vs_CTRL260)                    [100%] 4 of 4, cached: 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:STRELKA_SOMATIC                           [  0%] 0 of 496
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_SNVS                        -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_INDELS                      -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED                            [  0%] 0 of 496
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MERGE_MUTECT2                             -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MERGEMUTECTSTATS                          -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:LEARNREADORIENTATIONMODEL                 -
[38/9d6a5a] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GETPILEUPSUMMARIES_TUMOR (NK260)          [100%] 496 of 496, cached: 124 ✔
[cf/2c27d8] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GETPILEUPSUMMARIES_NORMAL (CTRL260)       [100%] 496 of 496, cached: 124 ✔
[2a/cfca33] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GATHERPILEUPSUMMARIES_NORMAL (CTRL260)    [100%] 4 of 4, cached: 1 ✔
[bd/fdfb12] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GATHERPILEUPSUMMARIES_TUMOR (NK260)       [100%] 4 of 4, cached: 1 ✔
[d4/5d6c51] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:CALCULATECONTAMINATION (NK260_vs_CTRL260) [100%] 4 of 4, cached: 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:FILTERMUTECTCALLS                         -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS                                                                       -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT                                                                  -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL                                                                   -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY                                                                     -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF                                                                   -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:TABIX_BGZIPTABIX                                                         -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:ENSEMBLVEP                                                           -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:TABIX_BGZIPTABIX                                                     -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                                                                                   -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC          

```                                                                                             
</details>


### Command used and terminal output

```console
nextflow run nf-core/sarek --input samplesheet.csv --outdir lgl1_wes_outs --genome GATK.GRCh38 -c cubi.config -profile singularity --email oliver.knight@drfz.de --tools cnvkit,mutect2,strelka,snpeff,vep -resume

where my cubi.config is

params {
    max_memory = 128.GB
    max_cpus = 32
    max_time = 144.h
}

process {

     withName:'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE' {
        time = 48.h
   }
}
process {

     withName:'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED' {
     time = 48.h
   }
}

process {

     withName:'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:STRELKA_SOMATIC' {
     time = 48.h
   }
}

process {

    withName:'NFCORE_SAREK:SAREK:CRAM_QC_RECAL:MOSDEPTH' {
    time = 48.h
   }
}

and
Error message

Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE (CTRL76)'

Caused by:
  Process exceeded running time limit (2d)

Command executed:

  configureStrelkaGermlineWorkflow.py \
      --bam CTRL76.recal.cram \
      --referenceFasta Homo_sapiens_assembly38.fasta \
      --callRegions chr3_93705575-198235559.bed.gz \
       \
      --runDir strelka

  python strelka/runWorkflow.py -m local -j 10
  mv strelka/results/variants/genome.*.vcf.gz     CTRL76.strelka.chr3_93705575-198235559.genome.vcf.gz
  mv strelka/results/variants/genome.*.vcf.gz.tbi CTRL76.strelka.chr3_93705575-198235559.genome.vcf.gz.tbi
  mv strelka/results/variants/variants.vcf.gz     CTRL76.strelka.chr3_93705575-198235559.variants.vcf.gz
  mv strelka/results/variants/variants.vcf.gz.tbi CTRL76.strelka.chr3_93705575-198235559.variants.vcf.gz.tbi

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE"":
      strelka: $( configureStrelkaGermlineWorkflow.py --version )
  END_VERSIONS

Command exit status:
  -

Command output:

  Successfully created workflow run script.
  To execute the workflow, run the following script and set appropriate options:

  /fast/scratch/users/knighto_c/tmp/b2/bc679ec9bd207d3c3f8d076bcc1458/strelka/runWorkflow.py

Command error:
  WARNING: DEPRECATED USAGE: Forwarding SINGULARITYENV_TMPDIR as environment variable will not be supported in the future, use APPTAINERENV_TMPDIR instead
  WARNING: DEPRECATED USAGE: Forwarding SINGULARITYENV_NXF_DEBUG as environment variable will not be supported in the future, use APPTAINERENV_NXF_DEBUG instead

Work dir:
  /fast/users/knighto_c/scratch/tmp/b2/bc679ec9bd207d3c3f8d076bcc1458

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`
```


### Relevant files

_No response_

### System information

Nextflow version (eg. 21.10.3)
- version 22.10.6 build 5843
Hardware (eg. HPC, Desktop, Cloud)
- HPC
Executor (eg. slurm, local, awsbatch)
 - srun bash 
Container engine: (e.g. Docker, Singularity, Conda, Podman, Shifter or Charliecloud)
- Singularity
OS (eg. CentOS Linux, macOS, Linux Mint)
Version of nf-core/sarek (eg. 1.1, 1.5, 1.8.2)",ollieeknight,https://github.com/nf-core/sarek/issues/932
I_kwDOCvwIC85ddjEJ,Consider adding SmuRF for Random Forest Ensembl Somatic Calling,CLOSED,2023-02-02T13:06:25Z,2023-02-02T15:03:54Z,2023-02-02T13:21:38Z,"### Description of feature

https://github.com/skandlab/SMuRF#input-alt

(most callers there, needs just varscan + one more I think) ",apeltzer,https://github.com/nf-core/sarek/issues/933
I_kwDOCvwIC85dk6tu,Perform a compatibility check between bed file specified at --targets and reference genome,OPEN,2023-02-03T14:49:01Z,2024-08-19T13:12:34Z,,"### Description of feature

If the bed file is incompatible with the reference genome (i.e. specifies regions outside the reference), the pipeline fails only after the alignment step at base recalibration. It would be nice to have a process that compares e.g. the fasta index (or sequence dictionary) with the bed file, and throws an error if the bed file is incompatible.",GeertvanGeest,https://github.com/nf-core/sarek/issues/934
I_kwDOCvwIC85d8Eqi,Add a section in the FAQs about annotation cache,OPEN,2023-02-08T12:26:20Z,2024-08-19T13:12:35Z,,"> Looks good to me overall. Can we have a section in the FAQs about this? I always have a hard time wrapping my head around it and there already is a section about custom cache that could be extended with this I think

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/936#pullrequestreview-1289038014_
            ",maxulysse,https://github.com/nf-core/sarek/issues/937
I_kwDOCvwIC85eNnCr,Process `NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_HAPLOTYPECALLER:JOINT_GERMLINE:MERGE_GENOTYPEGVCFS (joint_variant_calling)` terminated with an error exit status (3),CLOSED,2023-02-11T03:32:18Z,2023-06-14T19:06:36Z,2023-06-14T15:31:05Z,"### Description of the bug

When i adding paramaters(--tools mutect2,haplotypecaller --joint_germline) to the process, there is an error in ""MERGE_GENOTYPEGVCFS (joint_variant_calling)"".  And i can't found the right place to fix this problem. 

### Command used and terminal output

```console
Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_HAPLOTYPECALLER:JOINT_GERMLINE:MERGE_GENOTYPEGVCFS (joint_variant_calling)'

Caused by:
  Process `NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_HAPLOTYPECALLER:JOINT_GERMLINE:MERGE_GENOTYPEGVCFS (joint_variant_calling)` terminated with an error exit status (3)

Command executed:

  gatk --java-options ""-Xmx36g"" MergeVcfs \
      --INPUT [chr4_31832570-32833016.vcf.gz].sort.vcf.gz --INPUT [chr21_9246088-9377143.vcf.gz].sort.vcf.gz --INPUT [chr9_63968448-64135013.vcf.gz].sort.vcf.gz --INPUT [chr18_10001-15410899.vcf.gz].sort.vcf.gz --INPUT [chr21_5443559-5449012.vcf.gz].sort.vcf.gz --INPUT [chr6_95070791-167591393.vcf.gz].sort.vcf.gz --INPUT [chr22_10834644-10874572.vcf.gz].sort.vcf.gz --INPUT [chr22_18339130-18433513.vcf.gz].sort.vcf.gz --INPUT [chr21_12965809-43212462.vcf.gz].sort.vcf.gz --INPUT [chr10_39230137-39409792.vcf.gz].sort.vcf.gz --INPUT [chr4_49486925-49658100.vcf.gz].sort.vcf.gz --INPUT [chr22_12275589-12438690.vcf.gz].sort.vcf.gz --INPUT [chr16_46380683-90228345.vcf.gz].sort.vcf.gz --INPUT [chr22_11118988-11160921.vcf.gz].sort.vcf.gz --INPUT [chr1_228608365-248946422.vcf.gz].sort.vcf.gz --INPUT [chrY_10682443-10691573.vcf.gz].sort.vcf.gz --INPUT [chr4_8816478-9272916.vcf.gz].sort.vcf.gz --INPUT [chr3_90568829-90722458.vcf.gz].sort.vcf.gz --INPUT [chr21_43262463-46699983.vcf.gz].sort.vcf.gz --INPUT [chr18_54537529-80263285.vcf.gz].sort.vcf.gz --INPUT [chr11_71055697-87978202.vcf.gz].sort.vcf.gz --INPUT [chr10_124121503-133690466.vcf.gz].sort.vcf.gz --INPUT [chr17_491112-21795850.vcf.gz].sort.vcf.gz --INPUT [chr13_111843442-113673020.vcf.gz].sort.vcf.gz --INPUT [chr12_10001-7083650.vcf.gz].sort.vcf.gz --INPUT [chr12_7084651-34719407.vcf.gz].sort.vcf.gz --INPUT [chr4_10001-1429358.vcf.gz].sort.vcf.gz --INPUT [chr21_8099840-8260971.vcf.gz].sort.vcf.gz --INPUT [chr9_134185537-138334717.vcf.gz].sort.vcf.gz --INPUT [chr9_63008372-63202862.vcf.gz].sort.vcf.gz --INPUT [chr8_7667128-12234345.vcf.gz].sort.vcf.gz --INPUT [chrY_56673215-56771509.vcf.gz].sort.vcf.gz --INPUT [chr21_10324328-10814560.vcf.gz].sort.vcf.gz --INPUT [chr2_91402512-92138145.vcf.gz].sort.vcf.gz --INPUT [chr11_51078349-54425074.vcf.gz].sort.vcf.gz --INPUT [chr10_38573339-38906036.vcf.gz].sort.vcf.gz --INPUT [chr1_125173584-125184587.vcf.gz].sort.vcf.gz --INPUT [chr8_85714223-145078636.vcf.gz].sort.vcf.gz --INPUT [chr6_167641394-170745979.vcf.gz].sort.vcf.gz --INPUT [chr9_10001-41225986.vcf.gz].sort.vcf.gz --INPUT [chr9_61785369-62149738.vcf.gz].sort.vcf.gz --INPUT [chr20_29413578-29562970.vcf.gz].sort.vcf.gz --INPUT [chr2_97489619-238903659.vcf.gz].sort.vcf.gz --INPUT [chr14_16133336-16140527.vcf.gz].sort.vcf.gz --INPUT [chr16_18486487-33214595.vcf.gz].sort.vcf.gz --INPUT [chr20_29697631-30038348.vcf.gz].sort.vcf.gz --INPUT [chrY_9055175-9057608.vcf.gz].sort.vcf.gz --INPUT [chr5_46485901-47069162.vcf.gz].sort.vcf.gz --INPUT [chr20_60001-63215.vcf.gz].sort.vcf.gz --INPUT [chr4_49708101-51743951.vcf.gz].sort.vcf.gz --INPUT [chr22_49975366-50808468.vcf.gz].sort.vcf.gz --INPUT [chr9_43236168-43263290.vcf.gz].sort.vcf.gz --INPUT [chr5_49666174-50059807.vcf.gz].sort.vcf.gz --INPUT [chrY_11013047-11016992.vcf.gz].sort.vcf.gz --INPUT [chr20_31161626-36314371.vcf.gz].sort.vcf.gz --INPUT [chr8_12284346-43983744.vcf.gz].sort.vcf.gz --INPUT [chr22_12691731-12726204.vcf.gz].sort.vcf.gz --INPUT [chr12_132224363-133265309.vcf.gz].sort.vcf.gz --INPUT [chr16_34339330-34521510.vcf.gz].sort.vcf.gz --INPUT [chr18_47019913-54536574.vcf.gz].sort.vcf.gz --INPUT [chr2_92188146-94090557.vcf.gz].sort.vcf.gz --INPUT [chr1_10001-207666.vcf.gz].sort.vcf.gz --INPUT [chr16_34580966-34584085.vcf.gz].sort.vcf.gz --INPUT [chr2_32917626-89330679.vcf.gz].sort.vcf.gz --INPUT [chr21_5010001-5166246.vcf.gz].sort.vcf.gz --INPUT [chr16_10001-18436486.vcf.gz].sort.vcf.gz --INPUT [chr11_60001-50821348.vcf.gz].sort.vcf.gz --INPUT [chrX_144475607-156030895.vcf.gz].sort.vcf.gz --INPUT [chr4_58921382-190123121.vcf.gz].sort.vcf.gz --INPUT [chr19_24891357-24898313.vcf.gz].sort.vcf.gz --INPUT [chr20_28868453-28890335.vcf.gz].sort.vcf.gz --INPUT [chr9_66591388-67920552.vcf.gz].sort.vcf.gz --INPUT [chr13_18171249-18358106.vcf.gz].sort.vcf.gz --INPUT [chr7_58169654-60828234.vcf.gz].sort.vcf.gz --INPUT [chr2_89753993-90402511.vcf.gz].sort.vcf.gz --INPUT [chr5_49609715-49611721.vcf.gz].sort.vcf.gz --INPUT [chr13_18408107-86202979.vcf.gz].sort.vcf.gz --INPUT [chr17_21814104-21984549.vcf.gz].sort.vcf.gz --INPUT [chr17_26935981-81742542.vcf.gz].sort.vcf.gz --INPUT [chr20_30456078-30761898.vcf.gz].sort.vcf.gz --INPUT [chr22_11428057-11497337.vcf.gz].sort.vcf.gz --INPUT [chr8_45927266-85664222.vcf.gz].sort.vcf.gz --INPUT [chr21_8310972-8472360.vcf.gz].sort.vcf.gz --INPUT [chr5_155761325-181478259.vcf.gz].sort.vcf.gz --INPUT [chr21_7377866-7500890.vcf.gz].sort.vcf.gz --INPUT [chr20_31051509-31107036.vcf.gz].sort.vcf.gz --INPUT [chr2_10001-16145119.vcf.gz].sort.vcf.gz --INPUT [chr2_16146120-32867130.vcf.gz].sort.vcf.gz --INPUT [chr22_16302844-16304296.vcf.gz].sort.vcf.gz --INPUT [chr1_29553836-121976459.vcf.gz].sort.vcf.gz --INPUT [chr5_50109808-139452659.vcf.gz].sort.vcf.gz --INPUT [chr16_34584623-36260386.vcf.gz].sort.vcf.gz --INPUT [chrY_11674124-20207793.vcf.gz].sort.vcf.gz --INPUT [chr12_34829238-37185252.vcf.gz].sort.vcf.gz --INPUT [chr4_190173122-190204555.vcf.gz].sort.vcf.gz --INPUT [chr1_13004385-16799163.vcf.gz].sort.vcf.gz --INPUT [chr5_17580549-46435900.vcf.gz].sort.vcf.gz --INPUT [chrX_114331199-115738949.vcf.gz].sort.vcf.gz --INPUT [chr1_16849164-29552233.vcf.gz].sort.vcf.gz --INPUT [chrY_56821510-56887902.vcf.gz].sort.vcf.gz --INPUT [chr20_28752591-28843401.vcf.gz].sort.vcf.gz --INPUT [chr7_143700805-159335973.vcf.gz].sort.vcf.gz --INPUT [chr20_30088349-30425128.vcf.gz].sort.vcf.gz --INPUT [chr17_22813680-26627010.vcf.gz].sort.vcf.gz --INPUT [chr15_20729747-21193490.vcf.gz].sort.vcf.gz --INPUT [chr15_17000001-19725254.vcf.gz].sort.vcf.gz --INPUT [chr6_60001-58453888.vcf.gz].sort.vcf.gz --INPUT [chr10_47870369-124121200.vcf.gz].sort.vcf.gz --INPUT [chr22_11681289-11724629.vcf.gz].sort.vcf.gz --INPUT [chr9_41229379-41237752.vcf.gz].sort.vcf.gz --INPUT [chr20_29271827-29315342.vcf.gz].sort.vcf.gz --INPUT [chr11_88002897-135076622.vcf.gz].sort.vcf.gz --INPUT [chr7_61967064-61976104.vcf.gz].sort.vcf.gz --INPUT [chr16_33264596-33392411.vcf.gz].sort.vcf.gz --INPUT [chr3_10001-90565295.vcf.gz].sort.vcf.gz --INPUT [chr18_15791048-20564714.vcf.gz].sort.vcf.gz --INPUT [chrX_37285838-49348394.vcf.gz].sort.vcf.gz --INPUT [chr3_91256422-91257890.vcf.gz].sort.vcf.gz --INPUT [chr5_139453660-155760324.vcf.gz].sort.vcf.gz --INPUT [chr17_26698999-26720420.vcf.gz].sort.vcf.gz --INPUT [chr19_60001-24448980.vcf.gz].sort.vcf.gz --INPUT [chr22_12488691-12641730.vcf.gz].sort.vcf.gz --INPUT [chr13_86252980-111703855.vcf.gz].sort.vcf.gz --INPUT [chr1_143184588-223558935.vcf.gz].sort.vcf.gz --INPUT [chr21_10269869-10274327.vcf.gz].sort.vcf.gz --INPUT [chr1_122026460-124977944.vcf.gz].sort.vcf.gz --INPUT [chr6_61371373-95020790.vcf.gz].sort.vcf.gz --INPUT [chr15_84320067-101981189.vcf.gz].sort.vcf.gz --INPUT [chr8_60001-7617127.vcf.gz].sort.vcf.gz --INPUT [chrX_58605580-62412542.vcf.gz].sort.vcf.gz --INPUT [chr10_42066266-47780368.vcf.gz].sort.vcf.gz --INPUT [chr20_26436233-26590875.vcf.gz].sort.vcf.gz --INPUT [chr5_10001-17530548.vcf.gz].sort.vcf.gz \
      --OUTPUT joint_germline.vcf.gz \
      --SEQUENCE_DICTIONARY Homo_sapiens_assembly38.dict \
      --TMP_DIR . \
  
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:GERMLINE_VARIANT_CALLING:RUN_HAPLOTYPECALLER:JOINT_GERMLINE:MERGE_GENOTYPEGVCFS"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  3

Command output:
  (empty)

Command error:
  Using GATK jar /opt/conda/envs/python27/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx36g -jar /opt/conda/envs/python27/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar MergeVcfs --INPUT [chr4_31832570-32833016.vcf.gz].sort.vcf.gz --INPUT [chr21_9246088-9377143.vcf.gz].sort.vcf.gz --INPUT [chr9_63968448-64135013.vcf.gz].sort.vcf.gz --INPUT [chr18_10001-15410899.vcf.gz].sort.vcf.gz --INPUT [chr21_5443559-5449012.vcf.gz].sort.vcf.gz --INPUT [chr6_95070791-167591393.vcf.gz].sort.vcf.gz --INPUT [chr22_10834644-10874572.vcf.gz].sort.vcf.gz --INPUT [chr22_18339130-18433513.vcf.gz].sort.vcf.gz --INPUT [chr21_12965809-43212462.vcf.gz].sort.vcf.gz --INPUT [chr10_39230137-39409792.vcf.gz].sort.vcf.gz --INPUT [chr4_49486925-49658100.vcf.gz].sort.vcf.gz --INPUT [chr22_12275589-12438690.vcf.gz].sort.vcf.gz --INPUT [chr16_46380683-90228345.vcf.gz].sort.vcf.gz --INPUT [chr22_11118988-11160921.vcf.gz].sort.vcf.gz --INPUT [chr1_228608365-248946422.vcf.gz].sort.vcf.gz --INPUT [chrY_10682443-10691573.vcf.gz].sort.vcf.gz --INPUT [chr4_8816478-9272916.vcf.gz].sort.vcf.gz --INPUT [chr3_90568829-90722458.vcf.gz].sort.vcf.gz --INPUT [chr21_43262463-46699983.vcf.gz].sort.vcf.gz --INPUT [chr18_54537529-80263285.vcf.gz].sort.vcf.gz --INPUT [chr11_71055697-87978202.vcf.gz].sort.vcf.gz --INPUT [chr10_124121503-133690466.vcf.gz].sort.vcf.gz --INPUT [chr17_491112-21795850.vcf.gz].sort.vcf.gz --INPUT [chr13_111843442-113673020.vcf.gz].sort.vcf.gz --INPUT [chr12_10001-7083650.vcf.gz].sort.vcf.gz --INPUT [chr12_7084651-34719407.vcf.gz].sort.vcf.gz --INPUT [chr4_10001-1429358.vcf.gz].sort.vcf.gz --INPUT [chr21_8099840-8260971.vcf.gz].sort.vcf.gz --INPUT [chr9_134185537-138334717.vcf.gz].sort.vcf.gz --INPUT [chr9_63008372-63202862.vcf.gz].sort.vcf.gz --INPUT [chr8_7667128-12234345.vcf.gz].sort.vcf.gz --INPUT [chrY_56673215-56771509.vcf.gz].sort.vcf.gz --INPUT [chr21_10324328-10814560.vcf.gz].sort.vcf.gz --INPUT [chr2_91402512-92138145.vcf.gz].sort.vcf.gz --INPUT [chr11_51078349-54425074.vcf.gz].sort.vcf.gz --INPUT [chr10_38573339-38906036.vcf.gz].sort.vcf.gz --INPUT [chr1_125173584-125184587.vcf.gz].sort.vcf.gz --INPUT [chr8_85714223-145078636.vcf.gz].sort.vcf.gz --INPUT [chr6_167641394-170745979.vcf.gz].sort.vcf.gz --INPUT [chr9_10001-41225986.vcf.gz].sort.vcf.gz --INPUT [chr9_61785369-62149738.vcf.gz].sort.vcf.gz --INPUT [chr20_29413578-29562970.vcf.gz].sort.vcf.gz --INPUT [chr2_97489619-238903659.vcf.gz].sort.vcf.gz --INPUT [chr14_16133336-16140527.vcf.gz].sort.vcf.gz --INPUT [chr16_18486487-33214595.vcf.gz].sort.vcf.gz --INPUT [chr20_29697631-30038348.vcf.gz].sort.vcf.gz --INPUT [chrY_9055175-9057608.vcf.gz].sort.vcf.gz --INPUT [chr5_46485901-47069162.vcf.gz].sort.vcf.gz --INPUT [chr20_60001-63215.vcf.gz].sort.vcf.gz --INPUT [chr4_49708101-51743951.vcf.gz].sort.vcf.gz --INPUT [chr22_49975366-50808468.vcf.gz].sort.vcf.gz --INPUT [chr9_43236168-43263290.vcf.gz].sort.vcf.gz --INPUT [chr5_49666174-50059807.vcf.gz].sort.vcf.gz --INPUT [chrY_11013047-11016992.vcf.gz].sort.vcf.gz --INPUT [chr20_31161626-36314371.vcf.gz].sort.vcf.gz --INPUT [chr8_12284346-43983744.vcf.gz].sort.vcf.gz --INPUT [chr22_12691731-12726204.vcf.gz].sort.vcf.gz --INPUT [chr12_132224363-133265309.vcf.gz].sort.vcf.gz --INPUT [chr16_34339330-34521510.vcf.gz].sort.vcf.gz --INPUT [chr18_47019913-54536574.vcf.gz].sort.vcf.gz --INPUT [chr2_92188146-94090557.vcf.gz].sort.vcf.gz --INPUT [chr1_10001-207666.vcf.gz].sort.vcf.gz --INPUT [chr16_34580966-34584085.vcf.gz].sort.vcf.gz --INPUT [chr2_32917626-89330679.vcf.gz].sort.vcf.gz --INPUT [chr21_5010001-5166246.vcf.gz].sort.vcf.gz --INPUT [chr16_10001-18436486.vcf.gz].sort.vcf.gz --INPUT [chr11_60001-50821348.vcf.gz].sort.vcf.gz --INPUT [chrX_144475607-156030895.vcf.gz].sort.vcf.gz --INPUT [chr4_58921382-190123121.vcf.gz].sort.vcf.gz --INPUT [chr19_24891357-24898313.vcf.gz].sort.vcf.gz --INPUT [chr20_28868453-28890335.vcf.gz].sort.vcf.gz --INPUT [chr9_66591388-67920552.vcf.gz].sort.vcf.gz --INPUT [chr13_18171249-18358106.vcf.gz].sort.vcf.gz --INPUT [chr7_58169654-60828234.vcf.gz].sort.vcf.gz --INPUT [chr2_89753993-90402511.vcf.gz].sort.vcf.gz --INPUT [chr5_49609715-49611721.vcf.gz].sort.vcf.gz --INPUT [chr13_18408107-86202979.vcf.gz].sort.vcf.gz --INPUT [chr17_21814104-21984549.vcf.gz].sort.vcf.gz --INPUT [chr17_26935981-81742542.vcf.gz].sort.vcf.gz --INPUT [chr20_30456078-30761898.vcf.gz].sort.vcf.gz --INPUT [chr22_11428057-11497337.vcf.gz].sort.vcf.gz --INPUT [chr8_45927266-85664222.vcf.gz].sort.vcf.gz --INPUT [chr21_8310972-8472360.vcf.gz].sort.vcf.gz --INPUT [chr5_155761325-181478259.vcf.gz].sort.vcf.gz --INPUT [chr21_7377866-7500890.vcf.gz].sort.vcf.gz --INPUT [chr20_31051509-31107036.vcf.gz].sort.vcf.gz --INPUT [chr2_10001-16145119.vcf.gz].sort.vcf.gz --INPUT [chr2_16146120-32867130.vcf.gz].sort.vcf.gz --INPUT [chr22_16302844-16304296.vcf.gz].sort.vcf.gz --INPUT [chr1_29553836-121976459.vcf.gz].sort.vcf.gz --INPUT [chr5_50109808-139452659.vcf.gz].sort.vcf.gz -- [chr4_10001-1429358.vcf.gz].sort.vcf.gz --INPUT [chr21_8099840-8260971.vcf.gz].sort.vcf.gz --INPUT [chr9_134185537-138334717.vcf.gz].sort.vcf.gz --INPUT [chr9_63008372-63202862.vcf.gz].sort.vcf.gz --INPUT [chr8_7667128-12234345.vcf.gz].sort.vcf.gz --INPUT [chrY_56673215-56771509.vcf.gz].sort.vcf.gz --INPUT [chr21_10324328-10814560.vcf.gz].sort.vcf.gz --INPUT [chr2_91402512-92138145.vcf.gz].sort.vcf.gz --INPUT [chr11_51078349-54425074.vcf.gz].sort.vcf.gz --INPUT [chr10_38573339-38906036.vcf.gz].sort.vcf.gz --INPUT [chr1_125173584-125184587.vcf.gz].sort.vcf.gz --INPUT [chr8_85714223-145078636.vcf.gz].sort.vcf.gz --INPUT [chr6_167641394-170745979.vcf.gz].sort.vcf.gz --INPUT [chr9_10001-41225986.vcf.gz].sort.vcf.gz --INPUT [chr9_61785369-62149738.vcf.gz].sort.vcf.gz --INPUT [chr20_29413578-29562970.vcf.gz].sort.vcf.gz --INPUT [chr2_97489619-238903659.vcf.gz].sort.vcf.gz --INPUT [chr14_16133336-16140527.vcf.gz].sort.vcf.gz --INPUT [chr16_18486487-33214595.vcf.gz].sort.vcf.gz --INPUT [chr20_29697631-30038348.vcf.gz].sort.vcf.gz --INPUT [chrY_9055175-9057608.vcf.gz].sort.vcf.gz --INPUT [chr5_46485901-47069162.vcf.gz].sort.vcf.gz --INPUT [chr20_60001-63215.vcf.gz].sort.vcf.gz --INPUT [chr4_49708101-51743951.vcf.gz].sort.vcf.gz --INPUT [chr22_49975366-50808468.vcf.gz].sort.vcf.gz --INPUT [chr9_43236168-43263290.vcf.gz].sort.vcf.gz --INPUT [chr5_49666174-50059807.vcf.gz].sort.vcf.gz --INPUT [chrY_11013047-11016992.vcf.gz].sort.vcf.gz --INPUT [chr20_31161626-36314371.vcf.gz].sort.vcf.gz --INPUT [chr8_12284346-43983744.vcf.gz].sort.vcf.gz --INPUT [chr22_12691731-12726204.vcf.gz].sort.vcf.gz --INPUT [chr12_132224363-133265309.vcf.gz].sort.vcf.gz --INPUT [chr16_34339330-34521510.vcf.gz].sort.vcf.gz --INPUT [chr18_47019913-54536574.vcf.gz].sort.vcf.gz --INPUT [chr2_92188146-94090557.vcf.gz].sort.vcf.gz --INPUT [chr1_10001-207666.vcf.gz].sort.vcf.gz --INPUT [chr16_34580966-34584085.vcf.gz].sort.vcf.gz --INPUT [chr2_32917626-89330679.vcf.gz].sort.vcf.gz --INPUT [chr21_5010001-5166246.vcf.gz].sort.vcf.gz --INPUT [chr16_10001-18436486.vcf.gz].sort.vcf.gz --INPUT [chr11_60001-50821348.vcf.gz].sort.vcf.gz --INPUT [chrX_144475607-156030895.vcf.gz].sort.vcf.gz --INPUT [chr4_58921382-190123121.vcf.gz].sort.vcf.gz --INPUT [chr19_24891357-24898313.vcf.gz].sort.vcf.gz --INPUT [chr20_28868453-28890335.vcf.gz].sort.vcf.gz --INPUT [chr9_66591388-67920552.vcf.gz].sort.vcf.gz --INPUT [chr13_18171249-18358106.vcf.gz].sort.vcf.gz --INPUT [chr7_58169654-60828234.vcf.gz].sort.vcf.gz --INPUT [chr2_89753993-90402511.vcf.gz].sort.vcf.gz --INPUT [chr5_49609715-49611721.vcf.gz].sort.vcf.gz --INPUT [chr13_18408107-86202979.vcf.gz].sort.vcf.gz --INPUT [chr17_21814104-21984549.vcf.gz].sort.vcf.gz --INPUT [chr17_26935981-81742542.vcf.gz].sort.vcf.gz --INPUT [chr20_30456078-30761898.vcf.gz].sort.vcf.gz --INPUT [chr22_11428057-11497337.vcf.gz].sort.vcf.gz --INPUT [chr8_45927266-85664222.vcf.gz].sort.vcf.gz --INPUT [chr21_8310972-8472360.vcf.gz].sort.vcf.gz --INPUT [chr5_155761325-181478259.vcf.gz].sort.vcf.gz --INPUT [chr21_7377866-7500890.vcf.gz].sort.vcf.gz --INPUT [chr20_31051509-31107036.vcf.gz].sort.vcf.gz --INPUT [chr2_10001-16145119.vcf.gz].sort.vcf.gz --INPUT [chr2_16146120-32867130.vcf.gz].sort.vcf.gz --INPUT [chr22_16302844-16304296.vcf.gz].sort.vcf.gz --INPUT [chr1_29553836-121976459.vcf.gz].sort.vcf.gz --INPUT [chr5_50109808-139452659.vcf.gz].sort.vcf.gz --INPUT [chr16_34584623-36260386.vcf.gz].sort.vcf.gz --INPUT [chrY_11674124-20207793.vcf.gz].sort.vcf.gz --INPUT [chr12_34829238-37185252.vcf.gz].sort.vcf.gz --INPUT [chr4_190173122-190204555.vcf.gz].sort.vcf.gz --INPUT [chr1_13004385-16799163.vcf.gz].sort.vcf.gz --INPUT [chr5_17580549-46435900.vcf.gz].sort.vcf.gz --INPUT [chrX_114331199-115738949.vcf.gz].sort.vcf.gz --INPUT [chr1_16849164-29552233.vcf.gz].sort.vcf.gz --INPUT [chrY_56821510-56887902.vcf.gz].sort.vcf.gz --INPUT [chr20_28752591-28843401.vcf.gz].sort.vcf.gz --INPUT [chr7_143700805-159335973.vcf.gz].sort.vcf.gz --INPUT [chr20_30088349-30425128.vcf.gz].sort.vcf.gz --INPUT [chr17_22813680-26627010.vcf.gz].sort.vcf.gz --INPUT [chr15_20729747-21193490.vcf.gz].sort.vcf.gz --INPUT [chr15_17000001-19725254.vcf.gz].sort.vcf.gz --INPUT [chr6_60001-58453888.vcf.gz].sort.vcf.gz --INPUT [chr10_47870369-124121200.vcf.gz].sort.vcf.gz --INPUT [chr22_11681289-11724629.vcf.gz].sort.vcf.gz --INPUT [chr9_41229379-41237752.vcf.gz].sort.vcf.gz --INPUT [chr20_29271827-29315342.vcf.gz].sort.vcf.gz --INPUT [chr11_88002897-135076622.vcf.gz].sort.vcf.gz --INPUT [chr7_61967064-61976104.vcf.gz].sort.vcf.gz --INPUT [chr16_33264596-33392411.vcf.gz].sort.vcf.gz --INPUT [chr3_10001-90565295.vcf.gz].sort.vcf.gz --INPUT [chr18_15791048-20564714.vcf.gz].sort.vcf.gz --INPUT [chrX_37285838-49348394.vcf.gz].sort.vcf.gz --INPUT [chr3_91256422-91257890.vcf.gz].sort.vcf.gz --INPUT [chr5_139453660-155760324.vcf.gz].sort.vcf.gz --INPUT [chr17_26698999-26720420.vcf.gz].sort.vcf.gz --INPUT [chr19_60001-24448980.vcf.gz].sort.vcf.gz --INPUT [chr22_12488691-12641730.vcf.gz].sort.vcf.gz --INPUT [chr13_86252980-111703855.vcf.gz].sort.vcf.gz --INPUT [chr1_143184588-223558935.vcf.gz].sort.vcf.gz --INPUT [chr21_10269869-10274327.vcf.gz].sort.vcf.gz --INPUT [chr1_122026460-124977944.vcf.gz].sort.vcf.gz --INPUT [chr6_61371373-95020790.vcf.gz].sort.vcf.gz --INPUT [chr15_84320067-101981189.vcf.gz].sort.vcf.gz --INPUT [chr8_60001-7617127.vcf.gz].sort.vcf.gz --INPUT [chrX_58605580-62412542.vcf.gz].sort.vcf.gz --INPUT [chr10_42066266-47780368.vcf.gz].sort.vcf.gz --INPUT [chr20_26436233-26590875.vcf.gz].sort.vcf.gz --INPUT [chr5_10001-17530548.vcf.gz].sort.vcf.gz --OUTPUT joint_germline.vcf.gz --SEQUENCE_DICTIONARY Homo_sapiens_assembly38.dict --TMP_DIR .
  09:21:54.362 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/conda/envs/python27/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
  [Fri Feb 10 09:21:54 UTC 2023] MergeVcfs --INPUT [chr4_31832570-32833016.vcf.gz].sort.vcf.gz --INPUT [chr21_9246088-9377143.vcf.gz].sort.vcf.gz --INPUT [chr9_63968448-64135013.vcf.gz].sort.vcf.gz --INPUT [chr18_10001-15410899.vcf.gz].sort.vcf.gz --INPUT [chr21_5443559-5449012.vcf.gz].sort.vcf.gz --INPUT [chr6_95070791-167591393.vcf.gz].sort.vcf.gz --INPUT [chr22_10834644-10874572.vcf.gz].sort.vcf.gz --INPUT [chr22_18339130-18433513.vcf.gz].sort.vcf.gz --INPUT [chr21_12965809-43212462.vcf.gz].sort.vcf.gz --INPUT [chr10_39230137-39409792.vcf.gz].sort.vcf.gz --INPUT [chr4_49486925-49658100.vcf.gz].sort.vcf.gz --INPUT [chr22_12275589-12438690.vcf.gz].sort.vcf.gz --INPUT [chr16_46380683-90228345.vcf.gz].sort.vcf.gz --INPUT [chr22_11118988-11160921.vcf.gz].sort.vcf.gz --INPUT [chr1_228608365-248946422.vcf.gz].sort.vcf.gz --INPUT [chrY_10682443-10691573.vcf.gz].sort.vcf.gz --INPUT [chr4_8816478-9272916.vcf.gz].sort.vcf.gz --INPUT [chr3_90568829-90722458.vcf.gz].sort.vcf.gz --INPUT [chr21_43262463-46699983.vcf.gz].sort.vcf.gz --INPUT [chr18_54537529-80263285.vcf.gz].sort.vcf.gz --INPUT [chr11_71055697-87978202.vcf.gz].sort.vcf.gz --INPUT [chr10_124121503-133690466.vcf.gz].sort.vcf.gz --INPUT [chr17_491112-21795850.vcf.gz].sort.vcf.gz --INPUT [chr13_111843442-113673020.vcf.gz].sort.vcf.gz --INPUT [chr12_10001-7083650.vcf.gz].sort.vcf.gz --INPUT [chr12_7084651-34719407.vcf.gz].sort.vcf.gz --INPUT [chr4_10001-1429358.vcf.gz].sort.vcf.gz --INPUT [chr21_8099840-8260971.vcf.gz].sort.vcf.gz --INPUT [chr9_134185537-138334717.vcf.gz].sort.vcf.gz --INPUT [chr9_63008372-63202862.vcf.gz].sort.vcf.gz --INPUT [chr8_7667128-12234345.vcf.gz].sort.vcf.gz --INPUT [chrY_56673215-56771509.vcf.gz].sort.vcf.gz --INPUT [chr21_10324328-10814560.vcf.gz].sort.vcf.gz --INPUT [chr2_91402512-92138145.vcf.gz].sort.vcf.gz --INPUT [chr11_51078349-54425074.vcf.gz].sort.vcf.gz --INPUT [chr10_38573339-38906036.vcf.gz].sort.vcf.gz --INPUT [chr1_125173584-125184587.vcf.gz].sort.vcf.gz --INPUT [chr8_85714223-145078636.vcf.gz].sort.vcf.gz --INPUT [chr6_167641394-170745979.vcf.gz].sort.vcf.gz --INPUT [chr9_10001-41225986.vcf.gz].sort.vcf.gz --INPUT [chr9_61785369-62149738.vcf.gz].sort.vcf.gz --INPUT [chr20_29413578-29562970.vcf.gz].sort.vcf.gz --INPUT [chr2_97489619-238903659.vcf.gz].sort.vcf.gz --INPUT [chr14_16133336-16140527.vcf.gz].sort.vcf.gz --INPUT [chr16_18486487-33214595.vcf.gz].sort.vcf.gz --INPUT [chr20_29697631-30038348.vcf.gz].sort.vcf.gz --INPUT [chrY_9055175-9057608.vcf.gz].sort.vcf.gz --INPUT [chr5_46485901-47069162.vcf.gz].sort.vcf.gz --INPUT [chr20_60001-63215.vcf.gz].sort.vcf.gz --INPUT [chr4_49708101-51743951.vcf.gz].sort.vcf.gz --INPUT [chr22_49975366-50808468.vcf.gz].sort.vcf.gz --INPUT [chr9_43236168-43263290.vcf.gz].sort.vcf.gz --INPUT [chr5_49666174-50059807.vcf.gz].sort.vcf.gz --INPUT [chrY_11013047-11016992.vcf.gz].sort.vcf.gz --INPUT [chr20_31161626-36314371.vcf.gz].sort.vcf.gz --INPUT [chr8_12284346-43983744.vcf.gz].sort.vcf.gz --INPUT [chr22_12691731-12726204.vcf.gz].sort.vcf.gz --INPUT [chr12_132224363-133265309.vcf.gz].sort.vcf.gz --INPUT [chr16_34339330-34521510.vcf.gz].sort.vcf.gz --INPUT [chr18_47019913-54536574.vcf.gz].sort.vcf.gz --INPUT [chr2_92188146-94090557.vcf.gz].sort.vcf.gz --INPUT [chr1_10001-207666.vcf.gz].sort.vcf.gz --INPUT [chr16_34580966-34584085.vcf.gz].sort.vcf.gz --INPUT [chr2_32917626-89330679.vcf.gz].sort.vcf.gz --INPUT [chr21_5010001-5166246.vcf.gz].sort.vcf.gz --INPUT [chr16_10001-18436486.vcf.gz].sort.vcf.gz --INPUT [chr11_60001-50821348.vcf.gz].sort.vcf.gz --INPUT [chrX_144475607-156030895.vcf.gz].sort.vcf.gz --INPUT [chr4_58921382-190123121.vcf.gz].sort.vcf.gz --INPUT [chr19_24891357-24898313.vcf.gz].sort.vcf.gz --INPUT [chr20_28868453-28890335.vcf.gz].sort.vcf.gz --INPUT [chr9_66591388-67920552.vcf.gz].sort.vcf.gz --INPUT [chr13_18171249-18358106.vcf.gz].sort.vcf.gz --INPUT [chr7_58169654-60828234.vcf.gz].sort.vcf.gz --INPUT [chr2_89753993-90402511.vcf.gz].sort.vcf.gz --INPUT [chr5_49609715-49611721.vcf.gz].sort.vcf.gz --INPUT [chr13_18408107-86202979.vcf.gz].sort.vcf.gz --INPUT [chr17_21814104-21984549.vcf.gz].sort.vcf.gz --INPUT [chr17_26935981-81742542.vcf.gz].sort.vcf.gz --INPUT [chr20_30456078-30761898.vcf.gz].sort.vcf.gz --INPUT [chr22_11428057-11497337.vcf.gz].sort.vcf.gz --INPUT [chr8_45927266-85664222.vcf.gz].sort.vcf.gz --INPUT [chr21_8310972-8472360.vcf.gz].sort.vcf.gz --INPUT [chr5_155761325-181478259.vcf.gz].sort.vcf.gz --INPUT [chr21_7377866-7500890.vcf.gz].sort.vcf.gz --INPUT [chr20_31051509-31107036.vcf.gz].sort.vcf.gz --INPUT [chr2_10001-16145119.vcf.gz].sort.vcf.gz --INPUT [chr2_16146120-32867130.vcf.gz].sort.vcf.gz --INPUT [chr22_16302844-16304296.vcf.gz].sort.vcf.gz --INPUT [chr1_29553836-121976459.vcf.gz].sort.vcf.gz --INPUT [chr5_50109808-139452659.vcf.gz].sort.vcf.gz --INPUT [chr16_34584623-36260386.vcf.gz].sort.vcf.gz --INPUT [chrY_11674124-20207793.vcf.gz].sort.vcf.gz --INPUT [chr12_34829238-37185252.vcf.gz].sort.vcf.gz --INPUT [chr4_190173122-190204555.vcf.gz].sort.vcf.gz --INPUT [chr1_13004385-16799163.vcf.gz].sort.vcf.gz --INPUT [chr5_17580549-46435900.vcf.gz].sort.vcf.gz --INPUT [chrX_114331199-115738949.vcf.gz].sort.vcf.gz --INPUT [chr1_16849164-29552233.vcf.gz].sort.vcf.gz --INPUT [chrY_56821510-56887902.vcf.gz].sort.vcf.gz --INPUT [chr20_28752591-28843401.vcf.gz].sort.vcf.gz --INPUT [chr7_143700805-159335973.vcf.gz].sort.vcf.gz --INPUT [chr20_30088349-30425128.vcf.gz].sort.vcf.gz --INPUT [chr17_22813680-26627010.vcf.gz].sort.vcf.gz --INPUT [chr15_20729747-21193490.vcf.gz].sort.vcf.gz --INPUT [chr15_17000001-19725254.vcf.gz].sort.vcf.gz --INPUT [chr6_60001-58453888.vcf.gz].sort.vcf.gz --INPUT [chr10_47870369-124121200.vcf.gz].sort.vcf.gz --INPUT [chr22_11681289-11724629.vcf.gz].sort.vcf.gz --INPUT [chr9_41229379-41237752.vcf.gz].sort.vcf.gz --INPUT [chr20_29271827-29315342.vcf.gz].sort.vcf.gz --INPUT [chr11_88002897-135076622.vcf.gz].sort.vcf.gz --INPUT [chr7_61967064-61976104.vcf.gz].sort.vcf.gz --INPUT [chr16_33264596-33392411.vcf.gz].sort.vcf.gz --INPUT [chr3_10001-90565295.vcf.gz].sort.vcf.gz --INPUT [chr18_15791048-20564714.vcf.gz].sort.vcf.gz --INPUT [chrX_37285838-49348394.vcf.gz].sort.vcf.gz --INPUT [chr3_91256422-91257890.vcf.gz].sort.vcf.gz --INPUT [chr5_139453660-155760324.vcf.gz].sort.vcf.gz --INPUT [chr17_26698999-26720420.vcf.gz].sort.vcf.gz --INPUT [chr19_60001-24448980.vcf.gz].sort.vcf.gz --INPUT [chr22_12488691-12641730.vcf.gz].sort.vcf.gz --INPUT [chr13_86252980-111703855.vcf.gz].sort.vcf.gz --INPUT [chr1_143184588-223558935.vcf.gz].sort.vcf.gz --INPUT [chr21_10269869-10274327.vcf.gz].sort.vcf.gz --INPUT [chr1_122026460-124977944.vcf.gz].sort.vcf.gz --INPUT [chr6_61371373-95020790.vcf.gz].sort.vcf.gz --INPUT [chr15_84320067-101981189.vcf.gz].sort.vcf.gz --INPUT [chr8_60001-7617127.vcf.gz].sort.vcf.gz --INPUT [chrX_58605580-62412542.vcf.gz].sort.vcf.gz --INPUT [chr10_42066266-47780368.vcf.gz].sort.vcf.gz --INPUT [chr20_26436233-26590875.vcf.gz].sort.vcf.gz --INPUT [chr5_10001-17530548.vcf.gz].sort.vcf.gz --OUTPUT joint_germline.vcf.gz --SEQUENCE_DICTIONARY Homo_sapiens_assembly38.dict --TMP_DIR . --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX true --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false
  [Fri Feb 10 09:22:04 UTC 2023] Executing as root@26047e876e98 on Linux 3.10.0-862.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 11.0.13+7-b1751.21; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.3.0.0
  [Fri Feb 10 09:22:04 UTC 2023] picard.vcf.MergeVcfs done. Elapsed time: 0.18 minutes.
  Runtime.totalMemory()=2147483648
  To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp
  htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Not in GZIP format, for input source: file://%5Bchr4_31832570-32833016.vcf.gz%5D.sort.vcf.gz
  	at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:264)
  	at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:103)
  	at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:128)
  	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:121)
  	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:81)
  	at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:145)
  	at picard.vcf.MergeVcfs.doWork(MergeVcfs.java:182)
  	at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:309)
  	at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37)
  	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
  	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
  	at org.broadinstitute.hellbender.Main.main(Main.java:289)
  Caused by: java.util.zip.ZipException: Not in GZIP format
  	at java.base/java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:166)
  	at java.base/java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:80)
  	at java.base/java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:92)
  	at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:258)
  	... 11 more
```


### Relevant files

_No response_

### System information

_No response_",nagihayate,https://github.com/nf-core/sarek/issues/938
I_kwDOCvwIC85eQQT0,Download link for the configuration file,CLOSED,2023-02-12T16:23:21Z,2023-02-13T11:39:10Z,2023-02-13T07:25:28Z,"Hi, I want to ask a seemingly stupid question. Could you please provide the download link for the configuration file in the link below. Thank you very much!

https://github.com/nf-core/sarek/blob/master/conf/igenomes.config

G1000_alleles_hg19.zip G1000_loci_hg19.zip GC_G1000_hg19.zip RT_G1000_hg19.zip 

I need the above four files. Hope you can provide some help or clues. grateful～",songzhirui0o0,https://github.com/nf-core/sarek/issues/939
I_kwDOCvwIC85eUg3V,Igenomes download failed,CLOSED,2023-02-13T14:17:22Z,2023-03-08T16:19:34Z,2023-03-08T16:19:34Z,"Hi,

I used the following command and the attached samplesheet to test run sarek:
nextflow run sarek --input samplesheet.csv --outdir sarek_out --genome GATK.GRCh38 -profile docker

But I got the following error:
Unexpected error [UnsupportedOperationException]
-- Check script 'sarek/./workflows/sarek.nf' at line: 56 or see '.nextflow.log' file for more details

I attached error logs.
[.nextflow.log](https://github.com/nf-core/sarek/files/10723033/default.nextflow.log)


I’m not sure what went wrong. Please help.

Best wishes,

Olu
",olu2016,https://github.com/nf-core/sarek/issues/940
I_kwDOCvwIC85euTmq,Pipeline stalling,CLOSED,2023-02-17T11:15:05Z,2023-03-21T13:34:51Z,2023-03-21T13:34:51Z,"### Description of the bug

Trying to run the pipeline to call CNVs for WES data. This error comes up for multiple samples. It ultimately failed after no activity for 8h.

### Command used and terminal output

```console
nextflow run nf-core/sarek --input samples.csv --outdir results --genome GATK.GRCh38 -profile singularity -resume
```


### Relevant files

[Archive.zip](https://github.com/nf-core/sarek/files/10766077/Archive.zip)

Attached:
- log file
- samples.csv
- nextflow.config

### System information

- nextflow version 21.10.6.5660
- Hardware: [Crop Diversity HPC](https://help.cropdiversity.ac.uk/index.html); requested interactive session with 64cpus, 180Gb
- Executor: SLURM
- Container: Singularity
- OS: Linux
- sarek version: v3.1.2",kmarianski,https://github.com/nf-core/sarek/issues/943
I_kwDOCvwIC85euqVq,Add target capture performance via Picard HsMetrics module,OPEN,2023-02-17T12:33:09Z,2024-08-19T13:12:35Z,,"### Description of feature

If we have a covered (by probes) and target file, we can use Picard's CollectHsMetrics for target sequencing performance. 

Requires:
 - target and covered file in Picard Interval List format. 
 - (possible) requires GATK_BEDTOINTERVALLIST module to be ran on BED files.
 - Gather HsMetrics output and add to MultiQC.",adamrtalbot,https://github.com/nf-core/sarek/issues/944
I_kwDOCvwIC85e4ejP,DeepVariant: Mismatching input samples,CLOSED,2023-02-20T13:18:55Z,2023-02-21T12:42:15Z,2023-02-21T12:42:15Z,"### Description of the bug

When running Sarek pipeline it errors on the process ""MERGE_DEEPVARIANT_VCF"". I couldn't find any advises anywhere to fix this problem and what Error exit status (3) means.

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.1 \
--input ""/mnt/polkanowa2/nf-core_sarek_tests/input_1000genomes/samplesheet.csv"" \
--genome GATK.GRCh38 \
--igenomes_base ""/mnt/polkanowa2/nf-core_sarek_tests/references/"" \
-profile docker \
--max_cpus 100 \
--max_memory '400.GB' \
--max_time '2400.day' \
-c config_increased_time \
--vep_dbnsfp \
--vep_spliceregion \
--save_bam_mapped \
--save_trimmed \
--save_reference \
--tools vep,deepvariant,haplotypecaller \
--wes \
--outdir /mnt/polkanowa2/nf-core_sarek_tests/outdir_references_run_nextflow -with-report report.html \
-resume loving_joliot \
-ansi-log false -dump-channels


config:
process {
     time = 2400.day
     withName:'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:CNNSCOREVARIANTS'
}



OUTPUT:


[34/ff12df] Submitted process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:CNNSCOREVARIANTS (sample2)

Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_DEEPVARIANT:MERGE_DEEPVARIANT_VCF (sample2)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_DEEPVARIANT:MERGE_DEEPVARIANT_VCF (sample2)` terminated with an error exit status (3)

Command executed:

  gatk --java-options ""-Xmx4g"" MergeVcfs \
      --INPUT sample2.deepvariant.chr2_97489619-238903659.vcf.gz --INPUT sample2.deepvariant.chr12_7084651-34719407.vcf.gz --INPUT sample2.deepvariant.chr13_86252980-111703855.vcf.gz --I>
      --OUTPUT sample2.deepvariant.vcf.gz \
      --SEQUENCE_DICTIONARY Homo_sapiens_assembly38.dict \
      --TMP_DIR . \


  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_DEEPVARIANT:MERGE_DEEPVARIANT_VCF"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  3

Command output:
  (empty)

Command error:
  Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -jar /usr/loc>
  08:07:02.573 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compre>
  [Mon Feb 13 08:07:02 GMT 2023] MergeVcfs --INPUT sample2.deepvariant.chr2_97489619-238903659.vcf.gz --INPUT sample2.deepvariant.chr12_7084651-34719407.vcf.gz --INPUT sample2.deepvarian>
  [Mon Feb 13 08:07:02 GMT 2023] Executing as root@002a29a8c952 on Linux 5.4.0-132-generic amd64; OpenJDK 64-Bit Server VM 11.0.15-internal+0-adhoc..src; Deflater: Intel; Inflater: Intel>
  [Mon Feb 13 08:07:07 GMT 2023] picard.vcf.MergeVcfs done. Elapsed time: 0.09 minutes.
  Runtime.totalMemory()=696254464
  To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp
  java.lang.IllegalArgumentException: Input path sample1.deepvariant.chr20_30088349-30425128.vcf.gz has sample entries that don't match the other files.
        at picard.vcf.MergeVcfs.doWork(MergeVcfs.java:207)
        at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:309)
        at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37)
        at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
        at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
        at org.broadinstitute.hellbender.Main.main(Main.java:289)

Work dir:
  /mnt/polkanowa2/nf-core_sarek_tests/work/60/52a7fb896cfa380813a0083a58aef6

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`
```


### Relevant files

[igenomes_run_increased_memory_with_config_time_13Feb.log](https://github.com/nf-core/sarek/files/10784711/igenomes_run_increased_memory_with_config_time_13Feb.log)


### System information

N E X T F L O W  ~  version 21.10.6
Hardware: HPC
Executor: local
Container engine: Docker
OS: Linux Ubuntu 20.04.4 LTS
Version of nf-core/sarek: 3.1",sandragold,https://github.com/nf-core/sarek/issues/946
I_kwDOCvwIC85e9Zq_, mixing tumor-only and paired samples and having a tool that only works for paired samples causes the pipeline to fail,CLOSED,2023-02-21T09:45:15Z,2024-08-19T13:17:13Z,2024-08-19T13:17:13Z,"### Description of the bug

When specifying mixed samples (tumor-only and paired) and some tools do not work for tumor-only, like msisensorpro, the pipeline will fail.

Reported by @MolPath-Bioinfo on Slack https://nfcore.slack.com/archives/CGFUX04HZ/p1676972372078189?thread_ts=1676897338.792179&cid=CGFUX04HZ

I am guessing we went a bit overboard with validating the tools parameter on input sample sheet parsing.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/948
I_kwDOCvwIC85fEmsk,Running the annotation step does not work with the full samplesheet file described on the website or with the variantcalling.csv file generated by the pipeline in a previous step,CLOSED,2023-02-22T12:44:07Z,2023-05-29T11:46:44Z,2023-05-29T11:46:44Z,"### Description of the bug

Running only the **annotation step** using the csv file generated by the Sarek 3.1.2 pipeline in a previous run (variantcaller.csv) ends with the following error:

_`The sample-sheet only contains normal-samples, but the following tools, which were requested with ""--tools"", expect at least one tumor-sample : mutect2`_

The content of the variantcaller.csv corresponds to the full samplesheet described on the website and has the following format:
```
patient,sample,variantcaller,vcf
P1,H-2022-09665,mutect2,H-2022-09665.mutect2.filtered.vcf.gz
P1,H-2022-09665,strelka,H-2022-09665.strelka.variants.vcf.gz
```
**Solution**: adding the columns **sex** and **status** to the input file (directly after the **patient** column) solves the problem
```
patient,sex,status.sample,variantcaller,vcf
P1,XX,1,H-2022-09665,mutect2,H-2022-09665.mutect2.filtered.vcf.gz
P1,XX,1,H-2022-09665,strelka,H-2022-09665.strelka.variants.vcf.gz
```
The documentation on the website needs to be adapted and the 2 columns added to the full samplesheet example.

This behavior is not consistent in all steps. If I run the pipeline from the beginning with the tools 
`--tools mutect2,strelka,vep`
then the error does not occur.

The columns **sex** and **status** seems to be required in all cases when one runs the individual steps using the **--step** parameter. I encountered this also while running only the variant calling step.



### Command used and terminal output

```console
nextflow run /home/thiele/local_apps/nf-core-sarek-3.1.2/workflow -profile singularity -c ""/mnt/Molpath/users/mihaela/projects/variant_calling/WES/220728_A01542_0030_AHNCCMDRXY_3Aug_TWIST/VIP_tumorOnly/config/nf_wes.config"" -params-file ""/mnt/Molpath/users/mihaela/projects/variant_calling/WES/220728_A01542_0030_AHNCCMDRXY_3Aug_TWIST/VIP_tumorOnly/config/nf_params.json"" --step annotate --vep_species ""homo_sapiens"" --vep_version '106.1' --vep_include_fasta --vep_loftee --vep_spliceregion


The sample-sheet only contains normal-samples, but the following tools, which were requested with ""--tools"", expect at least one tumor-sample : mutect2
```


### Relevant files

_No response_

### System information

- System: Linux 5.4.0-136-generic Ubuntu
- nf-core/sarek 3.1.2
- executor: local
- hardware: Desktop
- nextflow version: 22.10.4 build 5836
- container engine: Singularity",MolPath-Bioinfo,https://github.com/nf-core/sarek/issues/949
I_kwDOCvwIC85fK4tX,More check for the samplesheet,CLOSED,2023-02-23T11:31:08Z,2023-03-29T13:39:13Z,2023-03-29T13:39:13Z,"### Description of feature

no space in sample or id",maxulysse,https://github.com/nf-core/sarek/issues/950
I_kwDOCvwIC85fK_03,CNVKIT_REFERENCE error,CLOSED,2023-02-23T11:51:42Z,2023-06-05T07:08:46Z,2023-06-05T07:08:45Z,"### Description of the bug

Dear All,
I am running sarek v 3.0.2, command as pasted below.
Seems like is not finding the referene genome.
Could you please assist me? 
Best


here is my output

```
-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:PREPARE_CNVKIT_REFERENCE:CNVKIT_REFERENCE (Homo_sapiens_assembly38.fasta)'

Caused by:
  Process `NFCORE_SAREK:SAREK:PREPARE_CNVKIT_REFERENCE:CNVKIT_REFERENCE (Homo_sapiens_assembly38.fasta)` terminated with an error exit status (1)

Command executed:

  cnvkit.py \
      reference \
      --fasta Homo_sapiens_assembly38.fasta \
      --targets Twist_Exome_RefSeq_targets_hg38.bed \
      --antitargets home.antitarget.bed \
      --output cnvkit.reference.cnn \
  
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:PREPARE_CNVKIT_REFERENCE:CNVKIT_REFERENCE"":
      cnvkit: $(cnvkit.py version | sed -e ""s/cnvkit v//g"")
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  Detected file format: bed
  Detected file format: bed
  Calculating GC and RepeatMasker content in Homo_sapiens_assembly38.fasta ...
  Traceback (most recent call last):
    File ""/usr/local/lib/python3.9/site-packages/pyfaidx/__init__.py"", line 369, in __init__
      self.file = self._fasta_opener(filename, 'r+b'
  FileNotFoundError: [Errno 2] No such file or directory: 'Homo_sapiens_assembly38.fasta'
  
  During handling of the above exception, another exception occurred:
  
  Traceback (most recent call last):
    File ""/usr/local/bin/cnvkit.py"", line 9, in <module>
      args.func(args)
    File ""/usr/local/lib/python3.9/site-packages/cnvlib/commands.py"", line 520, in _cmd_reference
      ref_probes = reference.do_reference_flat(args.targets, args.antitargets,
    File ""/usr/local/lib/python3.9/site-packages/cnvlib/reference.py"", line 30, in do_reference_flat
      gc, rmask = get_fasta_stats(ref_probes, fa_fname)
    File ""/usr/local/lib/python3.9/site-packages/cnvlib/reference.py"", line 461, in get_fasta_stats
      gc_rm_vals = [calculate_gc_lo(subseq)
    File ""/usr/local/lib/python3.9/site-packages/cnvlib/reference.py"", line 461, in <listcomp>
      gc_rm_vals = [calculate_gc_lo(subseq)
    File ""/usr/local/lib/python3.9/site-packages/cnvlib/reference.py"", line 487, in fasta_extract_regions
      with pyfaidx.Fasta(fa_fname, as_raw=True) as fa_file:
    File ""/usr/local/lib/python3.9/site-packages/pyfaidx/__init__.py"", line 996, in __init__
      self.faidx = Faidx(
    File ""/usr/local/lib/python3.9/site-packages/pyfaidx/__init__.py"", line 378, in __init__
      raise FastaNotFoundError(
  pyfaidx.FastaNotFoundError: Cannot read FASTA file Homo_sapiens_assembly38.fasta
```

### Command used and terminal output

```console
nextflow run nf-core/sarek --input $wd/samplesheet.csv --outdir $wd/outdir
--genome GATK.GRCh38 --only_paired_variant_calling TRUE \
--wes TRUE \
--intervals $panel/Twist_Exome_RefSeq_targets_hg38.bed \
--tools strelka,mutect2,msisensorpro,cnvkit,manta,snpeff,vep,merge -profile singularity \
--max_cpus $SLURM_NTASKS --save_output_as_bam TRUE \
-c $wd/nextflow.config -r 3.0.2
```


### Relevant files

_No response_

### System information

slurm
module load nextflow/22.10.4
module load graphviz/2.50.0
module load singularity/3.4.2",paolo-kunderfranco,https://github.com/nf-core/sarek/issues/951
I_kwDOCvwIC85fMvQN,Parallelisation of variant call across regions causes Manta to miss some variant classes,CLOSED,2023-02-23T16:33:26Z,2023-04-14T15:18:25Z,2023-04-14T15:18:25Z,"### Description of the bug

When an intervals bed file is provided, Sarek parallelises variant calling across regions. Unfortunately, in the case of SV calling this means that variants that cross multiple regions (for example interchromosomal translocations) appear to be missed.

Another issue with the parallelisation is that, when the variant calls are then merged for a sample, the IDs relating to each SV event are non-unique so it can make it difficult to identify the correct ""MateID"" for a translocation.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",lhonour,https://github.com/nf-core/sarek/issues/952
I_kwDOCvwIC85fQZ4M,Annotation not performed with hg19,CLOSED,2023-02-24T08:12:12Z,2023-03-02T09:09:23Z,2023-03-02T09:09:22Z,"### Description of the bug

VEP does not work with hg19 (at least,  other genomes has not been tested), neither when all pipeline is executed, nor when the annotation step is launched

### Command used and terminal output

```console
nextflow run nf-core/sarek -profile singularity --input results_Lpalomo_Wes_no_intervals/csv/variantcalled.csv  --genome hg19 -r 3.1.2 -c sarek.conf --outdir results_Lpalomo_Wes_no_intervals/ --tools vep -resume  --step annotate

and the sample sheet

patient,sample,vcf
WES_200209,WES_200209,results_Lpalomo_Wes_no_intervals/variant_calling/strelka/WES_200209/WES_200209.strelka.variants.vcf.gz
WES_200394,WES_200394,results_Lpalomo_Wes_no_intervals/variant_calling/strelka/WES_200394/WES_200394.strelka.variants.vcf.gz
WES_201085,WES_201085,results_Lpalomo_Wes_no_intervals/variant_calling/strelka/WES_201085/WES_201085.strelka.variants.vcf.gz
WES_201184,WES_201184,results_Lpalomo_Wes_no_intervals/variant_calling/strelka/WES_201184/WES_201184.strelka.variants.vcf.gz
WES_210490,WES_210490,results_Lpalomo_Wes_no_intervals/variant_calling/strelka/WES_210490/WES_210490.strelka.variants.vcf.gz
results_Lpalomo_Wes_no_intervals/csv/variantcalled.csv (END)
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/10821994/nextflow.log)


### System information

singularity --version
singularity-ce version 3ea83f6-dirty

Linux bioinf.vhio.org 5.4.0-91-generic #102-Ubuntu SMP Fri Nov 5 16:31:28 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux


",paumarc,https://github.com/nf-core/sarek/issues/953
I_kwDOCvwIC85fj1OO,Community feedback needed: Add tutorials/faqs describing different usages to the website,CLOSED,2023-02-28T14:55:29Z,2024-10-30T10:09:43Z,2024-10-30T10:09:41Z,"### Description of feature

We are looking into expanding the documentation by adding tutorials for different usages. For this we would love some community feedback: 

What are some questions you asked yourself when starting out? What would you like to see explained in more detail? Where do you find the documentation is lacking?

",FriederikeHanssen,https://github.com/nf-core/sarek/issues/955
I_kwDOCvwIC85fo_-L,Support for PureCN for estimating purity/ploidy,CLOSED,2023-03-01T09:21:27Z,2023-09-08T11:11:54Z,2023-03-01T10:15:47Z,"### Description of feature

As the subject says, [PureCN](https://github.com/lima1/PureCN) is a software for calling copy number and variants after estimating tumor purity and ploidy. In our experience it performs better than ASCAT with low-purity(< 40%) samples.

A typical PureCN analysis involves creating a panel of normals (PoN) which is then used as a reference to estimate purity/ploidy and from there CNV and SNV.  Currently there are no modules in nf-core to use PureCN (however it's in bioconda), so these would need to be done first.

I'm posting this to enquire whether there is interest, as if so, we are considering implementing this support ourselves.

cc @LMannarino @aldosr @rikizad",lbeltrame,https://github.com/nf-core/sarek/issues/956
I_kwDOCvwIC85fzwV7,Canfam4 genome,CLOSED,2023-03-02T19:56:19Z,2023-03-03T09:42:03Z,2023-03-03T09:42:02Z,"Hello,

Would it possible to add references and annotations for Canfam4 to perform somatic analysis? I can see canfan3.1 is already there but its very old now.

Thank you,
Keyur",KeyTals,https://github.com/nf-core/sarek/issues/958
I_kwDOCvwIC85f33bK,Use GATK small_exac_common_3.hg38.vcf.gz as default germline_resource,OPEN,2023-03-03T11:40:23Z,2023-03-03T13:17:46Z,,"### Description of feature

See https://github.com/broadinstitute/gatk/issues/7606 and #592.

In the GATK4 GetPileupSummaries code, the entire -V option populated by the sarek germline_resource parameter is read into memory. The current default for human hg38 is to use gnomad_af_only_hg38, which is huge and leads to Java heap out of memory errors. The request is to use the GATK file small_exac_common_3 file for this purpose instead. It's a subset of common variants found in gnomAD (https://gatk.broadinstitute.org/hc/en-us/community/posts/360067310872-How-to-find-or-generate-common-germline-variant-sites-VCF-required-by-GetPileupSummaries).",ameynert,https://github.com/nf-core/sarek/issues/959
I_kwDOCvwIC85gNDW7,Process exceeded running time limit with --max_time,CLOSED,2023-03-07T19:02:16Z,2023-03-11T20:38:48Z,2023-03-11T20:38:48Z,"### Description of the bug

When using haplotypecaller tool, the pipeline fails on the stage: 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:CNNSCOREVARIANTS'
and ignores the ""-max_time"" value in the main script or ""time"" set up in the config file.

It seems, that this parameter doesn't work?

### Command used and terminal output

```console
Command used:
nextflow run nf-core/sarek -r 3.1 \
--input ""/mnt/polkanowa2/nf-core_sarek_tests/input_1000genomes/samplesheet.csv"" \
--genome GATK.GRCh38 \
--igenomes_base ""/mnt/polkanowa2/nf-core_sarek_tests/references/"" \
-profile docker \
--max_cpus 100 \
--max_memory '400.GB' \
--max_time '3600.day' \
-c config_increased_time \
--vep_dbnsfp \
--vep_spliceregion \
--save_bam_mapped \
--save_trimmed \
--nucleotides_per_second 1500 \
--save_reference \
--tools vep,deepvariant,haplotypecaller \
--wes \
--outdir /mnt/polkanowa2/nf-core_sarek_tests/outdir -with-report report.html \
-ansi-log false -dump-channels

#########################################
Config file:
process {
     time = 3600.day
withName:'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:CNNSCOREVARIANTS'
}


######################################
Error:


[83/c302db] Submitted process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_DEEPVARIANT:MERGE_DEEPVARIANT_VCF (sample1)
[44/acbaac] Submitted process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:CNNSCOREVARIANTS (sample1)
[ca/a053a2] Submitted process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL (sample1)
[fa/fcb62f] Submitted process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS (sample1)
[45/ccadc3] Submitted process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:ENSEMBLVEP (sample1)
[6b/8f7e32] Submitted process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT (sample1)
[5c/356d78] Submitted process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY (sample1)
[f4/8960dd] Submitted process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:TABIX_BGZIPTABIX (sample1)
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:CNNSCOREVARIANTS (sample2)'

Caused by:
  Process exceeded running time limit (4h)

Command executed:

  gatk --java-options ""-Xmx12g"" CNNScoreVariants \
      --variant sample2.haplotypecaller.vcf.gz \
      --output sample2.cnn.vcf.gz \
      --reference Homo_sapiens_assembly38.fasta \
      --intervals wgs_calling_regions_noseconds.hg38.bed \
       \
       \
       \
      --tmp-dir . \


  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:VCF_VARIANT_FILTERING_GATK:CNNSCOREVARIANTS"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  -

Command output:
  (empty)

Command error:
  20:07:47.553 INFO  ProgressMeter -        chr8:81610486            229.6               2430000          10582.8
  20:07:59.954 INFO  ProgressMeter -        chr8:82847862            229.8               2432000          10582.0
  20:08:12.268 INFO  ProgressMeter -        chr8:84359094            230.0               2435000          10585.6
  20:08:24.326 INFO  ProgressMeter -        chr8:86157948            230.2               2437000          10585.1
  20:08:36.428 INFO  ProgressMeter -        chr8:87285201            230.4               2439000          10584.5
  20:08:48.525 INFO  ProgressMeter -        chr8:89045403            230.6               2441000          10583.9
...
  20:16:07.756 INFO  ProgressMeter -       chr8:138868734            238.0               2514000          10565.1
  20:16:20.051 INFO  ProgressMeter -       chr8:140043571            238.2               2516000          10564.4
  20:16:32.327 INFO  ProgressMeter -       chr8:141756234            238.4               2519000          10567.9
  20:16:44.314 INFO  ProgressMeter -       chr8:142656025            238.6               2521000          10567.4
  20:16:56.744 INFO  ProgressMeter -       chr8:143448771            238.8               2523000          10566.6
  20:17:08.926 INFO  ProgressMeter -       chr8:144730590            239.0               2525000          10566.0
  20:17:21.148 INFO  ProgressMeter -          chr9:682646            239.2               2527000          10565.4
  20:17:33.551 INFO  ProgressMeter -         chr9:1488919            239.4               2529000          10564.6
  20:17:45.835 INFO  ProgressMeter -         chr9:2583439            239.6               2531000          10563.9

Work dir:
  /mnt/polkanowa2/nf-core_sarek_tests/work/5e/270bc21ef45ac454d38380c5579503

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line


Execution cancelled -- Finishing pending tasks before exit
-^[[0;35m[nf-core/sarek]^[[0;31m Pipeline completed with errors^[[0m-
```


### Relevant files

[igenomes_run_increased_memory_with_config_time_28Feb.log](https://github.com/nf-core/sarek/files/10913078/igenomes_run_increased_memory_with_config_time_28Feb.log)


### System information

N E X T F L O W ~ version 21.10.6
Hardware: HPC
Executor: local
Container engine: Docker
OS: Linux Ubuntu 20.04.4 LTS
Version of nf-core/sarek: 3.1",sandragold,https://github.com/nf-core/sarek/issues/960
I_kwDOCvwIC85gRRgY,Info needed for --max_cpus,CLOSED,2023-03-08T11:49:58Z,2023-03-21T12:41:17Z,2023-03-21T12:41:17Z,"### Description of feature

I was wondering if there could be more information provided on max-cpus and max-memory? Right now I am trying to figure out whether I need to scale these depending on the number of samples in the sample sheet, or whether nextflow will handle allocations of memory and cpus to not pass processes for samples that can't be run yet?

Thanks!",TonyKess,https://github.com/nf-core/sarek/issues/961
I_kwDOCvwIC85gTo0d,Support for FilterSamReads,OPEN,2023-03-08T18:53:42Z,2023-03-08T18:53:42Z,,"### Description of feature

[FilterSamReads](https://gatk.broadinstitute.org/hc/en-us/articles/9570282347419-FilterSamReads-Picard-) can be used to subset a SAM/BAM/CRAM file. In some cases, it could be beneficial to keep only a subset of the alignment before the variant calling step. FilterSamReads also supports the usage of javascript where we can utilize SamRecord and SAMFileHeader from [htsjdk](https://github.com/samtools/htsjdk), which is quite flexible and powerful.

Thanks!
cc @maxulysse ",YeHW,https://github.com/nf-core/sarek/issues/962
I_kwDOCvwIC85gYm0g,controlfreec - Instance of 'std::length_error' of a genome,OPEN,2023-03-09T11:51:00Z,2023-03-21T12:47:23Z,,"### Description of the bug

My latest Sarek run on normal-tumor paired samples for one patient using some tools (freebayes,mutect2,strelka,tiddit,cnvkit,controlfreec,merge) ended up with an instance of 'std::length_error' related to  Homo_sapiens_assembly38.fasta.bed.

### Command used and terminal output

```console
cmd:

nextflow run -resume /AnalysisFolder/WES/SAREK2/sarek/main.nf \
        -profile vm03 \
        --input /AnalysisFolder/WES/SAREK2/sarek/samplesheet.csv \
        --outdir /AnalysisFolder/WES/SAREK2/sarek_out \
        --tools freebayes,mutect2,strelka,tiddit,cnvkit,controlfreec,merge \
        -with-conda true

Terminal output:

..Reading Homo_sapiens_assembly38.fasta.bed
  ..Your file must be in .BED format, and it must be sorted
  Number of exons analysed in chromosome HLA-DRB1*15:02:01 : 1
  Average exon length in chromosome HLA-DRB1*15:02:01 : 10313
  ..Reading Homo_sapiens_assembly38.fasta.bed
  ..Your file must be in .BED format, and it must be sorted
  Number of exons analysed in chromosome HLA-DRB1*15:03:01:01 : 1
  Average exon length in chromosome HLA-DRB1*15:03:01:01 : 11567
  ..Reading Homo_sapiens_assembly38.fasta.bed
  ..Your file must be in .BED format, and it must be sorted
  Number of exons analysed in chromosome HLA-DRB1*15:03:01:02 : 1
  Average exon length in chromosome HLA-DRB1*15:03:01:02 : 11569
  ..Reading Homo_sapiens_assembly38.fasta.bed
  ..Your file must be in .BED format, and it must be sorted
  Number of exons analysed in chromosome HLA-DRB1*16:02:01 : 1
  Average exon length in chromosome HLA-DRB1*16:02:01 : 11005
  ..[genomecopynumber] Starting reading tumor_sample_vs_normal_sample.normal.mpileup.gz
  ..finished reading tumor_sample_vs_normal_sample.normal.mpileup.gz
  terminate called after throwing an instance of 'std::length_error'
    what():  basic_string::_M_create
  .command.sh: line 60: 64724 Aborted                 (core dumped) freec -conf config.txt

Work dir:
  /AnalysisFolder/WES/SAREK2/work/72/a5ddf99e016e01cf77585832091f59

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`

/AnalysisFolder/WES/SAREK2/work/72/a5ddf99e016e01cf77585832091f59/.command.run
```


### Relevant files

[.nextflow.log](https://github.com/nf-core/sarek/files/10930981/default.nextflow.log)


### System information

Local Ubuntu machine",olu2016,https://github.com/nf-core/sarek/issues/963
I_kwDOCvwIC85gZ8ru,GATK4_GENOTYPEGVCFS error,OPEN,2023-03-09T14:55:34Z,2023-03-21T12:46:56Z,,"### Description of the bug

Dear All, I am facing a problem almost at the end of sarek version 3.1 for germline detection.

Could you please provide me assistance and help me to fix it?

Many Thanks


```
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOTYPEGVCFS (joint_variant_calling)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOTYPEGVCFS (joint_variant_calling)` terminated with an error exit status (3)

Command executed:

  gatk --java-options ""-Xmx72g"" GenotypeGVCFs \
      --variant gendb://chr15_75430187-75430375.joint \
      --output chr15_75430187-75430375.vcf.gz \
      --reference Homo_sapiens_assembly38.fasta \
       \
      --dbsnp dbsnp_146.hg38.vcf.gz \
      --tmp-dir . \
  
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOTYPEGVCFS"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  3

Command output:
  (empty)

Command error:
  09:40:19.358 INFO  ProgressMeter -         chr16:173125              4.4                  7000           1581.1
  09:40:54.996 INFO  ProgressMeter -         chr16:689239              5.0                  8000           1593.2
  09:41:18.531 INFO  ProgressMeter -        chr16:1228922              5.4                  9000           1notypeGVCFs - GCS max retries/reopens: 20
  09:35:52.270 INFO  GenotypeGVCFs - Requester pays: disabled
  09:35:52.270 INFO  GenotypeGVCFs - Initializing engine
  09:35:52.515 INFO  FeatureManager - Using codec VCFCodec to read file file://dbsnp_146.hg38.vcf.gz
  09:35:52.983 INFO  GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a
  10:35:53.420 info  NativeGenomicsDB - pid=50075 tid=50076 No valid combination operation found for INFO field DB  - the field will NOT be part of INFO fields in the generated VCF records
  10:35:53.420 info  NativeGenomicsDB - pid=50075 tid=50076 No valid combination operation found for INFO field InbreedingCoeff  - the field will NOT be part of INFO fields in the generated VCF records
  10:35:53.420 info  NativeGenomicsDB - pid=50075 tid=50076 No valid combination operation found for INFO field MLEAC  - the field will NOT be part of INFO fields in the generated VCF records
  10:35:53.420 info  NativeGenomicsDB - pid=50075 tid=50076 No valid combination operation found for INFO field MLEAF  - the field will NOT be part of INFO fields in the generated VCF records
  09:35:53.574 WARN  IndexUtils - Feature file ""file://dbsnp_146.hg38.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file
  09:35:53.632 WARN  IntelInflater - Zero Bytes Written : 0
  09:35:53.656 INFO  GenotypeGVCFs - Done initializing engine
  09:35:53.715 INFO  ProgressMeter - Starting traversal
  09:35:53.715 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute
  09:35:57.795 WARN  InbreedingCoeff - InbreedingCoeff will not be calculated at position chr15:75617462 and possibly subsequent; at least 10 samples must have called genotypes
  09:36:57.808 INFO  ProgressMeter -       chr15:80404516              1.1                  1000            936.1
  09:37:27.218 INFO  ProgressMeter -       chr15:82810185              1.6                  2000           1283.4
  09:37:43.219 INFO  ProgressMeter -       chr15:84240410              1.8                  3000           1643.8
  09:38:14.300 INFO  ProgressMeter -       chr15:88516787              2.3                  4000           1707.2
  09:39:01.694 INFO  ProgressMeter -       chr15:90762967              3.1                  5000           1595.9
  09:39:56.454 INFO  ProgressMeter -      chr15:100880106              4.0                  6000           1483.1
  09:40:19.358 INFO  ProgressMeter -         chr16:173125              4.4                  7000           1581.1
  09:40:54.996 INFO  ProgressMeter -         chr16:689239              5.0                  8000           1593.2
  09:41:18.531 INFO  ProgressMeter -        chr16:1228922              5.4                  9000           1662.5
  09:41:45.137 INFO  ProgressMeter -        chr16:1614825              5.9                 10000           1707.4
  09:42:25.324 INFO  ProgressMeter -        chr16:2135576              6.5                 11000           1685.4
  09:42:34.686 INFO  GenotypeGVCFs - Shutting down engine
  GENOMICSDB_TIMER,GenomicsDB iterator next() timer,Wall-clock time(s),0.2129274370000011,Cpu time(s),0.12280140800000039
  [March 9, 2023 at 9:42:34 AM GMT] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 6.71 minutes.
  Runtime.totalMemory()=3774873600
  java.lang.RuntimeException: GenomicsDB JNI Error: vector::_M_default_append
  	at org.genomicsdb.reader.GenomicsDBFeatureIterator.hasNext(GenomicsDBFeatureIterator.java:183)
  	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:132)
  	at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
  	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
  	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
  	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
  	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
  	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
  	at java.base/java.util.stream.ReferencePipeline.forEachOrdered(ReferencePipeline.java:502)
  	at org.broadinstitute.hellbender.engine.VariantLocusWalker.traverse(VariantLocusWalker.java:132)
  	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1095)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211)
  	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
  	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
  	at org.broadinstitute.hellbender.Main.main(Main.java:289)

Work dir:
  /home/pkf/gendata2/bioinformatica/pkunderfranco/HSR/nfcore_germline/work/8a/d79afc6faf08b26227cf24d9a4204b

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

```

### Command used and terminal output

```console
nextflow run nf-core/sarek -profile singularity -c $wd/nextflow.config --input $wd/samplesheet/HUM.csv --outdir $wd/outdir_HUM_paired --genome GATK.GRCh38 --only_paired_variant_calling FALSE --wes TRUE --joint_germline TRUE --intervals $panel/Twist_Exome_RefSeq_targets_hg38.bed --tools haplotypecaller,manta,snpeff,vep,merge --save_output_as_bam TRUE -r 3.1 -resume
```


### Relevant files

[logfile.txt](https://github.com/nf-core/sarek/files/10932821/logfile.txt)


### System information

nextflow/22.10.4
graphviz/2.50.0
singularity/3.4.2
slurm",paolo-kunderfranco,https://github.com/nf-core/sarek/issues/964
I_kwDOCvwIC85g9XGJ,Joint Genotype Calling Recalibrated VCFs Duplicate Entries; VEP is (Intentionally) Eating Variants ,CLOSED,2023-03-16T04:37:14Z,2023-08-16T19:26:31Z,2023-08-16T19:26:31Z,"Hi all, I encountered some unexpected results when testing the GATK joint genotype calling pipeline on my dataset. 

1. The number of variants in the final recalibrated VCF (`joint_germline_recalibrated.vcf.gz`) is exactly double that in the raw VCF (`joint_germline.vcf.gz`) outputted from GenotypeGVCFs. This is because all variants are completely duplicated due to merging separately recalibrated SNP and indel VCFs, the entries are also not identical as shown below. The fix is to sequentially recalibrate for SNPs then indels, or vice versa, on the same VCF. 

Raw VCF counts and example entries:
```
$ bcftools +counts joint_germline.vcf.gz
Number of samples: 15
Number of SNPs:    13717272
Number of INDELs:  3503929
Number of MNPs:    0
Number of others:  0
Number of sites:   17113477

#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT
1       10146   rs375931351     AC      A       102.67  .       AC=3;AF=0.1;AN=30;BaseQRankSum=0.967;DB;DP=698;ExcessHet=0;FS=0;InbreedingCoeff=0.5022;MLEAC=2;MLEAF=0.067;MQ=40.97;MQRankSum=0.967;QD=14.67;ReadPosRankSum=0.967;SOR=0.223     GT:AD:DP:GQ:PL
1       10327   rs112750067     T       C       106.77  .       AC=2;AF=0.067;AN=30;DB;DP=280;ExcessHet=0;FS=0;InbreedingCoeff=0.214;MLEAC=4;MLEAF=0.133;MQ=30.71;QD=25.36;SOR=2.833    GT:AD:DP:GQ:PGT:PID:PL:PS
```

Recalibrated VCF counts and example entries:
```
$ bcftools +counts joint_germline_recalibrated.vcf.gz
Number of samples: 15
Number of SNPs:    27434544
Number of INDELs:  7007858
Number of MNPs:    0
Number of others:  0
Number of sites:   34226954

#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT
1       10146   rs375931351     AC      A       102.67  PASS    AC=3;AF=0.1;AN=30;BaseQRankSum=0.967;DB;DP=698;ExcessHet=0;FS=0;InbreedingCoeff=0.5022;MLEAC=2;MLEAF=0.067;MQ=40.97;MQRankSum=0.967;NEGATIVE_TRAIN_SITE;POSITIVE_TRAIN_SITE;QD=14.67;ReadPosRankSum=0.967;SOR=0.223;VQSLOD=-9.728e-01;culprit=DP        GT:AD:DP:GQ:PL
1       10146   rs375931351     AC      A       102.67  .       AC=3;AF=0.1;AN=30;BaseQRankSum=0.967;DB;DP=698;ExcessHet=0;FS=0;InbreedingCoeff=0.5022;MLEAC=2;MLEAF=0.067;MQ=40.97;MQRankSum=0.967;QD=14.67;ReadPosRankSum=0.967;SOR=0.223     GT:AD:DP:GQ:PL
1       10327   rs112750067     T       C       106.77  .       AC=2;AF=0.067;AN=30;DB;DP=280;ExcessHet=0;FS=0;InbreedingCoeff=0.214;MLEAC=4;MLEAF=0.133;MQ=30.71;QD=25.36;SOR=2.833    GT:AD:DP:GQ:PGT:PID:PL:PS
1       10327   rs112750067     T       C       106.77  VQSRTrancheSNP99.90to100.00     AC=2;AF=0.067;AN=30;DB;DP=280;ExcessHet=0;FS=0;InbreedingCoeff=0.214;MLEAC=4;MLEAF=0.133;MQ=30.71;NEGATIVE_TRAIN_SITE;POSITIVE_TRAIN_SITE;QD=25.36;SOR=2.833;VQSLOD=-1.242e+00;culprit=SOR      GT:AD:DP:GQ:PGT:PID:PL:PS
```

2. I used the `--merge` annotation method and the final annotated VCF (`joint_germline_recalibrated_snpEff_VEP.ann.vcf.gz`) is missing almost half of the variants compared to the un-annotated VCFs, which was pretty alarming. After tracing the changes to the VCF it appears that VEP is intentionally filtering out common variants via the `--filter_common` external argument, which removes alleles with global AF > 0.01. While this is not a bug per-se, it would be helpful to make this an optional argument or at least document it somewhere to prevent scares and confusion. 

Variant counts following through the annotation pipeline: 
```
$ bcftools +counts joint_germline_snpEff.ann.vcf.gz
Number of samples: 15
Number of SNPs:    13717272
Number of INDELs:  3503929
Number of MNPs:    0
Number of others:  0
Number of sites:   17113477

$ bcftools +counts joint_germline_snpEff_VEP.ann.vcf.gz
Number of samples: 15
Number of SNPs:    5954775
Number of INDELs:  3370807
Number of MNPs:    0
Number of others:  0
Number of sites:   9223960
```
",evanwu1119,https://github.com/nf-core/sarek/issues/966
I_kwDOCvwIC85hK7Ax,Pipeline completed with errors : ControlFREEC,CLOSED,2023-03-18T09:53:22Z,2024-08-30T11:38:39Z,2023-03-22T09:17:09Z,"### Description of the bug

Dear SAREK Team,
I want to use Somatic variant calling, but pipeline completed with errors.
Thanks in advance for your help.

Best regards,

### Command used and terminal output

```console
nextflow run nf-core/sarek  --step variant_calling --input samplesheet.csv --outdir results/ --genome GATK.GRCh37 -profile docker --wes --intervals /home/user1/Target-panel/dataSet/data_run/tar_bla_cancer2.bed --tools cnvkit,controlfreec --only_paired_variant_calling true --max_cpus 7

-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_CONTROLFREEC:FREEC_SOMATIC (tumor_3468_S15_1_vs_normal_3468_S15_1)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_CONTROLFREEC:FREEC_SOMATIC (tumor_3468_S15_1_vs_normal_3468_S15_1)` terminated with an error exit status (1)

Command executed:

  touch config.txt
  
  echo ""[general]"" >> config.txt
  echo BedGraphOutput = TRUE >> config.txt
  echo breakPointThreshold = 1.2 >> config.txt
  echo breakPointType = 4 >> config.txt
  echo chrFiles =${PWD}/Chromosomes >> config.txt
  echo chrLenFile = ${PWD}/human_g1k_v37_decoy.fasta.fai >> config.txt
  echo coefficientOfVariation = 0.05 >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo forceGCcontentNormalization = 1 >> config.txt
  echo  >> config.txt
  echo gemMappabilityFile = ${PWD}/out100m2_hg19.gem >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo minimalSubclonePresence = 30 >> config.txt
  echo ""maxThreads = 2"" >> config.txt
  echo noisyData = TRUE >> config.txt
  echo  >> config.txt
  echo ploidy = 2 >> config.txt
  echo printNA = FALSE >> config.txt
  echo readCountThreshold = 50 >> config.txt
  echo sex = XY >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  
  echo ""[control]"" >> config.txt
  echo mateFile = ${PWD}/tumor_3468_S15_1_vs_normal_3468_S15_1.normal.mpileup.gz >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo inputFormat = pileup >> config.txt
  echo mateOrientation = FR >> config.txt
  
  echo ""[sample]"" >> config.txt
  echo mateFile = ${PWD}/tumor_3468_S15_1_vs_normal_3468_S15_1.tumor.mpileup.gz >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo inputFormat = pileup >> config.txt
  echo mateOrientation = FR >> config.txt
  
  echo ""[BAF]"" >> config.txt
  echo  >> config.txt
  echo fastaFile = ${PWD}/human_g1k_v37_decoy.fasta >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo  >> config.txt
  echo SNPfile = $PWD/dbsnp_138.b37.vcf.gz >> config.txt
  
  echo ""[target]"" >> config.txt
  echo captureRegions = tar_bla_cancer2.bed >> config.txt
  
  freec -conf config.txt
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_CONTROLFREEC:FREEC_SOMATIC"":
      controlfreec: $(echo $(freec -version 2>&1) | sed 's/^.*Control-FREEC  //; s/:.*$//' | sed -e ""s/Control-FREEC v//g"" )
  END_VERSIONS

Command exit status:
  1

Command output:
  Control-FREEC v11.6 : a method for automatic detection of copy number alterations, subclones and for accurate estimation of contamination and main ploidy using deep-sequencing data
  Multi-threading mode using 2 threads
  ..consider the sample being male
  ..Breakpoint threshold for segmentation of copy number profiles is 1.2
  ..telocenromeric set to 50000
  ..FREEC is not going to adjust profiles for a possible contamination by normal cells
  ..Coefficient Of Variation set equal to 0.05
  ..it will be used to evaluate window size
  ..Output directory:	.
  ..Directory with files containing chromosome sequences:	Chromosomes
  ..Sample file:	tumor_3468_S15_1_vs_normal_3468_S15_1.tumor.mpileup.gz
  ..Sample input format:	pileup
  ..Control file:	tumor_3468_S15_1_vs_normal_3468_S15_1.normal.mpileup.gz
  ..Input format for the control file:	pileup
  ..forceGCcontentNormalization was set to 1: will use GC-content to normalize the read count data
  ..minimal expected GC-content (general parameter ""minExpectedGC"") was set to 0.35
  ..maximal expected GC-content (general parameter ""maxExpectedGC"") was set to 0.55
  ..Polynomial degree for ""ReadCount ~ GC-content"" normalization is 3 or 4: will try both
  ..Minimal CNA length (in windows) is 3
  ..File with chromosome lengths:	human_g1k_v37_decoy.fasta.fai
  ..File human_g1k_v37_decoy.fasta.fai was read

Command error:
  For example, you can remove chromosome GL000217.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000216.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000216.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000215.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000215.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000205.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000205.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000219.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000219.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000224.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000224.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000223.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000223.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000195.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000195.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000212.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000212.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000222.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000222.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000200.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000200.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000193.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000193.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000194.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000194.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000225.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000225.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome GL000192.1 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome GL000192.1 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome NC_007605 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome NC_007605 from your human_g1k_v37_decoy.fasta.fai
  Error: chromosome hs37d5 present in your human_g1k_v37_decoy.fasta.fai file was not detected in your file with capture regions tar_bla_cancer2.bed
  Please solve this issue and rerun Control-FREEC
  For example, you can remove chromosome hs37d5 from your human_g1k_v37_decoy.fasta.fai
  Will exit

Work dir:
  /home/user1/nf-core/work/95/aeb42ee48ac9481504cef7bbd311f2

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/11008112/nextflow.log)
[samplesheet.csv](https://github.com/nf-core/sarek/files/11008115/samplesheet.csv)
[tar_bla_cancer2.bed.tar.gz](https://github.com/nf-core/sarek/files/11008174/tar_bla_cancer2.bed.tar.gz)


### System information

CPU: Intel(R) Xeon(R) CPU E5-1620 v3 @ 3.50GHz
RAM: 32 GB
Distribution: Ubuntu 22.04.1 LTS",Nour-EddineS,https://github.com/nf-core/sarek/issues/970
I_kwDOCvwIC85hTt32,Help with understanding & manipulate CNVkit output,OPEN,2023-03-20T17:26:10Z,2023-03-21T12:44:08Z,,"### Description of the bug

Dear All,

I need help to understand CNVkit output.
I'm looking to get the copy number of each region found by CNVkit, along with their type (loss or gain), thing like that : 

```
#CHRO #START #END #GENE #CNV
chr1 12345 23575 NRAS loss
chr2 4326 45887 TPM3 gain

```
My CNVkit results:
[CNVkit_Output.tar.gz](https://github.com/nf-core/sarek/files/11021164/CNVkit_Output.tar.gz)


Best regards,

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",VironicaZ,https://github.com/nf-core/sarek/issues/972
I_kwDOCvwIC85hZJbh,Add --step controlfreec back in,OPEN,2023-03-21T13:56:01Z,2023-03-21T13:56:01Z,,"### Description of feature

Aslo enable starting directly from the pileup files as we had in sarek 2.7 when running controlfreec",FriederikeHanssen,https://github.com/nf-core/sarek/issues/973
I_kwDOCvwIC85hZV-L,Can you add the trimming step?,CLOSED,2023-03-21T14:19:49Z,2023-03-21T14:40:40Z,2023-03-21T14:40:39Z,"### Description of feature

Hi, thank you for bulding this great pipeline. I have a small queation: why there isn't the trimming step before mapping? Is it necessary for general WGS or WES analysis? Can you add that step in your pipeline? Thank you.",JiahuaQu,https://github.com/nf-core/sarek/issues/974
I_kwDOCvwIC85hZXrv,Collection of issues regarding joining on metamaps ,CLOSED,2023-03-21T14:23:14Z,2023-06-14T06:23:49Z,2023-06-14T06:23:30Z,"### Description of feature

We have experienced several issues with respect to how we use meta maps. Some of them should hipefully be fixed on dev,
however there is still a few remaining that seem related:

@BrunoGrandePhD found that FILTERMUTECTCALLS is only run sporadically. Can confirm this, on the same input data sometimes it is run and sometimes not. `failOnDuplicate` and `failOnMismatch` were added to get better error messages and see what is happening. so far we know that the `join` operator then fails due to mismatches. So `join`ing on metamaps seems to be a bad idea. We could either just not merge on meta maps and extract the `id` anywhere we need to join. Although the issue seems to occur in particular with mutect2 (& deepvariant), so it may be just something in that subworkflow. Extracting the `id`s also seem to have some pitfalls, which makes me think that really something else is also going on
For the detailed discussion also see: https://nfcore.slack.com/archives/CGFUX04HZ/p1677742801147389",FriederikeHanssen,https://github.com/nf-core/sarek/issues/975
I_kwDOCvwIC85hZh12,Clean up the schema file,CLOSED,2023-03-21T14:43:47Z,2024-10-29T08:28:52Z,2024-10-29T08:28:51Z,"### Description of feature

Looking through the schema file recently I noticed that not all variables received the same love when it comes to adding a useful description or being consistent whether something is hidden or not. This would be good to clean up.",FriederikeHanssen,https://github.com/nf-core/sarek/issues/976
I_kwDOCvwIC85hkqFJ,sarek v3.1.2 -- error for ascat,OPEN,2023-03-23T07:34:01Z,2024-10-31T22:49:19Z,,"I have encountered the following error. I have tried both with out without --save_output_as_bam. It didn't help. I also ensured sufficient memory (256G) is available.

Thank you!

```
_-[nf-core/sarek] Pipeline completed with errors-
[27/e13d07] NOTE: Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (CGMH-FH-RCC1T_vs_CGMH-FH-RCC1N)` terminated with an error exit status (1) -- Execution is retried (7)
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (CGMH-FH-RCC1T_vs_CGMH-FH-RCC1N)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (CGMH-FH-RCC1T_vs_CGMH-FH-RCC1N)` terminated with an error exit status (1)

Command executed:

  #!/usr/bin/env Rscript
  library(RColorBrewer)
  library(ASCAT)
  options(bitmapType='cairo')
  
  #build prefixes: <abspath_to_files/prefix_chr>
  allele_path = normalizePath(""G1000_alleles_hg19"")
  allele_prefix = paste0(allele_path, ""/"", ""G1000_alleles_hg19"", ""_chr"")
  
  loci_path = normalizePath(""G1000_loci_hg19"")
  loci_prefix = paste0(loci_path, ""/"", ""G1000_loci_hg19"", ""_chr"")
  
  #prepare from BAM files
  ascat.prepareHTS(
      tumourseqfile = ""T.converted.cram"",
      normalseqfile = ""N.converted.cram"",
      tumourname = paste0(""T_vs_N"", "".tumour""),
      normalname = paste0(""T_vs_N"", "".normal""),
      allelecounter_exe = ""alleleCounter"",
      alleles.prefix = allele_prefix,
      loci.prefix = loci_prefix,
      gender = ""XY"",
      genomeVersion = ""hg19"",
      nthreads = 16
      ,minCounts = 10
      ,BED_file = 'wgs_calling_regions_Sarek.list'
      ,chrom_names = c(1:22, 'X', 'Y')
      ,min_base_qual = 20
      ,min_map_qual = 35
      ,ref.fasta = 'human_g1k_v37_decoy.fasta'
  
  
  )
  
  
  #Load the data
  ascat.bc = ascat.loadData(
      Tumor_LogR_file = paste0(""T_vs_N"", "".tumour_tumourLogR.txt""),
      Tumor_BAF_file = paste0(""T_vs_N"", "".tumour_tumourBAF.txt""),
      Germline_LogR_file = paste0(""T_vs_N"", "".tumour_normalLogR.txt""),
      Germline_BAF_file = paste0(""T_vs_N"", "".tumour_normalBAF.txt""),
      genomeVersion = ""hg19"",
      gender = ""XY""
  )
  
  #Plot the raw data
  ascat.plotRawData(ascat.bc, img.prefix = paste0(""T_vs_N"", "".before_correction.""))
  
  # optional LogRCorrection
  if(""GC_G1000_hg19"" != ""NULL"") {
      gc_input = paste0(normalizePath(""GC_G1000_hg19""), ""/"", ""GC_G1000_hg19"", "".txt"")
  
      if(""RT_G1000_hg19"" != ""NULL""){
          rt_input = paste0(normalizePath(""RT_G1000_hg19""), ""/"", ""RT_G1000_hg19"", "".txt"")
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = rt_input)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""T_vs_N"", "".after_correction_gc_rt.""))
      }
      else {
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = RT_G1000_hg19)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""T_vs_N"", "".after_correction_gc.""))
      }
  }
  
  #Segment the data
  ascat.bc = ascat.aspcf(ascat.bc)
  
  #Plot the segmented data
  ascat.plotSegmentedData(ascat.bc)
  
  #Run ASCAT to fit every tumor to a model, inferring ploidy, normal cell contamination, and discrete copy numbers
  #If psi and rho are manually set:
  if (!is.null(NULL) && !is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL, psi_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, psi_manual=NULL)
  } else {
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1)
  }
  
  #Extract metrics from ASCAT profiles
  QC = ascat.metrics(ascat.bc,ascat.output)
  
  #Write out segmented regions (including regions with one copy of each allele)
  write.table(ascat.output[[""segments""]], file=paste0(""T_vs_N"", "".segments.txt""), sep="" "", quote=F, row.names=F)
  
  #Write out CNVs in bed format
  cnvs=ascat.output[[""segments""]][2:6]
  write.table(cnvs, file=paste0(""T_vs_N"","".cnvs.txt""), sep=""    "", quote=F, row.names=F, col.names=T)
  
  #Write out purity and ploidy info
  summary <- tryCatch({
          matrix(c(ascat.output[[""aberrantcellfraction""]], ascat.output[[""ploidy""]]), ncol=2, byrow=TRUE)}, error = function(err) {
              # error handler picks up where error was generated
              print(paste(""Could not find optimal solution:  "",err))
              return(matrix(c(0,0),nrow=1,ncol=2,byrow = TRUE))
      }
  )
  colnames(summary) <- c(""AberrantCellFraction"",""Ploidy"")
  write.table(summary, file=paste0(""T_vs_N"","".purityploidy.txt""), sep="" "", quote=F, row.names=F, col.names=T)
  
  write.table(QC, file=paste0(""T_vs_N"", "".metrics.txt""), sep=""  "", quote=F, row.names=F)
  
  # version export
  f <- file(""versions.yml"",""w"")
  alleleCounter_version = system(paste(""alleleCounter --version""), intern = T)
  ascat_version = sessionInfo()$otherPkgs$ASCAT$Version
  writeLines(paste0('""', ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT"", '""', "":""), f)
  writeLines(paste(""    alleleCounter:"", alleleCounter_version), f)
  writeLines(paste(""    ascat:"", ascat_version), f)
  close(f)

Command exit status:
  1

Command output:
  (empty)

Command error:
  Done reading locis
  Multi pos start:
  Reading locis
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  Reading locis
  Done reading locis
  Multi pos start:
  munmap_chunk(): invalid pointer
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error   <<..............>>

Work dir:
  <<..............>>

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`_
```

",PyGuan,https://github.com/nf-core/sarek/issues/977
I_kwDOCvwIC85h90io,Process requirement exceeds available memory: mutect2,CLOSED,2023-03-28T09:55:42Z,2023-03-28T13:37:18Z,2023-03-28T13:37:18Z,"### Description of the bug

Dear All,
I can see that the process requirement exceeds the available memory, the command is launched with the option: --java-options ""**-Xmx36g**"". How can I adapt it according to the availability of my memory?

Best regards,

### Command used and terminal output

```console
nextflow run /home/nf-core/sarek --step variant_calling --input results/csv/recalibrated.csv --outdir results/ --genome GATK.GRCh37 -profile docker --wes --intervals targeted.bed --tools mutect2,strelka2 --only_paired_variant_calling true --max_cpus 7
 
Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED (tumor_3513_S1_vs_normal_3513_S1)'

Caused by:
  Process requirement exceeds available memory -- req: 36 GB; avail: 31.3 GB

Command executed:

  gatk --java-options ""-Xmx36g"" Mutect2 \
      --input normal_3513_S1.converted.cram --input tumor_3513_S1.converted.cram \
      --output tumor_3513_S1_vs_normal_3513_S1.mutect2.vcf.gz \
      --reference human_g1k_v37_decoy.fasta \
       \
      --germline-resource af-only-gnomad.raw.sites.vcf.gz \
      --intervals chr1_27022879-27024037.bed \
      --tmp-dir . \
      --f1r2-tar-gz tumor_3513_S1_vs_normal_3513_S1.mutect2.f1r2.tar.gz --normal-sample 3513_S1_normal_3513_S1
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  -

Command output:
  (empty)

Work dir:
  /home/work/data_reel/nfCore/preprocessing/work/cc/7942b97a486f9cf1d206d167ca9

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line
```


### Relevant files

_No response_

### System information

_No response_",Nour-EddineS,https://github.com/nf-core/sarek/issues/979
I_kwDOCvwIC85iOPK5,Fix caching for tests on custom runners,CLOSED,2023-03-30T15:58:25Z,2023-05-25T19:30:58Z,2023-05-25T19:30:58Z,"### Description of the bug

Path is not the same on custom runners, but cache is carried over.
@adamrtalbot found a fix:

cf https://github.com/nf-core/rnaseq/pull/987

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/983
I_kwDOCvwIC85iPoxK,Need to document snpeff_cache/vep_cache clashes with snpeff_version/vep_version,OPEN,2023-03-30T20:27:43Z,2023-03-31T08:01:19Z,,"### Description of feature

When using 

```
nextflow run nf-core/sarek -r 3.1.1 -profile cfc \
--input 'sampleSheet_sarek.csv' \
--genome null \
--save_reference \
--fasta './Synechocystis_sp_pcc_6803_gca_000009725.ASM972v1.dna.toplevel.fa' \
--skip_tools baserecalibrator \
--split_fastq 0 \
--igenomes_ignore \
--snpeff_cache './snpeff_cache' \
--tools 'freebayes,snpeff,vep' \
--snpeff_version '5.0-1' \
--annotation_cache true \
--snpeff_db 'Synechocystis_sp_pcc_6803_substr_gt_i' \
--vep_cache './vep_cache' \
--vep_genome 'Synechocystis_sp_pcc_6803_gca_000009725' \
-c custom.config \
--outdir 'results_sarek' \
-resume
```

The snpeff container was not actually updated but still container 5.1 was used",FriederikeHanssen,https://github.com/nf-core/sarek/issues/984
I_kwDOCvwIC85iSMxe,Singularity related-problems,CLOSED,2023-03-31T08:32:57Z,2023-06-07T06:11:49Z,2023-06-07T06:11:49Z,"### Description of the bug

I am using the pipeline sarek in my local PC for much time but now I find an error. I cannot run any sample neither any basic test.
My error happens when the pipeline is beginning. I am sure that is for singularity, but I try to install an updated version and reinstall my older version but the problem is not solved.

Please, can somebody helps me? 
 

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.1.2 -profile test,singularity
or
nextflow run ./workflow/main.nf -profile test,singularity

My output is:
Error executing process > 'NFCORE_SAREK:SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED (genome)'

Caused by:
  Process `NFCORE_SAREK:SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED (genome)` terminated with an error exit status (255)

Command executed:

  gatk --java-options ""-Xmx6g"" IntervalListToBed \
      --INPUT genome.interval_list \
      --OUTPUT genome.bed \
      --TMP_DIR . \
  
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:PREPARE_INTERVALS:GATK4_INTERVALLISTTOBED"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  255

Command output:
  (empty)

Command error:
  FATAL:   container creation failed: mount /proc/self/fd/3->/usr/local/var/singularity/mnt/session/rootfs error: while mounting image /proc/self/fd/3: failed to find loop device: could not attach image file to loop device: no loop devices available

Work dir:
  /home/jaume/Escritorio/nf-core-sarek-3.1.2/work/4a/55152df9acac9853ec372b4d6a7b12

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line
```


### Relevant files

_No response_

### System information

I am using: 
Nextflow version 22.10.7 build 5853 
nf-core v. 2.7.2
Singularity version 3.5.3 (I try 3.11 but the problem persist and I re-install this version)
java version 11.0.18
OS: Ubuntu 2020",jbague,https://github.com/nf-core/sarek/issues/986
I_kwDOCvwIC85iSn6a,Add Illumina/Hap.py module for germline samples with truth set.,CLOSED,2023-03-31T09:47:41Z,2024-02-15T09:53:33Z,2024-02-15T09:53:32Z,"### Description of feature

First step to delivering validation feature as discussed in #798 

Add the [happy](https://github.com/nf-core/modules/tree/master/modules/nf-core/happy/happy) module which runs when:
 - A truth set is supplied via the samplesheet (additional column required)
 - `--tools happy` is included in commmand
 - The sample has had germline short variant calling performed on it (e.g. haplotypecaller, freebayes)

Out of scope: Somatic calling, CNVs, structural variants, reporting of variants for some final validation step. 

Running the module should include the [stratifications]( https://github.com/genome-in-a-bottle/genome-stratifications) and write sample to the `--outdir`. Results should be passed to multiqc for [inclusion in the report](https://multiqc.info/docs/#hap.py).",adamrtalbot,https://github.com/nf-core/sarek/issues/987
I_kwDOCvwIC85i1NFe,Add optional capability of adding the BED file into samplesheet,OPEN,2023-04-06T22:17:49Z,2023-04-06T22:17:49Z,,"### Description of feature

Feature:
Add one additional column in samplesheet where the <path_to_BED_file> will be specified.

Reason for request:
While processing WES or TS files, it is recommended to use BED files. Since BED files can be different for different samples, it would be great to add this information directly in the samplesheet, where sample to BED file correlation can be maintained. It also consolidates all inputs required from a human user into one samplesheet, making it easier to deploy the pipelines using scripts.

Current use case:
I have a list of WES files where one batch used BED_file_A, and the other used BED_file_B. Currently, I am splitting up the samplesheet into two (one for each BED file). Then I am deploying two runs of sarek, one for each of the samplesheets and manually adding each BEDfile to respective runs using the parameter `--interval` .",jaybee84,https://github.com/nf-core/sarek/issues/989
I_kwDOCvwIC85i2fY1,"fgbio CallMolecularConsensusReads Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space",CLOSED,2023-04-07T07:00:01Z,2024-10-04T08:57:35Z,2024-10-04T08:57:35Z,"### Description of the bug

Hi,
I am trying to analyze with Sarek a few exomes with paired-end reads of 151nt, that incorporate UMIs in R1 reads. When executing fgbio CallMolecularConsensusReads fails because of a memory problem. 
I am running Sarek on an HPC cluster with a Slurm queuing system, which is equipped with large capacities. I have set the following parameters in the params.json file:

```
{
    ""input"": ""\/home\/user\/UMP421_pipe2nextflow\/UMIs_paired_exomes\/config\/sample.csv"",
    ""outdir"": ""\/home\/user\/UMP421_pipe2nextflow\/UMIs_paired_exomes\/analysis"",
    ""wes"": true,
    ""intervals"": ""\/home\/user\/info\/panel_designs\/HyperExome\/target.bed"",
    ""tools"": ""mutect2,VEP"",
    ""trim_fastq"": true,
    ""umi_read_structure"": ""9M151T 151T"",
    ""vep_include_fasta"": true,
    ""max_cpus"": 30,
    ""max_memory"": ""400.GB"",
    ""multiqc_title"": ""4WES"",
    ""nucleotides_per_second"": 100000 
}
```

In addition, in the nextflow.config file located in the running directory, I've also added the following lines:
```
process {
  executor = 'slurm'
  queue = 'long'
}
```

The queue long has almost 500GB of RAM, which should be enough to process one sample. 
I do not see any parameter in the fgbio command that is executed in Sarek related to memory usage  -Xms or -Xmx. 
```
#!/bin/bash -euo pipefail
fgbio \
    --tmp-dir=. \
    CallMolecularConsensusReads \
    -i 105-gm-1_umi-grouped.bam \
    -M 1 -S Coordinate \
    -o 105-gm-1_umi-consensus.bam

cat <<-END_VERSIONS > versions.yml
""NFCORE_SAREK:SAREK:FASTQ_CREATE_UMI_CONSENSUS_FGBIO:CALLUMICONSENSUS"":
    fgbio: $( echo $(fgbio --version 2>&1 | tr -d '[:cntrl:]' ) | sed -e 's/^.*Version: //;s/\[.*$//')
END_VERSIONS
```

Running the fgbio command this ways with the same input files that failed in the execution worked:

```
singularity exec --bind /clinicfs/userhomes/user/UMP421_pipe2nextflow/UMIs_paired_exomes/  /home/user/test_jf/singularity/singularity-images/depot.galaxyproject.org-singularity-fgbio-2.0.2--hdfd78af_0.img fgbio -Xms500m -Xmx100g --tmp-dir=. CallMolecularConsensusReads -i /clinicfs/userhomes/user/UMP421_pipe2nextflow/UMIs_paired_exomes/scripts/work/45/7653a0168e64e6540c9acb610591bf/105-gm-1_umi-grouped.bam -M 1 -S Coordinate -o 105-gm-1_umi-consensus.bam
```

So I think  I need to include -Xms500m -Xmx100g flags in the fgbio command but I do not know how to do this with the available options in sarek. 

I would be grateful if someone could tell me how to solve the problem.

Thanks in advance.

Sheila

### Command used and terminal output

```console
$ module load Nextflow/22.10.7 Singularity/3.6.4-GCC-5.4.0-2.26
$ nextflow -bg run nf-core/sarek -r 3.1.2 -name paired_exomes -profile singularity -params-file /home/user/UMP421_pipe2nextflow/UMIs_paired_exomes/config/nf-local-params.json



This is the error message:
[2023/04/06 15:39:43 | CallMolecularConsensusReads | Info] processed    38,000,000 records.  Elapsed time: 00:16:37s.  Time for last 1,000,000:   26s.  Last read position: chr4:9,268,435.  Last read name: A00718:490:H3LN2DSX5:2:2234:21748:27132
[2023/04/06 15:40:23 | FgBioMain | Info] CallMolecularConsensusReads failed. Elapsed time: 17.31 minutes.
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:281)
        at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:866)
        at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:840)
        at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:834)
        at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:802)
        at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:591)
        at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:570)
        at com.fulcrumgenomics.commons.CommonsDef$JavaIteratorAdapter.next(CommonsDef.scala:249)
        at scala.collection.Iterator$$anon$9.next(Iterator.scala:577)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator.maybeNext(BetterBufferedIterator.scala:45)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator.$anonfun$next$2(BetterBufferedIterator.scala:54)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator$$Lambda$437/0x000000010062f840.apply$mcV$sp(Unknown Source)
        at com.fulcrumgenomics.commons.CommonsDef.yieldAndThen(CommonsDef.scala:72)
        at com.fulcrumgenomics.commons.CommonsDef.yieldAndThen$(CommonsDef.scala:70)
        at com.fulcrumgenomics.commons.CommonsDef$.yieldAndThen(CommonsDef.scala:440)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator.next(BetterBufferedIterator.scala:54)
        at com.fulcrumgenomics.commons.collection.SelfClosingIterator.next(SelfClosingIterator.scala:46)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator$$anon$1.next(BetterBufferedIterator.scala:63)
        at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
        at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator$$anon$1.foreach(BetterBufferedIterator.scala:61)
        at com.fulcrumgenomics.bam.Template$.apply(Bams.scala:147)
        at com.fulcrumgenomics.bam.Template$.apply(Bams.scala:133)
        at com.fulcrumgenomics.bam.Bams$$anon$1.next(Bams.scala:314)
        at com.fulcrumgenomics.bam.Bams$$anon$1.next(Bams.scala:310)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator.maybeNext(BetterBufferedIterator.scala:45)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator.$anonfun$next$2(BetterBufferedIterator.scala:54)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator$$Lambda$437/0x000000010062f840.apply$mcV$sp(Unknown Source)
        at com.fulcrumgenomics.commons.CommonsDef.yieldAndThen(CommonsDef.scala:72)
        at com.fulcrumgenomics.commons.CommonsDef.yieldAndThen$(CommonsDef.scala:70)
        at com.fulcrumgenomics.commons.CommonsDef$.yieldAndThen(CommonsDef.scala:440)
        at com.fulcrumgenomics.commons.collection.BetterBufferedIterator.next(BetterBufferedIterator.scala:54)
```


### Relevant files

_No response_

### System information

_No response_",smzt,https://github.com/nf-core/sarek/issues/990
I_kwDOCvwIC85jQWeZ,nf-amazon does not exist,CLOSED,2023-04-12T20:49:57Z,2023-04-14T18:12:12Z,2023-04-14T18:12:12Z,"### Description of the bug

Hi, I created a conda virtual environment where I installed both nextflow (version: 22.10.6) and nf-core (version: 2.7.2). Then I downloaded nf-core/sarek (version: 3.1.2) Singularity. However, when I ran the sarek pipeling, Plugin with id nf-amazon did not exist in any repository. Later, I downloaded the plugin nf-amazon-1.16.1.zip from the website (https://github.com/nextflow-io/nf-amazon/releases/download/1.16.1/nf-amazon-1.16.1.zip) directly and specified it in the coding, but the pipeline still didn't recognize the plugin. Ask your help to solve this problem occuring when running the sarek Singularity in HPC （gcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC)). Thank you very much.

### Command used and terminal output

```console
log=./08-test_NextFlow-nf-core-sarek/test-01/my.log

workflow=./08-test_NextFlow-nf-core-sarek/nf-core-sarek-3.1.2/workflow

profile=singularity

input=./08-test_NextFlow-nf-core-sarek/test_data/samplesheet.csv

outdir=./08-test_NextFlow-nf-core-sarek/test-01/output

plugin=./08-test_NextFlow-nf-core-sarek/nf-core-sarek-3.1.2/plugins/nf-amazon-1.16.1

# run pipeline
nextflow -log $log run $workflow -profile $profile --input $input --outdir $outdir --genome GATK.GRCh38 -plugins $plugin
```


### Relevant files

[nf-amazon-1.16.1.zip](https://github.com/nf-core/sarek/files/11215865/nf-amazon-1.16.1.zip)
[my.log](https://github.com/nf-core/sarek/files/11215882/my.log)
[my2.log](https://github.com/nf-core/sarek/files/11215884/my2.log)


### System information

Linux version 3.10.0-1160.88.1.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) ) #1 SMP Tue Mar 7 15:41:52 UTC 2023",JiahuaQu,https://github.com/nf-core/sarek/issues/992
I_kwDOCvwIC85jSMrE,ControlFREEC: Error in plot.window(...) : need finite 'ylim' values,CLOSED,2023-04-13T05:02:47Z,2023-09-28T16:29:40Z,2023-09-28T16:29:40Z,"### Description of the bug

Hello nf-core SAREK team,
I met controlfreec errors when I was running SAREK offline.
Here is the command I run:
```linux
nextflow run /home/hyjforesight/nf-core-sarek-3.1.2/workflow sarek --input /home/hyjforesight/Sarek_design_copy.csv --outdir /mnt/d/WES/ --genome GATK.GRCh38 -profile docker --tools freebayes,mutect2,strelka,manta,tiddit,cnvkit,controlfreec,snpeff,vep
```
Error message:
```linux
[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_CONTROLFREEC:MAKEGRAPH (C1033)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_CONTROLFREEC:MAKEGRAPH (C1033)` terminated with an error exit status (1)

Command executed:

  cat $(which makeGraph.R) | R --slave --args 2 C1033.tumor.mpileup.gz_ratio.txt C1033.tumor.mpileup.gz_BAF.txt
  
  mv *_BAF.txt.png C1033_BAF.png
  mv *_ratio.txt.log2.png C1033_ratio.log2.png
  mv *_ratio.txt.png C1033_ratio.png
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_CONTROLFREEC:MAKEGRAPH"":
      controlfreec: $(echo $(freec -version 2>&1) | sed 's/^.*Control-FREEC  //; s/:.*$//' | sed -e ""s/Control-FREEC v//g"" )
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  Error in plot.window(...) : need finite 'ylim' values
  Calls: plot -> plot.default -> localWindow -> plot.window
  In addition: There were 17 warnings (use warnings() to see them)
  Execution halted

Work dir:
  /home/hyjforesight/work/73/248c092693a8d9612f76e8fb25e3d7

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line
```
Could you please help me with this issue? Thank you in advance!

### Command used and terminal output

```console
$ nextflow run /home/hyjforesight/nf-core-sarek-3.1.2/workflow sarek --input /home/hyjforesight/Sarek_design_copy.csv --outdir /mnt/d/WES/ --genome GATK.GRCh38 -profile docker --tools freebayes,mutect2,strelka,manta,tiddit,cnvkit,controlfreec,snpeff,vep
```


### Relevant files

[](url)
[nextflow (1).log](https://github.com/nf-core/sarek/files/11218304/nextflow.1.log)


### System information

Nextflow version: 22.10.6.5843
Hardware: Desktop
Executor: local
Container engine: Docker
OS: Win11 22H2, WSL2, Ubuntu 22.04
Version of nf-core/sarek: 3.1.2",hyjforesight,https://github.com/nf-core/sarek/issues/993
I_kwDOCvwIC85jTxWm,Check & update reference files,OPEN,2023-04-13T10:12:02Z,2023-04-13T10:12:02Z,,"### Description of feature

Things like the current dbsnp version have newer versions availble and should prob be updated",FriederikeHanssen,https://github.com/nf-core/sarek/issues/995
I_kwDOCvwIC85jTyYu,Evaluate possible drop-in replacements/additions to the GATK suite,OPEN,2023-04-13T10:14:57Z,2023-04-13T10:14:57Z,,"### Description of feature

- elPrep
- Parabricks 

Some aspects:
- feature parity
- documentation
- is it continuously developed?",FriederikeHanssen,https://github.com/nf-core/sarek/issues/996
I_kwDOCvwIC85jVmMm,samplesheet verification failed when running variant_calling step,OPEN,2023-04-13T15:01:51Z,2023-05-29T11:45:02Z,,"### Description of the bug

i runned sarek to performe a variant calling with a wrong samle file and the samplesheet was not correctly verified

### Command used and terminal output

```console
nextflow run nf-core/sarek -profile singularity --input results_CRESPOMAR_06/csv/markduplicates.csv --genome hg19 -r 3.1.2 -c sarek.conf --outdir results_CRESPOMAR_06 --pon /mnt/bioinfnas/bioinformatics/projects/20220808_MCrespo_22LLC/Mutect2-hg19-exome-panel.vcf.gz --known_indels /mnt/bioinfnas/bioinformatics/projects/20220808_MCrespo_22LLC/Mills_and_1000G_gold_standard.indels.b37.vcf.gz --tools mutect2,strelka,snpeff,vep --igenomes_base /mnt/bioinfnas/general/refs/igenomes/ --step variant_calling
```


### Relevant files

_No response_

### System information

using singularity 3ea83f6-dirty",paumarc,https://github.com/nf-core/sarek/issues/997
I_kwDOCvwIC85jdtI1,How to download reference data and tell the pipeline to locate those data in my customized folder?,CLOSED,2023-04-14T18:03:44Z,2023-04-14T20:05:33Z,2023-04-14T20:05:33Z,"### Description of feature

Because there're some kinds of restrictions to access to AWS (outside network) by our institute HPC, the sarek pipepline can't download the s3 references automatically. Therefore, I want to download those reference data and save them in a customized folder. Then, specificy that folder path to load those reference data into the pipeline when running the pipeline. Is there any way to meet this need? In addition, I want to save the downloaded reference data so that I can save time without need of dowloading the same reference data when running the pipeline to analyze another set of genomic data next time. Thank you.",JiahuaQu,https://github.com/nf-core/sarek/issues/999
I_kwDOCvwIC85je-k5,"Environment variable SINGULARITYENV_TMP is set, but APPTAINERENV_TMP is preferred",OPEN,2023-04-15T00:06:27Z,2023-06-05T07:07:07Z,,"### Description of the bug

I created a conda virtual environment where I installed nextflow (version: 23.04.0) and nf-core (version: 2.7.2) and dowloaded nf-core/sarek (version: 3.1.2) Singularity image. After running for long time, it failed with the error sent to my email:
[error.txt](https://github.com/nf-core/sarek/files/11237816/error.txt)
In brief: 
Command error:
  INFO:    Environment variable SINGULARITYENV_TMP is set, but APPTAINERENV_TMP is preferred
  INFO:    Environment variable SINGULARITYENV_TMPDIR is set, but APPTAINERENV_TMPDIR is preferred
  INFO:    Environment variable SINGULARITYENV_NXF_DEBUG is set, but APPTAINERENV_NXF_DEBUG is preferred
Hope can get your help to solve the problem and resume the program. Thank you very much.

### Command used and terminal output

```console
nextflow -log $log run $workflow -profile $profile --input $input --outdir $outdir --genome $genome --igenomes_base $my_base --email $email
```


### Relevant files

[my.log](https://github.com/nf-core/sarek/files/11237821/my.log)
[test-02.sh.o2546181.txt](https://github.com/nf-core/sarek/files/11237822/test-02.sh.o2546181.txt)


### System information

I created a conda virtual environment where I installed nextflow (version: 23.04.0) and nf-core (version: 2.7.2) and dowloaded nf-core/sarek (version: 3.1.2) Singularity image.
HPC
qsub SGE
CentOS Linux",JiahuaQu,https://github.com/nf-core/sarek/issues/1000
I_kwDOCvwIC85juEvG,ERROR while running Sarek (v3.1.2) with test_full profile - couldnt get data from s3://nf-core-awsmegatests/sarek/input,CLOSED,2023-04-18T12:29:21Z,2023-04-20T13:11:06Z,2023-04-20T13:11:06Z,"### Description of the bug

Hi team, 
I am trying to run Sarek (v3.1.2) with the test_full profile using nextflow 23.04.1 and I am getting this error:

ERROR ~ Unable to read script: '/Users/username/pipelines/sarek/./workflows/sarek.nf' -- cause: s3://nf-core-awsmegatests/sarek/input/S07604624_Padded_Agilent_SureSelectXT_allexons_V6_UTR.bed

It seems that the problem is while trying to get data for 'intervals' param from s3 bucket ('s3://nf-core-awsmegatests/sarek/input/S07604624_Padded_Agilent_SureSelectXT_allexons_V6_UTR.bed'):
https://github.com/nf-core/sarek/blob/master/conf/test/test_full.config#L23

Is the .bed file present in /sarek/input folder, please? Could you please point to the used .bed file? 
Thank you for your help in advance!



### Command used and terminal output

```console
❯ cd sarek
❯ git clone https://github.com/nf-core/sarek.git
❯ nextflow-23.04.1-all run . -profile test_full,docker --outdir OUTPUT_SAREK_FULL_PROFILE_23.04.1/
N E X T F L O W  ~  version 23.04.1
Launching `./main.nf` [lonely_bartik] DSL2 - revision: c2275107d1


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .´ _  `.
   /  |\`-_ \      __        __   ___
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /¯¯\ |  \ |___ |  \
    `|____\´

  nf-core/sarek v3.1.2
------------------------------------------------------
Core Nextflow options
  runName                   : lonely_bartik
  containerEngine           : docker
  launchDir                 : /Users/username/pipelines/sarek
  workDir                   : /Users/username/pipelines/sarek/work
  projectDir                : /Users/username/pipelines/sarek
  userName                  : username
  profile                   : test_full,docker
  configFiles               : /Users/username/pipelines/sarek/nextflow.config

Input/output options
  input                     : https://raw.githubusercontent.com/nf-core/test-datasets/sarek/testdata/csv/HCC1395_WXS_somatic_full_test.csv
  outdir                    : OUTPUT_SAREK_FULL_PROFILE_23.04.1/

Main options
  split_fastq               : 20000000
  wes                       : true
  intervals                 : s3://nf-core-awsmegatests/sarek/input/S07604624_Padded_Agilent_SureSelectXT_allexons_V6_UTR.bed
  nucleotides_per_second    : 100000
  tools                     : strelka,mutect2,freebayes,ascat,manta,cnvkit,tiddit,controlfreec,vep

Variant Calling
  cf_chrom_len              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/Length/Homo_sapiens_assembly38.len
  pon                       : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz
  pon_tbi                   : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz.tbi

Reference genome options
  ascat_genome              : hg38
  ascat_alleles             : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_alleles_hg38.zip
  ascat_loci                : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_loci_hg38.zip
  ascat_loci_gc             : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/GC_G1000_hg38.zip
  ascat_loci_rt             : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/RT_G1000_hg38.zip
  bwa                       : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAIndex/
  bwamem2                   : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAmem2Index/
  chr_dir                   : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/Chromosomes
  dbsnp                     : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz
  dbsnp_tbi                 : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz.tbi
  dbsnp_vqsr                : --resource:dbsnp,known=false,training=true,truth=false,prior=2.0 dbsnp_146.hg38.vcf.gz
  dict                      : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.dict
  dragmap                   : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/dragmap/
  fasta                     : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta
  fasta_fai                 : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai
  germline_resource         : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz
  germline_resource_tbi     : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz.tbi
  known_indels              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz
  known_indels_tbi          : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz.tbi
  known_indels_vqsr         : --resource:gatk,known=false,training=true,truth=true,prior=10.0 Homo_sapiens_assembly38.known_indels.vcf.gz --resource:mills,known=false,training=true,truth=true,prior=10.0 Mills_and_1000G_gold_standard.indels.hg38.vcf.gz
  known_snps                : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000G_omni2.5.hg38.vcf.gz
  known_snps_tbi            : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000G_omni2.5.hg38.vcf.gz.tbi
  known_snps_vqsr           : --resource:1000G,known=false,training=true,truth=true,prior=10.0 1000G_omni2.5.hg38.vcf.gz
  mappability               : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Control-FREEC/out100m2_hg38.gem
  snpeff_db                 : GRCh38.105
  snpeff_genome             : GRCh38
  snpeff_version            : 5.1
  vep_genome                : GRCh38
  vep_species               : homo_sapiens
  vep_cache_version         : 106
  vep_version               : 106.1
  igenomes_base             : s3://ngi-igenomes/igenomes

Institutional config options
  config_profile_name       : Full test profile
  config_profile_description: Full test dataset to check pipeline function

!! Only displaying parameters that differ from the pipeline defaults !!
------------------------------------------------------
If you use nf-core/sarek for your analysis please cite:

* The pipeline
  https://doi.org/10.12688/f1000research.16665.2
  https://doi.org/10.5281/zenodo.4468605

* The nf-core framework
  https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md
------------------------------------------------------
ERROR ~ Unable to read script: '/Users/username/pipelines/sarek/./workflows/sarek.nf' -- cause: s3://nf-core-awsmegatests/sarek/input/S07604624_Padded_Agilent_SureSelectXT_allexons_V6_UTR.bed

 -- Check script 'main.nf' at line: 79 or see '.nextflow.log' file for more details
```


### Relevant files

_No response_

### System information

_No response_",jsitarova-dnanexus,https://github.com/nf-core/sarek/issues/1001
I_kwDOCvwIC85j1G0K,Add vep_custom_args params to allow for VEP custom args,CLOSED,2023-04-19T13:04:43Z,2023-05-26T14:37:30Z,2023-05-26T14:36:37Z,"### Description of feature

We could then more easilly specify what we want, so you would be able to do `--vep_custom_args ""--hgvsg""` which is not possible at the moment due to bad interaction with params in the config files.",maxulysse,https://github.com/nf-core/sarek/issues/1003
I_kwDOCvwIC85j7QYK,error when providing a bed file for targeted samples,CLOSED,2023-04-20T10:47:15Z,2023-05-03T17:04:51Z,2023-05-03T17:04:51Z,"### Description of the bug

Hi! 
My input to sarek is fastq files from targeted gene panels, so I use `--wes` and a bed file after `--intervals`.
I get an error at the base recalibrator step which uses gatk:
```
Error executing process > 'NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (SampleB)'

Caused by:
  Essential container in task exited

Command executed:

  gatk --java-options ""-Xmx4g"" BaseRecalibrator  \
      --input SampleB.md.cram \
      --output SampleB_chr3_9166297-9166676.recal.table \
      --reference human_g1k_v37_decoy.fasta \
      --intervals chr3_9166297-9166676.bed \
      --known-sites dbsnp_138.b37.vcf.gz --known-sites 1000G_phase1.indels.b37.vcf.gz --known-sites Mills_and_1000G_gold_standard.indels.b37.vcf.gz \
      --tmp-dir . \
  
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  2

Command output:
  (empty)


A USER ERROR has occurred: Badly formed genome unclippedLoc: Contig chr3 given as location, but this contig isn't present in the Fasta sequence dictionary
```
In ran the pipeline several times and got a different 'chr' every time, so it does not look like a problem with the format of a specific chromosome.

The reference genome I am using is hg19 and the bed file is formatted accordingly (using chr).
I attach the command and the bed file I used. 

Thank you so much!

### Command used and terminal output

```console
nextflow run nf-core/sarek --input pancancer_spreadsheet.csv --outdir sarek_pancancer --genome GATK.GRCh37 --wes --intervals BED/AGL_hg19.bed -profile docker --tools strelka,freebayes,manta,tiddit,mpileup,haplotypecaller,merge
```


### Relevant files

[BED.zip](https://github.com/nf-core/sarek/files/11285271/BED.zip)


### System information

Nexflow 22.10.7.5853
nf-core/sarek 3.1.2
Docker
awsbatch
Linux Mint",albcarrasco,https://github.com/nf-core/sarek/issues/1004
I_kwDOCvwIC85kgwCf,Error while running sarek v3.1.2:  Not a valid S3 file system provider file attribute view,CLOSED,2023-04-27T07:49:45Z,2023-05-13T07:25:49Z,2023-05-13T07:25:49Z,"### Description of the bug

while i run the nf-core/sarek with my own data using nextflow 22.10.6 , the error shows:
Not a valid S3 file system provider file attribute view: java.nio.file.attribute.BasicWithKeyFileAttributeView
 -- Check script 'software/sarek/./workflows/sarek.nf' at line: 56 or see '.nextflow.log' file for more details
I checked the line 56, yet find nothing.


### Command used and terminal output

```console
$ nextflow run ~/software/sarek \
> -profile singularity \
> --input ~/project/test/patient.csv \ 
N E X T F L O W  ~  version 22.10.6
Launching `/dssg/home/acct-medlhp/medlhp-wzc/software/sarek/main.nf` [awesome_plateau] DSL2 - revision: c2275107d1


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .´ _  `.
   /  |\`-_ \      __        __   ___     
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /¯¯\ |  \ |___ |  \
    `|____\´

  nf-core/sarek v3.1.2
------------------------------------------------------
Core Nextflow options
  runName              : awesome_plateau
  containerEngine      : singularity
  launchDir            : /dssg/home/acct-medlhp/medlhp-wzc
  workDir              : /dssg/home/acct-medlhp/medlhp-wzc/work
  projectDir           : /dssg/home/acct-medlhp/medlhp-wzc/software/sarek
  userName             : medlhp-wzc
  profile              : singularity
  configFiles          : /dssg/home/acct-medlhp/medlhp-wzc/software/sarek/nextflow.config

Input/output options
  input                : /dssg/home/acct-medlhp/medlhp-wzc/project/test/patient.csv
  outdir               : results

Main options
  intervals            : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/intervals/wgs_calling_regions_noseconds.hg38.bed

Variant Calling
  cf_chrom_len         : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/Length/Homo_sapiens_assembly38.len
  pon                  : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz
  pon_tbi              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz.tbi

Reference genome options
  ascat_genome         : hg38
  ascat_alleles        : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_alleles_hg38.zip
  ascat_loci           : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_loci_hg38.zip
  ascat_loci_gc        : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/GC_G1000_hg38.zip
  ascat_loci_rt        : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/RT_G1000_hg38.zip
  bwa                  : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAIndex/
  bwamem2              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAmem2Index/
  chr_dir              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/Chromosomes
  dbsnp                : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz
  dbsnp_tbi            : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz.tbi
  dbsnp_vqsr           : --resource:dbsnp,known=false,training=true,truth=false,prior=2.0 dbsnp_146.hg38.vcf.gz
  dict                 : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.dict
  dragmap              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/dragmap/
  fasta                : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta
  fasta_fai            : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai
  germline_resource    : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz
  germline_resource_tbi: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz.tbi
  known_indels         : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz
  known_indels_tbi     : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz.tbi
  known_indels_vqsr    : --resource:gatk,known=false,training=true,truth=true,prior=10.0 Homo_sapiens_assembly38.known_indels.vcf.gz --resource:mills,known=false,training=true,truth=true,prior=10.0 Mills_and_1000G_gold_standard.indels.hg38.vcf.gz
  known_snps           : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000G_omni2.5.hg38.vcf.gz
  known_snps_tbi       : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000G_omni2.5.hg38.vcf.gz.tbi
  known_snps_vqsr      : --resource:1000G,known=false,training=true,truth=true,prior=10.0 1000G_omni2.5.hg38.vcf.gz
  mappability          : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Control-FREEC/out100m2_hg38.gem
  snpeff_db            : GRCh38.105
  snpeff_genome        : GRCh38
  snpeff_version       : 5.1
  vep_genome           : GRCh38
  vep_species          : homo_sapiens
  vep_cache_version    : 106
  vep_version          : 106.1
  igenomes_base        : s3://ngi-igenomes/igenomes

!! Only displaying parameters that differ from the pipeline defaults !!
------------------------------------------------------
If you use nf-core/sarek for your analysis please cite:

* The pipeline
  https://doi.org/10.12688/f1000research.16665.2
  https://doi.org/10.5281/zenodo.4468605

* The nf-core framework
  https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md
------------------------------------------------------
Not a valid S3 file system provider file attribute view: java.nio.file.attribute.BasicWithKeyFileAttributeView

 -- Check script 'software/sarek/./workflows/sarek.nf' at line: 56 or see '.nextflow.log' file for more details
```


### Relevant files

_No response_

### System information

_No response_",FrancisNietzsche,https://github.com/nf-core/sarek/issues/1006
I_kwDOCvwIC85lQ6eX,Error running VQSR in joint VC: nextflow.exception.MissingProcessException: Missing process or function mix,CLOSED,2023-05-07T07:15:00Z,2023-05-16T09:16:32Z,2023-05-16T09:16:31Z,"### Description of the bug

Hello,

I'm getting the error in the title when attempting to run a joint variant calling pipeline. The error appears almost as soon as the pipeline starts running and the stack trace contains the following:

```
[...]
	at nextflow.cli.CmdRun.run(CmdRun.groovy:368)
	at nextflow.cli.Launcher.run(Launcher.groovy:494)
	at nextflow.cli.Launcher.main(Launcher.groovy:653)
Caused by: groovy.lang.MissingMethodException: No signature of method: java.lang.String.mix() is applicable for argument types: (String) values: [--resource:ensemblvcf,known=false,training=true,truth=true,prior=10.0 Saccharomyces_cerevisiae.vcf.gz]
Possible solutions: md5(), is(java.lang.Object), wait(), trim(), next(), find()
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:70)
	at org.codehaus.groovy.runtime.callsite.PojoMetaClassSite.call(PojoMetaClassSite.java:46)
[...]
```

The error seems to refer to the way I've configured `--dbsnp_vqsr`, so I can't rule out a configuration error.

I'm attaching a script that reproduces the error using some yeast data. It should take less than 5 minutes to run.

### Relevant files


[sarek-error.sh.txt](https://github.com/nf-core/sarek/files/11414161/sarek-error.sh.txt)


### System information

Nextflow 23.04.0 or 23.04.1
nf-core/sarek dev -latest
",amizeranschi,https://github.com/nf-core/sarek/issues/1016
I_kwDOCvwIC85lXS9U,Error when running nextflow: ERROR ~ No signature of method: java.util.LinkedList.withInputStream(),CLOSED,2023-05-08T17:03:03Z,2023-05-29T14:37:19Z,2023-05-29T14:37:19Z,"### Description of the bug

Hi all,

After fixing some errors, I got another error. Would you please have a look? Let me know if you need the log file and sample sheet. Thanks in advance 😃.

### Command used and terminal output

```console
nextflow run nf-core/sarek --input sample_sheet_sarek.csv --outdir /labs/ --genome GATK.GRCh38 -profile singularity

WARN: There’s no process matching config selector: NFCORE_SAREK:sarek:CRAM_QC_NO_MD:SAMTOOLS_STATS -- Did you mean: NFCORE_SAREK:sarek:CRAM_QC_RECAL:SAMTOOLS
_STATS?
ERROR ~ No signature of method: java.util.LinkedList.withInputStream() is applicable for argument types: (Script_662cb5ec$_flowcellLaneFromFastq_closure6) va
lues: [Script_662cb5ec$_flowcellLaneFromFastq_closure6@6ad1c7a5]
```


### Relevant files
[nextflow_error.log](https://github.com/nf-core/sarek/files/11456385/nextflow_error.log)


### System information
Nextflow version 23.04.0",Chrisdoan9,https://github.com/nf-core/sarek/issues/1017
I_kwDOCvwIC85lgxUu,Please add CSI indexing,OPEN,2023-05-10T04:10:03Z,2023-05-29T11:43:31Z,,"Hi

I'm working with plant genomes and have particularly long chromosomes which cannot be indexed.

I am getting this error:

```
Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM (S6)` terminated with an error exit status (1)

Command executed:

  samtools \
      index \
      -@ 0 \
       \
      S6.sorted.bam
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM"":
      samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  [E::hts_idx_check_range] Region 536870772..536870923 cannot be stored in a bai index. Try using a csi index
  [E::sam_index] Read 'A00694:39:H2JGYDRX3:1:2270:20202:8484' with ref_name='chr2H', ref_length=672273650, flags=83, pos=536870773 cannot be indexed
  samtools index: failed to create index for ""S6.sorted.bam"": Numerical result out of range
```

Would it be possible to add CSI indexing as a parameter to Sarek? Thank you.

I get the same problem if I use intervals for speeding up analysis as tabix uses TBI indexing and not CSI.",brettChapman,https://github.com/nf-core/sarek/issues/1018
I_kwDOCvwIC85lyesR,Error while running ASCAT ,OPEN,2023-05-12T14:25:50Z,2024-12-10T17:42:12Z,,"### Description of the bug

Dear All,

I launched the variant calling step using the ASCAT tool, but I received an error message that I did not understand. Could you please help me resolve this issue?

I have attached a log file containing the executed command and the encountered error.

Thank you in advance for your help.

Best regards,

### Command used and terminal output

```console
nextflow run /home/NGS/nf-core/sarek  --step variant_calling --input /home/NGS/work/data_reel/nfcore2/results/csv/recalibrated.csv --outdir results/ --genome GATK.GRCh37 -profile docker --wes --intervals /home/NGS/work/data_reel/nfcore2/targeted.bed --tools ascat --only_paired_variant_calling true --max_cpus 7 --max_memory '31.GB'


N E X T F L O W  ~  version 22.10.7
Launching `/home/NGS/nf-core/sarek/main.nf` [voluminous_woese] DSL2 - revision: c2275107d1


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .´ _  `.
   /  |\`-_ \      __        __   ___     
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /¯¯\ |  \ |___ |  \
    `|____\´

  nf-core/sarek v3.1.2
------------------------------------------------------
Core Nextflow options
  runName                    : voluminous_woese
  containerEngine            : docker
  launchDir                  : /home/NGS/work/data_reel/nfcore2
  workDir                    : /home/NGS/work/data_reel/nfcore2/work
  projectDir                 : /home/NGS/nf-core/sarek
  userName                   : NGS
  profile                    : docker
  configFiles                : /home/NGS/nf-core/sarek/nextflow.config

Input/output options
  step                       : variant_calling
  input                      : /home/NGS/work/data_reel/nfcore2/results/csv/recalibrated.csv
  outdir                     : results/

Main options
  wes                        : true
  intervals                  : /home/NGS/work/data_reel/nfcore2/targeted.bed
  tools                      : ascat

Variant Calling
  only_paired_variant_calling: true

Reference genome options
  genome                     : GATK.GRCh37
  ascat_genome               : hg19
  ascat_alleles              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/ASCAT/G1000_alleles_hg19.zip
  ascat_loci                 : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/ASCAT/G1000_loci_hg19.zip
  ascat_loci_gc              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/ASCAT/GC_G1000_hg19.zip
  ascat_loci_rt              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/ASCAT/RT_G1000_hg19.zip
  bwa                        : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Sequence/BWAIndex/
  chr_dir                    : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Sequence/Chromosomes
  dbsnp                      : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/dbsnp_138.b37.vcf.gz
  dbsnp_tbi                  : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/dbsnp_138.b37.vcf.gz.tbi
  dbsnp_vqsr                 : --resource:dbsnp,known=false,training=true,truth=false,prior=2.0 dbsnp_138.b37.vcf.gz
  dict                       : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Sequence/WholeGenomeFasta/human_g1k_v37_decoy.dict
  fasta                      : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Sequence/WholeGenomeFasta/human_g1k_v37_decoy.fasta
  fasta_fai                  : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Sequence/WholeGenomeFasta/human_g1k_v37_decoy.fasta.fai
  germline_resource          : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/af-only-gnomad.raw.sites.vcf.gz
  germline_resource_tbi      : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/af-only-gnomad.raw.sites.vcf.gz.tbi
  known_indels               : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/{1000G_phase1,Mills_and_1000G_gold_standard}.indels.b37.vcf.gz
  known_indels_tbi           : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/{1000G_phase1,Mills_and_1000G_gold_standard}.indels.b37.vcf.gz.tbi
  known_indels_vqsr          : --resource:1000G,known=false,training=true,truth=true,prior=10.0 1000G_phase1.indels.b37.vcf.gz --resource:mills,known=false,training=true,truth=true,prior=10.0 Mills_and_1000G_gold_standard.indels.b37.vcf.gz
  known_snps                 : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/1000G_phase1.snps.high_confidence.b37.vcf.gz
  known_snps_tbi             : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/GATKBundle/1000G_phase1.snps.high_confidence.b37.vcf.gz.tbi
  known_snps_vqsr            : --resource:1000G,known=false,training=true,truth=true,prior=10.0 1000G_phase1.snps.high_confidence.b37.vcf.gz
  mappability                : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh37/Annotation/Control-FREEC/out100m2_hg19.gem
  snpeff_db                  : GRCh37.87
  snpeff_genome              : GRCh37
  snpeff_version             : 5.1
  vep_genome                 : GRCh37
  vep_species                : homo_sapiens
  vep_cache_version          : 106
  vep_version                : 106.1
  igenomes_base              : s3://ngi-igenomes/igenomes

Max job request options
  max_cpus                   : 7
  max_memory                 : 31.GB

!! Only displaying parameters that differ from the pipeline defaults !!
------------------------------------------------------
If you use nf-core/sarek for your analysis please cite:

* The pipeline
  https://doi.org/10.12688/f1000research.16665.2
  https://doi.org/10.5281/zenodo.4468605

* The nf-core framework
  https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md
------------------------------------------------------
WARN: Default reference files not suited for running ASCAT on WES data. It's recommended to use the reference files provided here: https://github.com/Wedge-lab/battenberg#required-reference-files
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                            -
executor >  local (1)
executor >  local (1)
executor >  local (2)
executor >  local (2)
executor >  local (2)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                            -
executor >  local (2)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                            -
executor >  local (2)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                            -
executor >  local (12)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP                                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                                      -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                             -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                                           -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                                                    -
[ff/11e81c] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_ALLELES (G1000_alleles_hg19.zip)                       [100%] 1 of 1 ✔
[be/66776e] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_LOCI (G1000_loci_hg19.zip)                             [100%] 1 of 1 ✔
[12/9fce90] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_GC (GC_G1000_hg19.zip)                                 [100%] 1 of 1 ✔
[4b/1b42c5] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_RT (RT_G1000_hg19.zip)                                 [100%] 1 of 1 ✔
[eb/7924d9] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (targeted.bed)                       [100%] 1 of 1 ✔
[e2/18b339] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (1_27022879-27024037)     [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_ANTITARGET                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_REFERENCE                                   -
[87/22306c] process > NFCORE_SAREK:SAREK:BAM_TO_CRAM (normal_HT1376_S5)                                              [100%] 4 of 4 ✔
[11/a4175d] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT ... [  0%] 0 of 2
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS                                     -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT                                -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL                                 -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY                                   -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                                                 -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC                                                                     -
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (tumor_HT1197_S4_vs_normal_HT1197_S4)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (tumor_HT1197_S4_vs_normal_HT1197_S4)` terminated with an error exit status (1)

Command executed:

  #!/usr/bin/env Rscript
  library(RColorBrewer)
  library(ASCAT)
  options(bitmapType='cairo')
  
  #build prefixes: <abspath_to_files/prefix_chr>
  allele_path = normalizePath(""G1000_alleles_hg19"")
  allele_prefix = paste0(allele_path, ""/"", ""G1000_alleles_hg19"", ""_chr"")
  
  loci_path = normalizePath(""G1000_loci_hg19"")
  loci_prefix = paste0(loci_path, ""/"", ""G1000_loci_hg19"", ""_chr"")
  
  #prepare from BAM files
  ascat.prepareHTS(
      tumourseqfile = ""tumor_HT1197_S4.converted.cram"",
      normalseqfile = ""normal_HT1197_S4.converted.cram"",
      tumourname = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour""),
      normalname = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".normal""),
      allelecounter_exe = ""alleleCounter"",
      alleles.prefix = allele_prefix,
      loci.prefix = loci_prefix,
      gender = ""XY"",
      genomeVersion = ""hg19"",
      nthreads = 6
      ,minCounts = 10
      ,BED_file = 'targeted.bed'
      ,chrom_names = c(1:22, 'X', 'Y')
      ,min_base_qual = 20
      ,min_map_qual = 35
      ,ref.fasta = 'human_g1k_v37_decoy.fasta'
  
  
  )
  
  
  #Load the data
  ascat.bc = ascat.loadData(
      Tumor_LogR_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_tumourLogR.txt""),
      Tumor_BAF_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_tumourBAF.txt""),
      Germline_LogR_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_normalLogR.txt""),
      Germline_BAF_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_normalBAF.txt""),
      genomeVersion = ""hg19"",
      gender = ""XY""
  )
  
  #Plot the raw data
  ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".before_correction.""))
  
  # optional LogRCorrection
  if(""GC_G1000_hg19"" != ""NULL"") {
      gc_input = paste0(normalizePath(""GC_G1000_hg19""), ""/"", ""GC_G1000_hg19"", "".txt"")
  
      if(""RT_G1000_hg19"" != ""NULL""){
          rt_input = paste0(normalizePath(""RT_G1000_hg19""), ""/"", ""RT_G1000_hg19"", "".txt"")
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = rt_input)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".after_correction_gc_rt.""))
      }
      else {
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = RT_G1000_hg19)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".after_correction_gc.""))
      }
  }
  
  #Segment the data
  ascat.bc = ascat.aspcf(ascat.bc)
  
  #Plot the segmented data
  ascat.plotSegmentedData(ascat.bc)
  
  #Run ASCAT to fit every tumor to a model, inferring ploidy, normal cell contamination, and discrete copy numbers
  #If psi and rho are manually set:
  if (!is.null(NULL) && !is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL, psi_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, psi_manual=NULL)
  } else {
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1)
  }
  
  #Extract metrics from ASCAT profiles
  QC = ascat.metrics(ascat.bc,ascat.output)
  
  #Write out segmented regions (including regions with one copy of each allele)
  write.table(ascat.output[[""segments""]], file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".segments.txt""), sep=""	"", quote=F, row.names=F)
  
  #Write out CNVs in bed format
  cnvs=ascat.output[[""segments""]][2:6]
  write.table(cnvs, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"","".cnvs.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  #Write out purity and ploidy info
  summary <- tryCatch({
          matrix(c(ascat.output[[""aberrantcellfraction""]], ascat.output[[""ploidy""]]), ncol=2, byrow=TRUE)}, error = function(err) {
              # error handler picks up where error was generated
              print(paste(""Could not find optimal solution:  "",err))
              return(matrix(c(0,0),nrow=1,ncol=2,byrow = TRUE))
      }
  )
  colnames(summary) <- c(""AberrantCellFraction"",""Ploidy"")
  write.table(summary, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"","".purityploidy.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  write.table(QC, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".metrics.txt""), sep=""	"", quote=F, row.names=F)
  
  # version export
  f <- file(""versions.yml"",""w"")
  alleleCounter_version = system(paste(""alleleCounter --version""), intern = T)
  ascat_version = sessionInfo()$otherPkgs$ASCAT$Version
  writeLines(paste0('""', ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT"", '""', "":""), f)
  writeLines(paste(""    alleleCounter:"", alleleCounter_version), f)
  writeLines(paste(""    ascat:"", ascat_version), f)
  close(f)

Command exit status:
  1

Command output:
  (empty)

Command error:
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Reading locis
  Reading locis
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
executor >  local (12)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP                                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                                      -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                             -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                                           -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                                                    -
[ff/11e81c] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_ALLELES (G1000_alleles_hg19.zip)                       [100%] 1 of 1 ✔
[be/66776e] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_LOCI (G1000_loci_hg19.zip)                             [100%] 1 of 1 ✔
[12/9fce90] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_GC (GC_G1000_hg19.zip)                                 [100%] 1 of 1 ✔
[4b/1b42c5] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_RT (RT_G1000_hg19.zip)                                 [100%] 1 of 1 ✔
[eb/7924d9] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (targeted.bed)                       [100%] 1 of 1 ✔
[e2/18b339] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (1_27022879-27024037)     [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_ANTITARGET                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_REFERENCE                                   -
[87/22306c] process > NFCORE_SAREK:SAREK:BAM_TO_CRAM (normal_HT1376_S5)                                              [100%] 4 of 4 ✔
[1f/a0ce39] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT ... [ 50%] 1 of 2, failed: 1
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS                                     -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT                                -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL                                 -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY                                   -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                                                 -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC                                                                     -
Execution cancelled -- Finishing pending tasks before exit
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (tumor_HT1197_S4_vs_normal_HT1197_S4)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (tumor_HT1197_S4_vs_normal_HT1197_S4)` terminated with an error exit status (1)

Command executed:

  #!/usr/bin/env Rscript
  library(RColorBrewer)
  library(ASCAT)
  options(bitmapType='cairo')
  
  #build prefixes: <abspath_to_files/prefix_chr>
  allele_path = normalizePath(""G1000_alleles_hg19"")
  allele_prefix = paste0(allele_path, ""/"", ""G1000_alleles_hg19"", ""_chr"")
  
  loci_path = normalizePath(""G1000_loci_hg19"")
  loci_prefix = paste0(loci_path, ""/"", ""G1000_loci_hg19"", ""_chr"")
  
  #prepare from BAM files
  ascat.prepareHTS(
      tumourseqfile = ""tumor_HT1197_S4.converted.cram"",
      normalseqfile = ""normal_HT1197_S4.converted.cram"",
      tumourname = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour""),
      normalname = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".normal""),
      allelecounter_exe = ""alleleCounter"",
      alleles.prefix = allele_prefix,
      loci.prefix = loci_prefix,
      gender = ""XY"",
      genomeVersion = ""hg19"",
      nthreads = 6
      ,minCounts = 10
      ,BED_file = 'targeted.bed'
      ,chrom_names = c(1:22, 'X', 'Y')
      ,min_base_qual = 20
      ,min_map_qual = 35
      ,ref.fasta = 'human_g1k_v37_decoy.fasta'
  
  
  )
  
  
  #Load the data
  ascat.bc = ascat.loadData(
      Tumor_LogR_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_tumourLogR.txt""),
      Tumor_BAF_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_tumourBAF.txt""),
      Germline_LogR_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_normalLogR.txt""),
      Germline_BAF_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_normalBAF.txt""),
      genomeVersion = ""hg19"",
      gender = ""XY""
  )
  
  #Plot the raw data
  ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".before_correction.""))
  
  # optional LogRCorrection
  if(""GC_G1000_hg19"" != ""NULL"") {
      gc_input = paste0(normalizePath(""GC_G1000_hg19""), ""/"", ""GC_G1000_hg19"", "".txt"")
  
      if(""RT_G1000_hg19"" != ""NULL""){
          rt_input = paste0(normalizePath(""RT_G1000_hg19""), ""/"", ""RT_G1000_hg19"", "".txt"")
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = rt_input)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".after_correction_gc_rt.""))
      }
      else {
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = RT_G1000_hg19)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".after_correction_gc.""))
      }
  }
  
  #Segment the data
  ascat.bc = ascat.aspcf(ascat.bc)
  
  #Plot the segmented data
  ascat.plotSegmentedData(ascat.bc)
  
  #Run ASCAT to fit every tumor to a model, inferring ploidy, normal cell contamination, and discrete copy numbers
  #If psi and rho are manually set:
  if (!is.null(NULL) && !is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL, psi_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, psi_manual=NULL)
  } else {
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1)
  }
  
  #Extract metrics from ASCAT profiles
  QC = ascat.metrics(ascat.bc,ascat.output)
  
  #Write out segmented regions (including regions with one copy of each allele)
  write.table(ascat.output[[""segments""]], file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".segments.txt""), sep=""	"", quote=F, row.names=F)
  
  #Write out CNVs in bed format
  cnvs=ascat.output[[""segments""]][2:6]
  write.table(cnvs, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"","".cnvs.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  #Write out purity and ploidy info
  summary <- tryCatch({
          matrix(c(ascat.output[[""aberrantcellfraction""]], ascat.output[[""ploidy""]]), ncol=2, byrow=TRUE)}, error = function(err) {
              # error handler picks up where error was generated
              print(paste(""Could not find optimal solution:  "",err))
              return(matrix(c(0,0),nrow=1,ncol=2,byrow = TRUE))
      }
  )
  colnames(summary) <- c(""AberrantCellFraction"",""Ploidy"")
  write.table(summary, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"","".purityploidy.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  write.table(QC, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".metrics.txt""), sep=""	"", quote=F, row.names=F)
  
  # version export
  f <- file(""versions.yml"",""w"")
  alleleCounter_version = system(paste(""alleleCounter --version""), intern = T)
  ascat_version = sessionInfo()$otherPkgs$ASCAT$Version
  writeLines(paste0('""', ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT"", '""', "":""), f)
  writeLines(paste(""    alleleCounter:"", alleleCounter_version), f)
  writeLines(paste(""    ascat:"", ascat_version), f)
  close(f)

Command exit status:
  1

Command output:
  (empty)

Command error:
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Reading locis
  Reading locis
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
executor >  local (12)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP                                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                                      -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                             -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                                           -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                                                    -
[ff/11e81c] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_ALLELES (G1000_alleles_hg19.zip)                       [100%] 1 of 1 ✔
[be/66776e] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_LOCI (G1000_loci_hg19.zip)                             [100%] 1 of 1 ✔
[12/9fce90] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_GC (GC_G1000_hg19.zip)                                 [100%] 1 of 1 ✔
[4b/1b42c5] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_RT (RT_G1000_hg19.zip)                                 [100%] 1 of 1 ✔
[eb/7924d9] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (targeted.bed)                       [100%] 1 of 1 ✔
[e2/18b339] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (1_27022879-27024037)     [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_ANTITARGET                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_REFERENCE                                   -
[87/22306c] process > NFCORE_SAREK:SAREK:BAM_TO_CRAM (normal_HT1376_S5)                                              [100%] 4 of 4 ✔
[11/a4175d] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT ... [100%] 2 of 2, failed: 2 ✘
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS                                     -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT                                -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL                                 -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY                                   -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                                                 -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC                                                                     -
Execution cancelled -- Finishing pending tasks before exit
-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (tumor_HT1197_S4_vs_normal_HT1197_S4)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (tumor_HT1197_S4_vs_normal_HT1197_S4)` terminated with an error exit status (1)

Command executed:

  #!/usr/bin/env Rscript
  library(RColorBrewer)
  library(ASCAT)
  options(bitmapType='cairo')
  
  #build prefixes: <abspath_to_files/prefix_chr>
  allele_path = normalizePath(""G1000_alleles_hg19"")
  allele_prefix = paste0(allele_path, ""/"", ""G1000_alleles_hg19"", ""_chr"")
  
  loci_path = normalizePath(""G1000_loci_hg19"")
  loci_prefix = paste0(loci_path, ""/"", ""G1000_loci_hg19"", ""_chr"")
  
  #prepare from BAM files
  ascat.prepareHTS(
      tumourseqfile = ""tumor_HT1197_S4.converted.cram"",
      normalseqfile = ""normal_HT1197_S4.converted.cram"",
      tumourname = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour""),
      normalname = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".normal""),
      allelecounter_exe = ""alleleCounter"",
      alleles.prefix = allele_prefix,
      loci.prefix = loci_prefix,
      gender = ""XY"",
      genomeVersion = ""hg19"",
      nthreads = 6
      ,minCounts = 10
      ,BED_file = 'targeted.bed'
      ,chrom_names = c(1:22, 'X', 'Y')
      ,min_base_qual = 20
      ,min_map_qual = 35
      ,ref.fasta = 'human_g1k_v37_decoy.fasta'
  
  
  )
  
  
  #Load the data
  ascat.bc = ascat.loadData(
      Tumor_LogR_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_tumourLogR.txt""),
      Tumor_BAF_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_tumourBAF.txt""),
      Germline_LogR_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_normalLogR.txt""),
      Germline_BAF_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_normalBAF.txt""),
      genomeVersion = ""hg19"",
      gender = ""XY""
  )
  
  #Plot the raw data
  ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".before_correction.""))
  
  # optional LogRCorrection
  if(""GC_G1000_hg19"" != ""NULL"") {
      gc_input = paste0(normalizePath(""GC_G1000_hg19""), ""/"", ""GC_G1000_hg19"", "".txt"")
  
      if(""RT_G1000_hg19"" != ""NULL""){
          rt_input = paste0(normalizePath(""RT_G1000_hg19""), ""/"", ""RT_G1000_hg19"", "".txt"")
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = rt_input)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".after_correction_gc_rt.""))
      }
      else {
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = RT_G1000_hg19)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".after_correction_gc.""))
      }
  }
  
  #Segment the data
  ascat.bc = ascat.aspcf(ascat.bc)
  
  #Plot the segmented data
  ascat.plotSegmentedData(ascat.bc)
  
  #Run ASCAT to fit every tumor to a model, inferring ploidy, normal cell contamination, and discrete copy numbers
  #If psi and rho are manually set:
  if (!is.null(NULL) && !is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL, psi_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, psi_manual=NULL)
  } else {
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1)
  }
  
  #Extract metrics from ASCAT profiles
  QC = ascat.metrics(ascat.bc,ascat.output)
  
  #Write out segmented regions (including regions with one copy of each allele)
  write.table(ascat.output[[""segments""]], file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".segments.txt""), sep=""	"", quote=F, row.names=F)
  
  #Write out CNVs in bed format
  cnvs=ascat.output[[""segments""]][2:6]
  write.table(cnvs, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"","".cnvs.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  #Write out purity and ploidy info
  summary <- tryCatch({
          matrix(c(ascat.output[[""aberrantcellfraction""]], ascat.output[[""ploidy""]]), ncol=2, byrow=TRUE)}, error = function(err) {
              # error handler picks up where error was generated
              print(paste(""Could not find optimal solution:  "",err))
              return(matrix(c(0,0),nrow=1,ncol=2,byrow = TRUE))
      }
  )
  colnames(summary) <- c(""AberrantCellFraction"",""Ploidy"")
  write.table(summary, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"","".purityploidy.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  write.table(QC, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".metrics.txt""), sep=""	"", quote=F, row.names=F)
  
  # version export
  f <- file(""versions.yml"",""w"")
  alleleCounter_version = system(paste(""alleleCounter --version""), intern = T)
  ascat_version = sessionInfo()$otherPkgs$ASCAT$Version
  writeLines(paste0('""', ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT"", '""', "":""), f)
  writeLines(paste(""    alleleCounter:"", alleleCounter_version), f)
  writeLines(paste(""    ascat:"", ascat_version), f)
  close(f)

Command exit status:
  1

Command output:
  (empty)

Command error:
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Reading locis
  Reading locis
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
executor >  local (12)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP                                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                                      -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                             -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                                           -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                                                    -
[ff/11e81c] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_ALLELES (G1000_alleles_hg19.zip)                       [100%] 1 of 1 ✔
[be/66776e] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_LOCI (G1000_loci_hg19.zip)                             [100%] 1 of 1 ✔
[12/9fce90] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_GC (GC_G1000_hg19.zip)                                 [100%] 1 of 1 ✔
[4b/1b42c5] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_RT (RT_G1000_hg19.zip)                                 [100%] 1 of 1 ✔
[eb/7924d9] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (targeted.bed)                       [100%] 1 of 1 ✔
[e2/18b339] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (1_27022879-27024037)     [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_ANTITARGET                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_REFERENCE                                   -
[87/22306c] process > NFCORE_SAREK:SAREK:BAM_TO_CRAM (normal_HT1376_S5)                                              [100%] 4 of 4 ✔
[11/a4175d] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT ... [100%] 2 of 2, failed: 2 ✘
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS                                     -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT                                -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL                                 -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY                                   -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                                                 -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC                                                                     -
Execution cancelled -- Finishing pending tasks before exit
-[nf-core/sarek] Pipeline completed with errors-
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (tumor_HT1197_S4_vs_normal_HT1197_S4)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (tumor_HT1197_S4_vs_normal_HT1197_S4)` terminated with an error exit status (1)

Command executed:

  #!/usr/bin/env Rscript
  library(RColorBrewer)
  library(ASCAT)
  options(bitmapType='cairo')
  
  #build prefixes: <abspath_to_files/prefix_chr>
  allele_path = normalizePath(""G1000_alleles_hg19"")
  allele_prefix = paste0(allele_path, ""/"", ""G1000_alleles_hg19"", ""_chr"")
  
  loci_path = normalizePath(""G1000_loci_hg19"")
  loci_prefix = paste0(loci_path, ""/"", ""G1000_loci_hg19"", ""_chr"")
  
  #prepare from BAM files
  ascat.prepareHTS(
      tumourseqfile = ""tumor_HT1197_S4.converted.cram"",
      normalseqfile = ""normal_HT1197_S4.converted.cram"",
      tumourname = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour""),
      normalname = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".normal""),
      allelecounter_exe = ""alleleCounter"",
      alleles.prefix = allele_prefix,
      loci.prefix = loci_prefix,
      gender = ""XY"",
      genomeVersion = ""hg19"",
      nthreads = 6
      ,minCounts = 10
      ,BED_file = 'targeted.bed'
      ,chrom_names = c(1:22, 'X', 'Y')
      ,min_base_qual = 20
      ,min_map_qual = 35
      ,ref.fasta = 'human_g1k_v37_decoy.fasta'
  
  
  )
  
  
  #Load the data
  ascat.bc = ascat.loadData(
      Tumor_LogR_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_tumourLogR.txt""),
      Tumor_BAF_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_tumourBAF.txt""),
      Germline_LogR_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_normalLogR.txt""),
      Germline_BAF_file = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".tumour_normalBAF.txt""),
      genomeVersion = ""hg19"",
      gender = ""XY""
  )
  
  #Plot the raw data
  ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".before_correction.""))
  
  # optional LogRCorrection
  if(""GC_G1000_hg19"" != ""NULL"") {
      gc_input = paste0(normalizePath(""GC_G1000_hg19""), ""/"", ""GC_G1000_hg19"", "".txt"")
  
      if(""RT_G1000_hg19"" != ""NULL""){
          rt_input = paste0(normalizePath(""RT_G1000_hg19""), ""/"", ""RT_G1000_hg19"", "".txt"")
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = rt_input)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".after_correction_gc_rt.""))
      }
      else {
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = RT_G1000_hg19)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".after_correction_gc.""))
      }
  }
  
  #Segment the data
  ascat.bc = ascat.aspcf(ascat.bc)
  
  #Plot the segmented data
  ascat.plotSegmentedData(ascat.bc)
  
  #Run ASCAT to fit every tumor to a model, inferring ploidy, normal cell contamination, and discrete copy numbers
  #If psi and rho are manually set:
  if (!is.null(NULL) && !is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL, psi_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, psi_manual=NULL)
  } else {
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1)
  }
  
  #Extract metrics from ASCAT profiles
  QC = ascat.metrics(ascat.bc,ascat.output)
  
  #Write out segmented regions (including regions with one copy of each allele)
  write.table(ascat.output[[""segments""]], file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".segments.txt""), sep=""	"", quote=F, row.names=F)
  
  #Write out CNVs in bed format
  cnvs=ascat.output[[""segments""]][2:6]
  write.table(cnvs, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"","".cnvs.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  #Write out purity and ploidy info
  summary <- tryCatch({
          matrix(c(ascat.output[[""aberrantcellfraction""]], ascat.output[[""ploidy""]]), ncol=2, byrow=TRUE)}, error = function(err) {
              # error handler picks up where error was generated
              print(paste(""Could not find optimal solution:  "",err))
              return(matrix(c(0,0),nrow=1,ncol=2,byrow = TRUE))
      }
  )
  colnames(summary) <- c(""AberrantCellFraction"",""Ploidy"")
  write.table(summary, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"","".purityploidy.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  write.table(QC, file=paste0(""tumor_HT1197_S4_vs_normal_HT1197_S4"", "".metrics.txt""), sep=""	"", quote=F, row.names=F)
  
  # version export
  f <- file(""versions.yml"",""w"")
  alleleCounter_version = system(paste(""alleleCounter --version""), intern = T)
  ascat_version = sessionInfo()$otherPkgs$ASCAT$Version
  writeLines(paste0('""', ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT"", '""', "":""), f)
  writeLines(paste(""    alleleCounter:"", alleleCounter_version), f)
  writeLines(paste(""    ascat:"", ascat_version), f)
  close(f)

Command exit status:
  1

Command output:
  (empty)

Command error:
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Reading locis
  Reading locis
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Reading locis
  Reading locis
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Reading locis
  Done reading locis
  Multi pos start:
  [E::sam_itr_next] Null iterator
  [ERROR] (src/bam_access.c: bam_access_get_multi_position_base_counts:379 errno: No such file or directory) Error detected (-2) when trying to iterate through region.
  [ERROR] (./src/alleleCounter.c: main:432 errno: None) Error scanning through bam file for loci list with dense snps.
  munmap_chunk(): invalid pointer
  Error in { : task 1 failed - ""EXIT_CODE == 0 is not TRUE""
  Calls: ascat.prepareHTS -> %dopar% -> <Anonymous>
  Execution halted

Work dir:
  /home/NGS/work/data_reel/nfcore2/work/1f/a0ce397d8fbaf666756097e1485aa1

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/11464851/nextflow.log)


### System information

_No response_",Nour-EddineS,https://github.com/nf-core/sarek/issues/1020
I_kwDOCvwIC85l20Bs,Passing `--wes true` intervals to mosdepth?,CLOSED,2023-05-14T08:51:19Z,2023-08-10T09:04:57Z,2023-08-10T09:04:57Z,"Sequencing depth calculation on exomes (_i.e._ with `--wes true`) currently reports the genome coverage in `mosdepth`, not the exome coverage.

I believe this is because the `--by` parameter is empty in this case. Shouldn't this pass the intervals bed file here? (either the default intervals or the user-specified)

https://github.com/nf-core/sarek/blob/c87f4eb694a7183e4f99c70fca0f1d4e91750b33/conf/modules/modules.config#L64",mschubert,https://github.com/nf-core/sarek/issues/1021
I_kwDOCvwIC85l6LhO,Problem running Strelka due to Python version difference,OPEN,2023-05-15T10:01:55Z,2023-05-29T11:33:38Z,,"### Description of the bug

Hello,

I'm getting Strelka error in running Test and variant calling, as Strelka is using Python2 which is outdated. I got no response from Strelka team regarding Python3 support. But I'm unable to finish test successfully without Strelka. I'm using Sarek with Conda environment on Ubuntu 22.04. If I change the default python from python3 to python2, I was able to run Strelka but there is another python script issue and it needs Python3 to run successfully.

### Command used and terminal output

```console
nextflow run nf-core/sarek -profile test,conda --outdir out
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/11476603/nextflow.log)


### System information

Nextflow version: 23.04
Hardware: Desktop workstation
Executor: Local
Container engine: Conda
OS: Ubuntu 22.04
Sarek version: 3.1.2",Venky2804,https://github.com/nf-core/sarek/issues/1023
I_kwDOCvwIC85l80s5,GATK 4.3.0.0 fatal error has been detected by the Java Runtime Environment,CLOSED,2023-05-15T16:38:20Z,2023-05-17T07:53:52Z,2023-05-17T07:53:52Z,"### Description of the bug

Hi,

I was running Sarek ver 3.1.2 on HPC using singularity profile. The pipeline ends with an error, which has been pasted below. Both, 'singularity version' and  'apptainer version' commands show me '1.1.7-1.el7' as an output. N E X T F L O W  ~  version 23.04.1. It would be nice when you might give me any advice on how to fix this.

Best!

Error executing process > 'NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (CLL_0267)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (CLL_0267)` terminated with an error exit status (250)
 
Command error:
  INFO:    Environment variable SINGULARITYENV_TMPDIR is set, but APPTAINERENV_TMPDIR is preferred
  INFO:    Environment variable SINGULARITYENV_NXF_DEBUG is set, but APPTAINERENV_NXF_DEBUG is preferred 

### Command used and terminal output

```console
./nextflow run nf-core/sarek -r 3.1.2 --wes --tools mutect2,vep --save_bam_mapped --save_output_as_bam --max_memory 60.GB --max_cpus 28 --input /lu/home/wes/sample_sheet_CLL_0267.csv --outdir /lu/home/wes/output --genome GATK.GRCh38 --save_reference -profile singularity
```


### Relevant files

_No response_

### System information

N E X T F L O W  ~  version 23.04.1
HPC
slurm
Singularity
sarek ver 3.1.2",kabanowa47,https://github.com/nf-core/sarek/issues/1024
I_kwDOCvwIC85mC7oB,Filtering of Haplotypecaller VC should only be done for the single-sample route,CLOSED,2023-05-16T13:34:47Z,2023-06-01T09:43:53Z,2023-06-01T09:43:53Z,"### Description of the bug

Basically a similar error as one reported not too long ago on the GATK forums:
https://gatk.broadinstitute.org/hc/en-us/community/posts/4412714547611-FilterVariantTranches-Error-no-variants-with-INFO-score-key-CNN-2D-

The fix mentioned on that page was to make sure that the VCF file passed as input to CNNScoreVariants was a proper VCF file, and NOT a GVCF file.

I've checked the file passed as input to FilterVariantTranches in the working directory where the error occurred, and it appears to be a GVCF file.

The nextflow command that I'm running is:

```
nextflow run nf-core/sarek -r dev -latest \
--input sample-sheet.csv \
--outdir sarek-test-jointVC \
-profile docker \
--max_time '336.h' \
--max_cpus '64' \
--max_memory '256.GB' \
--fasta ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae.R64-1-1-chr.fa \
--fasta_fai ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae.R64-1-1-chr.fa.fai \
--dict ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae.R64-1-1-chr.dict \
--save_mapped \
--save_output_as_bam \
--vep_cache ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae \
--bwa ${main_dir}/reproducible-sarek-error/bwa-index \
--igenomes_ignore \
--genome null \
--joint_germline \
--tools haplotypecaller,vep \
--dbsnp ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae.vcf.gz \
--dbsnp_tbi ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae.vcf.gz.tbi \
--dbsnp_vqsr '--resource:ensemblvcf,known=false,training=true,truth=true,prior=10.0 Saccharomyces_cerevisiae.vcf.gz' \
--known_indels ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae_indels.vcf.gz \
--known_indels_tbi ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae_indels.vcf.gz.tbi \
--known_indels_vqsr '--resource:ensemblsnps,known=false,training=true,truth=true,prior=10.0 Saccharomyces_cerevisiae_indels.vcf.gz' \
--known_snps ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae_snps.vcf.gz \
--known_snps_tbi ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae_snps.vcf.gz.tbi \
--known_snps_vqsr '--resource:ensemblindels,known=false,training=true,truth=true,prior=10.0 Saccharomyces_cerevisiae_snps.vcf.gz' \
--vep_out_format tab \
--vep_include_fasta
```


### Command used and terminal output

_No response_

### Relevant files

[nextflow-error.log.txt](https://github.com/nf-core/sarek/files/11488342/nextflow-error.log.txt)


### System information

_No response_",amizeranschi,https://github.com/nf-core/sarek/issues/1025
I_kwDOCvwIC85mOmyI,warning while running sarek 3.1.2 --step variant_calling ,CLOSED,2023-05-18T06:44:46Z,2023-05-19T08:49:57Z,2023-05-19T08:49:57Z,"### Description of the bug

First I tried to run the whole sarek workflow from the mapping step, yet it automatically stopped after finishing preprocessing steps, the remaining variant_calling and annotation steps weren't executed.
So I tried another way: step by step, and start by variant_calling step, however, no results regarding variant_calling were generated even though it showed that the pipeline was finished.

### Command used and terminal output

```console
nextflow run /dssg/home/acct-medlhp/medlhp-wzc/software/sarek --step variant_calling --tools Strelka --input /dssg/home/acct-medlhp/medlhp-wzc/project/sarek_jobs/results/0517/csv/recalibrated.csv --igenomes_base /dssg/home/acct-medlhp/medlhp-wzc/references  --outdir /dssg/home/acct-medlhp/medlhp-wzc/project/sarek_jobs/results/0517-2 --genome GATK.GRCh38 -profile singularity
```


### Relevant files


[25080362.zip](https://github.com/nf-core/sarek/files/11505103/25080362.zip)


### System information

Hardware (HPC)
Executor (slurm)
Container engine: (Singularity)
OS (CentOS Linux)",FrancisNietzsche,https://github.com/nf-core/sarek/issues/1026
I_kwDOCvwIC85mWHL0,Pytest for VQSR-flow,CLOSED,2023-05-19T11:09:08Z,2023-08-16T19:26:20Z,2023-08-16T19:26:20Z,"### Description of feature

We are lacking a pytest of the VQSR-flow. (Both the GATK version and the upcoming Sentieon version.)

The challenge is that VQSR crashes on our current testdata for joint-germline. (Problems include too little variation in testdata and testdata based on a different reference-genome than the one used for the VQSR-reference-files in iGenomes.)

Here is a long Slack conversation where I try to explain the problems with constructing the desired pytest.

https://nfcore.slack.com/archives/C02MDBZAYJK/p1684238360591329

This is how I run the VQSR-flow manually:

GATK:
` nextflow run main.nf -profile docker --input mapped_joint_bam_HG002_HG003_1perc_of_40x.csv  --tools haplotypecaller --step variant_calling --joint_germline --outdir results --skip_tools haplotypecaller_filter`

Sentieon:
`nextflow run main.nf -profile docker --input mapped_joint_bam_HG002_HG003_1perc_of_40x.csv  --tools sentieon_haplotyper --step variant_calling --joint_germline --outdir results --skip_tools haplotypecaller_filter --sentieon_haplotyper_out_format gvcf
`

(The syntax for the Sentieon-cmd may be changed.)",asp8200,https://github.com/nf-core/sarek/issues/1027
I_kwDOCvwIC85mXAyV,Shared results-folder between different pipeline runs?,OPEN,2023-05-19T14:04:48Z,2023-05-19T14:09:09Z,,"### Description of feature

It is possible to run Sarek repeatedly for the same sample or different samples using the same output-folder (results-folder), and some users do run Sarek like that.

Currently there are a number of issues with reusing the results-folder.

1. The multiqc-data gets overwritten,
2. joint-germline-data gets overwritten,
3. `csv/variantcalled.csv` gets overwritten, (there is probably more files that get overwritten)
4. if starting more pipelines at the exact same time, then different pipelines may construct pipeline-info-files with exactly the same name. (Tower actually did this at DNGC; bug reported to Rob Syme and Harshil Patel.)
5. Using the same results-folder for different pipeline-runs makes it very difficult to subsequently, say, find and delete all files from some particular pipeline-run.   

Should we try to solve those issue?

**Perhaps we just want to advise against using the same results-folder for different pipeline-runs?**

N.B.  What about work-folders?  On re-runs (`-resume`) one, of course, wants to use the same work-folder, but what about work-folders for pipeline-runs of different samples? I see no reason that they should share work-folders. (I know that the subfolders with the hash-strings make any clashes/conflicts between pipelines sharing the same work-folder very unlikely, but still?) ",asp8200,https://github.com/nf-core/sarek/issues/1028
I_kwDOCvwIC85mXWUj,Stop Sarek as early as possible when input-data has too low quality,OPEN,2023-05-19T15:01:03Z,2023-05-19T15:03:07Z,,"### Description of feature

If some QC-tool in Sarek like, say for instance FASTQC, early on in the pipeline-run finds that input-data has too low quality for passing QC, then perhaps the pipeline should stop automatically based on that instead of continuing to waste time and resources on running subsequent jobs which will only fail or produce useless data?

Apparently, the nf-core-pipeline `rnaseq` has some QC-functionality like that.

Maxime mentions that perhas we would add an option like `--qc_limit` or whatever to control the QC-treshold.

Let's take a look at how `rnaseq` is doing this.

N.B. Perhaps the user would be interested in the output from other QC-tools even if, say, FASTQC reports input-data with too low quality? But other steps like variantcalling and annotation should probably be skipped.",asp8200,https://github.com/nf-core/sarek/issues/1029
I_kwDOCvwIC85mcjg5,Exit status 250 and SIGBUS (0x7) errors on GATK processes,CLOSED,2023-05-22T02:11:54Z,2023-09-19T06:27:34Z,2023-09-19T06:27:33Z,"### Description of the bug

I'm encountering the error described in #1024.

Briefly, I'm running nf-sarek using standard parameters on an HPC using singularity. I encounter this error on GATK components intermittently - some steps succeed on resubmission. 

I've had a look at the following during debugging:

- The /tmp directories for the machines on which nf-sarek is running seem to have space.
- Adding a config.config with process.scratch = false doesn't fix the issue.
- Cloning the git repo and adding `gatk --java-options ""-Djava.io.tmpdir=. -Xmx4g""` to set the tmp dir still results in the error.

Any thoughts on how best to debug?

### Command used and terminal output

```console
../20230420_Garvan_SLOW5_conversion/nextflow run nf-core/sarek -profile wehi -work-dir /vast/scratch/users/fearnley.l/NGS_SAREK/ --step mapping --tools deepvariant,haplotypecaller,strelka,freebayes,manta,merge,cnvkit --save_mapped True --igenomes_base /vast/projects/fearnleyl_ukbiobank/20230518_NGS_Trios/references/ -resume --input /vast/projects/fearnleyl_ukbiobank/20230518_NGS_Trios/20230518_31666_sarek_manifest.csv --outdir /vast/projects/fearnleyl_ukbiobank/20230518_NGS_Trios/20230518_sarek_output/31666/ -c config.config

Results in

Caused by:                                                                                               [37/1961]
  Process `NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (31666)`terminated with an error exit status (250)

Command executed:

  gatk --java-options ""-Xmx4g"" BaseRecalibrator  \
      --input 31666.md.cram \
      --output 31666_chr17_491112-21795850.recal.table \
      --reference Homo_sapiens_assembly38.fasta \
      --intervals chr17_491112-21795850.bed \
      --known-sites dbsnp_146.hg38.vcf.gz --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-s
ites Homo_sapiens_assembly38.known_indels.vcf.gz \
      --tmp-dir . \

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  250

Command output:
  #
  # A fatal error has been detected by the Java Runtime Environment:
  #
  #  SIGBUS (0x7) at pc=0x00002b04cce71b0d, pid=93, tid=94
  #
  # JRE version:  (11.0.15) (build )
  # Java VM: OpenJDK 64-Bit Server VM (11.0.15-internal+0-adhoc..src, mixed mode, sharing, tiered, compressed oops
, g1 gc, linux-amd64)
  # Problematic frame:
  # C  [libc.so.6+0x15cb0d]
  #
  # Core dump will be written. Default location: Core dumps may be processed with ""/usr/libexec/abrt-hook-ccpp %s %c %p %u %g %t e %P %I %h"" (or dumping to core.93)
  #
  # An error report file with more information is saved as:
  # hs_err_pid93.log
  #
  #

Command error:
  WARNING: While bind mounting '/vast:/vast': destination is already in the mount point list
  Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=. -Xmx4g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar BaseRecalibrator --input 31666.md.cram --output 31666_chr17_491112-21795850.recal.table --reference Homo_sapiens_assembly38.fasta --intervals chr17_491112-21795850.bed --known-sites dbsnp_146.hg38.vcf.gz --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites Homo_sapiens_assembly38.known_indels.vcf.gz --tmp-dir .

Work dir:
  /vast/scratch/users/fearnley.l/NGS_SAREK/69/9f297ba2d5a7a7c16e2ab8304e0184

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`

 -- Check '.nextflow.log' file for details
```


### Relevant files

_No response_

### System information

nextflow version 23.04.1.5866
HPC
slurm
Singularity
CentOS
Sarek 3.1.2",lfearnley,https://github.com/nf-core/sarek/issues/1030
I_kwDOCvwIC85mpY9x, Inquiry Regarding Somatic Analysis and Normal Sample Requirement,CLOSED,2023-05-23T13:48:11Z,2023-06-14T06:21:10Z,2023-06-14T06:21:10Z,"### Description of the bug

Dear All,

I have two questions:

1.    I plan to perform somatic analysis on 10 different tumor BAM files to determine the SNPs, indels, and CNVs. Is it necessary to have 10 distinct normal files corresponding to each tumor BAM file, or can I use a single normal file for all 10 BAM files?

2.    If I don't have a normal file and still want to perform somatic analysis, is there a solution to do it without requiring a normal sample?

Best regards,

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",Sadiki-Noureddine,https://github.com/nf-core/sarek/issues/1033
I_kwDOCvwIC85mtiGR,onComplete-Mailing fails when running awsbatch-s3 and attaching multiQC report,CLOSED,2023-05-24T05:41:24Z,2024-08-30T16:20:29Z,2024-08-30T16:20:29Z,"### Description of the bug

When running Sarek in `--profile awsbatch` using s3 as work and results folder, the Sarek-oncomplete mail-out [activated here](https://github.com/nf-core/sarek/blob/c87f4eb694a7183e4f99c70fca0f1d4e91750b33/workflows/sarek.nf#LL1105C81-L1105C95) fails. When inserting `multiqc_report.view()` in the code directly above it prints:
```
/bucket/work/eb/fe52c40d4185bd22d6078b45ec32d7/multiqc_report.html
```
This would be a correct path to a multiqc-report when running e.g. local docker, but it seems to not resolve correctly when given to the sendmail command [here](https://github.com/nf-core/sarek/blob/c87f4eb694a7183e4f99c70fca0f1d4e91750b33/lib/NfcoreTemplate.groovy#L76-L92). Then the mail-out fails silently, [logging](https://github.com/nf-core/sarek/blob/c87f4eb694a7183e4f99c70fca0f1d4e91750b33/lib/NfcoreTemplate.groovy#L125) `Sent summary e-mail to $email_address (sendmail)-""`, but not sending any email.

I've made the following tests to establish this is the cause:
1. adding `-N myemail@gmail.com` to the nextflow command, for 'notify', does produce a generic _nextflow_-notification email, so sendmail system is configured correctly and ports are open
2. Setting `plaintext_email = true` in nextflow.config does produce an additional _nfcore/sarek_-notification email, without multiqc report as expected.
3. Setting `max_multiqc_email_size = 0` does produce the same additional _nfcore/sarek_-notification email, without multiqc report as expected.

This [github issue](https://github.com/Arcadia-Science/seqqc/pull/41) seems related, but still not exactly the same.

### Command used and terminal output

```console
nextflow run /home/ubuntu/Nucleus-sarek/main.nf -profile awsbatch --input 'samples.csv' --step annotate  --tools 'snpeff' --email 'myemail@gmail.com' -N 'myemail@gmail.com'
#no email

nextflow run /home/ubuntu/Nucleus-sarek/main.nf -profile awsbatch --input 'samples.csv' --step annotate  --tools 'snpeff' --email 'myemail@gmail.com' -N 'myemail@gmail.com' --max_multiqc_email_size 0
#yes email
```


### Relevant files

_No response_

### System information

- nextflow version 22.10.3.5834
- sarek version 3.0.1 (ok I'm slow, but the relevant code is unchanged up until the 3.1.2)
- Ubuntu 22.04 LTS
- AWS instance",lassefolkersen,https://github.com/nf-core/sarek/issues/1034
I_kwDOCvwIC85mvUOs,"Error with GATK4_GENOMICSDBIMPORT during joint calling: Badly formed genome unclippedLoc: Query interval ""[]"" is not valid for this input",CLOSED,2023-05-24T10:34:10Z,2023-06-08T12:14:42Z,2023-06-02T19:28:07Z,"### Description of the bug

Hello,

I'm getting the error from the title while running joint calling with GATK. I'm attaching the full log below.

I am running `nf-core/sarek -r cbdaaa0`. I can provide my script if needed, for reproducibility.


### Command used and terminal output

_No response_

### Relevant files

[nextflow-error.log.txt](https://github.com/nf-core/sarek/files/11553464/nextflow-error.log.txt)


### System information

_No response_",amizeranschi,https://github.com/nf-core/sarek/issues/1035
I_kwDOCvwIC85mzL_M,Fix fastp v 0.23.3,OPEN,2023-05-24T20:32:52Z,2023-08-15T09:07:53Z,,"Not working currently in Sarek, but CI is working in modules.
Need to investigate more.

cf https://github.com/nf-core/sarek/pull/1032",maxulysse,https://github.com/nf-core/sarek/issues/1036
I_kwDOCvwIC85mzMJF,Investigate failing concatenate_vcfs singularity tests,CLOSED,2023-05-24T20:33:26Z,2023-05-25T11:38:27Z,2023-05-25T11:38:27Z,cf https://github.com/nf-core/sarek/issues/1032,maxulysse,https://github.com/nf-core/sarek/issues/1037
I_kwDOCvwIC85m2JTL,Mosdepth in sarek runs default with --fast-mode. Could this be made configurable?,OPEN,2023-05-25T09:40:51Z,2023-08-05T22:41:53Z,,"### Description of feature

Mosdepth runs with --fast-mode per default - meaning that mosdepth dont look at internal cigar operations or correct mate overlaps. This is also recommended  by the creators of mosdepth in most use-cases.

However, some libraries are created with mate pair overlap between 10-30 % of bases, meaning that the coverage might loook artificial high. In variant calling mate-pair overlaps are often seen as one read. 

It could be nice if it was possible to skip the --fast-mode flag in mosdepth.

Otherwise thanks for a great tool! :) ",lasseringstedmark,https://github.com/nf-core/sarek/issues/1038
I_kwDOCvwIC85m5uwP,Rename channels genotype_intervals,OPEN,2023-05-25T19:57:00Z,2023-05-25T19:57:26Z,,"### Description of feature

Rename channels `genotype_intervals` to `genotype_intervals_and_crams`.

https://github.com/nf-core/sarek/pull/1007/files#r1205948269

Probably needs to be done here:

`bam_variant_calling_germline_all`

`bam_variant_calling_haplotypecaller`

`bam_variant_calling_sentieon_haplotyper`",asp8200,https://github.com/nf-core/sarek/issues/1042
I_kwDOCvwIC85m-5ZJ,Avoid redundant index-generation in Sentieon-flow.,CLOSED,2023-05-26T14:43:55Z,2024-12-09T16:22:53Z,2023-07-07T18:07:52Z,"### Description of feature

The sentieon-bwamem module actually generate index-files (default behaviour, cannot be disabled) but currently Sarek recomputes the index-files, since that is what is done for the other aligners. It would be great if we could avoid the redundant index generation when Sarek uses Sentieon-bwamem for alignment.

",asp8200,https://github.com/nf-core/sarek/issues/1046
I_kwDOCvwIC85m-9FL,Update Sentieon-modules,CLOSED,2023-05-26T14:54:15Z,2024-12-09T16:22:54Z,2023-07-07T18:07:31Z,"### Description of feature

First to https://github.com/nf-core/modules/issues/3460",asp8200,https://github.com/nf-core/sarek/issues/1047
I_kwDOCvwIC85nOwfu,Allow genotyping germline sites with Mutect 2,OPEN,2023-05-30T09:55:53Z,2023-06-01T05:51:10Z,,"### Description of feature

Some software, like PureCN, expects to have germline sites called along with somatic sites when using MuTect 2. However, currently there's no way to set `--genotype-germline-sites true` and  `--genotype-pon-sites true` to the caller arguments, or at least not that I've found with a search in the workflows.

This should be optional, and default to off. 

Reference: https://bioconductor.org/packages/3.16/bioc/vignettes/PureCN/inst/doc/Quick.html#3_Create_VCF_files",lbeltrame,https://github.com/nf-core/sarek/issues/1049
I_kwDOCvwIC85nXEqY,Update VEP version,CLOSED,2023-05-31T12:21:40Z,2023-08-06T08:14:32Z,2023-08-06T08:14:00Z,"### Description of the bug

The version currently included is v106, which is missing import annotation flags like --af_gnomag (gnomAD frequencies from genomes, not only exomes).

Please consider updating it.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",stfacc,https://github.com/nf-core/sarek/issues/1052
I_kwDOCvwIC85nd749,Refactor joint germline calling,CLOSED,2023-06-01T09:41:50Z,2023-08-16T19:27:01Z,2023-08-16T19:27:01Z,"So this established back the old behavior. The pipeline was not supposed to do filtering for joint germline, but single sample only, (where it sometimes can be skipped because it takes a long time): https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels- 

I think we might need to refactor this subworkflow a bit to make it more intuitive.  and put the whole FilterVarianttranches/CNNScore part into a singlesample haplotypecaller subworkflow, but I would do that in a separate PR

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/issues/1050#issuecomment-1569666254_",maxulysse,https://github.com/nf-core/sarek/issues/1053
I_kwDOCvwIC85nfCCi,results folder not seen in sarek output,CLOSED,2023-06-01T12:10:38Z,2023-06-06T06:27:43Z,2023-06-06T06:27:43Z,"### Description of the bug

Hi,

I'm running Sarek on N-T pair samples and would like to have access to recalibrated bam/bai files. Surprisingly, I have not been able find the results folder for me to locate the bam files as indicated in the documentation as follows:

`results/Preprocessing/[SAMPLE]/Recalibrated`

`[SAMPLE].recal.bam and [SAMPLE].recal.bai`

How can I find the bam files?

Thanks

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",olu2016,https://github.com/nf-core/sarek/issues/1054
I_kwDOCvwIC85nx8kw,Remove redundant index computation after MarkDuplicates,CLOSED,2023-06-05T07:22:27Z,2023-06-15T06:22:29Z,2023-06-15T06:22:29Z,"### Description of the bug

what the title says

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1065
I_kwDOCvwIC85nyCnI,Future proof Deepvariant output,OPEN,2023-06-05T07:41:17Z,2023-06-05T07:41:17Z,,"              If we ever do anything on both vcfs we'll have the same issue as with Manta. Maybe for safety we can fix this. ( can be after the release)

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1057#discussion_r1214603731_
            ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1066
I_kwDOCvwIC85nyC6D,Update samtools version for cnvkti,CLOSED,2023-06-05T07:41:54Z,2023-08-15T09:04:52Z,2023-08-15T09:04:51Z,"              need a new mulled container here

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1057#discussion_r1217560905_
            ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1067
I_kwDOCvwIC85nyDO4,Update conda gatk version,CLOSED,2023-06-05T07:42:40Z,2023-06-16T09:13:27Z,2023-06-16T09:13:27Z,"              Conda is still using older gatk version

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1057#discussion_r1214595005_
            ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1068
I_kwDOCvwIC85nyDdq,Remove redundant index computation for DeepVariant,CLOSED,2023-06-05T07:43:18Z,2023-06-12T15:38:36Z,2023-06-12T15:38:36Z,"              Looks like the deepvariant module now provides the indices directly, so this can be removed

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1057#discussion_r1217563943_
            ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1069
I_kwDOCvwIC85nyJH_,Revisit resource requests,OPEN,2023-06-05T07:58:30Z,2023-06-05T07:58:30Z,,"              hm this seems excessive for sarek.

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1057#discussion_r1217562527_
            ",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1072
I_kwDOCvwIC85n7-Hx,Test full is using wrong config name,CLOSED,2023-06-06T12:31:13Z,2023-06-08T07:45:05Z,2023-06-08T07:45:05Z,"### Description of the bug

Should be aws_tower, but currently says `public_aws_tower` which does not exist

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1075
I_kwDOCvwIC85n8Dbr,Feature: BAM inputs to align step for re-alignment,OPEN,2023-06-06T12:44:12Z,2023-06-06T13:01:58Z,,"### Description of feature

Differences between genome versions are a pain. Your GRCh37 is probably not 100% the same as my GRCh37. If someone sends me some BAM files I can't guarantee they are compatible, or worse still, don't produce a silent error. To solve this, we could re-align to the genome by extracting the sequencing data and re-processing, but this can be tricky and time consuming.

BWA (and some other tools such as FASTQC) accept a BAM file in place of a set of FASTQ files. If we want to re-align a set of BAM files, we could supply BAMs as inputs then realign them with BWA before processing as normal. It's not clear how we would implement this, perhaps it could be if `step = realign` or perhaps it just works if you supply a BAM and the step is `mapping`. The created realigned BAM channel will be injected into the normal BAM workflow.",adamrtalbot,https://github.com/nf-core/sarek/issues/1076
I_kwDOCvwIC85oKGfe,Fatal error when downloading sarek pipeline v2.7.1,OPEN,2023-06-08T09:30:11Z,2024-08-19T15:27:07Z,,"### Description of the bug

Singularity container retrieval failed when trying to download the nf-core/sarek pipeline. While making image from oci registry I got an error fetching image to cache and failed to get checksum for docker image.

### Command used and terminal output

```console
Input:
$ export NXF_SINGULARITY_CACHEDIR=""/tmp/singularity_tmp/"" && nf-core download sarek -r 2.7.1 --container singularity --compress none --force

Output:

INFO     Saving 'nf-core/sarek'                                                                                                                                                                                    
          Pipeline revision: '2.7.1'                                                                                                                                                                               
          Pull containers: 'singularity'                                                                                                                                                                           
          Using $NXF_SINGULARITY_CACHEDIR': /tmp/singularity_tmp/                                                                                                                                                  
          Output directory: 'nf-core-sarek-2.7.1'                                                                                                                                                                  
INFO     Downloading workflow files from GitHub                                                                                                                                                                    
INFO     Downloading centralised configs from GitHub                                                                                                                                                               
INFO     Found 4 containers                                                                                                                                                                                        
Pulling singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  50% • 2/4 completed
{(params.annotation_cache && params.snpeff_cache) ? 'nfcore/sarek:2.7.1' : ""nfcore/sareksnpeff:2.7.1.${params.genome}""}  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━INFO     Singularity container retrieval fialed with the following error:                                                                                                                                          
INFO     FATAL:   While making image from oci registry: error fetching image to cache: failed to get checksum for docker://{(params.annotation_cache && params.snpeff_cache) ? 'nfcore/sarek:2.7.1' :              
         ""nfcore/sareksnpeff:2.7.1.${params.genome}""}: unable to parse image name docker://{(params.annotation_cache && params.snpeff_cache) ? 'nfcore/sarek:2.7.1' : ""nfcore/sareksnpeff:2.7.1.${params.genome}""}:
         invalid reference format                                                                                                                                                                                  
Pulling singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  50% • 2/4 completed
{(params.annotation_cache && params.snpeff_cache) ? 'nfcor… FATAL:   While making image from oci registry: error fetching image to cache: failed to get checksum for docker://{(params.annotation_cache && params… 
CRITICAL The container ""{(params.annotation_cache && params.snpeff_cache) ? 'nfcore/sarek:2.7.1' : ""nfcore/sareksnpeff:2.7.1.${params.genome}""}"" is unavailable.                                                   
         FATAL:   While making image from oci registry: error fetching image to cache: failed to get checksum for docker://{(params.annotation_cache && params.snpeff_cache) ? 'nfcore/sarek:2.7.1' :              
         ""nfcore/sareksnpeff:2.7.1.${params.genome}""}: unable to parse image name docker://{(params.annotation_cache && params.snpeff_cache) ? 'nfcore/sarek:2.7.1' : ""nfcore/sareksnpeff:2.7.1.${params.genome}""}:
         invalid reference format
```


### Relevant files

![image](https://github.com/nf-core/sarek/assets/92781950/599eb66f-4a70-4c0c-b6eb-ab06b96a8bfc)


### System information

Nextflow version: 23.04.1.5866
Hardware: Desktop
Executor: Local
Container engine: Singularity
OS: Linux
Version of nf-core/sarek: 2.8",solcos,https://github.com/nf-core/sarek/issues/1083
I_kwDOCvwIC85oLMwF,Ensembl VEP is skipped when using custom a custom genome (--igenomes_ignore --genome null),CLOSED,2023-06-08T12:10:25Z,2023-06-08T21:12:21Z,2023-06-08T21:12:20Z,"### Description of the bug

Using Nextflow v. 23.04.1 and sarek v. 3.2.0, Ensembl VEP is being executed fine with a built-in genome, but it ends up getting silently skipped (with no error or warning in the logs) with a custom genome for the same species. I tested this in yeast, as follows:

With a custom genome, the command below skips VEP:

```
nextflow run nf-core/sarek -r 3.2.0 \
--skip_tools haplotypecaller_filter,baserecalibrator,markduplicates \
-profile docker \
--input sample-sheet.csv \
--outdir sarek-test-jointVC \
--max_time '336.h' \
--max_cpus '64' \
--max_memory '256.GB' \
--fasta ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae.R64-1-1-chr.fa \
--fasta_fai ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae.R64-1-1-chr.fa.fai \
--dict ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae.R64-1-1-chr.dict \
--save_mapped \
--vep_cache ${main_dir}/reproducible-sarek-error/Saccharomyces_cerevisiae \
--bwa ${main_dir}/reproducible-sarek-error/bwa-index \
--igenomes_ignore \
--genome null \
--tools haplotypecaller,vep \
--vep_out_format tab \
--vep_include_fasta
```

With the built-in genome, VEP is being executed:

```
nextflow run nf-core/sarek -r 3.2.0 \
--skip_tools haplotypecaller_filter,baserecalibrator,markduplicates \
-profile docker \
--input sample-sheet.csv \
--outdir sarek-test-jointVC \
--max_time '336.h' \
--max_cpus '64' \
--max_memory '256.GB' \
--save_mapped \
--genome R64-1-1 \
--tools haplotypecaller,vep \
--vep_out_format tab \
--vep_include_fasta
```

I am including two log files below, one for a test in which VEP was skipped and one where VEP was executed.


### Command used and terminal output

_No response_

### Relevant files

[this-skipped-vep.log.txt](https://github.com/nf-core/sarek/files/11687597/this-skipped-vep.log.txt)

[this-ran-vep.log.txt](https://github.com/nf-core/sarek/files/11687592/this-ran-vep.log.txt)


### System information

_No response_",amizeranschi,https://github.com/nf-core/sarek/issues/1084
I_kwDOCvwIC85oMVLJ,GATK4_CREATESEQUENCEDICTIONARY memory misspecification,CLOSED,2023-06-08T14:47:52Z,2023-06-09T11:51:17Z,2023-06-09T11:51:17Z,"Hi team, thanks for releasing sarek version 3.2.1!  

While running the test profile (`nextflow run nf-core/sarek -r 3.2.1 -profile test,singularity --outdir results -c my.config`) I found that `GATK4_CREATESEQUENCEDICTIONARY` fails with `Error: A JNI error has occurred, please check your installation and try again`.  It appears the module accidentally specifies 6MB memory rather than 6GB for this task.  
Relevant code:
https://github.com/nf-core/sarek/blob/6c0d335e17fb4406f527540631da7b26a5fe1464/modules/nf-core/gatk4/createsequencedictionary/main.nf#LL22C1-L30C72

Proposed downstream fix would be to change the module.  In the meantime, adding the following to my config file has worked for me:
```
withName: 'GATK4_CREATESEQUENCEDICTIONARY' {
        memory       = { 6.GB * task.attempt }
    }
```",mjakobs,https://github.com/nf-core/sarek/issues/1085
I_kwDOCvwIC85oNpbU,The Singularity fails while attempting to pull a specific image required for offline usage of Sarek.,CLOSED,2023-06-08T18:05:10Z,2023-06-08T19:27:24Z,2023-06-08T19:27:24Z,"### Description of the bug

Following the verification that I had the latest versions of nf-core, Nextflow, and Singularity installed, I tried to download Sarek for offline usage. Regardless of the Sarek version I aimed to download, Singularity always encountered a failure at the same stage: pulling the GATK image. The corresponding error message was as follows:

Found 36 containers
Pulling singularity images
• 34/36 completed
broadinstitute/gatk4.3.0.0 Copying from cache to target directory
**CRITICAL [Errno 2] No such file or directory:
path..../broadinstitute-gatk-4.3.0.0.img**

### Command used and terminal output

```console
nf-core download sarek -r 3.1.2 --container singularity
```


### Relevant files

<img width=""1440"" alt=""Screenshot 2023-06-08 at 12 49 45"" src=""https://github.com/nf-core/sarek/assets/100424993/ec32bfc1-7c33-4163-b569-8ac9cdbcdb1f"">


### System information

nextflow version 23.04.1.5866
Hardware HPC
executor local
singularity-ce version 3.11.3
""CentOS Linux""
sarek 3.1.2",Hadi90Eidgah,https://github.com/nf-core/sarek/issues/1086
I_kwDOCvwIC85oS3Am,Duplicated parameter setting line in the main workflow ,CLOSED,2023-06-09T12:09:39Z,2023-06-12T09:21:41Z,2023-06-12T09:21:40Z,"The main workflow has the following line duplicated twice:
https://github.com/nf-core/sarek/blob/6c0d335e17fb4406f527540631da7b26a5fe1464/workflows/sarek.nf#L149-L150

Either this is just a duplication and can be deleted or one line should be a different parameter.
",SPPearce,https://github.com/nf-core/sarek/issues/1090
I_kwDOCvwIC85oUc4I,nf-core download sarek --outdir nf-core-sarek --container singularity --compress none --revision 3.2.1,CLOSED,2023-06-09T16:38:25Z,2023-08-06T08:12:09Z,2023-08-06T08:12:09Z,"### Description of the bug

Hi, 

this is my first time to use not only nf-core but also nextflow. My task is to make the software running on a HPC from our university. I decided to first download the code as Singularity container. Somehow I cannot retrieve nf-core/deepvariant:1.5.0, please see below.

Best,
Christian

### Command used and terminal output

```console
nf-core download sarek --outdir nf-core-sarek --container singularity --compress none --revision 3.2.1 --force

                                          ,--./,-.
          ___     __   __   __   ___     /,-._.--~\
    |\ | |__  __ /  ` /  \ |__) |__         }  {
    | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                          `._,._,'

    nf-core/tools version 2.8 - https://nf-co.re



Nextflow and nf-core can use an environment variable called $NXF_SINGULARITY_CACHEDIR that is a path to a directory where remote Singularity images are stored. This allows downloaded 
images to be cached in a central location.
? Define $NXF_SINGULARITY_CACHEDIR for a shared Singularity image download folder? [y/n]: n
WARNING  Deleting existing output directory: 'nf-core-sarek'                                                                                                                                
INFO     Saving 'nf-core/sarek'                                                                                                                                                             
          Pipeline revision: '3.2.1'                                                                                                                                                        
          Pull containers: 'singularity'                                                                                                                                                    
          Output directory: 'nf-core-sarek'                                                                                                                                                 
INFO     Downloading workflow files from GitHub                                                                                                                                             
INFO     Downloading centralised configs from GitHub                                                                                                                                        
INFO     Found 35 containers                                                                                                                                                                
Downloading singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  46% • 16/35 completed
fgbio:2.0.2--hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.1% • 436.7/589.6 MB • 7.0 MB/s
gatk4:4.4.0.0--py36hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 92.4% • 628.9/680.5 MB • 10.9Downloading singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  46% • 16/35 completed
fgbio:2.0.2--hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.2% • 437.4/589.6 MB • 7.0 MB/s
gatk4:4.4.0.0--py36hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 92.6% • 629.9Downloading singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  46% • 16/35 complet
fgbio:2.0.2--hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.3% • 438.1/589.6 MB • 7.0 MB
gatk4:4.4.0.0--py36hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 92.7% • 631.0/680.5 Downloading singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  46% • 16/35 completed
fgbio:2.0.2--hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 74.4% • 438.8/589.6 MB • 7.0 MB/s
gatk4:4.4.0.0--py36hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 92.9% • 632.0/680.5Downloading singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  46% • 16/35 complete
fgbio:2.0.2--hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 74.5% • 439.5/589.6 MB • 7.0 MB/
gatk4:4.4.0.0--py36hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 93.0% • 633.1/680.5 MB Downloading singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  46% • 16/35 completed
fgbio:2.0.2--hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 74.7% • 440.1/589.6 MB • 7.0 MB/s
gatk4:4.4.0.0--py36hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 93.2% • 634.1/680.5 MB • Downloading singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  46% • 16/35 completed
fgbio:2.0.2--hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 74.8% • 440.9/589.6 MB • 7.0 MB/s
gatk4:4.4.0.0--py36hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 93.3% • 635.2/680.5 MB • 9.Downloading singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  46% • 16/35 completed
fgbio:2.0.2--hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 74.9% • 441.6/589.6 MB • 7.0 MB/s
gatk4:4.4.0.0--py36hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 93.5% • 636.2/680.5 MB • 9.8Downloading singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  46% • 16/35 completed
fgbio:2.0.2--hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 75.0% • 442.3/589.6 MB • 7.0 MB/s
gatk4:4.4.0.0--py36hdfd78af_0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 93.6% • 637.2/680.5 MB • 10.9 MB/DDoDDownloadinPulling singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━  94% • 33/35 completed
nf-core/deepvariant:1.5.0  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━INFO     Singularity container retrieval fialed with the following error:                                                           
INFO     FATAL:   While making image from oci registry: error fetching image to cache: failed to get checksum for                   
         docker://nf-core/deepvariant:1.5.0: reading manifest 1.5.0 in docker.io/nf-core/deepvariant: errors:                       
         denied: requested access to the resource is denied                                                                         
         unauthorized: authentication required                                                                                      
Pulling singularity images ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━  94% • 33/35 completed
nf-core/deepvariant:1.5.0 unauthorized: authentication required ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
CRITICAL The container ""nf-core/deepvariant:1.5.0"" is unavailable.                                                                  
         FATAL:   While making image from oci registry: error fetching image to cache: failed to get checksum for                   
         docker://nf-core/deepvariant:1.5.0: reading manifest 1.5.0 in docker.io/nf-core/deepvariant: errors:                       
         denied: requested access to the resource is denied                                                                         
         unauthorized: authentication required
```


### Relevant files

_No response_

### System information

- nextflow version 23.04.1.5866
- HPC
- local
- ?
- Red Hat Enterprise Linux 8.5
- 3.2.1",ChristianRohde,https://github.com/nf-core/sarek/issues/1092
I_kwDOCvwIC85oV8fK,Error reported at the `GATHERPILEUPSUMMARIES` step causing the workflow to abort,CLOSED,2023-06-09T22:53:28Z,2023-06-12T16:34:28Z,2023-06-12T13:32:07Z,"### Description of the bug

While running `sarek v3.2.1` with Mutect2 on two batches of WES files, I am repeatedly getting the following error:

```
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GATHERPILEUPSUMMARIES_NORMAL (XX-A)'
.
.
.
A USER ERROR has occurred: Bad input: format error in 'XX-A.mutect2.pileups.table' at line 0: premature end of table: header line not found
```
For one batch this error occurs for the same sample every time I ran the workflow, for the other batch this error occurs for different samples in different runs.


### Command used and terminal output
```
nextflow run nf-core/sarek
--input samplesheet.csv 
--wes true 
--igenomes_base sage-igenomes/igenomes
--genome GATK.GRCh38
--tools strelka,mutect2
--intervals custom_withChr_GRCh38_sorted.bed
--outdir ./batch1/
```
### Relevant files

_No response_

### System information

_No response_",jaybee84,https://github.com/nf-core/sarek/issues/1094
I_kwDOCvwIC85oc4ig,Error with BAM_MERGE_INDEX when using bwa-mem2: Unsorted positions on sequence #4: 1260047 followed by 1259502,CLOSED,2023-06-12T10:14:47Z,2023-06-14T06:30:37Z,2023-06-14T06:30:37Z,"### Description of the bug

I'm getting an error about unsorted reads at the `BAM_MERGE_INDEX` stage, when using BWA-mem2. As far as I could tell, it only happens with bwa-mem2, and not with regular bwa-mem.

I am attaching the full log and a script that consistently reproduces the error on my machine.

```
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM (SRR9041541)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM (SRR9041541)` terminated with an error exit status (1)

Command executed:

  samtools \
      index \
      -@ 0 \
       \
      SRR9041541.sorted.bam
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM"":
      samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  [E::hts_idx_push] Unsorted positions on sequence #4: 1260047 followed by 1259502
  [E::sam_index] Read 'SRR9041541.9001' with ref_name='IV', ref_length=1531933, flags=163, pos=1259502 cannot be indexed
  samtools index: failed to create index for ""SRR9041541.sorted.bam""
```


### Command used and terminal output

_No response_

### Relevant files

[sarek-joint-VC.sh.txt](https://github.com/nf-core/sarek/files/11721210/sarek-joint-VC.sh.txt)

[nextflow-error.log.txt](https://github.com/nf-core/sarek/files/11721218/nextflow-error.log.txt)


### System information

_No response_",amizeranschi,https://github.com/nf-core/sarek/issues/1097
I_kwDOCvwIC85od83Z,Fix bcftools mpileup,CLOSED,2023-06-12T12:40:30Z,2023-06-15T10:05:13Z,2023-06-15T10:05:13Z,"### Description of feature

Some versions of samtools and bcftools are not compatible so the pileup file generated by `samtools mpileup` cannot be read by `bcftools call`. Also, it seems `bcftools call` accepts **only a piped input** from `bcftools mpileup`. So the pileup files generated by `samtools mpileup` cannot finally be used for variant calling! I know there might be some ways to figure it out without adding a new feature to the pipeline but I have two suggestions to make Sarek even better: 

1. Replace `samtools mpileup` with `bcftools mpileup`. 
2. Add bcftool call for actual variant calling and generate the final vcf file. 

",amnghn,https://github.com/nf-core/sarek/issues/1099
I_kwDOCvwIC85ojR1B,"For joint variant calling, produce a final VCF file with filtered variants (those that PASS filters)",CLOSED,2023-06-13T05:40:17Z,2023-08-23T16:47:03Z,2023-08-16T19:26:44Z,"### Description of feature

Following [this discussion on Slack](https://nfcore.slack.com/archives/CGFUX04HZ/p1686590391814079?thread_ts=1686493658.456969&cid=CGFUX04HZ), after running joint calling and VQSR, it would be helpful to produce a final VCF file with filtered variants, i.e. those that have `PASS` on their filter column, and not `.` or anything else. Something like the following bcftools command:

```
bcftools view -f PASS joint_germline_recalibrated.vcf.gz > joint_germline_recalibrated_final.vcf.gz
```

The current behavior is to have a mix of both calibrated and uncalibrated variants in `joint_germline_recalibrated.vcf.gz`, which means that there are two records for each variant.
",amizeranschi,https://github.com/nf-core/sarek/issues/1102
I_kwDOCvwIC85omKaa,Fix output docs to also have bcftools mpileup output,CLOSED,2023-06-13T12:58:47Z,2023-06-15T10:05:01Z,2023-06-15T10:05:01Z,"### Description of feature

as title says",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1103
I_kwDOCvwIC85omLAY,Don't run samtools mpileup when only tools mpileup is selected,CLOSED,2023-06-13T12:59:55Z,2023-06-15T10:04:48Z,2023-06-15T10:04:48Z,"### Description of the bug

currently samtools mielup and bcftools mpileup are both run when `tools mpileup ` is selected, however only run bcftools then + fix the output docs if this is done

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1104
I_kwDOCvwIC85ozMJ0,Use AWS cache Annotation Depot,CLOSED,2023-06-15T07:51:32Z,2024-01-23T08:42:33Z,2024-01-23T08:42:33Z,"### Description of feature

- [x] Set it up
- [x] Point to it with the `--snpeff_cache` and `--vep_cache` params
- [x] Test
- [x] :rocket:",maxulysse,https://github.com/nf-core/sarek/issues/1110
I_kwDOCvwIC85pE_ro,"Pipeline doesn't properly use resource setting from custom.config, when starting analysis from --step variant_calling",CLOSED,2023-06-19T07:21:48Z,2023-06-30T12:37:34Z,2023-06-30T11:43:53Z,"### Description of the bug

As the title says, the resource limits from `custom.config` appear to be ignored and I am seeing messages such as the ones below, during the pipeline's runtime. I am attaching a stand-alone script that reproduces this issue for me. The `custom.config` is embedded within it and I have used this config in previous runs without issues.

These are some of the messages that appear:

```
   ### ERROR ###   Max cpus '[:]' is not valid! Using default value: 2
[GATK HaplotypeCaller] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.
[GATK MergeVcfs] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.
etc.
```


### Command used and terminal output

_No response_

### Relevant files

[test-small.sh.txt](https://github.com/nf-core/sarek/files/11785346/test-small.sh.txt)


### System information

Tested with `nf-core/sarek -r 3.2.2` and Nextflow v. `23.04.1`.
",amizeranschi,https://github.com/nf-core/sarek/issues/1117
I_kwDOCvwIC85pNaeb,Optimisation: Drop samtools index processes,OPEN,2023-06-20T10:38:07Z,2023-11-23T13:00:19Z,,"### Description of feature

Most `samtools` tools now include the option to `--write-index`. By leveraging this we could omit a bunch of `SAMTOOLS_INDEX` processes, saving a lot of data shuffling and processes.

",matthdsm,https://github.com/nf-core/sarek/issues/1121
I_kwDOCvwIC85pXNnp,Joint calling with mpileup ,OPEN,2023-06-21T14:09:43Z,2023-06-26T15:25:10Z,,"### Description of feature

Currently, it is not possible to do a joint calling and create one VCF file for all samples using bcftools. Sarek creates one VCF file for each sample separately. If the user tries to merge the VCF files, those variants present in only one of the initial VCF files will be 0/0 (homozygous for reference) or ./. (missing) for other samples. This could be a big issue for many studies. 
",amnghn,https://github.com/nf-core/sarek/issues/1124
I_kwDOCvwIC85pXRXj,Adding more keys in the INFO and FORMAT fields of the VCF file,OPEN,2023-06-21T14:15:50Z,2023-06-21T15:49:37Z,,"### Description of feature

It would be great if the user can modify the keys in the INFO and FORMAT fields. Some keys may (not) be needed for some data sets. For example, in the FORMAT field, Sarek retunes only GT and PL keys, and the user cannot add other keys such as DP, SP, ADF, ADR, AD, GP, and GQ. ",amnghn,https://github.com/nf-core/sarek/issues/1125
I_kwDOCvwIC85pYHBO,Sarek copies dbSNP for each invocation of HaplotypeCaller ,OPEN,2023-06-21T15:58:33Z,2023-09-01T08:47:03Z,,"### Description of the bug

I am running Sarek using a custom reference and dbSNP version on a SLURM cluster. Each invocation of HaplotypeCaller produces a copy of dbSNP in the process's working directory. This fills up my project space because each copy is 24 GB.

I explicitly set stageInMode to 'symlink' in my nextflow.config and the issue is still occuring. A look at the .command.run produced by Nextflow also shows that the file should be symlinked and not copied.

I am suspecting this happens because the dbSNP files match the `*.vcf.gz` and `*.tbi` globs defined in the output of GATK4_HAPLOTYPECALLER and therefore, Nextflow stages them for output by copying them to the working directory.

### Command used and terminal output

```console
nextflow run $NF_CORE_PIPELINES/sarek/3.1.1/workflow \
  -profile uppmax \
  -resume \
  -c nextflow.config \
  --project $PROJECT \
  --input samplesheet.csv \
  --fasta <long_path>/remapping_resources/reference/T2T-CHM13.fna \
  --dbsnp <long_path>/remapping_resources/dbSNP/chm13v2.0_dbSNPv155.vcf.gz \
  --step variant_calling \
  --tools snpeff,haplotypecaller \
  --save_reference \
  --igenomes_ignore \
  --joint_germline \
  --outdir out
```


### Relevant files

custom config and .command.run from affected job: [sarek_files.zip](https://github.com/nf-core/sarek/files/11820053/sarek_files.zip)


### System information

* Nextflow version 23.04.2 build 5870
* Hardware: HPC (UPPMAX/Bianca)
* Executor: Slurm
* Container Engine: Singularity
* OS: Linux
* Version of nf-core/sarek: 3.11",Schmytzi,https://github.com/nf-core/sarek/issues/1127
I_kwDOCvwIC85pj6Be,ERROR: terminated for an unknown reason -- Likely it has been terminated by the external system,OPEN,2023-06-23T08:11:04Z,2023-08-15T10:50:57Z,,"### Description of the bug

I was running Sarek for about 100 samples, and I ran out of disk quota at some point. After free up more space on the cluster (Uppmax), I `-resume` the previous run, but I got this error message for several samples, and the pipeline shot down. 

`  error [nextflow.exception.ProcessFailedException]: Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_MPILEUP:BCFTOOLS_MPILEUP (sample_ID)` terminated for an unknown reason -- Likely it has been terminated by the external system`


### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.2.2 -resume -profile uppmax -params-file nf-params.json --project naissXXXX-X-XXX
```


### Relevant files

[username@rackham4 b747de569aa5ee9e086c3234d3d971]$ cat .command.log
INFO:    Environment variable SINGULARITYENV_TMPDIR is set, but APPTAINERENV_TMPDIR is preferred
INFO:    Environment variable SINGULARITYENV_NXF_DEBUG is set, but APPTAINERENV_NXF_DEBUG is preferred
INFO:    Environment variable SINGULARITYENV_SNIC_TMP is set, but APPTAINERENV_SNIC_TMP is preferred
Note: none of --samples-file, --ploidy or --ploidy-file given, assuming all sites are diploid
[mpileup] 1 samples in 1 input files
[mpileup] maximum number of reads per input file set to -d 250
slurmstepd: error: *** JOB 38839689 ON r151 CANCELLED AT 2023-06-22T13:18:59 ***

### System information

HPC: Uppmax
Congfig: `-profile uppmax`",amnghn,https://github.com/nf-core/sarek/issues/1133
I_kwDOCvwIC85pvsEj,ERROR ~ Cannot invoke method startsWith() on null object,OPEN,2023-06-26T07:22:14Z,2024-11-18T09:18:03Z,,"### Description of the bug

I've just upgraded sarek from 3.14 to the latest version. When I run it now it crashes out almost immediately with:

`ERROR ~ Cannot invoke method startsWith() on null object`

### Command used and terminal output

```console
nextflow run nf-core/sarek -config ~/nextflow-bear.config --input samplesheet.csv --outdir results/ --genome GATK.GRCh38 -resume --tools strelka,manta,tiddit,cnvkit,vep --umi_read_structure ""+T 18M11S+T""

N E X T F L O W  ~  version 23.04.2
Launching `https://github.com/nf-core/sarek` [stupefied_carlsson] DSL2 - revision: ed1cc84993 [master]


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .´ _  `.
   /  |\`-_ \      __        __   ___
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /¯¯\ |  \ |___ |  \
    `|____\´

  nf-core/sarek v3.2.3-ged1cc84
------------------------------------------------------
Core Nextflow options
  revision                  : master
  runName                   : stupefied_carlsson
  containerEngine           : singularity
  launchDir                 : /rds/projects/b/beggsa-hpbcancer/BILCAP3/panelseq/DNA
  workDir                   : /rds/projects/b/beggsa-hpbcancer/BILCAP3/panelseq/DNA/work
  projectDir                : /rds/homes/b/beggsa/.nextflow/assets/nf-core/sarek
  userName                  : beggsa
  profile                   : standard
  configFiles               : /rds/homes/b/beggsa/.nextflow/assets/nf-core/sarek/nextflow.config, /rds/homes/b/beggsa/nextflow-bear.config

Input/output options
  input                     : samplesheet.csv
  outdir                    : results/

Main options
  intervals                 : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/intervals/wgs_calling_regions_noseconds.hg38.bed
  tools                     : strelka,manta,tiddit,cnvkit,vep

FASTQ Preprocessing
  umi_read_structure        : +T 18M11S+T

Variant Calling
  cf_chrom_len              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/Length/Homo_sapiens_assembly38.len
  pon                       : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz
  pon_tbi                   : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz.tbi

Reference genome options
  ascat_genome              : hg38
  ascat_alleles             : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_alleles_hg38.zip
  ascat_loci                : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_loci_hg38.zip
  ascat_loci_gc             : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/GC_G1000_hg38.zip
  ascat_loci_rt             : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/RT_G1000_hg38.zip
  bwa                       : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAIndex/
  bwamem2                   : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAmem2Index/
  chr_dir                   : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/Chromosomes
  dbsnp                     : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz
  dbsnp_tbi                 : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz.tbi
  dbsnp_vqsr                : --resource:dbsnp,known=false,training=true,truth=false,prior=2.0 dbsnp_146.hg38.vcf.gz
  dict                      : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.dict
  dragmap                   : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/dragmap/
  fasta                     : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta
  fasta_fai                 : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta.fai
  germline_resource         : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz
  germline_resource_tbi     : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz.tbi
  known_indels              : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz
  known_indels_tbi          : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/{Mills_and_1000G_gold_standard.indels.hg38,beta/Homo_sapiens_assembly38.known_indels}.vcf.gz.tbi
  known_indels_vqsr         : --resource:gatk,known=false,training=true,truth=true,prior=10.0 Homo_sapiens_assembly38.known_indels.vcf.gz --resource:mills,known=false,training=true,truth=true,prior=10.0 Mills_and_1000G_gold_standard.indels.hg38.vcf.gz
  known_snps                : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000G_omni2.5.hg38.vcf.gz
  known_snps_tbi            : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000G_omni2.5.hg38.vcf.gz.tbi
  known_snps_vqsr           : --resource:1000G,known=false,training=true,truth=true,prior=10.0 1000G_omni2.5.hg38.vcf.gz
  mappability               : s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Control-FREEC/out100m2_hg38.gem
  snpeff_db                 : 105
  snpeff_genome             : GRCh38
  snpeff_version            : 5.1
  vep_genome                : GRCh38
  vep_species               : homo_sapiens
  vep_cache_version         : 108
  vep_version               : 108.2
  igenomes_base             : s3://ngi-igenomes/igenomes

Institutional config options
  config_profile_description: The Francis Crick Institute CAMP HPC cluster profile provided by nf-core/configs.
  config_profile_contact    : Chris Cheshire (@chris-cheshire)
  config_profile_url        : https://www.crick.ac.uk/research/platforms-and-facilities/scientific-computing/technologies

Max job request options
  max_cpus                  : 32
  max_memory                : 128 GB
  max_time                  : 96.h

!! Only displaying parameters that differ from the pipeline defaults !!
Displaying all reference genome parameters
------------------------------------------------------
If you use nf-core/sarek for your analysis please cite:

* The pipeline
  https://doi.org/10.12688/f1000research.16665.2
  https://doi.org/10.5281/zenodo.4468605

* The nf-core framework
  https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md
------------------------------------------------------
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -

[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_... -
WARN: There's no process matching config selector: NFCORE_SAREK:SAREK:CRAM_QC_NO_MD:SAMTOOLS_STATS -- Did you mean: NFCORE_SAREK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS?
ERROR ~ Cannot invoke method startsWith() on null object

 -- Check script '/rds/homes/b/beggsa/.nextflow/assets/nf-core/sarek/./workflows/sarek.nf' at line: 1356 or see '.nextflow.log' file for more details
-[nf-core/sarek] Pipeline completed with errors-
```


### Relevant files

[samplesheet.csv](https://github.com/nf-core/sarek/files/11865482/samplesheet.csv)


### System information

nextflow version 23.04.2.5870
HPOC
Slurm
Singularity
RedHat Enterprise 8.6
",adbeggs,https://github.com/nf-core/sarek/issues/1134
I_kwDOCvwIC85qCpHT,GenomicsDBImport : Mismatching intervals for input-vcf-files and bed-interval-file,CLOSED,2023-06-28T14:29:07Z,2023-08-16T19:26:54Z,2023-08-16T19:26:54Z,"### Description of the bug

There seems to be a mismatch in the GenomicsDBImport process between the intervals for the input-vcf-files and the input-interval-bed-file. Here is an example:
```
gatk --java-options ""-Xmx6553M"" GenomicsDBImport \
    --variant HG002.haplotypecaller.chr16_46380683-90228345.g.vcf.gz --variant HG003.haplotypecaller.chr2_89753993-90402511.g.vcf.gz \
    --genomicsdb-workspace-path chr16_46380683-90228345.joint \
    --intervals chr16_46380683-90228345.bed \
    --tmp-dir . \
    --genomicsdb-shared-posixfs-optimizations true --bypass-feature-reader 
```

More info on Slack:

https://nfcore.slack.com/archives/CGFUX04HZ/p1686902281599399?thread_ts=1686493658.456969&cid=CGFUX04HZ

### Command used and terminal output

```console
nextflow run main.nf -profile docker --input /home/ubuntu/test_data/mapped_joint_bam_HG002_HG003_1perc_of_40x.csv --tools haplotypecaller --step variant_calling --joint_germline --outdir results --skip_tools haplotypecaller_filter
```
```


### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1137
I_kwDOCvwIC85qHfh8,ERROR while running annotation step with vep,CLOSED,2023-06-29T08:16:54Z,2024-10-30T14:17:41Z,2023-08-04T03:01:07Z,"### Description of the bug

So I chose to manually downloaded the vep_cache files and use dbNSFP plugin for the annotation, but the annotating step terminated with an error exit status. And I cannot find out whether it was because the vep cache file was not correctly directed or anything else.

### Command used and terminal output

```console
Nextflow command
nextflow run /dssg/home/acct-medlhp/medlhp-wzc/software/sarek --step annotate --tools vep --input //dssg/home/acct-medlhp/medlhp-wzc/project/sarek_jobs/results/0518-2/csv/variantcalled.csv --outdir /dssg/home/acct-medlhp/medlhp-wzc/project/sarek_jobs/results/anno_test --vep_dbnsfp --vep_cache /dssg/home/acct-medlhp/medlhp-wzc/cache_files/homo_sapiens/106_GRCh38 --igenomes_base /dssg/home/acct-medlhp/medlhp-wzc/references --genome GATK.GRCh38 -profile singularity
```


### Relevant files


[26161760.txt](https://github.com/nf-core/sarek/files/11903534/26161760.txt)


### System information

_No response_",FrancisNietzsche,https://github.com/nf-core/sarek/issues/1139
I_kwDOCvwIC85qb1lE,Pass on the number of tumor samples per patient,OPEN,2023-07-03T08:54:51Z,2023-07-03T08:54:51Z,,"### Description of feature

As discussed [here](https://github.com/nf-core/sarek/pull/1013#discussion_r1248888191), using the number of tumor samples as the `size` parameter  in `groupTuple` operations would avoid potential bottlenecks in the pipeline. It would be nice to collect this info at the early stages of the pipeline and pass it on as metadata/channel to downstream subworkflows.",berguner,https://github.com/nf-core/sarek/issues/1141
I_kwDOCvwIC85qjfKG,Time course sampling analysis,OPEN,2023-07-04T10:50:56Z,2023-07-06T07:25:48Z,,"### Description of feature

Can Sarek handle a time course analysis of exome samples? (where a patient was profiled in multiple timepoints)
I couldn't find whether it's possible to handle this explicitly, or it's better to label each sample individually (and pair with somatic control)? 

",frenkiboy,https://github.com/nf-core/sarek/issues/1142
I_kwDOCvwIC85qqLus,Filtering reads by mapping to multiple reference genome,CLOSED,2023-07-05T12:23:32Z,2023-09-08T14:04:25Z,2023-09-08T14:04:25Z,"### Description of feature

I would like to extend sarek to be able to perform multiple mapping steps in order to filter out reads from contaminating organisms.

Specifically, in my case, my research group would like to filter out human reads from malaria parasites (taken from human blood samples). The method that our lab has used in the past is the same one as described in [this publication by MalariaGEN](https://doi.org/10.12688%2Fwellcomeopenres.16168.2) (one of the most recent and largest studies on malaria genome variation by a big consortium) and boils down to the following:

1) map reads to human reference genome, e.g. via `bwa mem`.
2) filter out proper paired reads / keep non-mapped reads, e.g. via `samtools --exclude-flags 2` =  read mapped in proper pair).
3) extract fastq files from the filtered bam file, e.g. via `samtools fastq`.
4) map reads to the actual reference genome of interest.

One question that immediately comes up is whether this should be coded as a two step approach (assuming most people will want to filter out only a single organism), or whether it should be flexible to as many additional reference genomes as the user desires. The latter is probably more useful to a wider audience, but might be a bit more challenging to code (cleanly)?

Another question could be whether the flag to filter on in samtools should be fixed or customizable.

---

I'm happy to start working on this myself and to start a PR, since my lab already has some existing scripts that perform this step (not Nextflow though unfortunately). 

---

For the sake of completion, we've had some internal discussions on whether or not a ""competitive mapping"" approach might be a better approach than this filtering one. In a nutshell, this would entail mapping a single time to the concatenated reference genome and only retaining reads that map to the part of the reference corresponding to the genome of interest. An example can be found [here](https://doi.org/10.1186/s12864-020-07229-y). I'm just mentioning it because we might want to add this feature as well at some point in the future and they serve the same purpose.
",pmoris,https://github.com/nf-core/sarek/issues/1144
I_kwDOCvwIC85q7ovx,"Better ""error handling"" around FilterVariantTranches",OPEN,2023-07-07T18:25:42Z,2023-07-07T18:25:42Z,,"### Description of feature

 Sarek occasionally crashes on `FilterVariantTranches` when the input VCF contains no variants or the key `CNN_1D` is missing from some variant in the input VCF. (I see the error from time to time - perhaps because I often run on tiny test data.)

Apparently, it is intended behaviour from `FilterVariantTranches`, but we could at some upfront check on the input data and/or ""catch"" the error after the call to `FilterVariantTranches` :

https://gatk.broadinstitute.org/hc/en-us/community/posts/16422702608283--FilterVariantTranches-fails-hard-on-input-VCF-without-variants-or-with-no-variants-with-info-key?page=1#community_comment_16953159164955

I guess if the input VCF contains no variants, then there is no point in trying to run `FilterVariantTranches` and it should just be skipped.

As for the missing `CNN_1D`, I don't know exactly how that should be handled nicely. I guess one would need to know the reasons why it might be missing. (Could it be missing due to some upstream bug? 🤔 Then the bug should also have been already have been caught upstream, right? )

I guess this ""error handling"" is just a nice-to-have thing and not in anyway urgent.",asp8200,https://github.com/nf-core/sarek/issues/1146
I_kwDOCvwIC85rAaKY,Intervals BED file processing: remove headers and validate any timing information,OPEN,2023-07-09T06:53:23Z,2023-07-09T06:53:23Z,,"### Description of feature

The BED file (or, alternatively,  `*.intervals_list` file  as used by Picard Tools) file that is being passed to sarek (since version 3.0) with the `--intervals` parameter serves two functions:
* Define the intervals of interest, e.g. for performing BQSR and targeted variant calling
* Give hints to the scatter & gather algorithm on who to set the scatter intervals for optimal processing

Because the latter function allows the user to provide timing information in the 5th column (originally designated for a ""score""), this can lead to spectacular derailments of the scattering/gathering if the BED/intervals file contains additional columns with name/score/strand or possibly annotation information, which is being interpreted as timing hints.  (I experienced a case where that lead to a splitting into several hundreds of thousands scatter intervals, instead of dozens at most, completely paralyzing the pipeline.  In that sense this might be considered a bug report.)

An unrelated, but equally annoying issue is that header lines in a BED added for display in the UCSC genome browser (see https://www.genome.ucsc.edu/FAQ/FAQformat.html#format1) lead to a crash, since they are ""invalid"" when being interpreted as BED lines denoting intervals (i.e. as `chr\tstart\tend`).  So if I have a BED file (e.g. provided by a WES kit manufacturer for identifying targets) that has more than 3 columns and/or one or more header lines, I would have to preprocess this file before using it, which is an extra step that may not be intuitive for new users and one more avoidable thing to do for anyone, never mind that another opportunity for making mistakes.

The provided BED/intervals file is being processed by the pipeline anyway (e.g. the staged copy is gzipped, and an `*.intervals_list` is converted to BED), so this problem can potentially be solved by making this preprocessing step ""smarter"".  This would involve the following steps:
* Scan the start of the provided BED for lines that match the regex `^(browser|track) ` (note the space at the end) and remove matching lines (this doesn't need to be done with a regex, since it is pretty simple).  This solves the crash problem when headers are present (`*.intervals_list` files don't have this problem and seems to be processed appropriately).
* Check if a 5th column is present, and if so, if it looks like timing information.  (Sorry, I've never used that feature, so I can't say exactly what would be valid or invalid for that, but it presumably would be strictly numeric.  Possibly a sanity check could be run to see if the numbers are plausible and lead to a manageable set of scatter intervals.)  If that is the case, use it as timing information, otherwise discard any columns after the 3rd (any names and scores don't matter, and strand information does not make sense in this context for DNA).

Thank you!",tdanhorn,https://github.com/nf-core/sarek/issues/1148
I_kwDOCvwIC85rOOIl,Make patient optional in the samplesheet,OPEN,2023-07-11T12:59:31Z,2024-01-10T12:58:29Z,,"### Description of feature

I often work with non-human germline data and in sarek it is required to specify a patient ID and a sample ID in the samplesheet. The IDs that are then written to the cram and VCF files are in the form patient_sample. In my case the concept of patient does not really apply and I just set patient and sample to the same value. However, this results in having IDs in the vcf and cram in the form ID_ID, while I would want them to be jus§t ID.

I suggest to make patient optional when running germline variant calling.",saulpierotti,https://github.com/nf-core/sarek/issues/1152
I_kwDOCvwIC85rwFvA,Missing plugin nf-amazon,OPEN,2023-07-17T13:17:36Z,2023-07-18T08:22:53Z,,"Downloading plugin nf-amazon@1.16.2
WARN: Unable to start plugin 'nf-amazon' required by s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_alleles_hg38.zip

ERROR ~ Missing plugin 'nf-amazon' required to read file: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/ASCAT/G1000_alleles_hg38.zip
",zprobot,https://github.com/nf-core/sarek/issues/1156
I_kwDOCvwIC85sT1Ei,Nucleotides per second default to high,OPEN,2023-07-23T13:52:14Z,2023-08-15T10:50:19Z,,"### Description of the bug

In the parameter docs for the `intervals` parameter the following is stated:

```
If no runtime is given, a time of 200000 nucleotides per second is assumed. See --nucleotides_per_second on how to customize this.
Actual figures vary from 2 nucleotides/second to 30000 nucleotides/second.
```

The default  for `nucleotides_per_second` is indeed 200000. This is well outside the range suggested. I suppose the actual value desired is a factor of 10 smaller (20000).

Practically for some real data that I have where the actual runtime is ~2 days per interval the default provides an estimate of 180 seconds, which seems orders of magnitude out of scale. This is reflected in merging intervals together that should not be merged, resulting in unreasonably long wall-time.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",saulpierotti,https://github.com/nf-core/sarek/issues/1161
I_kwDOCvwIC85sW2kU,Allow on restart to not convert the bam files to cram,CLOSED,2023-07-24T08:40:15Z,2024-11-12T15:27:57Z,2024-11-12T15:27:57Z,"### Description of feature

Dear nf-core community, thank you very much for your incredible work!

I'm working with WGS samples with 100 X coverage. I have bam and bai files.
When I run sarek on these files I have to wait from 4 to 6 hour to convert them from bam to cram and I don't even need those cram files.
I was wondering if it could be possible to create and option to work exclusively with bam instead of cram.

Furthermore, looking at the code and modules, I found that BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES is the main gateway to cram instead of bam outputs.

https://github.com/nf-core/sarek/blob/3.1.2/modules/nf-core/gatk4/markduplicates/main.nf

Probably with a slight modification to this modules, making it optional to convert from bam to cram after markduplicates, it'll also be possible to have bam outputs without converting cram to bam when using the full pipeline starting from fastq inputs.

Thank you in advance and best regards!
Youssef
",YussAb,https://github.com/nf-core/sarek/issues/1162
I_kwDOCvwIC85svnAZ,sarek in slurm : issue,CLOSED,2023-07-27T13:51:01Z,2023-08-17T12:56:31Z,2023-08-17T12:56:31Z,"I am trying to run sarek on slurm , with the script,
#!/bin/bash
#SBATCH --qos=standard
#SBATCH --job-name=cd_301
#SBATCH --nodes=1
#SBATCH --tasks=2
#SBATCH --cpus-per-task=8
#SBATCH --tasks-per-node=1
#SBATCH --output=/TEST/sarek_301_%j.out
#SBATCH --chdir=/TEST/
#SBATCH --error=/TEST/sarek_300_%j.err

nextflow run  /TEST/sarek/main.nf -profile docker --input /TEST/samplesheet.csv --genome GATK.GRCh38 --outdir /TEST/results/
##########
The error I am getting is ,
cat sarek_1.err
tput: terminal attributes: No such device or address
tput: terminal attributes: No such device or address
tput: terminal attributes: No such device or address
tput: terminal attributes: No such device or address
tput: terminal attributes: No such device or address

cat sarek_1.out
WARN: There's no process matching config selector: NFCORE_SAREK:SAREK:CRAM_QC_NO_MD:SAMTOOLS_STATS -- Did you mean: NFCORE_SAR
EK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS?
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:FASTP (test_sample-lane_1)'

the sample sheet ,
patient,sample,lane,fastq_1,fastq_2
I1,test_sample,lane_1,test_sample_R1_xxx.fastq.gz,test_sample_R2_xxx.fastq.gz


Can anyone give a solution to this. 


",ChitrArpita,https://github.com/nf-core/sarek/issues/1165
I_kwDOCvwIC85tCIPi,Sarek stops at MULTIQC step.,CLOSED,2023-07-31T14:08:02Z,2023-11-22T11:14:52Z,2023-11-22T11:14:52Z,"### Description of the bug

Sarek abrubtly stops at MULTIQC stage, 

Jul-27 19:22:28.557 [Actor Thread 398] ERROR nextflow.processor.TaskProcessor - Error executing process > 'NFCORE_SAREK:SAREK:MULTIQC'

Caused by:
  cannot spread the type java.lang.Integer with value 1

### Command used and terminal output

```console
Command:
nextflow run nf-core/sarek -r 3.2.3     -c fullpath/sarek_config_alma.config    -with-tower  --input fullpath/sarek/newsample.csv   --outdir fullpath/sarek/  --genome GATK.GRCh38 -resume

output:
Jul-27 19:22:28.610 [Actor Thread 398] DEBUG nextflow.Session - The following nodes are still active:
[process] NFCORE_SAREK:SAREK:MULTIQC
  status=ACTIVE
  port 0: (value) bound ; channel: multiqc_files
  port 1: (value) bound ; channel: multiqc_config
  port 2: (value) bound ; channel: extra_multiqc_config
  port 3: (value) bound ; channel: multiqc_logo
  port 4: (cntrl) -     ; channel: $

Jul-27 19:22:28.612 [Task submitter] DEBUG nextflow.processor.TaskProcessor - Handling unexpected condition for
  task: name=NFCORE_SAREK:SAREK:MULTIQC; work-dir=/data/scratch/DGE/DUDGE/MOPOPGEN/jayaramv/Project_K78/sarek/work/9f/970e83940be3d51f6e59c1e26fbe99
  error [nextflow.exception.ProcessFailedException]: Error submitting process 'NFCORE_SAREK:SAREK:MULTIQC' for execution
```


### Relevant files

[nextflow_log.txt](https://github.com/nf-core/sarek/files/12215888/nextflow_log.txt)


### System information

_No response_",portbo,https://github.com/nf-core/sarek/issues/1167
I_kwDOCvwIC85tWPzs,"Unsucessful, at the `MergeVcfs` step when using `--joint_germline true`",OPEN,2023-08-03T08:28:54Z,2023-09-18T08:24:03Z,,"### Description of the bug

I ran the pipeline with `--tools haplotypecaller,vep`, which finished successfully. When adding `--joint_germline true` to do the joint calling, it fails. Resuming the pipeline restarts the calling step and stops at the exact point. Similar issues arise when adding other callers.

### Command used and terminal output

```console
nextflow run nf-core/sarek
		 -r 3.2.3
		 -profile singularity
		 -with-singularity nfcore/sarek
		 -c code/sarek-config.config
		 --wes true
		 --genome GATK.GRCh38
		 --igenomes_base references/
		 --input data/sarek-input.csv
		 --outdir output/
		 --tools haplotypecaller,vep
		 --skip_tools multiqc
		 --joint_germline true

Workflow execution completed unsuccessfully
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_JOINT_CALLING_GERMLINE_GATK:MERGE_GENOTYPEGVCFS (joint_variant_calling)'

Caused by:
  cannot spread the type java.lang.Integer with value 1 -- Check script '/data/scratch/DGE/DUDGE/MOPOPGEN/mahmed03/nextflow/assets/nf-core/sarek/./workflows/../subworkflows/local/bam_variant_calling_germline_all/../bam_variant_calling_deepvariant/../../../modules/nf-core/gatk4/mergevcfs/main.nf' at line: 29

Source block:
  def args = task.ext.args ?: ''
  def prefix = task.ext.prefix ?: ""${meta.id}""
  def input_list = vcf.collect{ ""--INPUT $it""}.join(' ')
  def reference_command = dict ? ""--SEQUENCE_DICTIONARY $dict"" : """"
  def avail_mem = 3072
  if (!task.memory) {
          log.info '[GATK MergeVcfs] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'
      } else {
          avail_mem = (task.memory.mega*0.8).intValue()
      }
  """"""
      gatk --java-options ""-Xmx${avail_mem}M"" MergeVcfs \\
          $input_list \\
          --OUTPUT ${prefix}.vcf.gz \\
          $reference_command \\
          --TMP_DIR . \\
          $args
  
      cat <<-END_VERSIONS > versions.yml
      ""${task.process}"":
          gatk4: \$(echo \$(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*\$//')
      END_VERSIONS
      """"""

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`
```


### Relevant files

I attached the config, input, and log file here, [Archive.zip](https://github.com/nf-core/sarek/files/12248582/Archive.zip)


### System information

- Nextflow version (23.04.2.5870)
- Hardware (HPC)
- Executor (slurm)
- Container engine: (Singularity)
- Version of nf-core/sarek (3.2.3)",MahShaaban,https://github.com/nf-core/sarek/issues/1168
I_kwDOCvwIC85tYjBy,Request to un-hide common parameters,CLOSED,2023-08-03T14:13:15Z,2023-08-17T08:22:34Z,2023-08-17T08:22:34Z,"### Description of feature

There are some parameters that I would argue should not be hidden by default. 

Recommendations include: aligner and umi_tools to start. Will send a PR with changes today.",robsyme,https://github.com/nf-core/sarek/issues/1170
I_kwDOCvwIC85tj-kD,nf-validation plugin: Status inference does not work,CLOSED,2023-08-06T10:27:37Z,2023-08-15T14:52:41Z,2023-08-15T14:52:41Z,"### Description of the bug

When providing a samplesheet without a status column, it should default 0 to 0. This does not apper to be the case and various validation on tools are triggered. 

Affected versions: `dev` 

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1175
I_kwDOCvwIC85tt6Vm,Add Ascat tumor-only & targeted mode,OPEN,2023-08-08T07:40:25Z,2023-08-08T07:40:25Z,,"### Description of feature

Ascat supports tumor-only mode and also has functions for targeted data we should add.",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1176
I_kwDOCvwIC85uhw5c,Managing CRAM compression level,OPEN,2023-08-17T06:31:56Z,2023-08-17T06:54:30Z,,"### Description of feature

Hi. I appreciate your effort in developing this pipeline.
I noticed that the cram file size generated by Sarek is much larger than the one from another pipeline.
For example, an identical sample (WGS) produces a cram file of approximately 30 GB (Sarek) compared to 10 GB (other pipeline) in size.
It would be nice if we could adjust the compression level of the resulting cram files for more efficient storage.
Thank you :)",dr-yoon,https://github.com/nf-core/sarek/issues/1182
I_kwDOCvwIC85uk3Tx,sarek failed at BQSR,CLOSED,2023-08-17T14:46:51Z,2023-08-22T08:53:14Z,2023-08-22T08:53:14Z,"Hi, I am trying to run sarek somatic pipeline with dummy tumor and normal sample present in sarek github repository : https://github.com/nf-core/test-datasets/tree/sarek/testdata/dummy

the sample sheet : 
```
patient,sex,status,sample,lane,fastq_1,fastq_2
patient1,XX,0,normal_sample,lane_1,/dummy_n_R1_xxx.fastq.gz,/dummy_n_R2_xxx.fastq.gz
patient1,XX,1,tumor_sample,lane_1,/dummy_t_R1_xxx.fastq.gz,/dummy_t_R2_xxx.fastq.gz
```

the sbatch script  : 

```
#!/bin/bash
#SBATCH --qos=standard
#SBATCH --job-name=cd_301
#SBATCH --nodes=1
#SBATCH --tasks=2
#SBATCH --cpus-per-task=8
#SBATCH --tasks-per-node=1
#SBATCH --output=/TEST/sarek_301_%j.out
#SBATCH --chdir=/TEST/
#SBATCH --error=/TEST/sarek_300_%j.err

nextflow /TEST/sarek/main.nf -c my_config.config -profile docker --input /TEST/samplesheet_somatic.csv --genome GATK.GRCh38 --igenomes_base '/TEST/references' --outdir /TEST/results/ --max_cpus 8 --save_output_as_bam TRUE --save_mapped TRUE --split_fastq 0
```

###########################################################################################

I am getting the error at BQSR step it seems:

```
 error [nextflow.exception.ProcessFailedException]: Process `NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS (tumor_sample)` terminated with an error exit status (3)
Aug-17 14:29:48.065 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'NFCORE_SAREK:SAREK:BAM_BASER
ECALIBRATOR:GATK4_GATHERBQSRREPORTS (tumor_sample)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS (tumor_sample)` terminated with an error exit status (3)
```",ChitrArpita,https://github.com/nf-core/sarek/issues/1185
I_kwDOCvwIC85uoy63,Annote unfiltered mutect reads if no filtering is done,OPEN,2023-08-18T07:12:19Z,2023-10-16T10:05:49Z,,"### Description of feature

Currently unfiltered mutect calls are not annotated when filtering is skipped. Add those to the annotation channel",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1186
I_kwDOCvwIC85uqvvA,Improvement: make --tools case insensitive,OPEN,2023-08-18T13:08:23Z,2023-08-19T12:29:56Z,,"### Description of feature

`--tools` can be tricky to understand. A little change we could add would make it case insensitive, meaning if someone wrote `--tools Mutect2` it would still work. This could be done by changing every:

```
params.skip_tools.split(',').contains('mutect2')
```

to:

```
params.skip_tools.split(',').toLowerCase().contains('mutect2')
```

Given how many times this pattern is used, we could also add it to a function.",adamrtalbot,https://github.com/nf-core/sarek/issues/1187
I_kwDOCvwIC85vC5dp,Add a tower.yml and populate it with the expected output from sarek,CLOSED,2023-08-23T10:19:04Z,2023-09-14T11:37:47Z,2023-09-14T11:37:47Z,"### Description of feature

cf https://github.com/nf-core/rnaseq/blob/master/tower.yml",maxulysse,https://github.com/nf-core/sarek/issues/1190
I_kwDOCvwIC85vQ9ag,Missing output documentation for ascat ASCATprofile plot,CLOSED,2023-08-25T09:41:54Z,2023-08-25T12:02:38Z,2023-08-25T12:02:38Z,"The ASCAT output image
`<tumorsample_vs_normalsample>.tumour.ASCATprofile.png`
should be listed in the output documentation.

Mainly created this issue to test contributing ;)",JohannesKersting,https://github.com/nf-core/sarek/issues/1191
I_kwDOCvwIC85vmYU8,Start pipeline from intermediate cram files,CLOSED,2023-08-29T19:59:59Z,2024-12-16T00:01:01Z,2023-09-01T11:50:29Z,"### Description of the bug

I ran the pipeline with a batch of paired fastq but the server failed after the markduplicate stage and before the recalibration stage. So now I have the md cram files and would like to complete the processing on another server. I tried an input file with the following format:

```
patient,sample,lane,cram,crai
sample,sample,L0,/path/to/preprocessing/markduplicates/sample/sample.md.cram,/path/to/preprocessing/markduplicates/sample/sample.md.cram.crai
```

And used the `--step recalibrate` parameter, but this doesn't seem to work as I expected. What would be the proper way to resume from this intermediate step, on a different server?

Thanks.

### Command used and terminal output

```console
NXF_VER=""23.04.1"" nextflow run nf-core/sarek -r 3.0.1 -profile docker -params-file X202SC23012617_md_n120.yaml -config sxv.config --step recalibrate

ERROR ~ Invalid method invocation `call` with arguments: [[patient:sample, sample:sample, sex:NA, status:0, id:sample, data_type:cram], /path/to/preprocessing/markduplicates/sample/sample.md.cram, /path/to/preprocessing/markduplicates/sample/sample.md.cram.crai, /path/batch/work/5e/0eb3f688eb245e1587859d7a2ff723/chr1_143184588-223558935.bed, 133] (java.util.LinkedList) on _closure3 type

 -- Check '.nextflow.log' file for details
```


### Relevant files

_No response_

### System information

_No response_",sxv,https://github.com/nf-core/sarek/issues/1194
I_kwDOCvwIC85voI76,Access Denied (Service: Amazon S3; ...),CLOSED,2023-08-30T02:49:20Z,2024-10-14T19:02:53Z,2023-08-30T08:10:41Z,"### Description of the bug

Unable to download resource files from AWS S3. 

### Command used and terminal output

```console
nextflow run nf-core/sarek \
   -profile docker \
   --genome GATK.GRCh38 \
   --input samplesheet.csv \
   --outdir results
```


### Relevant files

```
Aug-30 11:31:32.578 [main] DEBUG com.upplication.s3fs.AmazonS3Client - Setting S3 glacierRetrievalTier=null
Aug-30 11:31:35.192 [main] DEBUG nextflow.Session - Session aborted -- Cause: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 9P1JTF6P1XWPW1CG; S3 Extended Request ID: KDG+NqrqOy52cKq3QKFICh6DqqUUaDPpyxiWJ0
irtK5XYw5yWnu5KY6QjcV5F8p3dEFKztGCDgk=; Proxy: null)
Aug-30 11:31:35.208 [main] ERROR nextflow.cli.Launcher - @unknown
com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 9P1JTF6P1XWPW1CG; S3 Extended Request ID: KDG+NqrqOy52cKq3QKFICh6DqqUUaDPpyxiWJ0irtK5XYw5yWnu5KY6QjcV5F8p3d
EFKztGCDgk=; Proxy: null)
```

### System information

Nextflow version: nextflow version 23.04.1.5866
Hardware: HPC
Executor: local
Container engine: Docker
OS: CentOS 7
sarek version: nf-core/sarek v3.2.3-ged1cc84",davetang,https://github.com/nf-core/sarek/issues/1195
I_kwDOCvwIC85wHkQ0,`WARN`: Uninitialized parameters,CLOSED,2023-09-05T01:28:24Z,2023-09-14T11:38:23Z,2023-09-14T11:38:23Z,"### Description of the bug

The pipeline starts with warnings complaining:

```
Launching `https://github.com/nf-core/sarek` [nice_murdock] DSL2 - revision: ed1cc84993 [master]
WARN: Access to undefined parameter `snpeff_genome` -- Initialise it to a default value eg. `params.snpeff_genome = some_value`
WARN: Access to undefined parameter `snpeff_version` -- Initialise it to a default value eg. `params.snpeff_version = some_value`
WARN: Access to undefined parameter `genome` -- Initialise it to a default value eg. `params.genome = some_value`
WARN: Access to undefined parameter `vep_genome` -- Initialise it to a default value eg. `params.vep_genome = some_value`
WARN: Access to undefined parameter `vep_version` -- Initialise it to a default value eg. `params.vep_version = some_value`
```

They are fixed in the `dev` version but not merged into the master yet.

### Command used and terminal output

```console
nextflow run nf-core/sarek \
-latest \
-profile docker \
--step mapping \
--tools freebayes,mpileup,mutect2,strelka,manta,tiddit,ascat,cnvkit,controlfreec,msisensorpro,snpeff,vep,merge \
--input 'samplesheet_sarek.csv' \
--outdir 's3://nextflow/sarek/results/' \
-work-dir 's3://nextflow/sarek/work/' \
-c 'custom.config' \
-r master \
-resume
```


### Relevant files

_No response_

### System information

N E X T F L O W  ~  version 23.08.1-edge
awsbatch
Docker
Linux
nf-core/sarek master",bounlu,https://github.com/nf-core/sarek/issues/1206
I_kwDOCvwIC85wPU7m,`snpeff_cache` folder is empty,CLOSED,2023-09-06T02:41:08Z,2024-08-30T15:32:14Z,2024-08-30T15:32:13Z,"### Description of the bug

The default location for SnpEff cache on AWS S3 is empty:

s3://annotation-cache/snpeff_cache/

Hence, the pipeline results in error:

```
Execution cancelled -- Finishing pending tasks before exit
-[nf-core/sarek] Pipeline completed with errors-
WARN: Input tuple does not match input set cardinality declared by process `NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF` -- offending value: [/annotation-cache/snpeff_cache]
WARN: Killing running tasks (87)
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF (1)'

Caused by:
  Not a valid path value type: org.codehaus.groovy.runtime.NullObject (null)


Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

 -- Check '.nextflow.log' file for details
Join mismatch for the following entries: key=[id:LC_AC35_vs_LC_AC00, normal_id:LC_AC00, patient:LC_AC, sex:XX, tumor_id:LC_AC35] values=


Staging foreign file: s3://annotation-cache/vep_cache
ERROR ~ Stream Closed

 -- Check '.nextflow.log' file for details
```

### Command used and terminal output

```console
nextflow run nf-core/sarek \
-latest \
-profile docker \
--step mapping \
--tools freebayes,mpileup,mutect2,strelka,manta,tiddit,ascat,cnvkit,controlfreec,msisensorpro,snpeff,vep,merge \
--input 'samplesheet_sarek.csv' \
--outdir '/data/nextflow/sarek/results/' \
-work-dir '/data/nextflow/sarek/work/' \
-c 'custom.config' \
-r dev \
-resume
```


### Relevant files

_No response_

### System information

N E X T F L O W ~ version 23.08.1-edge
local
Docker
Linux
nf-core/sarek dev",bounlu,https://github.com/nf-core/sarek/issues/1207
I_kwDOCvwIC85wTmLY,"Replace param haplotypecaller_filter, haplotyper_filter and dnascope_filter with variant_filter",CLOSED,2023-09-06T14:49:24Z,2023-09-17T21:45:02Z,2023-09-17T21:45:02Z,"### Description of feature

No need to have three (or more) params for skipping the variant-filtering.

Replace the ""options"" `haplotypecaller_filter`, `haplotyper_filter` and `dnascope_filter` for `--skip_tools` with `variant_filter`

https://nfcore.slack.com/archives/C02MDBZAYJK/p1694010751853099

Also, add variant-filtering for mutect2.",asp8200,https://github.com/nf-core/sarek/issues/1210
I_kwDOCvwIC85wX-12,GATK time limit excedded error,CLOSED,2023-09-07T08:04:26Z,2023-09-07T08:09:41Z,2023-09-07T08:09:41Z,"### Description of the bug

When I run sarek on large exome data, GATK dies with the time excedded error.

How can I increase the running time for GATK?

### Command used and terminal output

```console
Sep-06 16:57:48.158 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (P1697_DNA_69_1)'

Caused by:
  Process exceeded running time limit (8h)
```


### Relevant files

_No response_

### System information

_No response_",frenkiboy,https://github.com/nf-core/sarek/issues/1213
I_kwDOCvwIC85wx9KJ,Error when write a file running Mutect2 with Sarek,OPEN,2023-09-12T10:02:27Z,2024-08-20T13:23:53Z,,"### Description of the bug

I'm running Sarek v3.2.3 on an HPC with Slurm as management system. When Mutect2 is running, execution stops due to an error when creating a file. I have made several tests:

- Running the same Sarek pipeline with the same version but in a different directory and with different samples, Mutect2 terminates successfully. I have checked the quality of the bams files and it is correct. 
- Running another Sarek version (v 3.2.1) the same error comes up.
 
This two facts makes me suspect it could be a space or permissions issue, but I'm not quite clear. I leave the error here in case you can help me to clarify it. Thank you very much.

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.2.3 --input ${SAMPLESHEET} --wes -profile singularity --max_cpus 24 --intervals ${INTERVALS} --outdir $OUTDIR --step variant_calling --tools mutect2 --email miriam.ferreirop@gmail.com

Execution cancelled -- Finishing pending tasks before exit
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED (B1R2_vs_G1)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED (B1R2_vs_G1)` terminated with an error exit status (1)

Command executed:

  gatk --java-options ""-Xmx29491M"" Mutect2 \
      --input G1-lane_1.converted.cram --input B1R2-lane_1.converted.cram \
      --output B1R2_vs_G1.mutect2.chr9_66591388-67920552.vcf.gz \
      --reference Homo_sapiens_assembly38.fasta \
      --panel-of-normals 1000g_pon.hg38.vcf.gz \
      --germline-resource af-only-gnomad.hg38.vcf.gz \
      --intervals chr9_66591388-67920552.bed \
      --tmp-dir . \
      --f1r2-tar-gz B1R2_vs_G1.mutect2.chr9_66591388-67920552.f1r2.tar.gz --normal-sample P01_G1
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  1

Command output:
  [0.001s][warning][os,container] Duplicate cpuset controllers detected. Picking /sys/fs/cgroup/cpuset, skipping /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/var/singularity/mnt/session/final/sys/fs/cgroup/cpuset.

Command error:
  23:07:43.140 INFO  ProgressMeter -       chr9:133405690             60.2                437880           7274.5
  23:07:53.502 INFO  ProgressMeter -       chr9:133540799             60.4                438710           7267.4
  23:08:03.594 INFO  ProgressMeter -       chr9:133725565             60.5                439840           7265.9
  23:08:13.701 INFO  ProgressMeter -       chr9:133925123             60.7                441090           7266.3
  23:08:23.706 INFO  ProgressMeter -       chr9:134088694             60.9                442150           7263.8
  23:08:32.545 INFO  Mutect2 - 245998 read(s) filtered by: MappingQualityReadFilter 
  0 read(s) filtered by: MappingQualityAvailableReadFilter 
  0 read(s) filtered by: MappingQualityNotZeroReadFilter 
  0 read(s) filtered by: MappedReadFilter 
  0 read(s) filtered by: NotSecondaryAlignmentReadFilter 
  1900483 read(s) filtered by: NotDuplicateReadFilter 
  0 read(s) filtered by: PassesVendorQualityCheckReadFilter 
  0 read(s) filtered by: NonChimericOriginalAlignmentReadFilter 
  0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter 
  47 read(s) filtered by: ReadLengthReadFilter 
  0 read(s) filtered by: GoodCigarReadFilter 
  0 read(s) filtered by: WellformedReadFilter 
  2146528 total reads filtered out of 8009276 reads processed
  23:08:32.545 INFO  ProgressMeter -       chr9:134182911             61.0                442750           7256.1
  23:08:32.545 INFO  ProgressMeter - Traversal complete. Processed 442750 total regions in 61.0 minutes.
  23:08:32.553 INFO  VectorLoglessPairHMM - Time spent in setup for JNI call : 1.1748570740000002
  23:08:32.553 INFO  PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 154.751703055
  23:08:32.553 INFO  SmithWatermanAligner - Total compute time in java Smith-Waterman : 2227.11 sec
  23:08:32.554 INFO  Mutect2 - Shutting down engine
  [September 11, 2023 at 11:08:32 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 61.03 minutes.
  Runtime.totalMemory()=3238002688
  htsjdk.samtools.SAMException: Could not write metrics to file: /tmp/untarred10106267742814969916/P01_B1R2.ref_histogram
  	at htsjdk.samtools.metrics.MetricsFile.write(MetricsFile.java:138)
  	at org.broadinstitute.hellbender.tools.walkers.readorientation.F1R2CountsCollector.writeHistograms(F1R2CountsCollector.java:164)
  	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.lambda$writeExtraOutputs$13(Mutect2Engine.java:435)
  	at java.base/java.util.Optional.ifPresent(Optional.java:178)
  	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.writeExtraOutputs(Mutect2Engine.java:434)
  	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalSuccess(Mutect2.java:297)
  	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1102)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:149)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:198)
  	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:217)
  	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
  	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
  	at org.broadinstitute.hellbender.Main.main(Main.java:289)
  Caused by: java.io.FileNotFoundException: /tmp/untarred10106267742814969916/P01_B1R2.ref_histogram (No such file or directory)
  	at java.base/java.io.FileOutputStream.open0(Native Method)
  	at java.base/java.io.FileOutputStream.open(FileOutputStream.java:293)
  	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:235)
  	at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:184)
  	at java.base/java.io.FileWriter.<init>(FileWriter.java:96)
  	at htsjdk.samtools.metrics.MetricsFile.write(MetricsFile.java:135)
  	... 12 more
  .command.run: line 155: kill: (33) - No such process
  INFO:    Cleaning up image...

Work dir:
  /mnt/lustre/scratch/nlsas/home/usc/mg/translational_oncology/1_tools/01_nf_core_sarek/1_src/b1r2_job/work/2a/832da1404e27cee67fe5d89ea2ffe5

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`

 -- Check '.nextflow.log' file for details
```


### Relevant files

_No response_

### System information

- Nextflow version: 23.04.2 
- Hardware: HPC Finisterrae III
- Executor: slurm
- Container engine: Singularity v3.9.7
- OS: Linux
- Version os nf-core/sarek: 3.2.3 and tested with 3.2.1 too",mimifp,https://github.com/nf-core/sarek/issues/1226
I_kwDOCvwIC85w4xn0,CI tests for spark are not passing,CLOSED,2023-09-13T07:36:46Z,2023-09-13T10:04:48Z,2023-09-13T10:04:48Z,"### Description of the bug

The CI tests for spark are not passing anymore. We are back to the :

```
Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
```

issue when testing with docker. Notable recent change was using the docker image frmo quay.io/nf-core instead of the one from the broadinstitute, although the image was just mirrored.


### Command used and terminal output

```console
nextflow run main.nf -profile test_cache,use_gatk_spark,docker --outdir results -resume
```


### Relevant files

_No response_

### System information


    Nextflow version: 23.04.1.5866
    Hardware: Desktop
    Executor: local
    Container engine: Docker
    OS some linux system
    Version of nf-core/sarek: 19395081df",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1228
I_kwDOCvwIC85xB-OB,Question regarding sarek dockers : ,OPEN,2023-09-14T11:11:00Z,2023-09-28T06:36:15Z,,"Is there any way we can download all the docker images used by sarek locally, and some how  change the paths for reading the docker files. Basically I want to run sarek from some server where I do not have access to internet.",ChitrArpita,https://github.com/nf-core/sarek/issues/1233
I_kwDOCvwIC85xDCjg,Add gene annotation on cnvkit for WGS (--annotate),OPEN,2023-09-14T13:49:53Z,2023-09-15T08:38:17Z,,"### Description of feature

Opening this issue as suggested by @FriederikeHanssen on slack, to get fixed: https://nfcore.slack.com/archives/CGFUX04HZ/p1693559870404319?thread_ts=1693235207.631999&cid=CGFUX04HZ

I tried to use CNVkit on WGS data where the bed file is not labeled with genes in a specific columns. This can be done on cnvkit with --annotate refFlat.txt argument (http://hgdownload.cse.ucsc.edu/goldenPath/hg38/database/refFlat.txt.gz).

I solve this using cnv_annotate.py from cnvkit utilities that allow you to annotate them after running cnvkit using refFlat.txt

Thanks!

",migmarbor,https://github.com/nf-core/sarek/issues/1234
I_kwDOCvwIC85xEdnH,Running out of disk space,CLOSED,2023-09-14T17:14:43Z,2023-09-19T05:30:44Z,2023-09-19T05:30:44Z,"### Description of the bug

Hi,

Running sarek locally on a disk with 3TB available, however the total space used by the pipeline (mostly the work/ directory) exceeds this and the pipeline fails.

Questions:

1) is there a way to prevent this from happening other than by breaking down the batch and running the pipeline on smaller subsets of samples?

2) When I resume the pipeline (with --resume), should I delete the contents of work/ before resuming?

Thanks,
Sujay



### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",sxv,https://github.com/nf-core/sarek/issues/1235
I_kwDOCvwIC85xQJ67,`ASSESS_SIGNIFICANCE` fails with Rscript error,CLOSED,2023-09-18T02:31:16Z,2024-08-08T08:14:21Z,2024-08-08T08:14:21Z,"### Description of the bug

`ASSESS_SIGNIFICANCE` step keeps failing at Rscript with the below error:

```
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_CONTROLFREEC:ASSESS_SIGNIFICANCE (AF60_72U_vs_AF00_69U)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_CONTROLFREEC:ASSESS_SIGNIFICANCE (AF60_72U_vs_AF00_69U)` terminated with an error exit status (1)

Command executed:

  cat $(which assess_significance.R) | R --slave --args AF60_72U_vs_AF00_69U.tumor.mpileup.gz_CNVs AF60_72U_vs_AF00_69U.tumor.mpileup.gz_normal_CNVs AF60_72U_vs_AF00_69U.tumor.mpileup.gz_normal_ratio.txt AF60_72U_vs_AF00_69U.tumor.mpileup.gz_ratio.txt
  
  mv *.p.value.txt AF60_72U_vs_AF00_69U.p.value.txt
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_CONTROLFREEC:ASSESS_SIGNIFICANCE"":
      controlfreec: $(echo $(freec -version 2>&1) | sed 's/^.*Control-FREEC  //; s/:.*$//' | sed -e ""s/Control-FREEC v//g"" )
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  Loading required package: GenomicRanges
  Loading required package: stats4
  Loading required package: BiocGenerics
  Loading required package: parallel
  
  Attaching package: ‘BiocGenerics’
  
  The following objects are masked from ‘package:parallel’:
  
      clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,
      clusterExport, clusterMap, parApply, parCapply, parLapply,
      parLapplyLB, parRapply, parSapply, parSapplyLB
  
  The following objects are masked from ‘package:stats’:
  
      IQR, mad, sd, var, xtabs
  
  The following objects are masked from ‘package:base’:
  
      anyDuplicated, append, as.data.frame, basename, cbind, colnames,
      dirname, do.call, duplicated, eval, evalq, Filter, Find, get, grep,
      grepl, intersect, is.unsorted, lapply, Map, mapply, match, mget,
      order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,
      rbind, Reduce, rownames, sapply, setdiff, sort, table, tapply,
      union, unique, unsplit, which.max, which.min
  
  Loading required package: S4Vectors
  
  Attaching package: ‘S4Vectors’
  
  The following object is masked from ‘package:base’:
  
      expand.grid
  
  Loading required package: IRanges
  Loading required package: GenomeInfoDb
  Error in `$<-.data.frame`(`*tmp*`, Ratio, value = logical(0)) : 
    replacement has 0 rows, data has 701
  Calls: $<- -> $<-.data.frame
  Execution halted

Work dir:
  /nextflow/sarek/work/bc/7f398053d6401db4e9aa9a25eca18c

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`

 -- Check '.nextflow.log' file for details
```

### Command used and terminal output

```console
nextflow run nf-core/sarek \
-latest \
-profile docker \
--step mapping \
--tools freebayes,mpileup,mutect2,strelka,manta,tiddit,ascat,cnvkit,controlfreec,msisensorpro \
--input 'samplesheet_sarek.csv' \
--outdir '/nextflow/sarek/results/' \
-work-dir '/nextflow/sarek/work/' \
-c 'custom.config' \
-r master \
-resume
```


### Relevant files

_No response_

### System information

N E X T F L O W ~ version 23.08.1-edge
local
Docker
Linux
nf-core/sarek master v3.3.0",bounlu,https://github.com/nf-core/sarek/issues/1239
I_kwDOCvwIC85xjFtn,Add Azure megatests,OPEN,2023-09-20T13:37:53Z,2023-11-28T12:04:13Z,,"### Description of feature

Add megatests also for Azure platform",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1249
I_kwDOCvwIC85xoNsL,"Stop ignoring parameter validation, lenient mode is sufficient",OPEN,2023-09-21T08:06:43Z,2023-09-21T08:06:54Z,,"### Description of feature

https://nfcore.slack.com/archives/C056RQB10LU/p1695283527310929?thread_ts=1695280559.241039&cid=C056RQB10LU",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1250
I_kwDOCvwIC85xtfaT,Output files for CNVkit incorrect,OPEN,2023-09-21T20:57:35Z,2024-08-20T15:57:31Z,,"### Description of the bug

Hello, I am a newbie to nf-core sarek and I am running it on WES data for tumor-only, using CNVkit tool. Even though the pipeline completes successfully, the CNVkit tool directory in the variant_calling directory does not produce all the required outputs.

The output files generated in my case are:

- cnvkit.reference.antitarget-tmp.bed  
- cnvkit.reference.target-tmp.bed  
- tumor.antitargetcoverage.cnn 
- tumor.targetcoverage.cnn

I have provided the command that I run.

### Command used and terminal output

```console
nextflow run nf-core-sarek-3.2.3/workflow --input design.csv --outdir output/ --genome GATK.GRCh38 -profile singularity --wes -c nfcore-RT-wes.config --tools cnvkit --intervals S03723314_Padded_hg38_Agi_v4_PDMR_3_column_only_primary.bed
```


### Relevant files

_No response_

### System information

- nf-core-sarek-3.2.3
- HPC 
- singularity/3.7.0
- nextflow/23.04.3.",parth2608,https://github.com/nf-core/sarek/issues/1251
I_kwDOCvwIC85x7ruI,"Error out when user provides both custom genome files (fasta, fai, dict, etc) in addition to --genome param",OPEN,2023-09-25T13:04:53Z,2023-09-25T13:59:04Z,,"### Description of feature

At the moment, it is possible to mix and match custom reference genome files (using --fasta param, for example) with iGenomes reference files.

This has the potential to cause challenging reference conflicts that can be difficult to debug or even notice.

I would suggest that sarek should error out if the user mixes custom and built-in reference genomes.",robsyme,https://github.com/nf-core/sarek/issues/1253
I_kwDOCvwIC85yDYd_,flowcellLaneFromFastq cannot handle weird fastq header,OPEN,2023-09-26T13:14:20Z,2023-09-29T13:25:32Z,,"### Description of the bug

Sarek v3.1.1, 3.1.2 (required for legacy compatibility) fails with error:
```
Check script '/<path>/nextflow/assets/nf-core/sarek/./workflows/sarek.nf' at line: 1358
```
i.e. `flowcellLaneFromFastq` closure.

Fastq header (data is from BGI and this is standard for their output fastq apparently) looks like:
```
@E100052094L1C001R00100000009/2
```
so obviously mis-formatted for schema:
```
// expected format:
// xx:yy:FLOWCELLID:LANE:... (seven fields)
// or
// FLOWCELLID:LANE:xx:... (five fields)
```

Is there a way I can skip the check on the fastq's or force the required settings?



### Command used and terminal output

```console
#!/bin/bash -l
#SBATCH --job-name=1180
#SBATCH --output=/scratch/moranb/projects/U50_CRC/1180/1180.log
#SBATCH --ntasks=1
#SBATCH --time 120:00:00
#SBATCH --cpus-per-task=1
#SBATCH --tasks-per-node=1
module load nextflow/22.04.5.5708 singularity/3.5.2
export NXF_SINGULARITY_CACHEDIR=/scratch/moranb/nextflow/sarek/3.1.1/singularity-images
srun nextflow -log /scratch/moranb/projects/U50_CRC/1180/.nextflow.log run nf-core/sarek -r 3.1.2 -profile singularity,ucd_sonic --outdir /scratch/moranb/projects/U50_CRC/1180/sarek --input /scratch/moranb/projects/U50_CRC/data/1180.sample_sarek.csv --tools cnvkit,deepvariant,freebayes,haplotypecaller,manta,merge,mpileup,msisensorpro,mutect2,strelka,tiddit,vep --email bruce.moran@ucd.ie -c /scratch/moranb/projects/U50_CRC/1180/.1180.nextflow.config -w /scratch/moranb/projects/U50_CRC/1180/work -with-mpi;
```
```


### Relevant files

[1180.log](https://github.com/nf-core/sarek/files/12727856/1180.log)


### System information

- nextflow/22.04.5.5708 
- HPC
- slurm
- singularity/3.5.2
- Ubuntu 18.04
- sarek v3.1.1, v3.1.2",brucemoran,https://github.com/nf-core/sarek/issues/1255
I_kwDOCvwIC85yHvhB,SNPEFF annotation process interrupted when no cache available,CLOSED,2023-09-27T03:42:52Z,2023-10-03T08:00:38Z,2023-10-03T08:00:38Z,"### Description of the bug

I am using the sarek pipeline latest version (**3.3.1**) with the utilization of `sentieon` workflow in order to perform **joint variant calling** using `sentieon_gvcftyper` for 20 WES samples. However the annotation process with `SNPEFF` is not working with the following errors:

```
-[nf-core/sarek] Pipeline completed with errors-
WARN: Input tuple does not match input set cardinality declared by process `NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF` -- offending value: []
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF (1)'

Caused by:
  Not a valid path value type: org.codehaus.groovy.runtime.NullObject (null)
``` 
I have included the `nextflow.log` file and also the `WES_sentieon_jointcalling_headnode.log` run log file.

### Command used and terminal output

```console
nextflow run main.nf \
    --input /mnt/rdisk/hieule/sarek_sentieon/input/samplesheet.csv \
    --outdir /mnt/rdisk/hieule/sarek_sentieon/output_3.3.1/ \
    -w /mnt/rdisk/hieule/sarek_sentieon/work_dir/ \
    --igenomes_base /mnt/rdisk/hieule/sarek_sentieon/input/references/ \
    --wes \
    --intervals /mnt/rdisk/hieule/sarek_sentieon/input/hg38_Exome_2.0_Plus_Panel_chr.bed \
    -profile docker \
    --aligner sentieon-bwamem \
    --tools sentieon_dedup,sentieon_haplotyper,merge \
    --joint_germline \
    --sentieon_haplotyper_emit_mode gvcf
```


### Relevant files

[WES_sentieon_jointcalling_headnode.log](https://github.com/nf-core/sarek/files/12734166/WES_sentieon_jointcalling_headnode.log)
[nextflow.log](https://github.com/nf-core/sarek/files/12734168/nextflow.log)


### System information

- Nextflow version 23.04.2
- Hardware: HPC
- Container engine: Docker
- OS: Linux Ubuntu 22.04
- Version of nf-core/sarek: 3.3.1",ducminhnguyenle,https://github.com/nf-core/sarek/issues/1257
I_kwDOCvwIC85yXoXS,The sample-sheet only contains tumor-samples,CLOSED,2023-09-29T08:08:02Z,2024-08-14T10:37:40Z,2024-06-17T17:19:14Z,"### Description of the bug

I am getting a new type of error when running the same sample samplesheet as it used to work before:

```
The sample-sheet only contains tumor-samples, but the following tools, which were requested by the option ""tools"", expect at least one normal-sample : ascat, msisensorpro
Files within --vep_cache invalid. Make sure there is a directory named homo_sapiens/110_GRCh38 in s3://annotation-cache/vep_cache/.
https://nf-co.re/sarek/dev/usage#how-to-customise-snpeff-and-vep-annotation
The sample-sheet only contains normal-samples, but the following tools, which were requested with ""--tools"", expect at least one tumor-sample : ascat, controlfreec, mutect2, msisensorpro
```

The sample sheet contains both normal and tumor samples though.

### Command used and terminal output

```console
nextflow run nf-core/sarek \
-latest \
-profile docker \
--step mapping \
--tools freebayes,mpileup,mutect2,strelka,manta,tiddit,ascat,cnvkit,controlfreec,msisensorpro,snpeff,vep,merge \
--input 'samplesheet_sarek.csv' \
--outdir '/nextflow/sarek/results/' \
-work-dir '/nextflow/sarek/work/' \
-c 'custom.config' \
-r master \
-resume
```


### Relevant files

_No response_

### System information

N E X T F L O W ~ version 23.08.1-edge
local
Docker
Linux
nf-core/sarek master v3.3.1",bounlu,https://github.com/nf-core/sarek/issues/1260
I_kwDOCvwIC85yiaiJ,AlphaMissense VEP Plugin Integration,OPEN,2023-10-02T10:12:17Z,2023-10-02T10:12:17Z,,"### Description of feature

Dear Sarek Developer Team, 

I think it would be a great add-on to integrate the recently published AlphaMissense Plugin into the vepsensembl module providing predictions on pathogenicity of all single–amino acid substitutions. 

https://www.science.org/doi/10.1126/science.adg7492

Best, 
Moritz",Moe94,https://github.com/nf-core/sarek/issues/1265
I_kwDOCvwIC85ylEEe,EnsemblVEP output only contains HTML report,CLOSED,2023-10-02T17:18:55Z,2023-10-02T18:45:06Z,2023-10-02T18:45:06Z,"### Description of the bug

I am wondering where the output files for EnsemblVEP are located. The output folder only contains HTML reports: /reports/EnsemblVEP/haplotypecaller/test
test.filtered_snpEff_VEP.ann.summary.html
test.filtered_VEP.ann.summary.html

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.2.3 -profile singularity --input samplesheet_test.csv --outdir output/ sarek_output_test_haplotypecaller_annotate --genome GATK.GRCh38 --tools haplotypecaller,merge,snpeff,vep
```


### Relevant files

_No response_

### System information

_No response_",slives-lab,https://github.com/nf-core/sarek/issues/1268
I_kwDOCvwIC85yum95,Receiving new error (ERROR ~ Unexpected error [EOFException]),OPEN,2023-10-03T19:39:20Z,2023-10-27T09:20:13Z,,"When beginning the pipeline with this command:

`NXF_VER=""23.04.1"" nextflow run nf-core/sarek -r 3.0.1 -profile docker -params-file 720.yaml -config sxv.config --save_output_as_bam`

### Description of the bug

```
ERROR ~ Unexpected error [EOFException]

 -- Check script '/home/lab/.nextflow/assets/nf-core/sarek/./workflows/sarek.nf' at line: 1319 or see '.nextflow.log' file for more details
```

The log file shows:

```
Oct-03 12:32:53.373 [main] DEBUG nextflow.Session - Session await > all processes finished
Oct-03 12:32:53.374 [main] DEBUG nextflow.Session - Session await > all barriers passed
Oct-03 12:32:53.405 [main] INFO  nextflow.Nextflow - -^[[0;35m[nf-core/sarek]^[[0;31m Pipeline completed with errors^[[0m-
Oct-03 12:32:53.413 [Actor Thread 56] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Oct-03 12:32:53.415 [Actor Thread 58] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Oct-03 12:32:53.415 [Actor Thread 36] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Oct-03 12:32:53.415 [Actor Thread 57] DEBUG nextflow.file.SortFileCollector - FileCollector temp dir not removed: null
Oct-03 12:32:53.415 [Actor Thread 13] DEBUG nextflow.processor.TaskProcessor - Handling unexpected condition for
  task: name=NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY; work-dir=null
  error [java.lang.InterruptedException]: java.lang.InterruptedException
Oct-03 12:32:53.415 [Actor Thread 22] DEBUG nextflow.processor.TaskProcessor - Handling unexpected condition for
  task: name=NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP; work-dir=null
  error [java.lang.InterruptedException]: java.lang.InterruptedException
Oct-03 12:32:53.415 [Actor Thread 14] DEBUG nextflow.processor.TaskProcessor - Handling unexpected condition for
  task: name=NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX; work-dir=null
  error [java.lang.InterruptedException]: java.lang.InterruptedException
Oct-03 12:32:53.415 [Actor Thread 19] DEBUG nextflow.processor.TaskProcessor - Handling unexpected condition for
  task: name=NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX; work-dir=null
  error [java.lang.InterruptedException]: java.lang.InterruptedException
Oct-03 12:32:53.416 [Actor Thread 33] DEBUG nextflow.processor.TaskProcessor - Handling unexpected condition for
  task: name=NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE; work-dir=null
  error [java.lang.InterruptedException]: java.lang.InterruptedException
Oct-03 12:32:53.416 [Actor Thread 11] DEBUG nextflow.processor.TaskProcessor - Handling unexpected condition for
  task: name=NFCORE_SAREK:SAREK:PREPARE_CNVKIT_REFERENCE:CNVKIT_ANTITARGET; work-dir=null
```

Seems maybe to be related to the genome download? Any steps that would help troubleshoot? I tried reducing to a test batch of 1 sample, but still getting the same error. Thanks.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",sxv,https://github.com/nf-core/sarek/issues/1269
I_kwDOCvwIC85zB4J1,Missing bam files,OPEN,2023-10-06T10:27:50Z,2023-10-13T01:45:11Z,,"### Description of the bug

> while running the Sarek 3.3.1 pre-processing step on a tumor-normal pair, the pipeline completed successfully, but it only generated the bam file for the normal sample

> I tried with both 3.3.0 and 3.3.2, the same problem are still there…

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1273
I_kwDOCvwIC85zB4tN,null value in mutect2 subworkflow,CLOSED,2023-10-06T10:29:12Z,2023-10-11T10:10:17Z,2023-10-11T10:10:16Z,"### Description of the bug

> A USER ERROR has occurred: Bad input: Sample E14_null is not in BAM header: [E14_E14_1_BM_T, E14_E14_2_BM_T, E14_E14_3_LN_T, E14_E14_N]

> gatk --java-options ""-Xmx29491M -XX:-UsePerfData"" \ Mutect2 \ --input E14_2_BM_T.converted.cram --input E14_N.converted.cram --input E14_3_LN_T.converted.cram --input E14_1_BM_T.converted.cram \ --output E14.mutect2.vcf.gz \ --reference human_g1k_v37_decoy.fasta \ --panel-of-normals pon.vcf.gz \ --germline-resource af-only-gnomad.raw.sites.vcf.gz \ --intervals 1_65565-65573.bed \ --tmp-dir . \ --f1r2-tar-gz E14.mutect2.f1r2.tar.gz --normal-sample E14_null

> I am also hitting this error with version 3.3.2. The weird thing is that some MUTECT2_PAIRED jobs finish successfully while others fail due to null normal_id . I checked the contents of the input_intervals in the somatic_mutect2 subworkflow and the normal_id meta values are set as they are supposed to. Very confusing...

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1274
I_kwDOCvwIC85zNaOo,Merge and normalise VCF files,OPEN,2023-10-09T11:52:09Z,2023-10-09T11:52:09Z,,"### Description of feature

Sarek pipeline includes different variant callers, but no variant normalisation is performed after variant calling. It would be useful to add the following features:

1. Normalise variants in VCF files for each variant caller by splitting multiallelic sites and remove duplicates using bcftools norm
2. Merge VCF files from different variant callers after variant normalisation",Matteodigg,https://github.com/nf-core/sarek/issues/1275
I_kwDOCvwIC85zNpSO,lane should be optional if unique,OPEN,2023-10-09T12:27:19Z,2023-10-09T12:27:19Z,,"### Description of feature

Lane should be optional, we really need one if a sample is spread over more than one.",maxulysse,https://github.com/nf-core/sarek/issues/1276
I_kwDOCvwIC85zaE6_,Parameter validation doesn't catch of unzipped vcf is supplied,OPEN,2023-10-10T20:51:16Z,2023-10-10T20:51:16Z,,"### Description of the bug

Currently parameter validation does not catch when the vcf file for PON is not zipped. This causes issues within the pipeline and is not intended. 



### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1279
I_kwDOCvwIC85zeWVq,Readme uses bith light and dark logo at all times,OPEN,2023-10-11T09:46:04Z,2023-10-11T09:46:04Z,,"### Description of the bug

![Screenshot 2023-10-11 at 11 42 53](https://github.com/nf-core/sarek/assets/12273093/a4a46c69-1f59-406c-a82b-382473ac9fdc)


### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1281
I_kwDOCvwIC85ze-sE,The pipeline should fail early when --no_intervals is used with joint germline calling,OPEN,2023-10-11T11:07:39Z,2024-08-03T12:40:33Z,,"### Description of the bug

As title states, we used to have it but apparently not anymore, see here: https://nfcore.slack.com/archives/CGFUX04HZ/p1697021363120009

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1282
I_kwDOCvwIC85zf5cq,Usage.md: link to example samplesheet is broken,OPEN,2023-10-11T13:07:35Z,2023-11-07T13:40:18Z,,"### Description of the bug

https://nf-co.re/sarek/3.3.2/docs/usage#overview-samplesheet-columns
-> https://github.com/nf-core/sarek/blob/dev/docs/usage.md?plain=1#L99
### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1283
I_kwDOCvwIC85z6-zA,new subworkflow: samplesheet handling,OPEN,2023-10-16T10:03:01Z,2023-10-16T13:24:38Z,,"The samplesheet is now being handled by a mapping function in the main workflow [here](https://github.com/nf-core/sarek/blob/dev/workflows/sarek.nf#L84-L177). However, we want to translate this to a proper subworkflow to make it easier for updates and tidy up the main workflow. ",RaqManzano,https://github.com/nf-core/sarek/issues/1285
I_kwDOCvwIC850VjmU,CNVKIT - Export cns file into seg format for IGV,OPEN,2023-10-19T10:15:15Z,2023-10-19T10:15:15Z,,,maxulysse,https://github.com/nf-core/sarek/issues/1292
I_kwDOCvwIC850o4Wp,Samplesheets with more normal-samples per patient pass nf-validation,OPEN,2023-10-23T10:43:20Z,2024-04-30T11:25:09Z,,"### Description of the bug

Samplesheets with more normal-samples per patient pass nf-validation, but they (probably) shouldn't.

https://nfcore.slack.com/archives/CGFUX04HZ/p1698056996871649?thread_ts=1697539431.495609&cid=CGFUX04HZ

The issue can be reproduced by using this csv:

```
$ cat my_recalibrated_somatic_joint.csv
patient,sex,status,sample,cram,crai
test,XX,0,sample1,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/cram/test.paired_end.recalibrated.sorted.cram,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/cram/test.paired_end.recalibrated.sorted.cram.crai
test,XX,0,sample2,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/cram/test2.paired_end.recalibrated.sorted.cram,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/cram/test2.paired_end.recalibrated.sorted.cram.crai
test,XX,1,sample3,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/cram/test3.paired_end.recalibrated.sorted.cram,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/cram/test3.paired_end.recalibrated.sorted.cram.crai
```

which is just `tests/csv/3.0/recalibrated_somatic_joint.csv` where `sample2` has changed status from `0` (normal) to `1` (tumor).
```
nextflow run main.nf -profile test_cache,tools_somatic,docker --tools mutect2 --outdir results --input my_recalibrated_somatic_joint.csv
```

@FriederikeHanssen knows what this is about ;-) 

### Command used and terminal output

```console
nextflow run main.nf -profile test_cache,tools_somatic,docker --tools mutect2 --outdir results --input my_recalibrated_somatic_joint.csv

....

Detected join operation duplicate emission on right channel -- offending element: key=test; value=[patient:test, sample:sample2, sex:XX, status:0, id:sample2, data_type:cram],/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/cram/test2.paired_end.recalibrated.sorted.cram,/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/cram/test2.paired_end.recalibrated.sorted.cram.crai
```


### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1293
I_kwDOCvwIC850wigI,Sarek stops and cannot connect to amazonaws,OPEN,2023-10-24T09:41:21Z,2023-10-26T07:25:34Z,,"### Description of the bug

As seen in the attached log file, the pipeline stops working and cycles in error with amazonaws. I am new to nf-core so I am unfamiliar with how all the parts of the pipeline work. Did this happen to anyone else?

### Command used and terminal output

```console
nextflow run nf-core/sarek --input fullpath/samplesheet.csv --outdir fullpath/results --tools freebayes,mutect2,strelka,snpeff,vep,merge -profile singularity --max_cpus 30 --step mapping --igenomes_base fullpath/references --use_annotation_cache_key


executor >  local (6977)
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                                                                 -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                                                                 -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                                                             -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                                                             -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                                                                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP                                                                                   -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                                                                       -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                                                              -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                                                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                                                                                     -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_ALLELES                                                                                 -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_LOCI                                                                                    -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_GC                                                                                      -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_RT                                                                                      -
[d3/8aa26a] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (wgs_calling_regions_noseconds.hg38.bed)                              [100%] 1 of 1 ✔
[9a/080ce7] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (chr6_95070791-167591393)                                  [100%] 21 of 21 ✔
[e0/0f1bfc] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED (wgs_calling_regions_noseconds.hg38)                    [100%] 1 of 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP                                                                    -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP                                                                -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP                                                                  -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP                                                                  -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP                                                                     -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_UNMAP                                                                      -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_MAP                                                                        -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:CAT_FASTQ                                                                                -
[58/65590e] process > NFCORE_SAREK:SAREK:FASTQC (R198-a)                                                                                              [100%] 38 of 38 ✔
[dc/9d8f01] process > NFCORE_SAREK:SAREK:FASTP (R15-a)                                                                                                [100%] 38 of 38 ✔
[bd/ab3cba] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (R15-a)                                                 [100%] 456 of 456 ✔
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM                                                         -
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN                                                       -
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM                                                     -
[43/e11fc4] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (R212)                                                               [100%] 38 of 38 ✔
[1f/6884db] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS (R212)                                           [100%] 38 of 38 ✔
[e2/10b8d7] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH (R212)                                                 [100%] 38 of 38 ✔
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM                                                                                                  -
[59/02d181] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (R212)                                                           [100%] 798 of 798 ✔
[f9/ff8a81] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS (R212)                                                          [100%] 38 of 38 ✔
[f8/f3d9b1] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR (R212)                                                                         [100%] 798 of 798 ✔
[7d/51e68d] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:MERGE_CRAM (R212)                                                    [100%] 38 of 38 ✔
[fe/f6f190] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM (R212)                                                    [100%] 38 of 38 ✔
[61/f96072] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS (R212)                                                                          [100%] 38 of 38 ✔
[f1/8d2956] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:MOSDEPTH (R212)                                                                                [100%] 38 of 38 ✔
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM_RECAL                                                                                            -
[63/574e87] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_FREEBAYES:FREEBAYES (R83)                               [100%] 399 of 399 ✔
[be/39f200] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_FREEBAYES:BCFTOOLS_SORT (R83)                           [100%] 399 of 399 ✔
[2d/1c2ce6] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_FREEBAYES:MERGE_FREEBAYES (R83)                         [100%] 19 of 19 ✔
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_FREEBAYES:TABIX_VC_FREEBAYES                            -
[8b/8b43ef] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE (R83)                     [100%] 399 of 399 ✔
[d7/220eb4] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA (R83)                      [100%] 19 of 19 ✔
[cb/ad1f55] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME (R83)               [100%] 19 of 19 ✔
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_FREEBAYES:FREEBAYES                                   -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_FREEBAYES:BCFTOOLS_SORT                               -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_FREEBAYES:MERGE_FREEBAYES                             -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_FREEBAYES:TABIX_VC_FREEBAYES                          -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:MUTECT2                            -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:MERGE_MUTECT2                      -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:MERGEMUTECTSTATS                   -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:LEARNREADORIENTATIONMODEL          -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:GETPILEUPSUMMARIES                 -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:GATHERPILEUPSUMMARIES              -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:CALCULATECONTAMINATION             -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:FILTERMUTECTCALLS                  -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE                         -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA                          -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME                   -
[2e/47d601] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_FREEBAYES:FREEBAYES (R212_vs_R210)                       [100%] 399 of 399 ✔
[93/3c8e8c] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_FREEBAYES:BCFTOOLS_SORT (R212_vs_R210)                   [100%] 399 of 399 ✔
[63/74013b] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_FREEBAYES:MERGE_FREEBAYES (R212_vs_R210)                 [100%] 19 of 19 ✔
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_FREEBAYES:TABIX_VC_FREEBAYES                             -
[0b/9cf2ed] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:STRELKA_SOMATIC (R212_vs_R210)           [100%] 399 of 399 ✔
[d3/ed61e0] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_INDELS (R212_vs_R210)      [100%] 19 of 19 ✔
[bd/0306e8] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_SNVS (R212_vs_R210)        [100%] 19 of 19 ✔
[57/d6f8c8] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED (R212_vs_R210)            [100%] 399 of 399 ✔
[01/72d767] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MERGE_MUTECT2 (R212_vs_R210)             [100%] 19 of 19 ✔
[73/b9901e] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MERGEMUTECTSTATS (R212_vs_R210)          [100%] 19 of 19 ✔
[82/bffe19] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:LEARNREADORIENTATIONMODEL (R212_vs_R210) [100%] 19 of 19 ✔
[8e/30c650] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GETPILEUPSUMMARIES_NORMAL (R210)         [100%] 399 of 399 ✔
[7d/f5a375] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GETPILEUPSUMMARIES_TUMOR (R212)          [100%] 399 of 399 ✔
[f5/6e564f] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GATHERPILEUPSUMMARIES_NORMAL (R210)      [100%] 19 of 19 ✔
[fb/102294] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GATHERPILEUPSUMMARIES_TUMOR (R212)       [100%] 19 of 19 ✔
[0e/63a4ca] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:CALCULATECONTAMINATION (R212_vs_R210)    [100%] 19 of 19 ✔
[a9/da52fb] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:FILTERMUTECTCALLS (R212_vs_R210)         [100%] 19 of 19 ✔
[42/6f01f9] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS (R212_vs_R210)                                                       [100%] 114 of 114 ✔
[43/218914] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT (R212_vs_R210)                                                  [100%] 114 of 114 ✔
[9f/57ab17] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL (R212_vs_R210)                                                   [100%] 114 of 114 ✔
[83/e2f11b] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY (R212_vs_R210)                                                     [100%] 114 of 114 ✔
[8d/b7c4ab] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF (R15_vs_R16)                                              [100%] 114 of 114 ✔
[d0/ea632a] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:TABIX_BGZIPTABIX (R15_vs_R16)                                           [100%] 114 of 114 ✔
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_MERGE:ENSEMBLVEP_VEP                                                           -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_MERGE:TABIX_TABIX                                                              -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:ENSEMBLVEP_VEP                                                      -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:TABIX_TABIX                                                         -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                                                                                  -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC
```


### Relevant files

[.nextflow.log](https://github.com/nf-core/sarek/files/13113076/default.nextflow.log)


### System information

nextflow 23.04.2
nf-core/sarek 3.3.2
System: Linux
Container: singularity",vmelichar,https://github.com/nf-core/sarek/issues/1295
I_kwDOCvwIC850wxFG,ContaminationAssessment in the Sentieon-subworkflows TNseq and TNscope?,OPEN,2023-10-24T10:19:22Z,2024-12-09T16:22:14Z,,"### Description of feature

Figure out if the Sentieon Algo [ContaminationAssessment](https://support.sentieon.com/manual/usages/general/#contaminationassessment-algorithm) should be called in the TNseq and TNscope subworkflows.

If so, how should the contamination values be passed into the tnfilter-module. (In the GATK4_FILTERMUTECTCALLS-module, the contamination value is passed into the module as `val(estimate)` in the input-section.)

(This issue is basically just a reminder for @asp8200 ) ",asp8200,https://github.com/nf-core/sarek/issues/1296
I_kwDOCvwIC8504PSv,Sarek --download_cache does not work properly,CLOSED,2023-10-25T09:00:04Z,2023-11-15T10:22:21Z,2023-11-15T10:22:21Z,"### Description of the bug

I've repeatedly attempted to download the vep_cache and snpeff_cache files to bypass the need for staging these files every time the pipeline processes samples. However, I consistently encounter similar error messages, whether I follow the guidelines from [Sarek's documentation on customizing snpEff and VEP annotation](https://nf-co.re/sarek/3.3.2/docs/usage#how-to-customise-snpeff-and-vep-annotation) or the instructions provided on Slack. Interestingly, when executing the pipeline on samples using the --use_annotation_cache_keys option, the pipeline stages the required annotation files without any issues.

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.3.2 --outdir results --outdir_cache /data/home/hadi.eidgah/small_germline/cache/ --tools vep,snpeff --download_cache --build_only_index --input false

or 

nextflow run /data/home/hadi.eidgah/pipelines/nf-core-sarek_3.3.2/3_3_2 --wes true --intervals /data/home/hadi.eidgah/refrences/Annotation/intervals/SureSelectXT_HS_Human_All_Exon_V8_S33266340_hs_hg38/S33266340_Padded.bed --input /data/home/hadi.eidgah/raw_data/SampleSheets/luisa_small.csv --outdir /data/home/hadi.eidgah/small_germline -profile singularity --tools haplotypecaller,snpeff,vep --use_annotation_cache_keys --download_cache

both raise error:

Files within --vep_cache invalid. Make sure there is a directory named homo_sapiens/110_GRCh38 in s3://annotation-cache/vep_cache/.
https://nf-co.re/sarek/usage#how-to-customise-snpeff-and-vep-annotation
```


### Relevant files

_No response_

### System information

_No response_",Hadi90Eidgah,https://github.com/nf-core/sarek/issues/1297
I_kwDOCvwIC851Lvgh,Config setting `prov.formats`,CLOSED,2023-10-27T18:33:54Z,2023-10-27T20:34:31Z,2023-10-27T19:42:55Z,"### Description of the bug

Hello everyone,

I installed nf-core/sarek -r 3.3.2 three weeks ago, and it was running smoothly until today. However, when I ran it again today, I encountered an error related to ""prov.formats."" I attempted to reinstall both Nextflow and Sarek version 3.3.2, but the error persisted. As a temporary solution, I downgraded to version 3.3.1, and it worked without any issues.

Could you please advise on how I can resolve this error and continue using Sarek 3.3.2?

Thank you, and I wish you all a great weekend.

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.3.2 -profile test --outdir .
N E X T F L O W  ~  version 23.10.0
Pulling nf-core/sarek ...
 downloaded from https://github.com/nf-core/sarek.git
Launching `https://github.com/nf-core/sarek` [chaotic_bassi] DSL2 - revision: f034b73763 [3.3.2]
Downloading plugin nf-validation@1.1.1
Downloading plugin nf-prov@1.2.0
Config setting `prov.formats` is required to specify provenance output formats
```


### Relevant files

_No response_

### System information

_No response_",lucianhu,https://github.com/nf-core/sarek/issues/1299
I_kwDOCvwIC851MI35,Fix the version of the plugins used in Sarek,CLOSED,2023-10-27T19:56:37Z,2023-11-01T09:35:42Z,2023-11-01T09:35:42Z,"### Description of feature

I suggest that we fix the version of the plugins used in Sarek in order to avoid sudden errors like [this](https://github.com/nf-core/sarek/issues/1299) popping up. We previously had a somewhat similar problem in connection with a new release of nf-validation. 

Discussed on [slack](https://nfcore.slack.com/archives/C05V9FRJYMV/p1698675361365649?thread_ts=1698674043.937679&cid=C05V9FRJYMV).",asp8200,https://github.com/nf-core/sarek/issues/1300
I_kwDOCvwIC851rkLt,Strelka requires `.crai` files to match format `PREFIX.cram.crai`,OPEN,2023-11-02T14:14:30Z,2024-01-17T10:40:05Z,,"### Description of the bug

Per [this slack thread](https://nfcore.slack.com/archives/CGFUX04HZ/p1698875675291879) the `Strelka` tool can take `bam` and `bai` or `cram` and `crai` but the requirements for the index filenames are subtly different.

For `bam` files the formats `PREFIX.bam.bai` and `PREFIX.bai` are both accepted, but for `cram` only `PREFIX.cram.crai` and not `PREFIX.crai`.

This is not well documented or checked for currently. 

In theory this could be checked for at runtime when processing the samplesheet and I think it may be as simple as:

```
if ((cram && crai) && crai.getName() != (cram.getName() + '.crai')){
     // throw some error
}
```

I couldn't currently see any check that if provided a `cram` that `crai` is also provided but I may have missed that. 

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",awgymer,https://github.com/nf-core/sarek/issues/1306
I_kwDOCvwIC851xHbu,Be consistent with the meta map for the reference files,OPEN,2023-11-03T09:50:40Z,2023-11-03T09:50:40Z,,"              vould we be consistent with either .baseName or fasta?

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1304#discussion_r1381397178_
            ",maxulysse,https://github.com/nf-core/sarek/issues/1308
I_kwDOCvwIC8518HKQ,Sarek stops after alignment and does not perform variant calling,CLOSED,2023-11-06T09:34:30Z,2023-11-13T15:35:50Z,2023-11-13T15:00:00Z,"### Description of the bug

Hi,
I recently run sarek with the UMI pipeline for WES data, it successfully finishs but only performs alignment and does not perform variant calling.
For the UMI pipeline I skipped markduplicates and baserecalibration and extract read names from FASTQ file according to issue #746 .
Also, this workflow is running in offline mode.
I do not know what configuration to change so the workflow also perform variant calling.

### Command used and terminal output

```console
nextflow run nf-core-sarek_3.3.2/3_3_2 \
-profile singularity \
-c config/run.config \
-params-file config/nf-params.json \
-plugins nf-validation@1.0.0 \
-resume
```
The workflow performs software dump and multiQC after finishing the following process for each sample:
`NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM`
```


### Relevant files

`run.config` file:

process {
    // extract UMIS from read names, also see https://github.com/nf-core/sarek/issues/746
    withName: 'FASTQTOBAM' {
        ext.args         = '--extract-umis-from-read-names'
    }

    // Increase memory for fgbio processes
    withName: 'GROUPREADSBYUMI' {
        publishDir       = [
            [   path: { ""${params.outdir}/reports/umi/"" },
                mode: params.publish_dir_mode,
                pattern: ""*.{txt}""
            ]
        ]
        memory           = 90.GB
        ext.args         = '-Xmx80g'
    }

    withName: 'CALLUMICONSENSUS' {
        memory           = 90.GB
        ext.args         = '-Xmx80g -M 1 -S Coordinate'
        ext.prefix       = {""${meta.id}_umi-consensus""}
        publishDir       = [
            path: { ""${params.outdir}/preprocessing/umi/${meta.sample}"" },
            mode: params.publish_dir_mode,
            pattern: ""*.{bam}""
        ]
    }
}


`nf-params.json file:
```
{
    ""input"": ""config/samplesheet.csv"",
    ""step"": ""mapping"",
    ""outdir"": ""results/"",
    ""fasta"": ""db/references/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"",
    ""split_fastq"": 50000000,
    ""wes"": true,
    ""intervals"": ""data/Target_Region_Hg38_V8_chr.bed"",
    ""nucleotides_per_second"": 200000.0,
    ""no_intervals"": false,
    ""tools"": ""freebayes,mutect2,strelka"",
    ""skip_tools"": ""markduplicates,baserecalibrator"",
    ""trim_fastq"": false,
    ""clip_r1"": 0,
    ""clip_r2"": 0,
    ""three_prime_clip_r1"": 0,
    ""three_prime_clip_r2"": 0,
    ""trim_nextseq"": 0,
    ""save_trimmed"": false,
    ""umi_read_structure"": ""NA"",
    ""group_by_umi_strategy"": ""Adjacency"",
    ""save_split_fastqs"": false,
    ""aligner"": ""bwa-mem2"",
    ""save_mapped"": true,
    ""save_output_as_bam"": true,
    ""concatenate_vcfs"": false,
    ""only_paired_variant_calling"": true,
    ""joint_germline"": false,
    ""joint_mutect2"": false,
    ""ascat_min_base_qual"": 20.0,
    ""ascat_min_counts"": 10.0,
    ""ascat_min_map_qual"": 35.0,
    ""cf_coeff"": 0.05,
    ""cf_contamination_adjustment"": false,
    ""cf_contamination"": 0.0,
    ""cf_minqual"": 0.0,
    ""cf_mincov"": 0.0,
    ""cf_ploidy"": ""2"",
    ""ignore_soft_clipped_bases"": false,
    ""sentieon_haplotyper_emit_mode"": ""variant"",
    ""vep_cache"": ""db/vep/"",
    ""vep_include_fasta"": false,
    ""vep_dbnsfp"": false,
    ""dbnsfp_fields"": ""rs_dbSNP,HGVSc_VEP,HGVSp_VEP,1000Gp3_EAS_AF,1000Gp3_AMR_AF,LRT_score,GERP++_RS,gnomAD_exomes_AF"",
    ""vep_loftee"": false,
    ""vep_spliceai"": false,
    ""vep_spliceregion"": false,
    ""vep_custom_args"": ""--everything --filter_common --per_gene --total_length --offline --format vcf"",
    ""use_annotation_cache_keys"": false,
    ""vep_out_format"": ""vcf"",
    ""genome"": ""GATK.GRCh38"",
    ""save_reference"": true,
    ""build_only_index"": false,
    ""download_cache"": false,
    ""igenomes_base"": ""db/references"",
    ""igenomes_ignore"": false,
    ""custom_config_version"": ""master"",
    ""custom_config_base"": ""nf-core-sarek_3.3.2/configs"",
    ""test_data_base"": null,
    ""seq_platform"": ""ILLUMINA"",
    ""max_cpus"": 16,
    ""max_memory"": ""128.GB"",
    ""max_time"": ""240.h"",
    ""help"": false,
    ""version"": false,
    ""publish_dir_mode"": ""copy"",
    ""plaintext_email"": false,
    ""max_multiqc_email_size"": ""25.MB"",
    ""monochrome_logs"": false,
    ""validate_params"": true,
    ""validationShowHiddenParams"": false,
    ""validationFailUnrecognisedParams"": false,
    ""validationLenientMode"": false,
    ""snpeff_db"": null,
}
```

### System information

Nextflow version: 23.04.3
Hardware: HPC
Executor: local
Container engine: singularity
version of sarek: 3.3.2",sci-kai,https://github.com/nf-core/sarek/issues/1313
I_kwDOCvwIC852EzCM,"When both sentieon_haplotype and haplotypecaller are provided, config scopes are clashing",CLOSED,2023-11-07T09:56:17Z,2024-12-09T16:22:15Z,2023-11-07T19:21:36Z,"### Description of the bug

https://github.com/nf-core/sarek/issues/1260#issuecomment-1795810035

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1314
I_kwDOCvwIC852HJvW,Fix UMI-link,OPEN,2023-11-07T15:08:04Z,2023-11-07T15:08:04Z,,"### Description of the bug

https://nfcore.slack.com/archives/C05V9FRJYMV/p1699267623821589

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1316
I_kwDOCvwIC852PWsB,How to output allele-specific copy number in CNVkit?,OPEN,2023-11-08T14:26:44Z,2023-11-08T14:26:44Z,,"### Description of feature

Hello, I am using CNVkit and the sample.call.sns file, there is only copy number information (column ""cn""). May I ask how to output allele specific copy number (columns ""cn1"" and ""cn2"" ) as mentioned here: https://cnvkit.readthedocs.io/en/stable/pipeline.html#call
Thank you very much in advance,",TuongViDang,https://github.com/nf-core/sarek/issues/1322
I_kwDOCvwIC852Pm1e,Error with ASCAT,OPEN,2023-11-08T14:58:25Z,2024-01-03T21:03:19Z,,"### Description of the bug

Hello, I run ASCAT and got this error. Because it's a sample from male. I guess it has something to do with the chromosome Y but I am not sure if it is the case.

### Command used and terminal output

```console
nextflow run nf-core/sarek --input samplesheet.csv --outdir results --genome GATK.GRCh38 -profile singularity -revision 3.2.1 \
--tools strelka,mutect2,vep,cnvkit,ascat -c config_ori.txt  --save_output_as_bam -resume

output: 


Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (OO99_tumor_vs_OO99_normal)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (OO99_tumor_vs_OO99_normal)` terminated with an error exit status (1)

Command executed:

  #!/usr/bin/env Rscript
  library(RColorBrewer)
  library(ASCAT)
  options(bitmapType='cairo')
  
  #build prefixes: 
  allele_path = normalizePath(""ascat_alleles"")
  allele_prefix = paste0(allele_path, ""/"", ""ascat_alleles"", ""_chr"")
  
  loci_path = normalizePath(""ascat_loci"")
  loci_prefix = paste0(loci_path, ""/"", ""ascat_loci"", ""_chr"")
  
  #prepare from BAM files
  ascat.prepareHTS(
      tumourseqfile = ""OO99_tumor.recal.cram"",
      normalseqfile = ""OO99_normal.recal.cram"",
      tumourname = paste0(""OO99_tumor_vs_OO99_normal"", "".tumour""),
      normalname = paste0(""OO99_tumor_vs_OO99_normal"", "".normal""),
      allelecounter_exe = ""alleleCounter"",
      alleles.prefix = allele_prefix,
      loci.prefix = loci_prefix,
      gender = ""XY"",
      genomeVersion = ""hg38"",
      nthreads = 6
      ,minCounts = 10
  
      ,chrom_names = c(1:22, 'X', 'Y')
      ,min_base_qual = 20
      ,min_map_qual = 35
      ,ref.fasta = 'Homo_sapiens_assembly38.fasta'
  
      ,
      seed = 42
  )
  
  
  #Load the data
  ascat.bc = ascat.loadData(
      Tumor_LogR_file = paste0(""OO99_tumor_vs_OO99_normal"", "".tumour_tumourLogR.txt""),
      Tumor_BAF_file = paste0(""OO99_tumor_vs_OO99_normal"", "".tumour_tumourBAF.txt""),
      Germline_LogR_file = paste0(""OO99_tumor_vs_OO99_normal"", "".tumour_normalLogR.txt""),
      Germline_BAF_file = paste0(""OO99_tumor_vs_OO99_normal"", "".tumour_normalBAF.txt""),
      genomeVersion = ""hg38"",
      gender = ""XY""
  )
  
  #Plot the raw data
  ascat.plotRawData(ascat.bc, img.prefix = paste0(""OO99_tumor_vs_OO99_normal"", "".before_correction.""))
  
  # optional LogRCorrection
  if(""ascat_loci_gc"" != ""NULL"") {
      gc_input = paste0(normalizePath(""ascat_loci_gc""), ""/"", ""ascat_loci_gc"", "".txt"")
  
      if(""ascat_loci_rt"" != ""NULL""){
          rt_input = paste0(normalizePath(""ascat_loci_rt""), ""/"", ""ascat_loci_rt"", "".txt"")
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = rt_input)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""OO99_tumor_vs_OO99_normal"", "".after_correction_gc_rt.""))
      }
      else {
          ascat.bc = ascat.correctLogR(ascat.bc, GCcontentfile = gc_input, replictimingfile = ascat_loci_rt)
          #Plot raw data after correction
          ascat.plotRawData(ascat.bc, img.prefix = paste0(""OO99_tumor_vs_OO99_normal"", "".after_correction_gc.""))
      }
  }
  
  #Segment the data
  ascat.bc = ascat.aspcf(ascat.bc, seed=42)
  
  #Plot the segmented data
  ascat.plotSegmentedData(ascat.bc)
  
  #Run ASCAT to fit every tumor to a model, inferring ploidy, normal cell contamination, and discrete copy numbers
  #If psi and rho are manually set:
  if (!is.null(NULL) && !is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL, psi_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, rho_manual=NULL)
  } else if(!is.null(NULL) && is.null(NULL)){
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1, psi_manual=NULL)
  } else {
      ascat.output <- ascat.runAscat(ascat.bc, gamma=1)
  }
  
  #Extract metrics from ASCAT profiles
  QC = ascat.metrics(ascat.bc,ascat.output)
  
  #Write out segmented regions (including regions with one copy of each allele)
  write.table(ascat.output[[""segments""]], file=paste0(""OO99_tumor_vs_OO99_normal"", "".segments.txt""), sep=""	"", quote=F, row.names=F)
  
  #Write out CNVs in bed format
  cnvs=ascat.output[[""segments""]][2:6]
  write.table(cnvs, file=paste0(""OO99_tumor_vs_OO99_normal"","".cnvs.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  #Write out purity and ploidy info
  summary <- tryCatch({
          matrix(c(ascat.output[[""aberrantcellfraction""]], ascat.output[[""ploidy""]]), ncol=2, byrow=TRUE)}, error = function(err) {
              # error handler picks up where error was generated
              print(paste(""Could not find optimal solution:  "",err))
              return(matrix(c(0,0),nrow=1,ncol=2,byrow = TRUE))
      }
  )
  colnames(summary) <- c(""AberrantCellFraction"",""Ploidy"")
  write.table(summary, file=paste0(""OO99_tumor_vs_OO99_normal"","".purityploidy.txt""), sep=""	"", quote=F, row.names=F, col.names=T)
  
  write.table(QC, file=paste0(""OO99_tumor_vs_OO99_normal"", "".metrics.txt""), sep=""	"", quote=F, row.names=F)
  
  # version export
  f <- file(""versions.yml"",""w"")
  alleleCounter_version = system(paste(""alleleCounter --version""), intern = T)
  ascat_version = sessionInfo()$otherPkgs$ASCAT$Version
  writeLines(paste0('""', ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT"", '""', "":""), f)
  writeLines(paste(""    alleleCounter:"", alleleCounter_version), f)
  writeLines(paste(""    ascat:"", ascat_version), f)
  close(f)

Command exit status:
  1

Command output:
  (empty)

Command error:
  Loading required package: GenomicRanges
  Loading required package: stats4
  Loading required package: BiocGenerics
  
  Attaching package: ‘BiocGenerics’
  
  The following objects are masked from ‘package:stats’:
  
      IQR, mad, sd, var, xtabs
  
  The following objects are masked from ‘package:base’:
  
      anyDuplicated, aperm, append, as.data.frame, basename, cbind,
      colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,
      get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,
      match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,
      Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,
      table, tapply, union, unique, unsplit, which.max, which.min
  
  Loading required package: S4Vectors
  
  Attaching package: ‘S4Vectors’
  
  The following objects are masked from ‘package:data.table’:
  
      first, second
  
  The following objects are masked from ‘package:base’:
  
      expand.grid, I, unname
  
  Loading required package: IRanges
  
  Attaching package: ‘IRanges’
  
  The following object is masked from ‘package:data.table’:
  
      shift
  
  Loading required package: GenomeInfoDb
  Loading required package: parallel
  Loading required package: doParallel
  Loading required package: foreach
  Loading required package: iterators
  Warning message:
  package ‘ASCAT’ was built under R version 4.2.3 
  Error in readAlleleCountFiles(tumourAlleleCountsFile.prefix, "".txt"", chrom_names,  : 
    length(files) > 0 is not TRUE
  Calls: ascat.prepareHTS ... ascat.getBAFsAndLogRs -> readAlleleCountFiles -> stopifnot
  Execution halted

Work dir:
  /group/poetsch_projects/Projects/poetsch_sc/nf-core_run/work/5d/0f4ea23bcdcec31c68e1dcef8fb1fa

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line
```


### Relevant files

_No response_

### System information

_No response_",TuongViDang,https://github.com/nf-core/sarek/issues/1323
I_kwDOCvwIC852q5Ae,Misleading info in error message concerning invalid path to VEP-cache,OPEN,2023-11-13T15:58:24Z,2024-08-14T02:28:21Z,,"### Description of the bug

Observed on Sarek 3.3.2 (and on dev 51c7676c0947849d191359bf2c66bf32b10dc88b) :

The error message for invalid VEP-cache contains some seeming misleading info about invalid sample-sheet.
```
The sample-sheet only contains tumor-samples, but the following tools, which were requested by the option ""tools"", expect at least one normal-sample : ascat
The sample-sheet only contains normal-samples, but the following tools, which were requested with ""--tools"", expect at least one tumor-sample : ascat, controlfreec, mutect2
Path provided with VEP cache is invalid.
Make sure there is a directory named homo_sapiens/110_GRCh38 in /home/ubuntu/dev/./nPlease refer to https://nf-co.re/sarek/docs/usage/#how-to-customise-snpeff-and-vep-annotation for more information.
```
My guess is that the lines about the invalid sample-sheet comes from nf-validation.

### Command used and terminal output

```console
nextflow run main.nf -profile test_full,docker --vep_cache /some/invalid/path --outdir foo
```


### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1326
I_kwDOCvwIC852zHCq,haplotypecaller output unusual behavior without dbsnps/known-sites,OPEN,2023-11-14T16:31:01Z,2023-11-14T16:31:01Z,,"### Description of the bug

When running haplotypecaller without a dbsnps or known-sites parameter (e.g., with non-human organisms) two unusual behaviors occur with the output:

1.  If not ran with --skip_step haplotypecaller_filter, the haplotypecaller vcf files are not carried downstream into any further steps like multiqc.
2. If ran with --skip_step haplotypecaller_filter the outputs are carried into multiqc, but are not ran with add_info_to_vcf.  

The problem appears to be in the VCF_VARIANT_FILTERING_GATK subworkflow and drops the haplotypecaller output after attempting to call FILTERVARIANTTRANCHES

### Command used and terminal output

```console
#1 nextflow run /mnt/storage/data/nf-core-sarek_3.3.2/3_3_2/main.nf --input /mnt/storage/data/samplesheet.csv --outdir /mnt/storage/data/results --fasta /mnt/storage/data/reference/mRatBN7.fa --fasta_fai /mnt/storage/data/reference/mRatBN7.fa.fai --igenomes_ignore --skip_tools baserecalibrator -profile docker --tools deepvariant,freebayes,haplotypecaller

#2 nohup nextflow run /mnt/storage/data/nf-core-sarek_3.3.2/3_3_2/main.nf --input /mnt/storage/data/samplesheet.csv --outdir /mnt/storage/data/results --fasta /mnt/storage/data/reference/mRatBN7.fa --fasta_fai /mnt/storage/data/reference/mRatBN7.fa.fai --igenomes_ignore --skip_tools baserecalibrator,haplotypecaller_filter -profile docker --tools deepvariant,freebayes,haplotypecaller
```


### Relevant files

_No response_

### System information

nextflow v23.10.0.5889
Cloud (32cpu, 512Gb RAM)
Executor (local)
Container: Docker
OS: Ubuntu
Sarek: 3.3.2",AJTDaedalus,https://github.com/nf-core/sarek/issues/1329
I_kwDOCvwIC8526Bha,Add nf-test scatter CI testing,OPEN,2023-11-15T14:44:19Z,2023-11-15T14:44:19Z,,,edmundmiller,https://github.com/nf-core/sarek/issues/1331
I_kwDOCvwIC8526tEj,Java applications request too much memory,OPEN,2023-11-15T16:15:51Z,2024-07-09T18:21:27Z,,"### Description of the bug

The sarek pipeline repeatedly errors out at the GATK MarkDuplicates step with the message:

""Error occurred during initialization of VM
  Could not reserve enough space for 50331648KB object heap""

Sometimes an error message for ""FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM"" also appears.

With help from our HPC support team, I have tried:

- Adding “NXF_OPTS='-Xms1g -Xmx4g'” to my .bashrc as per https://nf-co.re/sarek/3.3.2/docs/usage
- Editing the custom config file to include combinations of:
--	clusterOptions = {""-l h_vmem=${(task.memory + 4.GB).bytes/task.cpus}""}
--	memory = 32.GB
--	memory = 10000
--	ext.args = ""-Xmx8000M""
--	ext.args = ""-Xmx8g""

In all cases, GATK still tries to request a larger heap size, so the custom settings don't seem to be getting passed to the program. A member of the HPC support team suggested the issue might be solvable by adding 'ext.args = ""-Xmx8g""' in the module-level config files rather than the user-level config file.

### Command used and terminal output

```console
nextflow run nf-core/sarek \
--genome null \
--igenomes_ignore \
--fasta ${ref} \
--bwamem2 ${basedir}refs/ \
--dict ${basedir}refs/VectorBase-65_AfunestusFUMOZ_Genome.dict \
--fasta_fai ${basedir}refs/VectorBase-65_AfunestusFUMOZ_Genome.fasta.fai \
--input ${basedir}metadata/sarek_test_metadata.csv \
--outdir . \
--joint_germline \
--skip_tools baserecalibrator \
--tools haplotypecaller \
-profile sge,singularity \
-c ${basedir}custom_resources_w_markduplicates.conf \
-resume

ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (Ken4590)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (Ken4590)` terminated with an error exit status (1)

Command executed:

  gatk --java-options ""-Xmx49152M -XX:-UsePerfData"" \
      MarkDuplicates \
      --INPUT Ken4590-1.0012.bam --INPUT Ken4590-1.0005.bam --INPUT Ken4590-1.0003.bam --INPUT Ken4590-1.0009.bam --INPUT Ken4590-1.0004.bam --INPUT Ken4590-1.0002.bam --INPUT Ken4590-1.0001.bam --INPUT Ken4590-1.0011.bam --INPUT Ken4590-1.0008.bam --INPUT Ken4590-1.0007.bam --INPUT Ken4590-1.0010.bam --INPUT Ken4590-1.0006.bam \
      --OUTPUT Ken4590.md.bam \
      --METRICS_FILE Ken4590.md.cram.metrics \
      --TMP_DIR . \      
--REFERENCE_SEQUENCE VectorBase-65_AfunestusFUMOZ_Genome.fasta \
      -REMOVE_DUPLICATES false -VALIDATION_STRINGENCY LENIENT

  # If cram files are wished as output, the run samtools for conversion
  if [[ Ken4590.md.cram == *.cram ]]; then
      samtools view -Ch -T VectorBase-65_AfunestusFUMOZ_Genome.fasta -o Ken4590.md.cram Ken4590.md.bam
      rm Ken4590.md.bam
      samtools index Ken4590.md.cram
  fi

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
      samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
  END_VERSIONS

Command exit status:
  1

Command output:
  Error occurred during initialization of VM
  Could not reserve enough space for 50331648KB object heap

Command error:
  Using GATK jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx49152M -XX:-UsePerfData -jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar MarkDuplicates --INPUT Ken4590-1.0012.bam --INPUT Ken4590-1.0005.bam --INPUT Ken4590-1.0003.bam --INPUT Ken4590-1.0009.bam --INPUT Ken4590-1.0004.bam --INPUT Ken4590-1.0002.bam --INPUT Ken4590-1.0001.bam --INPUT Ken4590-1.0011.bam --INPUT Ken4590-1.0008.bam --INPUT Ken4590-1.0007.bam --INPUT Ken4590-1.0010.bam --INPUT Ken4590-1.0006.bam --OUTPUT Ken4590.md.bam --METRICS_FILE Ken4590.md.cram.metrics --TMP_DIR . 
--REFERENCE_SEQUENCE VectorBase-65_AfunestusFUMOZ_Genome.fasta -REMOVE_DUPLICATES false -VALIDATION_STRINGENCY LENIENT

Work dir:
  <path>/response/test/work/2f/47a0d3b493240c590a3540d1a88bd5

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

 -- Check '.nextflow.log' file for details
```


### Relevant files

[nextflow.log.zip](https://github.com/nf-core/sarek/files/13366913/nextflow.log.zip)
[sarek_test.zip](https://github.com/nf-core/sarek/files/13366914/sarek_test.zip)


### System information

Nextflow version 23.04.1,  nf-core/sarek v3.3.2-gf034b73
HPC + singularity on SGE
CentOS
",rrlove-cdc,https://github.com/nf-core/sarek/issues/1332
I_kwDOCvwIC853F5KG,Annotations for custom genome,CLOSED,2023-11-17T00:37:21Z,2023-12-04T09:40:35Z,2023-12-04T09:40:35Z,"Is there a way, we can skip annotations step since I have a custom genome ?",rakesh-res,https://github.com/nf-core/sarek/issues/1336
I_kwDOCvwIC853TVlU,Validate that only vcf.gz(.tbi) are provided,OPEN,2023-11-20T07:42:15Z,2024-01-17T10:41:11Z,,"### Description of feature

Currently we expect users to provide any reference vcfs as vcf.gz(tbi), but we don't validate for it in the schema.",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1337
I_kwDOCvwIC854GL4q,sarek on Kubernetes documentation,CLOSED,2023-11-28T16:57:16Z,2023-12-04T09:45:09Z,2023-12-04T09:45:08Z,"### Sarek on Kubernetes

I am sorry, this might not be a feature request, rather a general question but I couldn't find where to ask it except here.
I am wondering if there is any documentation on how to run Sarek on a Kubernetes cluster, I am especially concerned about the best configuration to adopt for permanent storage when the pipeline runs, if there is any guidance already published online I would be very grateful.
Cheers,
Rad
",radaniba,https://github.com/nf-core/sarek/issues/1341
I_kwDOCvwIC854Lcc-,VEP on Strelka indel output,CLOSED,2023-11-29T10:24:07Z,2024-01-17T10:40:58Z,2024-01-17T10:40:58Z,"### Description of feature

Does the pipeline run VEP on both the SNV and indel output from Strelka?

If it does, it would be awesome to know where the results are located.
",frenkiboy,https://github.com/nf-core/sarek/issues/1342
I_kwDOCvwIC854bgiA,"No parameter for ""penalty"" in ASCAT segmentation",OPEN,2023-12-01T09:33:04Z,2023-12-01T09:50:16Z,,"### Description of feature

Hi, I found that no ""penalty"" parameter has been set for ASCAT segmentation (ASPCF) by sarek. From the [main.nf](http://main.nf/) I see:
```
    #Segment the data
    ascat.bc = ascat.aspcf(ascat.bc)
```
Best, 
Giulia 	",MGCarta,https://github.com/nf-core/sarek/issues/1346
I_kwDOCvwIC855HaUl,Error while FASTP is running,CLOSED,2023-12-08T06:07:09Z,2023-12-14T00:29:41Z,2023-12-14T00:29:41Z,"Hi,
I ran sarek for WGS data with hg19 genome, but it reports error when sarek perform FASTP.
Here is my command line and terminal output.

### Command line
```shell
nextflow run nf-core/sarek \
--input /BiO/home/pmc09/pipeline_test/WGS_germline/00.script/samplesheet_1.csv \
--outdir /BiO/home/pmc09/pipeline_test/WGS_germline/02.results/NA12877/hg19 \
--genome hg19 \
--dbsnp ""/BiO/Reference/dbsnp_b151_37p13.vcf.gz"" \
-profile docker \
-bg
``` 
### Terminal output
```
N E X T F L O W  ~  version 23.10.0
Launching `https://github.com/nf-core/sarek` [shrivelled_lalande] DSL2 - revision: 6aeac929c9 [master]


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .´ _  `.
   /  |\`-_ \      __        __   ___
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /¯¯\ |  \ |___ |  \
    `|____\´

  nf-core/sarek v3.4.0-g6aeac92
------------------------------------------------------
Core Nextflow options
  revision             : master
  runName              : shrivelled_lalande
  containerEngine      : docker
  launchDir            : /BiO/home/pmc09/pipeline_test/WGS_germline/00.script
  workDir              : /BiO/home/pmc09/pipeline_test/WGS_germline/00.script/work
  projectDir           : /BiO/home/pmc09/.nextflow/assets/nf-core/sarek
  userName             : pmc09
  profile              : docker
  configFiles          :

Input/output options
  input                : /BiO/home/pmc09/pipeline_test/WGS_germline/00.script/samplesheet_1.csv
  outdir               : /BiO/home/pmc09/pipeline_test/WGS_germline/02.results/NA12877/hg19

Reference genome options
  genome               : hg19
  bwa                  : s3://ngi-igenomes/igenomes//Homo_sapiens/UCSC/hg19/Sequence/BWAIndex/version0.6.0/
  dbsnp                : /BiO/Reference/dbsnp_b151_37p13.vcf.gz
  fasta                : s3://ngi-igenomes/igenomes//Homo_sapiens/UCSC/hg19/Sequence/WholeGenomeFasta/genome.fa
  snpeff_db            : 87
  snpeff_genome        : GRCh37
  vep_genome           : GRCh37
  vep_species          : homo_sapiens
  vep_cache_version    : 110

Generic options
  validationLenientMode: true

!! Only displaying parameters that differ from the pipeline defaults !!
------------------------------------------------------
If you use nf-core/sarek for your analysis please cite:

* The pipeline
  https://doi.org/10.12688/f1000research.16665.2
  https://doi.org/10.5281/zenodo.3476425

* The nf-core framework
  https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md

WARN: There's no process matching config selector: NFCORE_SAREK:SAREK:CRAM_QC_NO_MD:SAMTOOLS_STATS -- Did you mean: NFCORE_SAREK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS?
WARN: There's no process matching config selector: .*BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP
WARN: There's no process matching config selector: .*BAM_NGSCHECKMATE:NGSCHECKMATE_NCM
WARN: There's no process matching config selector: .*:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES
[b7/ad2f2e] Submitted process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP (dbsnp_b151_37p13.vcf)
[3d/a799ce] Submitted process > NFCORE_SAREK:SAREK:FASTP (NA12877-1)
[57/5ae9d6] Submitted process > NFCORE_SAREK:SAREK:FASTQC (NA12877-1)
Staging foreign file: s3://ngi-igenomes/igenomes/Homo_sapiens/UCSC/hg19/Sequence/WholeGenomeFasta/genome.fa
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:FASTP (NA12877-1)'

Caused by:
  Process `NFCORE_SAREK:SAREK:FASTP (NA12877-1)` terminated with an error exit status (255)

Command executed:

  [ ! -f  NA12877-1_1.fastq.gz ] && ln -sf ERR194146_1.fastq.gz NA12877-1_1.fastq.gz
  [ ! -f  NA12877-1_2.fastq.gz ] && ln -sf ERR194146_2.fastq.gz NA12877-1_2.fastq.gz
  fastp \
      --in1 NA12877-1_1.fastq.gz \
      --in2 NA12877-1_2.fastq.gz \
      --out1 NA12877-1_1.fastp.fastq.gz \
      --out2 NA12877-1_2.fastp.fastq.gz \
      --json NA12877-1.fastp.json \
      --html NA12877-1.fastp.html \
       \
       \
       \
      --thread 12 \
      --detect_adapter_for_pe \
      --disable_adapter_trimming      --split_by_lines 200000000 \
      2> NA12877-1.fastp.log

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:FASTP"":
      fastp: $(fastp --version 2>&1 | sed -e ""s/fastp //g"")
  END_VERSIONS

Command exit status:
  255

Command output:
  (empty)

Work dir:
  /BiO/home/pmc09/pipeline_test/WGS_germline/00.script/work/3d/a799cee87395a369afdf75e5c9d362
Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

 -- Check '.nextflow.log' file for details
Execution cancelled -- Finishing pending tasks before exit
``` ",cuppacuppa,https://github.com/nf-core/sarek/issues/1347
I_kwDOCvwIC855Jo-_,Unpatch dragmap modules,OPEN,2023-12-08T12:27:06Z,2023-12-08T12:27:06Z,,"              I think we don't need to patch anymore and it should work now with the latest version. The rebuild conda recipe fixed it.

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1345#discussion_r1420339658_
            ",maxulysse,https://github.com/nf-core/sarek/issues/1348
I_kwDOCvwIC855JpPH,Fix nf-tests coming from nf-core/modules,OPEN,2023-12-08T12:27:52Z,2023-12-08T12:27:52Z,,"              the nf-test is not working, I'll fix all the nf-tests in a separate PR

_Originally posted by @maxulysse in https://github.com/nf-core/sarek/pull/1345#discussion_r1420368234_
            ",maxulysse,https://github.com/nf-core/sarek/issues/1349
I_kwDOCvwIC855LF-m,Strelka malformed VCF header,CLOSED,2023-12-08T16:14:20Z,2023-12-14T14:54:28Z,2023-12-14T14:54:28Z,"### Description of the bug

I have been trying to run Strelka using Sarek. The initial steps (FASTQC, FASTP) seem to run successfully, as well as the actual calling. Unfortunately, it looks like the downstream step of merging Strelka's VCFs into one single VCF causes the pipeline to crash. 

```
Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA (S-9998)` terminated with an error exit status (3)

Command executed:

  gatk --java-options ""-Xmx3276M -XX:-UsePerfData"" \
      MergeVcfs \
      --INPUT S-9998.strelka.chr20_36314720-64334167.variants.vcf.gz --INPUT S-9998.strelka.chr2_16146120-32867130.variants.vcf.gz --INPUT S-9998.strelka.chr6_95070791-167591393.variants.vcf.gz --INPUT S-9998.strelka.chr4_10001-1429358
.variants.vcf.gz --INPUT S-9998.strelka.chr5_139453660-155760324.variants.vcf.gz --INPUT S-9998.strelka.chr11_51078349-54425074.variants.vcf.gz --INPUT S-9998.strelka.chr15_20729747-21193490.variants.vcf.gz --INPUT S-9998.strelka.chr18
_47019913-54536574.variants.vcf.gz --INPUT S-9998.strelka.chr12_37235253-37240944.variants.vcf.gz --INPUT S-9998.strelka.chr1_122026460-124977944.variants.vcf.gz --INPUT S-9998.strelka.chr17_60001-448188.variants.vcf.gz --INPUT S-9998.
strelka.chr13_86252980-111703855.variants.vcf.gz --INPUT S-9998.strelka.chrX_37285838-49348394.variants.vcf.gz --INPUT S-9998.strelka.chr8_44033745-45877265.variants.vcf.gz --INPUT S-9998.strelka.chr4_190173122-190204555.variants.vcf.g
z --INPUT S-9998.strelka.chrY_9055175-9057608.variants.vcf.gz --INPUT S-9998.strelka.chr1_10001-207666.variants.vcf.gz --INPUT S-9998.strelka.chr7_58169654-60828234.variants.vcf.gz --INPUT S-9998.strelka.chr9_41229379-41237752.variants
.vcf.gz --INPUT S-9998.strelka.chr10_39590436-39593013.variants.vcf.gz --INPUT S-9998.strelka.chr2_238904048-242183529.variants.vcf.gz \
      --OUTPUT S-9998.strelka.variants.vcf.gz \
      --SEQUENCE_DICTIONARY Homo_sapiens_assembly38.dict \
      --TMP_DIR . \

 htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file, for input sour
ce: file://S-9998.strelka.chr4_190173122-190204555.variants.vcf.gz
        at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:264)
        at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:103)
        at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:128)
        at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:121)
        at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:81)
        at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:145)
        at picard.vcf.MergeVcfs.doWork(MergeVcfs.java:182)
        at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:289)
        at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37)
        at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160)
        at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203)
        at org.broadinstitute.hellbender.Main.main(Main.java:289)
  Caused by: htsjdk.tribble.TribbleException$InvalidHeader: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file
        at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:115)
        at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:79)
        at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:37)
        at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:262)
        ... 11 more
```

Checking one of the VCF files' header, seems like there is an issue indeed. There is no #CHROM line, and the last ##contig line is merged with the first variant:
```
##contig=<ID=chrUn_JTFH01000515v1_decoy,length=1548>
##contig=<ID=chrUn_JTFH01000516v1_decoy,length=1546>
##contig=<ID=chrUn_JTFH01000517v1_decoy,length=1541>
##contig=<ID=chrUn_JTFH01000518v1_decoy,length=1536>
##contig=<ID=chrUn_JTFH01000519v1_decoy,length=1533>
##contig=<ID=chrUn_JTFH0100chr4 190174313       .       CGCCCACACCGGCGCGT       C       21      LowDepth;NoPassedVariantGTs     CIGAR=1M16D;RU=GCCCACACCGGCGCGT;REFREP=1;IDREP=0;MQ=1   GT:GQ:GQX:DPI:AD:ADF:ADR:FT:PL  0/1:64:9:2:3,1:3,1:
0,0:LowDepth:61,0,240
chr4    190174547       .       G       C       3       LowGQX;NoPassedVariantGTs       SNVHPOL=2;MQ=3  GT:GQ:GQX:DP:DPF:AD:ADF:ADR:SB:FT:PL    0/1:32:0:12:0:7,5:7,5:0,0:0.0:LowGQX:34,0,52
chr4    190175118       .       G       T       3       LowGQX;LowDepth;NoPassedVariantGTs      SNVHPOL=2;MQ=3  GT:GQ:GQX:DP:DPF:AD:ADF:ADR:SB:FT:PL    0/1:22:0:2:0:1,1:0,1:1,0:0.0:LowGQX;LowDepth:35,0,20
chr4    190175160       .       C       T       47      LowGQX;LowDepth;NoPassedVariantGTs      SNVHPOL=4;MQ=8  GT:GQ:GQX:DP:DPF:AD:ADF:ADR:SB:FT:PL    1/1:5:1:2:0:0,2:0,1:0,1:-9.2:LowGQX;LowDepth:83,6,0
chr4    190175166       .       G       C       48      LowGQX;LowDepth;NoPassedVariantGTs      SNVHPOL=2;MQ=9  GT:GQ:GQX:DP:DPF:AD:ADF:ADR:SB:FT:PL    1/1:5:1:2:0:0,2:0,1:0,1:-9.4:LowGQX;LowDepth:84,6,0
```

Is this a known issue?
Thanks

### Command used and terminal output

```console
nextflow run ../../.NEXTFLOW/nf-core-sarek_3.4.0/3_4_0/ -profile singularity,cluster --input sampleSheet.csv --outdir results/  --tools strelka
```


### Relevant files

_No response_

### System information

Nextflow v23.04.3.5875
Ran on HPC with Slurm and Singularity
Sarek v3.4.0",gitMakeCoffee,https://github.com/nf-core/sarek/issues/1352
I_kwDOCvwIC855ebUa,Allow for snpEff db without versions,OPEN,2023-12-12T15:31:53Z,2023-12-12T15:31:53Z,,"### Description of feature

Currently in Sarek we suppose that the database is build following the format `genome.db`, like `GRCh37.75`, `GRCh38.105`, `GRCh38.99`, and so on...

But actually there are lots of databases with only `genome` like `hg19`, `hg19kg`, `hg38`, `hg38kg`, `testHg19ChrM` for `Homo_Sapiens`.

Or for other organisms, only the former could be available, such as `Oryza_sativa`",maxulysse,https://github.com/nf-core/sarek/issues/1354
I_kwDOCvwIC855eer-,Allow to specify a local config file for snpEff,OPEN,2023-12-12T15:38:00Z,2023-12-12T15:38:00Z,,"### Description of feature

This would allow for usage of locally build genomes",maxulysse,https://github.com/nf-core/sarek/issues/1355
I_kwDOCvwIC855im32,ngscheckmate_bed is set incorrectly for GATK.GRCh37 ,OPEN,2023-12-13T07:26:34Z,2023-12-13T07:26:46Z,,"### Description of the bug

ngscheckmate_bed is set incorrectly for GATK.GRCh37 - this genome has no ""chr"".

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",SPPearce,https://github.com/nf-core/sarek/issues/1356
I_kwDOCvwIC8550atS,Large discrepancy between number of reads in the same sample but in different lanes causes a sample to get dropped,CLOSED,2023-12-15T14:02:02Z,2024-01-17T10:36:16Z,2024-01-17T10:36:15Z,"### Description of the bug

When analysing a sample that has been sequenced on multiple lanes, if there is a large discrepancy between samples the grouping key will be waiting for `n lanes * n split reads` for a sample, which may never occur. 

Here's an example:
- sample1, lane 1: 10 million reads  
- sample1, lane 2: 1 million reads

- split into 1 million read chunks by FASTP
- the groupKey [here](https://github.com/nf-core/sarek/blob/3.4.0/workflows/sarek.nf#L494-L506) calculates there should be 2 * 10 chunks for a total of 20
- Only 12 ever appear, so the sample is dropped out of the pipeline.
- The pipeline green ticks because nothing 'fails'


I've attached a samplesheet using the test data from profile `test` and `test_full` which recreates the problem.

Setting splitFastq to size 0 is a workaround for this problem.


### Command used and terminal output

```console
nextflow run nf-core/sarek -r v3.4.0 --splitFastq 5000000 --input sarek-uneven-size-error.csv --outdir results/
```


### Relevant files

[sarek-uneven-size-error.csv](https://github.com/nf-core/sarek/files/13685979/sarek-uneven-size-error.csv)

### System information

All",adamrtalbot,https://github.com/nf-core/sarek/issues/1357
I_kwDOCvwIC856Ja0b,Increase run time,CLOSED,2023-12-19T19:15:10Z,2024-01-17T15:07:57Z,2024-01-17T15:07:57Z,"### Description of the bug

I have large FASTQ files I would like to process with Sarek. Unfortunately, most tasks fail due to insufficient time for most tasks. The cluster I'm using runs Slurm.
I tried overriding Sarek's time allocation config with the following config file:
```
nextflow.enable.dsl=2

process {
        time = '168h'
}
```

Which I've passed using ```-c moretime.config```.

However, this is not taken into account by Sarek, which still allocates time according to its default config.

Any idea how to override time allocation?
How can I make it as default for all Nextflow runs, regardless of the pipeline?
Is the first DLS2 line mandatory?
Thank you.

### Command used and terminal output

```console
nextflow run ../../.NEXTFLOW/nf-core-sarek_3.4.0/3_4_0/ -profile singularity,cluster -c moretime.config --input fastqSamplesheet.csv --outdir result/ --tools strelka,mutect2,freebayes,mpileup -with-report report.html -with-timeline timeline.html -with-dag dag.png --wes --intervals exons.bed --concatenate_vcfs
```


### Relevant files

_No response_

### System information

_No response_",gitMakeCoffee,https://github.com/nf-core/sarek/issues/1361
I_kwDOCvwIC856Mm7H,Check docker.runOptions for spark,OPEN,2023-12-20T08:59:04Z,2023-12-20T08:59:04Z,,"Check `docker.runOptions = '-u $(id -u):$(id -g)'` for spark

>  I am getting anxious about any spark related issues :D

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1360#discussion_r1432427884_",maxulysse,https://github.com/nf-core/sarek/issues/1362
I_kwDOCvwIC856Pr4x,Minimum split_fastq value is 250,CLOSED,2023-12-20T16:50:26Z,2024-01-17T10:38:35Z,2024-01-17T10:38:34Z,"### Description of the bug

The minimum value for split_fastq is 250 because this the smallest value accepted by FASTP is 1000 but this isn't reflected in the schema. Discovered while trying to fix #1357.

This could be addressed in the schema.

### Command used and terminal output

```console
nextflow run . -profile test_cache,docker --outdir results --split_fastq 249 --test_data_base ~/test-datasets


ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:FASTP (test-test_L2)'

Caused by:
  Process `NFCORE_SAREK:SAREK:FASTP (test-test_L2)` terminated with an error exit status (255)

Command executed:

  [ ! -f  test-test_L2_1.fastq.gz ] && ln -sf test_1.fastq.gz test-test_L2_1.fastq.gz
  [ ! -f  test-test_L2_2.fastq.gz ] && ln -sf test_2.fastq.gz test-test_L2_2.fastq.gz
  fastp \
      --in1 test-test_L2_1.fastq.gz \
      --in2 test-test_L2_2.fastq.gz \
      --out1 test-test_L2_1.fastp.fastq.gz \
      --out2 test-test_L2_2.fastp.fastq.gz \
      --json test-test_L2.fastp.json \
      --html test-test_L2.fastp.html \
       \
       \
       \
      --thread 2 \
      --detect_adapter_for_pe \
      --disable_adapter_trimming      --split_by_lines 996 \
      2> >(tee test-test_L2.fastp.log >&2)
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:FASTP"":
      fastp: $(fastp --version 2>&1 | sed -e ""s/fastp //g"")
  END_VERSIONS

Command exit status:
  255

Command output:
  (empty)

Command error:
  ERROR: you have enabled splitting output by file lines, the file lines (--split_by_lines) should be >= 1000.
```
```


### Relevant files

_No response_

### System information

_No response_",adamrtalbot,https://github.com/nf-core/sarek/issues/1363
I_kwDOCvwIC856osK7,NGSCHECKMATE_NCM,OPEN,2023-12-27T15:12:56Z,2024-01-10T12:00:46Z,,"### Description of feature

I noticed that nf-core/sarek has added a new step, CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM. I'm curious about how to use it as I couldn't find any documentation or shortcuts. Is this step intended to be used solely in conjunction with bcftools mileup?""",lucianhu,https://github.com/nf-core/sarek/issues/1366
I_kwDOCvwIC856tW8b,sambamba/markdup,OPEN,2023-12-28T18:24:02Z,2024-01-10T12:57:36Z,,"### Description of feature

Use sambamba as a gatk-markdup alternative to increase processing time

https://lomereiter.github.io/sambamba/docs/sambamba-markdup.html",stavgrossfeld,https://github.com/nf-core/sarek/issues/1367
I_kwDOCvwIC857ADrX,Mutect2 error with Sarek 3.4.0,OPEN,2024-01-03T09:35:07Z,2024-01-17T10:57:42Z,,"### Description of the bug

After 5h56min of runtime, Sarek's pipeline gave me an error telling me that it cannot find typical linux commands such as sed and cat. I checked that those commands were installed on the HPC where I am launching the tool and they are. Also, Mutect2 with Sarek worked for me for another sample using the same command line. 

For the same sample had given me a different error before, I tried to solve it by increasing the RAM memory (although the error did not say anything about that, but a problem writing a file) and now I got the error I am talking about.

Any ideas?

### Command used and terminal output

```console
Command:

#!/bin/bash
#SBATCH -c 16
#SBATCH -t 08:00:00
#SBATCH --mem=40G
#SBATCH --mail-type=begin,end,fail
#SBATCH --mail-user=xxx@xxx.xxx

# $1 sample ID [e.g. P01M2]
# sbatch -J P01M2_mutect2 launch_mutect2.sh P01M2

module load cesga/2020 nextflow/23.04.2
NXF_SINGULARITY_CACHEDIR=/mnt/lustre/scratch/nlsas/home/usc/mg/translational_oncology/5_tmp/singularity_cache
NXF_WORK=/mnt/lustre/scratch/nlsas/home/usc/mg/translational_oncology/5_tmp/nextflow_cache

outdir=/mnt/lustre/scratch/nlsas/home/usc/mg/translational_oncology/2_projects/7_ENDEVO/noalt
samplesheet=/mnt/lustre/scratch/nlsas/home/usc/mg/translational_oncology/2_projects/7_ENDEVO/2_data/samplesheets/samplesheet_$1.csv
reference=/mnt/lustre/scratch/nlsas/home/usc/mg/translational_oncology/0_reference

nextflow run nf-core/sarek -r 3.4.0 \
    -profile singularity \
    --step variant_calling \
    --max_cpus 16 \
    --wes \
    --input $samplesheet \
    --fasta ${reference}/2_GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \
    --fasta_fai ${reference}/2_GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai \
    --dict ${reference}/2_GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.dict \
    --intervals ${reference}/2_GRCh38/SureSelect_v6cosmic_macrogen_short.bed  \
    --tools mutect2 \
    --known_snp ${reference}/4_others/dbsnp_146.hg38.vcf.gz \
    --known_snp ${reference}/4_others/dbsnp_146.hg38.vcf.gz.tbi \
    --outdir $outdir \
    --email xxx@xxx.xxx


Output:
```bash
nf-core/sarek execution completed unsuccessfully!
The exit status of the task that caused the workflow execution to fail was: 1.

The full error message was:

Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GETPILEUPSUMMARIES_TUMOR (P01B2R1)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GETPILEUPSUMMARIES_TUMOR (P01B2R1)` terminated with an error exit status (1)

Command executed:

  gatk --java-options ""-Xmx9830M -XX:-UsePerfData"" \
      GetPileupSummaries \
      --input P01B2R1.converted.cram \
      --variant af-only-gnomad.hg38.vcf.gz \
      --output P01B2R1.mutect2.pileups.table \
      --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \
      --intervals chr1_12081-12251.bed \
      --tmp-dir . \
  
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:GETPILEUPSUMMARIES_TUMOR"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  1

Command output:
  [0.001s][warning][os,container] Duplicate cpuset controllers detected. Picking /sys/fs/cgroup/cpuset, skipping /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/var/singularity/mnt/session/final/sys/fs/cgroup/cpuset.
  Tool returned:
  SUCCESS

Command error:
  23:50:48.892 INFO  ProgressMeter -       chr18:54941098             26.4              53954000        2040779.8
  23:50:58.903 INFO  ProgressMeter -       chr18:79710373             26.6              54207000        2037490.8
  23:51:08.907 INFO  ProgressMeter -        chr19:4267861             26.8              54608000        2039779.8
  23:51:18.965 INFO  ProgressMeter -       chr19:10004118             26.9              55099000        2045313.2
  23:51:29.020 INFO  ProgressMeter -       chr19:14941611             27.1              55535000        2048754.2
  23:51:39.076 INFO  ProgressMeter -       chr19:20646443             27.3              55979000        2052442.5
  23:51:49.090 INFO  ProgressMeter -       chr19:34445005             27.4              56148000        2046117.9
  23:51:59.093 INFO  ProgressMeter -       chr19:40577025             27.6              56661000        2052343.6
  23:52:09.133 INFO  ProgressMeter -       chr19:46675279             27.8              57129000        2056828.7
  23:52:19.173 INFO  ProgressMeter -       chr19:52034473             27.9              57650000        2063156.8
  23:52:29.178 INFO  ProgressMeter -       chr19:57491540             28.1              58072000        2065931.8
  23:52:39.226 INFO  ProgressMeter -       chr20:11923047             28.3              58472000        2067841.2
  23:52:49.268 INFO  ProgressMeter -       chr20:33367038             28.4              58795000        2067029.5
  23:52:59.273 INFO  ProgressMeter -       chr20:45802029             28.6              59229000        2070151.5
  23:53:09.288 INFO  ProgressMeter -       chr20:62546277             28.8              59632000        2072148.0
  23:53:19.300 INFO  ProgressMeter -       chr21:29555308             28.9              59893000        2069219.3
  23:53:29.322 INFO  ProgressMeter -       chr21:43985141             29.1              60248000        2069541.2
  23:53:39.325 INFO  ProgressMeter -       chr22:20564354             29.3              60597000        2069676.9
  23:53:49.335 INFO  ProgressMeter -       chr22:31262697             29.4              61059000        2073640.5
  23:53:59.345 INFO  ProgressMeter -       chr22:41758344             29.6              61499000        2076816.4
  23:54:09.359 INFO  ProgressMeter -         chrX:3320041             29.8              61892000        2078373.9
  23:54:19.384 INFO  ProgressMeter -        chrX:24060153             29.9              62192000        2076795.7
  23:54:29.416 INFO  ProgressMeter -        chrX:44969044             30.1              62474000        2074629.2
  23:54:39.466 INFO  ProgressMeter -        chrX:53984790             30.3              62903000        2077320.6
  23:54:49.498 INFO  ProgressMeter -        chrX:77584251             30.4              63286000        2078492.2
  23:54:59.500 INFO  ProgressMeter -       chrX:103077681             30.6              63591000        2077137.2
  23:55:09.503 INFO  ProgressMeter -       chrX:123888804             30.8              63909000        2076218.0
  23:55:19.507 INFO  ProgressMeter -       chrX:150771813             30.9              64212000        2074822.9
  23:55:25.217 INFO  GetPileupSummaries - 0 read(s) filtered by: MappingQualityAvailableReadFilter 
  5145854 read(s) filtered by: MappingQualityNotZeroReadFilter 
  0 read(s) filtered by: MappedReadFilter 
  3879470 read(s) filtered by: PrimaryLineReadFilter 
  98716086 read(s) filtered by: NotDuplicateReadFilter 
  0 read(s) filtered by: PassesVendorQualityCheckReadFilter 
  0 read(s) filtered by: NonZeroReferenceLengthAlignmentReadFilter 
  495025 read(s) filtered by: MateOnSameContigOrNoMappedMateReadFilter 
  0 read(s) filtered by: GoodCigarReadFilter 
  0 read(s) filtered by: WellformedReadFilter 
  108236435 total reads filtered out of 148946276 reads processed
  23:55:25.217 INFO  ProgressMeter -        chrY:19732575             31.0              64456547        2076339.9
  23:55:25.217 INFO  ProgressMeter - Traversal complete. Processed 64456547 total loci in 31.0 minutes.
  23:55:25.218 INFO  GetPileupSummaries - Shutting down engine
  [January 2, 2024 at 11:55:25 PM GMT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 31.08 minutes.
  Runtime.totalMemory()=1048576000
  Tool returned:
  SUCCESS
  .command.sh: line 12: sed: command not found
  .command.sh: line 12: cat: command not found
  .command.run: line 155: kill: (33) - No such process
  INFO:    Cleaning up image...

Work dir:
  /mnt/lustre/scratch/nlsas/home/usc/mg/translational_oncology/2_projects/7_ENDEVO/1_src/work/83/a77fbdf266bed78ccb2bd97c2957d2

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`
```
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/13817457/nextflow.log)


### System information

- Nextflow version - 23.04.2
- Hardware - HPC
- Executor - slurm
- Container engine - Singularity
- OS - Linux
- Version os nf-core/Sarek - 3.4.0",mimifp,https://github.com/nf-core/sarek/issues/1369
I_kwDOCvwIC857AL-G,Adapting nf-core/sarek for Ion Torrent Single-End Data Analysis,CLOSED,2024-01-03T09:52:23Z,2024-01-10T13:08:40Z,2024-01-10T12:50:01Z,"### Description of the bug


Hello nf-core/sarek team!

I'm attempting to utilize the Sarek pipeline for analyzing **Ion Torrent data**, but I'm facing challenges with configuring **single-end files**. Unlike Illumina, the Ion Torrent data I have comes in the form of a single FASTQ file per sample.

My issue lies in adapting the sample sheet to accommodate these single-end data. According to the documentation, the sample sheet structure is designed for paired-end data, whereas in my case, I only have one FASTQ file per sample.

Here's an example of my data structure:

patient,sample,lane,fastq_1
patient1,test_sample,lane_1,test.fastq.gz

I've created this simplified configuration for my single FASTQ file, but I'm wondering if there's a specific method to correctly adapt the sample sheet for Ion Torrent single-end data.

Additionally, are there specific parameters or configurations within the Sarek pipeline that need adjustment to support this single-end data type from Ion Torrent?

Any help or guidance in correctly adapting the nf-core/sarek configuration for Ion Torrent single-end data would be greatly appreciated!

Thank you very much for your assistance!

Best regards,

Nour-Eddine

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",Sadiki-Noureddine,https://github.com/nf-core/sarek/issues/1370
I_kwDOCvwIC857flTt,Not valid SNP_GRCh38_hg38_wChr.bed on GATK.GRCh38,CLOSED,2024-01-09T08:34:09Z,2024-02-06T15:11:14Z,2024-02-06T15:11:14Z,"### Description of the bug

The pipeline is running correctly when, before to begin variant calling step, the job is aborted. All genome files are downloaded previously and annotation files are downloaded during the pipeline. 

### Command used and terminal output

```console
The command was:

nextflow run './3_4_0/main.nf' -profile singularity --input 1.csv --tools 'strelka,merge' --genome GATK.GRCh38 --igenomes_base ./references --outdir ./aqui --max_cpus 12 --max_memory 28.GB --download_cache -resume elegant_murdock --save_output_as_bam

The sequences files were germinals.

The output was:
executor >  local (123)
[fd/731b21] process > NFCORE_SAREK:SAREK:DOWNLOAD_CACHE_SNPEFF_VEP:ENSEMBLVEP_DOWNLOAD (110_GRCh38)                                 [100%] 1 of 1, cached: 1 ✔
[da/565a31] process > NFCORE_SAREK:SAREK:DOWNLOAD_CACHE_SNPEFF_VEP:SNPEFF_DOWNLOAD (GRCh38.105)                                     [100%] 1 of 1, cached: 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                                           -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                                              -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                                           -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                                              -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP                                                                 -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                                                     -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                                                          -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                                                                   -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_ALLELES                                                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_LOCI                                                                  -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_GC                                                                    -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_RT                                                                    -
[51/f9bb31] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (wgs_calling_regions_noseconds.hg38.bed)            [100%] 1 of 1, cached: 1 ✔
[d4/1b1008] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (chr9_41229379-41237752)                 [100%] 21 of 21, cached: 21 ✔
[8b/f5a63c] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED (wgs_calling_regions_noseconds.hg38)  [100%] 1 of 1, cached: 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP                                                  -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP                                              -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP                                                -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP                                                -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP                                                   -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_UNMAP                                                    -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_MAP                                                      -
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:CAT_FASTQ                                                              -
[4f/c262f1] process > NFCORE_SAREK:SAREK:FASTQC (qG170290127-2)                                                                     [100%] 2 of 2, cached: 2 ✔
[b8/3c4512] process > NFCORE_SAREK:SAREK:FASTP (qG170290127-2)                                                                      [100%] 2 of 2, cached: 2 ✔
[06/33314a] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (qG170290127-2)                       [100%] 24 of 24 ✔
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM                                       -
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN                                     -
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM                                   -
[b0/e4b15c] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (qG170290127)                                      [100%] 2 of 2 ✔
[0d/82bab3] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS (qG170290127)                  [100%] 2 of 2 ✔
[10/6fb2ce] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH (qG170290127)                        [100%] 2 of 2 ✔
[fc/def103] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM (qG170290127)                                                                  [100%] 2 of 2 ✔
[4d/31d01b] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (qG170290127)                                  [100%] 42 of 42 ✔
[26/9cf8b9] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS (qG170290127)                                 [100%] 2 of 2 ✔
[cc/942490] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR (qG170290127)                                                [100%] 42 of 42 ✔
[24/d2ac7e] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:MERGE_CRAM (qG170290127)                           [100%] 2 of 2 ✔
[9f/199c69] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM (qG170290123)                           [100%] 1 of 1
[b6/fc4f28] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS (qG170290123)                                                 [100%] 1 of 1
[1f/020470] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:MOSDEPTH (qG170290123)                                                       [100%] 1 of 1
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM_RECAL                                                                          [  0%] 0 of 1
[-        ] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP                                            -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM                                            -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE         [  0%] 0 of 12
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA          -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME   -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:STRELKA_SINGLE       -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA        -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_SINGLE_STRELKA:MERGE_STRELKA_GENOME -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:STRELKA_SOMATIC        -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_INDELS   -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_STRELKA:MERGE_STRELKA_SNVS     -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS                                                    -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT                                               -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL                                                -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY                                                  -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF                                         -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:TABIX_BGZIPTABIX                                      -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_MERGE:ENSEMBLVEP_VEP                                         -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_MERGE:TABIX_TABIX                                            -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                                                                -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC                                                                                    -
-[nf-core/sarek] Pipeline completed with errors-
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP (1)'

Caused by:
  Not a valid path value: './references/Homo_sapiens/GATK/GRCh38/Annotation/NGSCheckMate/SNP_GRCh38_hg38_wChr.bed'


Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`

 -- Check '.nextflow.log' file for details

There isn't any custom config file.
```


### Relevant files

_No response_

### System information

**System information:**
    Nextflow version: 23.04.3.5875
    Hardware: Desktop
    Executor: bash
    Container engine: Singularity
    OS: Ubuntu Linux 20.04 
    Version of nf-core/sarek: 3.4.0

",jbague,https://github.com/nf-core/sarek/issues/1371
I_kwDOCvwIC858GNka,"improve documentation to point out that when using ASCAT and the Format in the BAM file is chr, one has to change the loci ascat reference file to be also in the chr format",OPEN,2024-01-15T13:07:58Z,2024-01-15T13:07:58Z,,"### Description of the bug

Running Ascat on the dev. branch of the SAREK pipeline in tumor-normal mode with WES data raised an error, because chromosome format in BAM file (e.g. chr1) did not match format of acat loci reference file (e.g. 1). This is mentioned in the ascat documentation (https://github.com/VanLoo-lab/ascat/tree/master/ReferenceFiles/WES), but is should be also made clear in the Sarek documentation. 

### Command used and terminal output

```console
export NXF_OPTS='-Xms1g -Xmx10g'
sudo nextflow run nf-core/sarek \
	-r 3.4.0 \
	--input ./sampleSheetShort.csv \
	--outdir ./results \
	--genome GATK.GRCh38 \
	--tools freebayes,mutect2,strelka,manta,tiddit,cnvkit,controlfreec,ascat,snpeff,vep,merge \
	-profile docker \


##############################Error Output###########################################
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (VUC133_1_
vs_VUC133_0)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_ASCAT:ASCAT (VUC133_1_vs_VUC133_0)` terminated
 with an error exit status (1)

Command executed:

  #!/usr/bin/env Rscript
  library(RColorBrewer)
  library(ASCAT)
  options(bitmapType='cairo')
 
  #build prefixes: <abspath_to_files/prefix_chr>
  allele_path = normalizePath(""battenberg_alleles_on_target_hg38"")
  allele_prefix = paste0(allele_path, ""/"", ""battenberg_alleles_on_target_hg38"", ""_chr"")
 
  loci_path = normalizePath(""battenberg_loci_on_target_hg38"")
  loci_prefix = paste0(loci_path, ""/"", ""battenberg_loci_on_target_hg38"", ""_chr"")
 
  #prepare from BAM files
  ascat.prepareHTS(
      tumourseqfile = ""VUC133_1.recal.cram"",
      normalseqfile = ""VUC133_0.recal.cram"",
      tumourname = paste0(""VUC133_1_vs_VUC133_0"", "".tumour""),
      normalname = paste0(""VUC133_1_vs_VUC133_0"", "".normal""),
      allelecounter_exe = ""alleleCounter"",
      alleles.prefix = allele_prefix,
      loci.prefix = loci_prefix,
      gender = ""XY"",
      genomeVersion = ""hg38"",
      nthreads = 6
      ,minCounts = 10
      ,BED_file = 'Twist_Exome_RefSeq_targets_hg38_c3.bed'
      ,chrom_names = c(1:22, 'X', 'Y')
      ,min_base_qual = 20
      ,min_map_qual = 35
      ,ref.fasta = 'Homo_sapiens_assembly38.fasta'
 
      ,
      seed = 42
  )
	--wes \
	--intervals ""/scratch/rgraus29/projects/wes_emily/emilyDataVsc/Grubb/supporting_files/gatk/Twist_Exome_RefSeq_targets_hg38_c3.bed"" \
	--ascat_alleles ""/localmirror_sata/ascat/correctionFiles/1000G_alleles_hg38/battenberg_alleles_on_target_hg38/battenberg_alleles_on_target_hg38.zip"" \
	--ascat_loci ""/localmirror_sata/ascat/correctionFiles/1000G_loci_hg38/battenberg_loci_on_target_hg38/battenberg_loci_on_target_hg38.zip"" \
	--ascat_loci_gc ""/localmirror_sata/ascat/correctionFiles/GC_G1000_on_target_hg38.zip"" \
	--ascat_loci_rt ""/localmirror_sata/ascat/correctionFiles/RT_G1000_on_target_hg38.zip"" \
	-c nextflow.config \
	-resume
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/13937067/nextflow.log)


### System information

_No response_",rgraus,https://github.com/nf-core/sarek/issues/1375
I_kwDOCvwIC858GZ6o,control-freec throws an error when running WES data in tumor-only mode because parameter minimalsubclonepresence is set,OPEN,2024-01-15T13:37:44Z,2024-01-15T13:37:44Z,,"### Description of the bug

I am running the dev version of Sarek, WES data in tumor-only mode, and I get an error in the control-freec model. Based on another issue (https://github.com/BoevaLab/FREEC/issues/79), I removed this line from the controllfree config.txt: `minimalSubclonePresence = 30 `. This allowed me to run this code `freec -conf config.txt` without any errors. I then created an own config file for controllfree and modified my command to run the pipeline (s. attached files). 
[filesFreecodeError.zip](https://github.com/nf-core/sarek/files/13939414/filesFreecodeError.zip)


### Command used and terminal output

```console
# s. zipped files
```


### Relevant files

s. zipped files

### System information

_No response_",rgraus,https://github.com/nf-core/sarek/issues/1376
I_kwDOCvwIC858U-CY,bwamem2 indexing resources,OPEN,2024-01-17T10:22:55Z,2024-02-13T10:38:20Z,,"### Description of the bug

When running bwamem2 indexing, the workflow fails due to not enough memory being allocated. The following label in the `base.config` solved the issue:

```
withName: 'BWAMEM2_INDEX' {
        cpus            = { check_max( 24 * task.attempt, 'cpus' ) }
        memory          = { check_max( 100.GB * task.attempt, 'memory' ) }
    }
```

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",Matteodigg,https://github.com/nf-core/sarek/issues/1377
I_kwDOCvwIC858fXgu,Run BAM QC after alignment,OPEN,2024-01-18T16:02:51Z,2024-01-18T16:03:07Z,,"### Description of feature

Currently sarek v.3.4.0 runs BAM QC after alignment only if marking of duplicates is skipped. It would be useful to have the flexibility of running BAM QC at any steps: after alignment, after MD, after BQSR or after each of the steps.",Matteodigg,https://github.com/nf-core/sarek/issues/1379
I_kwDOCvwIC859EZvX,Sentieon Refactor,OPEN,2024-01-24T13:46:57Z,2024-12-09T16:39:26Z,,"cf https://github.com/nf-core/modules/pull/5856

```[tasklist]
### Tasks
- [x] ~~Sentieon tests running locally~~
- [x] https://github.com/nf-core/modules/pull/5856
- [x] Sentieon tests running on pipelines (Sarek as an example)
- [ ] Sentieon running on AWS Megatests
```

Been out for 6-8 months, worked nicely for 6 months. Don't know what's in the GitHub secrets.

```[tasklist]
### Modules to be updated
- [x] https://github.com/nf-core/modules/pull/5856
- [ ] applyvarcal
- [ ] https://github.com/nf-core/modules/pull/6027
- [ ] dnamodelapply
- [ ] dnascope
- [ ] gvcftyper
- [ ] haplotyper
- [ ] varcal
- [ ] https://github.com/nf-core/modules/pull/5823
- [ ] https://github.com/nf-core/modules/pull/5945
- [ ] https://github.com/nf-core/modules/pull/6025
```

Checklist for each Module(make a seperate PR for each):

- [ ] Convert to nf-test(pytest-workflow isn't setup)
- [ ] Update the license logic
- [ ] Add @DonFreed as a maintainer in `meta.yml`",maxulysse,https://github.com/nf-core/sarek/issues/1380
I_kwDOCvwIC859IPAF,"Typo in `--three_prime_clip_r{1,2}` parameter documentation ",CLOSED,2024-01-24T23:44:37Z,2024-04-22T10:42:33Z,2024-04-22T10:42:33Z,"### Description of the bug

The documentation for parameter `--three_prime_clip_r{1,2}` refers to a FastP parameter that doesn't exist, it should be `--trim_tail{1,2}`.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",tdanhorn,https://github.com/nf-core/sarek/issues/1382
I_kwDOCvwIC859Sym5,plugin/nf-validation error on HPC,CLOSED,2024-01-26T11:39:06Z,2024-01-31T16:32:37Z,2024-01-31T16:32:37Z,"### Description of the bug

Hi,
when I try to use the pipeline on HPC environment, different errors
about nextflow plugin appeared. This errors is caused because the HPC
environment cannot connect to internet to download these plugins.

I try to modify the file called nextflow.config changing:
    validationFailUnrecognisedParams = false
    validationLenientMode            = false
    validationSchemaIgnoreParams     = 'cf_ploidy,genomes,igenomes_base'
    validationShowHiddenParams       = false
    validate_params                  = false

but the error continue.

My idea was trying to skip this validation step previous to begin the pipeline because
I validated in local previously (I run completely with 1 sample in Linux). 

### Command used and terminal output

```console
The terminal output is:
N E X T F L O W  ~  version 21.10.6
Launching `./3_4_0/main.nf` [happy_yalow] - revision: 48ed053221
Cannot find latest version of nf-validation plugin
```


### Relevant files

_No response_

### System information

    Nextflow version (eg. 21.10.6)
    Hardware (HPC)
    Executor (slurm)
    Container engine: (Singularity)
    Version of nf-core/sarek (3.4.0)

",jbague,https://github.com/nf-core/sarek/issues/1384
I_kwDOCvwIC859uRuA,Manta (germline) terminated with an error exit status (1) ,OPEN,2024-01-31T04:56:07Z,2024-01-31T04:56:07Z,,"### Description of the bug

Hi, thanks for actively maintaining a great user-friendly tool.
While I am running Sarek with tools Manta and Haplotypecaller, I have encountered the below error even after resuming.
I could not get any insight by myself what caused this and how to resolve. Your help will be much appreciated.
I have attached nexflow.log and my parameter file. Please let me know if you need anything else, Thanks.


### Command used and terminal output

```console
run nf-core/sarek -r 3.3.2 \
-profile singularity -work-dir ${MCPAN}/${PREFIX}-pg/aligned/sarek -resume -params-file ${MCPAN}/${PREFIX}-pg/aligned/nf-params.json


Core Nextflow options
  revision             : 3.3.2
  runName              : adoring_booth
  containerEngine      : singularity
  launchDir            : /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek
  workDir              : /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek
  projectDir           : /home/jeon96/.nextflow/assets/nf-core/sarek
  userName             : jeon96
  profile              : singularity
  configFiles          : 

Input/output options
  input                : /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/mapped.csv
  step                 : markduplicates
  outdir               : /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/called/snp/

Main options
  tools                : haplotypecaller,manta
  skip_tools           : baserecalibrator

FASTQ Preprocessing
  trim_fastq           : true

Reference genome options
  genome               : custom
  fasta                : /scratch/negishi/jeon96/swallow/linpan/short-read_panref2_resorted.renamed.fa
  fasta_fai            : /scratch/negishi/jeon96/swallow/linpan/short-read_panref2_resorted.renamed.fa.fai
  igenomes_base        : s3://ngi-igenomes/igenomes
  igenomes_ignore      : true

Generic options
  email                : jeon96@purdue.edu
  multiqc_title        : short-read-pan_swallow
  validationLenientMode: true

!! Only displaying parameters that differ from the pipeline defaults !!
------------------------------------------------------
If you use nf-core/sarek for your analysis please cite:

* The pipeline
  https://doi.org/10.12688/f1000research.16665.2
  https://doi.org/10.5281/zenodo.3476425

* The nf-core framework
  https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md

---erased description of progress status---

ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_GERMLINE_MANTA:MANTA_GERMLINE (SRR14131089)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_GERMLINE_MANTA:MANTA_GERMLINE (SRR14131089)` terminated with an error exit status (1)

Command executed:

  configManta.py         --bam SRR14131089.md.cram         --reference short-read_panref2_resorted.renamed.fa         --runDir manta         --callRegions [short-read_panref2_resorted.renamed.fa].bed.gz         
  
  python manta/runWorkflow.py -m local -j 16
  
  mv manta/results/variants/candidateSmallIndels.vcf.gz         SRR14131089.manta.candidate_small_indels.vcf.gz
  mv manta/results/variants/candidateSmallIndels.vcf.gz.tbi         SRR14131089.manta.candidate_small_indels.vcf.gz.tbi
  mv manta/results/variants/candidateSV.vcf.gz         SRR14131089.manta.candidate_sv.vcf.gz
  mv manta/results/variants/candidateSV.vcf.gz.tbi         SRR14131089.manta.candidate_sv.vcf.gz.tbi
  mv manta/results/variants/diploidSV.vcf.gz         SRR14131089.manta.diploid_sv.vcf.gz
  mv manta/results/variants/diploidSV.vcf.gz.tbi         SRR14131089.manta.diploid_sv.vcf.gz.tbi
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_GERMLINE_MANTA:MANTA_GERMLINE"":
      manta: $( configManta.py --version )
  END_VERSIONS

Command exit status:
  1

Command output:
  
  Successfully created workflow run script.
  To execute the workflow, run the following script and set appropriate options:
  
  manta/runWorkflow.py

Command error:
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.067660Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	forwardTranscriptStrandReadCount: 0 ; reverseTranscriptStrandReadCount: 1
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.068190Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	index candidate:assemblyAlign:assemblySegment: 0:0:0
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.068699Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Alignment: 1I139D
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.069234Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	BreakendInsertSeq: A
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.069737Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Breakend: GenomeInterval: 1071:[117784086,117784087) RIGHT_OPEN
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.070269Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	SVBreakendLowResEvidence: pair: 0 local_pair: 1 cigar: 3 softclip: 0 semialign: 2 shadow: 0 split_align: 0 unknown: 0
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.070847Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.071339Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Breakend: GenomeInterval: 1071:[117784226,117784227) LEFT_OPEN
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.071813Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	SVBreakendLowResEvidence: pair: 0 local_pair: 0 cigar: 3 softclip: 0 semialign: 0 shadow: 0 split_align: 0 unknown: 0
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.072304Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.073185Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.073685Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.074199Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] cmdline:	/usr/local/share/manta-1.6.0-1/libexec/GenerateSVCandidates --threads 16 --align-stats manta/workspace/alignmentStats.xml --graph-file manta/workspace/svLocusGraph.bin --bin-index 0 --bin-count 1 --max-edge-count 10 --min-candidate-sv-size 8 --min-candidate-spanning-count 3 --min-scored-sv-size 50 --ref short-read_panref2_resorted.renamed.fa --candidate-output-file manta/workspace/svHyGen/candidateSV.0000.vcf --diploid-output-file manta/workspace/svHyGen/diploidSV.0000.vcf --min-qual-score 10 --min-pass-qual-score 20 --min-pass-gt-score 15 --enable-remote-read-retrieval --chrom-depth manta/workspace/chromDepth.txt --edge-runtime-log manta/workspace/svHyGen/edgeRuntimeLog.0000.txt --edge-stats-log manta/results/stats/svCandidateGenerationStats.xml --edge-stats-report manta/results/stats/svCandidateGenerationStats.tsv --align-file SRR14131089.md.cram
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.074999Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] version:	1.6.0
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.075505Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] buildTime:	2019-06-28T22:06:27.673004Z
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.075997Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] compiler:	g++-6.3.1
  [2024-01-31T04:26:55.125345Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] Shutting down task submission. Waiting for remaining tasks to complete.
  [2024-01-31T04:27:07.094634Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] Workflow terminated due to the following task errors:
  [2024-01-31T04:27:07.094833Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] Failed to complete command task: 'generateCandidateSV_0000' launched from master workflow, error code: 1, command: '/usr/local/share/manta-1.6.0-1/libexec/GenerateSVCandidates --threads 16 --align-stats manta/workspace/alignmentStats.xml --graph-file manta/workspace/svLocusGraph.bin --bin-index 0 --bin-count 1 --max-edge-count 10 --min-candidate-sv-size 8 --min-candidate-spanning-count 3 --min-scored-sv-size 50 --ref short-read_panref2_resorted.renamed.fa --candidate-output-file manta/workspace/svHyGen/candidateSV.0000.vcf --diploid-output-file manta/workspace/svHyGen/diploidSV.0000.vcf --min-qual-score 10 --min-pass-qual-score 20 --min-pass-gt-score 15 --enable-remote-read-retrieval --chrom-depth manta/workspace/chromDepth.txt --edge-runtime-log manta/workspace/svHyGen/edgeRuntimeLog.0000.txt --edge-stats-log manta/results/stats/svCandidateGenerationStats.xml --edge-stats-report manta/results/stats/svCandidateGenerationStats.tsv --align-file SRR14131089.md.cram'
  [2024-01-31T04:27:07.094891Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] Error Message:
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] Last 29 stderr lines from task (of 29 total lines):
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:54.839234Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] ERROR: Exception caught in getReadSplitScore() while scoring read: SRR14131089.28554754/1 tid:pos:strand 1071:117784086:+ cigar: 1M templSize: 452 mate_tid:pos:strand 1071:117784388:-
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.058376Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] FATAL_ERROR: 2024-Jan-30 23:26:55 /builder/src/c++/lib/applications/GenerateSVCandidates/SplitReadAlignment.cpp(271): Throw in function void splitReadAligner(unsigned int, const string&, const qscore_snp&, const uint8_t*, const string&, const known_pos_range2&, SRAlignmentInfo&)
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.061687Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] Dynamic exception type: boost::exception_detail::clone_impl<illumina::common::GeneralException>
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.062506Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] std::exception::what: Unexpected split read alignment input condition: scanEnd < scanStart. scanEnd: 841 scanStart: 842 querySize: 1 targetSize: 1856
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.063053Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	targetRange: [841,841)
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.063587Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.064121Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] [processEdge(int, GSCOptions const&, SVLocusSet const&, std::vector<EdgeThreadLocalData, std::allocator<EdgeThreadLocalData> >&, EdgeInfo)::current_edge_info*] = Exception caught in thread 12 while processing graph edge: edgeinfo locus:node1:node2: 59252:0:0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.064644Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	node1:LocusNode: GenomeInterval: 1071:[117784045,117784439) n_edges: 1 out_count: 16 evidence: [117783991,117784300)
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.065167Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	EdgeTo: 0 out_count: 16
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.065665Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	node1:EndUserGenomeInterval: bHirRus1_LinPan#0#NC_053450.1:117784046-117784439
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.066178Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.066677Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] [SVScorer::scoreSV(SVCandidateSetData const&, std::vector<SVCandidateAssemblyData, std::allocator<SVCandidateAssemblyData> > const&, SVMultiJunctionCandidate const&, std::vector<SVId, std::allocator<SVId> > const&, std::vector<bool, std::allocator<bool> > const&, bool, bool, std::vector<SVModelScoreInfo, std::allocator<SVModelScoreInfo> >&, SVModelScoreInfo&, bool&, SVEvidenceWriterData&)::scoring_candidate_info*] = Exception caught while attempting to score SVCandidate:
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.067180Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	isImprecise?: 0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.067660Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	forwardTranscriptStrandReadCount: 0 ; reverseTranscriptStrandReadCount: 1
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.068190Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	index candidate:assemblyAlign:assemblySegment: 0:0:0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.068699Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Alignment: 1I139D
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.069234Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	BreakendInsertSeq: A
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.069737Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Breakend: GenomeInterval: 1071:[117784086,117784087) RIGHT_OPEN
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.070269Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	SVBreakendLowResEvidence: pair: 0 local_pair: 1 cigar: 3 softclip: 0 semialign: 2 shadow: 0 split_align: 0 unknown: 0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.070847Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.071339Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Breakend: GenomeInterval: 1071:[117784226,117784227) LEFT_OPEN
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.071813Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	SVBreakendLowResEvidence: pair: 0 local_pair: 0 cigar: 3 softclip: 0 semialign: 0 shadow: 0 split_align: 0 unknown: 0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.072304Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.073185Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.073685Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.074199Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] cmdline:	/usr/local/share/manta-1.6.0-1/libexec/GenerateSVCandidates --threads 16 --align-stats manta/workspace/alignmentStats.xml --graph-file manta/workspace/svLocusGraph.bin --bin-index 0 --bin-count 1 --max-edge-count 10 --min-candidate-sv-size 8 --min-candidate-spanning-count 3 --min-scored-sv-size 50 --ref short-read_panref2_resorted.renamed.fa --candidate-output-file manta/workspace/svHyGen/candidateSV.0000.vcf --diploid-output-file manta/workspace/svHyGen/diploidSV.0000.vcf --min-qual-score 10 --min-pass-qual-score 20 --min-pass-gt-score 15 --enable-remote-read-retrieval --chrom-depth manta/workspace/chromDepth.txt --edge-runtime-log manta/workspace/svHyGen/edgeRuntimeLog.0000.txt --edge-stats-log manta/results/stats/svCandidateGenerationStats.xml --edge-stats-report manta/results/stats/svCandidateGenerationStats.tsv --align-file SRR14131089.md.cram
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.074999Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] version:	1.6.0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.075505Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] buildTime:	2019-06-28T22:06:27.673004Z
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.075997Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] compiler:	g++-6.3.1

Work dir:
  /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/98/f3d9af86f05f52d79825893eb8a74b

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`

 -- Check '.nextflow.log' file for details
Join mismatch for the following entries: 
- key=[patient:SRR14131116, sample:SRR14131116, sex:NA, status:0, id:SRR14131116, data_type:cram, interval_name:bHirRus1_LinPan#0#unmap_tig00000034_1-10465, num_intervals:8, variantcaller:haplotypecaller] values=[/scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram.crai, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/86/9b352d85fffbeddd3d71776dbd956d/bHirRus1_LinPan#0#unmap_tig00000034_1-10465.bed, []] 
- key=[patient:SRR14131116, sample:SRR14131116, sex:NA, status:0, id:SRR14131116, data_type:cram, interval_name:bHirRus1_LinPan#0#NC_053451, num_intervals:8, variantcaller:haplotypecaller] values=[/scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram.crai, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/86/9b352d85fffbeddd3d71776dbd956d/bHirRus1_LinPan#0#NC_053451.1_1-119023421.bed, []] 
- key=[patient:SRR14131116, sample:SRR14131116, sex:NA, status:0, id:SRR14131116, data_type:cram, interval_name:bHirRus1_LinPan#0#NC_053455, num_intervals:8, variantcaller:haplotypecaller] values=[/scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram.crai, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/86/9b352d85fffbeddd3d71776dbd956d/bHirRus1_LinPan#0#NC_053455.1_1-63258489.bed, []] 
- key=[patient:SRR14131116, sample:SRR14131116, sex:NA, status:0, id:SRR14131116, data_type:cram, interval_name:bHirRus1_LinPan#0#NC_080708, num_intervals:8, variantcaller:haplotypecaller] values=[/scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram.crai, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/86/9b352d85fffbeddd3d71776dbd956d/bHirRus1_LinPan#0#NC_080708.1_1-220485.bed, []] 
- key=[patient:SRR14131116, sample:SRR14131116, sex:NA, status:0, id:SRR14131116, data_type:cram, interval_name:bHirRus1_LinPan#0#NC_053464, num_intervals:8, variantcaller:haplotypecaller] values=[/scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/47/9fc07b7df60e5f78b1308d31e7a16f/SRR14131116.md.cram.crai, /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/86/9b352d85fffbeddd3d71776dbd956d/bHirRus1_LinPan#0#NC_053464.1_1-16541138.bed, []]

---erased description of progress status---

ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_GERMLINE_MANTA:MANTA_GERMLINE (SRR14131089)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_GERMLINE_MANTA:MANTA_GERMLINE (SRR14131089)` terminated with an error exit status (1)

Command executed:

  configManta.py         --bam SRR14131089.md.cram         --reference short-read_panref2_resorted.renamed.fa         --runDir manta         --callRegions [short-read_panref2_resorted.renamed.fa].bed.gz         
  
  python manta/runWorkflow.py -m local -j 16
  
  mv manta/results/variants/candidateSmallIndels.vcf.gz         SRR14131089.manta.candidate_small_indels.vcf.gz
  mv manta/results/variants/candidateSmallIndels.vcf.gz.tbi         SRR14131089.manta.candidate_small_indels.vcf.gz.tbi
  mv manta/results/variants/candidateSV.vcf.gz         SRR14131089.manta.candidate_sv.vcf.gz
  mv manta/results/variants/candidateSV.vcf.gz.tbi         SRR14131089.manta.candidate_sv.vcf.gz.tbi
  mv manta/results/variants/diploidSV.vcf.gz         SRR14131089.manta.diploid_sv.vcf.gz
  mv manta/results/variants/diploidSV.vcf.gz.tbi         SRR14131089.manta.diploid_sv.vcf.gz.tbi
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_GERMLINE_MANTA:MANTA_GERMLINE"":
      manta: $( configManta.py --version )
  END_VERSIONS

Command exit status:
  1

Command output:
  
  Successfully created workflow run script.
  To execute the workflow, run the following script and set appropriate options:
  
  manta/runWorkflow.py

Command error:
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.067660Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	forwardTranscriptStrandReadCount: 0 ; reverseTranscriptStrandReadCount: 1
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.068190Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	index candidate:assemblyAlign:assemblySegment: 0:0:0
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.068699Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Alignment: 1I139D
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.069234Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	BreakendInsertSeq: A
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.069737Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Breakend: GenomeInterval: 1071:[117784086,117784087) RIGHT_OPEN
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.070269Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	SVBreakendLowResEvidence: pair: 0 local_pair: 1 cigar: 3 softclip: 0 semialign: 2 shadow: 0 split_align: 0 unknown: 0
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.070847Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.071339Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Breakend: GenomeInterval: 1071:[117784226,117784227) LEFT_OPEN
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.071813Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	SVBreakendLowResEvidence: pair: 0 local_pair: 0 cigar: 3 softclip: 0 semialign: 0 shadow: 0 split_align: 0 unknown: 0
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.072304Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.073185Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.073685Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.074199Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] cmdline:	/usr/local/share/manta-1.6.0-1/libexec/GenerateSVCandidates --threads 16 --align-stats manta/workspace/alignmentStats.xml --graph-file manta/workspace/svLocusGraph.bin --bin-index 0 --bin-count 1 --max-edge-count 10 --min-candidate-sv-size 8 --min-candidate-spanning-count 3 --min-scored-sv-size 50 --ref short-read_panref2_resorted.renamed.fa --candidate-output-file manta/workspace/svHyGen/candidateSV.0000.vcf --diploid-output-file manta/workspace/svHyGen/diploidSV.0000.vcf --min-qual-score 10 --min-pass-qual-score 20 --min-pass-gt-score 15 --enable-remote-read-retrieval --chrom-depth manta/workspace/chromDepth.txt --edge-runtime-log manta/workspace/svHyGen/edgeRuntimeLog.0000.txt --edge-stats-log manta/results/stats/svCandidateGenerationStats.xml --edge-stats-report manta/results/stats/svCandidateGenerationStats.tsv --align-file SRR14131089.md.cram
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.074999Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] version:	1.6.0
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.075505Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] buildTime:	2019-06-28T22:06:27.673004Z
  [2024-01-31T04:26:55.124755Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] [2024-01-31T04:26:55.075997Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] compiler:	g++-6.3.1
  [2024-01-31T04:26:55.125345Z] [a264.negishi.rcac.purdue.edu] [2425_1] [TaskManager] [ERROR] Shutting down task submission. Waiting for remaining tasks to complete.
  [2024-01-31T04:27:07.094634Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] Workflow terminated due to the following task errors:
  [2024-01-31T04:27:07.094833Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] Failed to complete command task: 'generateCandidateSV_0000' launched from master workflow, error code: 1, command: '/usr/local/share/manta-1.6.0-1/libexec/GenerateSVCandidates --threads 16 --align-stats manta/workspace/alignmentStats.xml --graph-file manta/workspace/svLocusGraph.bin --bin-index 0 --bin-count 1 --max-edge-count 10 --min-candidate-sv-size 8 --min-candidate-spanning-count 3 --min-scored-sv-size 50 --ref short-read_panref2_resorted.renamed.fa --candidate-output-file manta/workspace/svHyGen/candidateSV.0000.vcf --diploid-output-file manta/workspace/svHyGen/diploidSV.0000.vcf --min-qual-score 10 --min-pass-qual-score 20 --min-pass-gt-score 15 --enable-remote-read-retrieval --chrom-depth manta/workspace/chromDepth.txt --edge-runtime-log manta/workspace/svHyGen/edgeRuntimeLog.0000.txt --edge-stats-log manta/results/stats/svCandidateGenerationStats.xml --edge-stats-report manta/results/stats/svCandidateGenerationStats.tsv --align-file SRR14131089.md.cram'
  [2024-01-31T04:27:07.094891Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] Error Message:
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] Last 29 stderr lines from task (of 29 total lines):
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:54.839234Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] ERROR: Exception caught in getReadSplitScore() while scoring read: SRR14131089.28554754/1 tid:pos:strand 1071:117784086:+ cigar: 1M templSize: 452 mate_tid:pos:strand 1071:117784388:-
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.058376Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] FATAL_ERROR: 2024-Jan-30 23:26:55 /builder/src/c++/lib/applications/GenerateSVCandidates/SplitReadAlignment.cpp(271): Throw in function void splitReadAligner(unsigned int, const string&, const qscore_snp&, const uint8_t*, const string&, const known_pos_range2&, SRAlignmentInfo&)
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.061687Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] Dynamic exception type: boost::exception_detail::clone_impl<illumina::common::GeneralException>
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.062506Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] std::exception::what: Unexpected split read alignment input condition: scanEnd < scanStart. scanEnd: 841 scanStart: 842 querySize: 1 targetSize: 1856
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.063053Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	targetRange: [841,841)
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.063587Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.064121Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] [processEdge(int, GSCOptions const&, SVLocusSet const&, std::vector<EdgeThreadLocalData, std::allocator<EdgeThreadLocalData> >&, EdgeInfo)::current_edge_info*] = Exception caught in thread 12 while processing graph edge: edgeinfo locus:node1:node2: 59252:0:0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.064644Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	node1:LocusNode: GenomeInterval: 1071:[117784045,117784439) n_edges: 1 out_count: 16 evidence: [117783991,117784300)
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.065167Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	EdgeTo: 0 out_count: 16
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.065665Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	node1:EndUserGenomeInterval: bHirRus1_LinPan#0#NC_053450.1:117784046-117784439
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.066178Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.066677Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] [SVScorer::scoreSV(SVCandidateSetData const&, std::vector<SVCandidateAssemblyData, std::allocator<SVCandidateAssemblyData> > const&, SVMultiJunctionCandidate const&, std::vector<SVId, std::allocator<SVId> > const&, std::vector<bool, std::allocator<bool> > const&, bool, bool, std::vector<SVModelScoreInfo, std::allocator<SVModelScoreInfo> >&, SVModelScoreInfo&, bool&, SVEvidenceWriterData&)::scoring_candidate_info*] = Exception caught while attempting to score SVCandidate:
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.067180Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	isImprecise?: 0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.067660Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	forwardTranscriptStrandReadCount: 0 ; reverseTranscriptStrandReadCount: 1
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.068190Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	index candidate:assemblyAlign:assemblySegment: 0:0:0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.068699Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Alignment: 1I139D
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.069234Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	BreakendInsertSeq: A
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.069737Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Breakend: GenomeInterval: 1071:[117784086,117784087) RIGHT_OPEN
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.070269Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	SVBreakendLowResEvidence: pair: 0 local_pair: 1 cigar: 3 softclip: 0 semialign: 2 shadow: 0 split_align: 0 unknown: 0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.070847Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.071339Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	Breakend: GenomeInterval: 1071:[117784226,117784227) LEFT_OPEN
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.071813Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 	SVBreakendLowResEvidence: pair: 0 local_pair: 0 cigar: 3 softclip: 0 semialign: 0 shadow: 0 split_align: 0 unknown: 0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.072304Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.073185Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.073685Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] 
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.074199Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] cmdline:	/usr/local/share/manta-1.6.0-1/libexec/GenerateSVCandidates --threads 16 --align-stats manta/workspace/alignmentStats.xml --graph-file manta/workspace/svLocusGraph.bin --bin-index 0 --bin-count 1 --max-edge-count 10 --min-candidate-sv-size 8 --min-candidate-spanning-count 3 --min-scored-sv-size 50 --ref short-read_panref2_resorted.renamed.fa --candidate-output-file manta/workspace/svHyGen/candidateSV.0000.vcf --diploid-output-file manta/workspace/svHyGen/diploidSV.0000.vcf --min-qual-score 10 --min-pass-qual-score 20 --min-pass-gt-score 15 --enable-remote-read-retrieval --chrom-depth manta/workspace/chromDepth.txt --edge-runtime-log manta/workspace/svHyGen/edgeRuntimeLog.0000.txt --edge-stats-log manta/results/stats/svCandidateGenerationStats.xml --edge-stats-report manta/results/stats/svCandidateGenerationStats.tsv --align-file SRR14131089.md.cram
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.074999Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] version:	1.6.0
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.075505Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] buildTime:	2019-06-28T22:06:27.673004Z
  [2024-01-31T04:27:07.094928Z] [a264.negishi.rcac.purdue.edu] [2425_1] [WorkflowRunner] [ERROR] [2024-01-31T04:26:55.075997Z] [a264.negishi.rcac.purdue.edu] [2425_1] [generateCandidateSV_0000] compiler:	g++-6.3.1

Work dir:
  /scratch/negishi/jeon96/swallow/mcpan/short-read-pg/aligned/sarek/98/f3d9af86f05f52d79825893eb8a74b

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`

 -- Check '.nextflow.log' file for details

-[nf-core/sarek] Pipeline completed with errors-
WARN: Killing running tasks (5)
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/14107246/nextflow.log)
[nf-params.json](https://github.com/nf-core/sarek/files/14107247/nf-params.json)


### System information

Slurm job scheduler
Sarek version 3.3.2",jyj5558,https://github.com/nf-core/sarek/issues/1387
I_kwDOCvwIC8598SnM,Error by haplotypecaller and GATK.GRCh37,CLOSED,2024-02-01T16:45:46Z,2024-02-06T15:10:26Z,2024-02-06T15:09:25Z,"### Description of the bug

I am trying to use haplotypecaller variant caller with germinal samples and I need to use GRCh37 reference genome.
On the final step before to annotation phase, the pipeline finish with error.
I am doing test and I believe that there is a problem with the path of 1000G_phase1.snps.high_confidence.b37.vcf.gz following the output description. The error appears independently if you are using local reference files or the internet connection. 


### Command used and terminal output

```console
nextflow run './3_4_0/main.nf' -profile singularity --input 1.csv --tools 'haplotypecaller,merge' --genome GATK.GRCh37 --outdir ./provant --max_cpus 12 --max_memory 28.GB --ngscheckmate_bed 'false'

And the output:
-[nf-core/sarek] Pipeline completed with errors-
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES (D16)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES (D16)` terminated with an error exit status (2)

Command executed:

  gatk --java-options ""-Xmx9830M -XX:-UsePerfData"" \
      FilterVariantTranches \
      --variant D16.cnn.vcf.gz \
      --resource dbsnp_138.b37.vcf.gz --resource 1000G_phase1.indels.b37.vcf.gz --resource Mills_and_1000G_gold_standard.indels.b37.vcf.gz --resource 1000G_phase1.snps.high_confidence.b37.vcf.gz \
      --output D16.haplotypecaller.filtered.vcf.gz \
      --tmp-dir . \
      --info-key CNN_1D
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  2

Command output:
  (empty)

Command error:
  16:29:45.398 INFO  ProgressMeter -          4:162713145              1.2                151000         125758.2
  16:29:55.568 INFO  ProgressMeter -          nalysis Toolkit (GATK) v4.4.0.0
  16:28:32.702 INFO  FilterVariantTranches - For support and documentation go to https://software.broadinstitute.org/gatk/
  16:28:32.702 INFO  FilterVariantTranches - Executing as bague@cbp10055 on Linux v5.15.0-92-generic amd64
  16:28:32.702 INFO  FilterVariantTranches - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src
  16:28:32.702 INFO  FilterVariantTranches - Start Date/Time: February 1, 2024 at 4:28:32 PM GMT
  16:28:32.702 INFO  FilterVariantTranches - ------------------------------------------------------------
  16:28:32.702 INFO  FilterVariantTranches - ------------------------------------------------------------
  16:28:32.703 INFO  FilterVariantTranches - HTSJDK Version: 3.0.5
  16:28:32.703 INFO  FilterVariantTranches - Picard Version: 3.0.0
  16:28:32.703 INFO  FilterVariantTranches - Built for Spark Version: 3.3.1
  16:28:32.703 INFO  FilterVariantTranches - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  16:28:32.704 INFO  FilterVariantTranches - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  16:28:32.704 INFO  FilterVariantTranches - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  16:28:32.704 INFO  FilterVariantTranches - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  16:28:32.704 INFO  FilterVariantTranches - Deflater: IntelDeflater
  16:28:32.704 INFO  FilterVariantTranches - Inflater: IntelInflater
  16:28:32.704 INFO  FilterVariantTranches - GCS max retries/reopens: 20
  16:28:32.705 INFO  FilterVariantTranches - Requester pays: disabled
  16:28:32.705 INFO  FilterVariantTranches - Initializing engine
  16:28:32.775 INFO  FeatureManager - Using codec VCFCodec to read file file://dbsnp_138.b37.vcf.gz
  16:28:32.850 INFO  FeatureManager - Using codec VCFCodec to read file file://1000G_phase1.indels.b37.vcf.gz
  16:28:32.938 INFO  FeatureManager - Using codec VCFCodec to read file file://Mills_and_1000G_gold_standard.indels.b37.vcf.gz
  16:28:33.092 INFO  FeatureManager - Using codec VCFCodec to read file file://1000G_phase1.snps.high_confidence.b37.vcf.gz
  16:28:33.176 INFO  FeatureManager - Using codec VCFCodec to read file file://D16.cnn.vcf.gz
  16:28:33.256 WARN  IndexUtils - Feature file ""file://1000G_phase1.snps.high_confidence.b37.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file
  16:28:33.330 WARN  IntelInflater - Zero Bytes Written : 0
  16:28:33.334 INFO  FilterVariantTranches - Done initializing engine
  16:28:33.355 INFO  ProgressMeter - Starting traversal
  16:28:33.355 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute
  16:28:33.355 INFO  FilterVariantTranches - Starting pass 0 through the variants
  16:28:43.667 INFO  ProgressMeter -          1:115569621              0.2                 24000         139670.2
  16:28:53.761 INFO  ProgressMeter -             2:552227              0.3                 48000         141135.0
  16:29:04.257 INFO  ProgressMeter -          2:120415348              0.5                 69000         133971.9
  16:29:14.262 INFO  ProgressMeter -          2:239039075              0.7                 89000         130540.0
  16:29:24.610 INFO  ProgressMeter -          3:116266658              0.9                110000         128770.4
  16:29:35.347 INFO  ProgressMeter -           4:37615199              1.0                133000         128728.4
  16:29:45.398 INFO  ProgressMeter -          4:162713145              1.2                151000         125758.2
  16:29:55.568 INFO  ProgressMeter -           5:94306756              1.4                171000         124799.3
  16:30:05.813 INFO  ProgressMeter -           6:29839472              1.5                190000         123299.2
  16:30:05.814 INFO  FilterVariantTranches - Filtered 0 SNPs out of 174319 and filtered 0 indels out of 15732 with INFO score: CNN_1D.
  16:30:05.816 INFO  FilterVariantTranches - Shutting down engine
  [February 1, 2024 at 4:30:05 PM GMT] org.broadinstitute.hellbender.tools.walkers.vqsr.FilterVariantTranches done. Elapsed time: 1.55 minutes.
  Runtime.totalMemory()=1090519040
  ***********************************************************************
  
  A USER ERROR has occurred: Bad input: The provided variant file(s) have inconsistent references for the same position(s) at 6:29857105, AC* in input vs. AA* in resource
  
  ***********************************************************************
 Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.
```


### Relevant files

_No response_

### System information

Nextflow version 23.04.3.5875
openjdk version ""17.0.9"" 2023-10-17
OpenJDK Runtime Environment (build 17.0.9+9-Ubuntu-120.04)
OpenJDK 64-Bit Server VM (build 17.0.9+9-Ubuntu-120.04, mixed mode, sharing)

Hardware: Desktop
Executor: local
Container engine: Singularity
OS: Ubuntu
Sarek: 3.4.0",jbague,https://github.com/nf-core/sarek/issues/1388
I_kwDOCvwIC85-FBcV,Default Bed file GATK.GRCh37 contains timings: remove those,OPEN,2024-02-02T16:00:34Z,2024-02-02T16:00:34Z,,"### Description of the bug

The default bed for 37 contains the seconds column and then isn't tunable with the nps parameter. We should add one without to igenomes and replace it in the config to allow users to better tune performance.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1389
I_kwDOCvwIC85-ZVRh,"Check and update local and nf-core modules, to make sure output does not encapsulate input",OPEN,2024-02-06T11:37:06Z,2024-02-06T11:42:41Z,,"### Description of feature

local:
- [x] add_info_to_vcf
- [x] build_intervals
- [ ] create_intervals_bed

nf-core:
- [x] ascat
- [ ] bcftools/annotate
- [ ] bcftools/concat
- [x] bcftools/mpileup
- [ ] bcftools/sort
- [x] bcftools/stats
- [x] bwa/index
- [x] bwa/mem
- [ ] bwamem2/index
- [ ] bwamem2/mem
- [ ] cat/cat
- [ ] cat/fastq
- [ ] cnvkit/antitarget
- [ ] cnvkit/batch
- [ ] cnvkit/genemetrics
- [ ] cnvkit/reference
- [ ] controlfreec/assesssignificance
- [ ] controlfreec/freec
- [ ] controlfreec/freec2bed
- [ ] controlfreec/freec2circos
- [ ] controlfreec/makegraph
- [ ] custom/dumpsoftwareversions
- [ ] deepvariant
- [ ] dragmap/align
- [ ] dragmap/hashtable
- [ ] ensemblvep/download
- [ ] ensemblvep/vep
- [ ] fastp
- [ ] fastqc
- [ ] fgbio/callmolecularconsensusreads
- [ ] fgbio/fastqtobam
- [ ] fgbio/groupreadsbyumi
- [ ] freebayes
- [ ] gatk4/applybqsr
- [ ] gatk4/applyvqsr
- [ ] gatk4/baserecalibrator
- [ ] gatk4/calculatecontamination
- [ ] gatk4/cnnscorevariants
- [ ] gatk4/createsequencedictionary
- [ ] gatk4/estimatelibrarycomplexity
- [ ] gatk4/filtermutectcalls
- [ ] gatk4/filtervarianttranches
- [ ] gatk4/gatherbqsrreports
- [ ] gatk4/gatherpileupsummaries
- [ ] gatk4/genomicsdbimport
- [ ] gatk4/genotypegvcfs
- [ ] gatk4/getpileupsummaries
- [ ] gatk4/haplotypecaller
- [ ] gatk4/intervallisttobed
- [ ] gatk4/learnreadorientationmodel
- [ ] gatk4/markduplicates
- [ ] gatk4/mergemutectstats
- [ ] gatk4/mergevcfs
- [ ] gatk4/mutect2
- [ ] gatk4/variantrecalibrator
- [ ] gatk4spark/applybqsr
- [ ] gatk4spark/baserecalibrator
- [ ] gatk4spark/markduplicates
- [ ] manta/germline
- [ ] manta/somatic
- [ ] manta/tumoronly
- [ ] mosdepth
- [ ] msisensorpro/msisomatic
- [ ] msisensorpro/scan
- [ ] multiqc
- [ ] ngscheckmate/ncm
- [ ] samblaster
- [ ] samtools/bam2fq
- [ ] samtools/collatefastq
- [ ] samtools/convert
- [ ] samtools/faidx
- [ ] samtools/index
- [ ] samtools/merge
- [ ] samtools/mpileup
- [ ] samtools/stats
- [ ] samtools/view
- [ ] sentieon/applyvarcal
- [ ] sentieon/bwamem
- [ ] sentieon/dedup
- [ ] sentieon/dnamodelapply
- [ ] sentieon/dnascope
- [ ] sentieon/gvcftyper
- [ ] sentieon/haplotyper
- [ ] sentieon/varcal
- [ ] snpeff/download
- [ ] snpeff/snpeff
- [ ] strelka/germline
- [ ] strelka/somatic
- [ ] svdb/merge
- [ ] tabix/bgziptabix
- [ ] tabix/tabix
- [ ] tiddit/sv
- [ ] untar
- [ ] unzip
- [ ] vcftools
",maxulysse,https://github.com/nf-core/sarek/issues/1392
I_kwDOCvwIC85-gSkr,Output files are getting overwritten when sample ids are not unique,OPEN,2024-02-07T07:46:23Z,2024-02-07T15:24:16Z,,"### Description of the bug

When the ""sampleID"" in the samplesheet is not unique, only the output files for the final patient are kept. For example the following samplesheet would only keep the outout of the patient called 10perc.
```
patient,status,sample,lane,bam
5perc,0,normal,lane_1,imgag-benchmark/NA12877_21.bam
5perc,1,tumor,lane_1,imgag-benchmark/NA12878x3_23_NA12877_21_5.bam
10perc,0,normal,lane_1,imgag-benchmark/NA12877_21.bam
10perc,1,tumor,lane_1,imgag-benchmark/NA12878x3_23_NA12877_21_10.bam
```
When the samplesheet is altered, all output is kept:
```
patient,status,sample,lane,bam
5perc,0,normal_5perc,lane_1,imgag-benchmark/NA12877_21.bam
5perc,1,tumor_5perc,lane_1,imgag-benchmark/NA12878x3_23_NA12877_21_5.bam
10perc,0,normal_10perc,lane_1,imgag-benchmark/NA12877_21.bam
10perc,1,tumor_10perc,lane_1,imgag-benchmark/NA12878x3_23_NA12877_21_10.bam
```
The sarek runs throws no errors just the files are missing in the end.

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.0 -profile cfc -work-dir work-bm -params-file params.json
```


### Relevant files

params.json:
```json
{
    ""input"": ""/sfs/9/ws/paifb01-pm4onco/imgag-benchmark/benchmark_samplesheet.csv"",
    ""outdir"": ""/sfs/9/ws/paifb01-pm4onco/imgag-benchmark/results"",
    ""tools"": ""mutect2,strelka,manta,vep"",
    ""save_output_as_bam"": true,
    ""only_paired_variant_calling"": true,
    ""email"": ""famke.baeuerle@qbic.uni-tuebingen.de""
}
```

### System information

- Nextflow version: 23.10.1
- Hardware: HPC
- Executor: slurm
- Container engine: Singularity
- OS: Red Hat Enterprise Linux 8.8 (Ootpa)
- Version: 3.4.0",famosab,https://github.com/nf-core/sarek/issues/1393
I_kwDOCvwIC85-g1Mg,Issues with intervals when chromosomes have a dot in their name,CLOSED,2024-02-07T09:12:46Z,2024-02-15T09:55:49Z,2024-02-15T09:55:49Z,"### Description of the bug

Issue might be in the way we deal with intervals cf https://nfcore.slack.com/archives/CGFUX04HZ/p1706539054046349

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1394
I_kwDOCvwIC85-kEMq,error in test by haplotypecaller,OPEN,2024-02-07T16:11:30Z,2024-02-08T15:23:52Z,,"### Description of the bug

I cannot finish the test of haplotypecaller tool on singularity container.
Of course, when I run my germline samples, I obtained the same error.
The pipeline fails on the last step of variant calling step.
I check with strelka tool, and the pipeline test finish correctly.

### Command used and terminal output

```console
nextflow run 3_4_0/main.nf -profile test,singularity --tools haplotypecaller --outdir ./results

[15/8868cb] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:VCF_VARIANT_FILTERING_GATK:FILTERVA... [100%] 1 of 1, failed: 1 ✘
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS                                 -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT                            -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL                             -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY                               -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                                             -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC                                                                 -
Execution cancelled -- Finishing pending tasks before exit
-[nf-core/sarek] Pipeline completed with errors-
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES (test)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES (test)` terminated with an error exit status (2)

Command executed:

  gatk --java-options ""-Xmx5324M -XX:-UsePerfData"" \
      FilterVariantTranches \
      --variant test.cnn.vcf.gz \
      --resource dbsnp_146.hg38.vcf.gz --resource mills_and_1000G.indels.vcf.gz \
      --output test.haplotypecaller.filtered.vcf.gz \
      --tmp-dir . \
      --info-key CNN_1D
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:VCF_VARIANT_FILTERING_GATK:FILTERVARIANTTRANCHES"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  2

Command output:
  (empty)

Command error:
  Using GATK jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5324M -XX:-UsePerfData -jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar FilterVariantTranches --variant test.cnn.vcf.gz --resource dbsnp_146.hg38.vcf.gz --resource mills_and_1000G.indels.vcf.gz --output test.haplotypecaller.filtered.vcf.gz --tmp-dir . --info-key CNN_1D
  16:07:18.011 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
  16:07:18.048 INFO  FilterVariantTranches - ------------------------------------------------------------
  16:07:18.052 INFO  FilterVariantTranches - The Genome Analysis Toolkit (GATK) v4.4.0.0
  16:07:18.052 INFO  FilterVariantTranches - For support and documentation go to https://software.broadinstitute.org/gatk/
  16:07:18.052 INFO  FilterVariantTranches - Executing as bague@cbp10055 on Linux v5.15.0-92-generic amd64
  16:07:18.052 INFO  FilterVariantTranches - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src
  16:07:18.052 INFO  FilterVariantTranches - Start Date/Time: February 7, 2024 at 4:07:17 PM GMT
  16:07:18.052 INFO  FilterVariantTranches - ------------------------------------------------------------
  16:07:18.052 INFO  FilterVariantTranches - ------------------------------------------------------------
  16:07:18.053 INFO  FilterVariantTranches - HTSJDK Version: 3.0.5
  16:07:18.053 INFO  FilterVariantTranches - Picard Version: 3.0.0
  16:07:18.053 INFO  FilterVariantTranches - Built for Spark Version: 3.3.1
  16:07:18.053 INFO  FilterVariantTranches - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  16:07:18.054 INFO  FilterVariantTranches - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  16:07:18.054 INFO  FilterVariantTranches - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  16:07:18.054 INFO  FilterVariantTranches - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  16:07:18.054 INFO  FilterVariantTranches - Deflater: IntelDeflater
  16:07:18.054 INFO  FilterVariantTranches - Inflater: IntelInflater
  16:07:18.054 INFO  FilterVariantTranches - GCS max retries/reopens: 20
  16:07:18.054 INFO  FilterVariantTranches - Requester pays: disabled
  16:07:18.055 INFO  FilterVariantTranches - Initializing engine
  16:07:18.129 INFO  FeatureManager - Using codec VCFCodec to read file file://dbsnp_146.hg38.vcf.gz
  16:07:18.132 WARN  IntelInflater - Zero Bytes Written : 0
  16:07:18.139 INFO  FeatureManager - Using codec VCFCodec to read file file://mills_and_1000G.indels.vcf.gz
  16:07:18.140 WARN  IntelInflater - Zero Bytes Written : 0
  16:07:18.146 INFO  FeatureManager - Using codec VCFCodec to read file file://test.cnn.vcf.gz
  16:07:18.147 WARN  IntelInflater - Zero Bytes Written : 0
  16:07:18.148 WARN  IntelInflater - Zero Bytes Written : 0
  16:07:18.152 INFO  FilterVariantTranches - Done initializing engine
  16:07:18.168 INFO  ProgressMeter - Starting traversal
  16:07:18.169 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute
  16:07:18.169 INFO  FilterVariantTranches - Starting pass 0 through the variants
  16:07:18.170 WARN  IntelInflater - Zero Bytes Written : 0
  16:07:18.171 INFO  FilterVariantTranches - Finished pass 0 through the variants
  16:07:18.171 INFO  FilterVariantTranches - Found 0 SNPs and 0 indels with INFO score key:CNN_1D.
  16:07:18.171 INFO  FilterVariantTranches - Found 0 SNPs and 0 indels in the resources.
  16:07:18.171 INFO  FilterVariantTranches - Filtered 0 SNPs out of 0 and filtered 0 indels out of 0 with INFO score: CNN_1D.
  16:07:18.173 INFO  FilterVariantTranches - Shutting down engine
  [February 7, 2024 at 4:07:18 PM GMT] org.broadinstitute.hellbender.tools.walkers.vqsr.FilterVariantTranches done. Elapsed time: 0.00 minutes.
  Runtime.totalMemory()=125829120
  ***********************************************************************
  
  A USER ERROR has occurred: Bad input: VCF contains no variants or no variants with INFO score key ""CNN_1D""
  
  ***********************************************************************
  Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.

Work dir:
  /media/bague/D_2/descriptiu_marato_tv3/prova_nf_3_4_0/work/15/8868cbf90b18654a33c937f88a5bc9

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

 -- Check '.nextflow.log' file for details
```


### Relevant files

_No response_

### System information

    Nextflow version  23.04.0
    Hardware Desktop
    Executor  local
    Container engine: Singularity
    Os: Ubuntu 20.0.04
    Version of nf-core/sarek 3.4.0

",jbague,https://github.com/nf-core/sarek/issues/1396
I_kwDOCvwIC85-kV9O,There's a mount issue using apptainer,OPEN,2024-02-07T16:46:31Z,2024-03-01T10:25:31Z,,"### Description of the bug

I get this error that kills the pipeline using the apptainer pipeline


Caused by:
  Process `NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX (genome.fasta)` terminated with an error exit status (255)

Command executed:

  mkdir bwa
  bwa \
      index \
       \
      -p bwa/genome \
      genome.fasta
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX"":
      bwa: $(echo $(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*$//')
  END_VERSIONS

Command exit status:
  255

Command output:
  (empty)

Command error:
  WARNING: Skipping mount /var/apptainer/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container
  FATAL:   container creation failed: mount hook function failure: mount /tmp/nxf.7ZcU6MZzH9->/tmp/nxf.7ZcU6MZzH9 error: while mounting /tmp/nxf.7ZcU6MZzH9: destination /tmp/nxf.7ZcU6MZzH9 doesn't exist in container

Work dir:
  /ocean/projects/aurora/achang4/sarek/Part2_FASTQ2SAREK/work/70/36e5cb0114519a69f081bd329e4960


### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.0 -profile test,apptainer \
    --input /ocean/projects/aurora/achang4/sarek/Part1.2_ASSEMBLE_BAM_INPUT/samplesheet.csv \
    --outdir /ocean/projects/aurora/achang4/sarek/Part2_FASTQ2SAREK/results/ \
    --genome GATK.GRCh38 \
    --tools mutect2,strelka,manta \
    -c slurm.config  \
    -resume
```


### Relevant files

[nextflow.zip](https://github.com/nf-core/sarek/files/14196745/nextflow.zip)


### System information

Nextflow Version 23.10.0 build 5889
Hardware: HPC
Executor: Slurm
Container enginer: Apptainer
OS: Ubuntu
Version of Sarek: 3.4.0",alexanderchang1,https://github.com/nf-core/sarek/issues/1397
I_kwDOCvwIC85-qGkI,ERROR  java.net.SocketTimeoutException: Read timed out,OPEN,2024-02-08T11:32:35Z,2024-02-08T11:32:35Z,,"### Description of the bug

Hi, I used SAREK successfully on single samples, however, when I want to process multiple samples I often get  java.net.SocketTimeoutException: Read timed out errors, e.g.:
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:ENSEMBLVEP_VEP (7)'

Caused by:
  java.net.SocketTimeoutException: Read timed out

A similar error appeared regarding tabix.

My system: ubuntu server, 44 cores, 256GB RAM, export NXF_OPTS=""-Xms200g -Xmx200g""

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.0 -profile docker -params-file nf-params_germline_healthy.json

ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:ENSEMBLVEP_VEP (7)'

Caused by:
  java.net.SocketTimeoutException: Read timed out
```


### Relevant files

_No response_

### System information

N E X T F L O W  ~  version 23.10.1
ubuntu server, 44 cores, 256GB RAM
nf-core/sarek v3.4.0-g6aeac92
Launching `https://github.com/nf-core/sarek` [furious_nightingale] DSL2 - revision: 6aeac929c9 [3.4.0]
Docker",Magisterh1,https://github.com/nf-core/sarek/issues/1398
I_kwDOCvwIC85-uqqh,Sentieon bundle file,OPEN,2024-02-08T22:19:49Z,2024-12-09T16:22:16Z,,"### Description of the bug

Hi, I am unable to load sentieon dnascope model using the command below.
The SENTIEONDNASCOPEMODEL points to to location of the Sentieon model
$PATH/DNAscopeIlluminaWES2.0.bundle/dnascope.model

I think. the bunde file is not recognised by the installed version of Sentieon


### Command used and terminal output

```console
nextflow run nf-core/sarek \
    -r 3.4.0 \
    -c ""$inDIR/my.config"" \
    -profile apptainer \
    -work-dir $workDIR \
    --input $inDIR/samplesheet.csv \
    --outdir $outPATH/$outDIR \
    --wes \
    --intervals $WESINTERVAL \
    --tools sentieon_dnascope,sentieon_dedup,mutect2,snpeff,vep,merge \
    --trim_fastq True \
    --aligner sentieon-bwamem \
    --joint_germline True \
    --joint_mutect2 True \
    --igenomes_ignore \
    --genome null \
    --fasta $GENOME \
    --fasta_fai $GENOMEIDX \
    --sentieon_dnascope_emit_mode variant,gvcf \
    --sentieon_dnascope_pcr_indel_model CONSERVATIVE \
    --dbsnp $DBSNP \
    --dbsnp_tbi $DBSNPTBI \
    --known_snps $SNPS \
    --known_snps_tbi $SNPSTBI \
    --known_snps_vqsr $SNPSVSQ \
    --known_indels $INDELS \
    --known_indels_tbi $INDELSTBI \
    --germline_resource $GERMLINERES \
    --germline_resource_tbi $GERMLINERESTBI \
    --sentieon_dnascope_model $SENTIEONDNASCOPEMODEL \
    --pon $PON \
    --pon_tbi $PONTBI \
    --snpeff_db 105 \
    --snpeff_genome 'GRCh38' \
    --vep_cache_version 110 \
    --vep_genome 'GRCh38' \
    --vep_species 'homo_sapiens' -resume
```


### Relevant files

_No response_

### System information

Nextflow/23.04.2
HPC
Slurm or local
Apptainer
nf-core/sarek 3.4.0
",gianfilippo,https://github.com/nf-core/sarek/issues/1399
I_kwDOCvwIC85_A-Mr,FASTP section missing in multiqc report from Sarek v3.4,OPEN,2024-02-12T20:50:44Z,2024-02-12T20:51:01Z,,"### Description of the bug

When running

```
nextflow run main.nf -profile test,docker --trim_fastq --save_trimmed --outdir results
```
on Sare v3.1.2, the multiqc-report, as expected, contains a FASTP section:

![image](https://github.com/nf-core/sarek/assets/37172585/f26e70e2-7941-4038-9c6a-8697a481d086)

However, when running the same test cmd through Sarek v3.4 there is not FASTP section in the resulting multiqc report.

Issue initially reported on [slack](https://nfcore.slack.com/archives/CGFUX04HZ/p1707755252764149?thread_ts=1707592870.675969&cid=CGFUX04HZ).

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1402
I_kwDOCvwIC85_LzY6,docker.userEmulation is deprecated with latest Nextflow edge,OPEN,2024-02-14T08:25:25Z,2024-02-14T11:57:05Z,,"### Description of the bug

used in `conf/test.config` and in `nextflow.config` for gatk spark.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1404
I_kwDOCvwIC85_NrPA,Figure out if Sarek-dev works with Spark,CLOSED,2024-02-14T12:53:59Z,2024-06-14T12:50:08Z,2024-06-14T12:50:08Z,"### Description of the bug

Spark may have stopped working after docker.userEmulation was removed in https://github.com/nf-core/sarek/pull/1405.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1406
I_kwDOCvwIC85_1iPQ,ERROR - failed to load the meta schema ,CLOSED,2024-02-20T16:11:38Z,2024-02-20T18:42:38Z,2024-02-20T16:49:52Z,"### Description of the bug

All of a sudden sarek stops launching with the error concerning meta schema. Tried updating nextflow, sarek and re pulling sarek to a fresh user

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.0 -profile test

N E X T F L O W  ~  version 23.10.1
Launching `https://github.com/nf-core/sarek` [drunk_jennings] DSL2 - revision: 6aeac929c9 [3.4.0]
Downloading plugin nf-validation@2.0.0
Downloading plugin nf-prov@1.2.1
WARN: The following invalid input values have been detected:

* --input: /home/webadm/.nextflow/assets/nf-core/sarek/tests/csv/3.0/fastq_single.csv
* --step: mapping
* --genome: null
* --igenomes_base: s3://ngi-igenomes/igenomes/
* --snpeff_cache: null
* --vep_cache: null
* --igenomes_ignore: true
* --save_reference: false
* --build_only_index: false
* --download_cache: false
* --no_intervals: false
* --nucleotides_per_second: 200000
* --tools: strelka
* --skip_tools: null
* --split_fastq: 0
* --trim_fastq: false
* --clip_r1: 0
* --clip_r2: 0
* --three_prime_clip_r1: 0
* --three_prime_clip_r2: 0
* --trim_nextseq: 0
* --save_trimmed: false
* --save_split_fastqs: false
* --umi_read_structure: null
* --group_by_umi_strategy: Adjacency
* --aligner: bwa-mem
* --use_gatk_spark: null
* --save_mapped: false
* --save_output_as_bam: false
* --seq_center: null
* --seq_platform: ILLUMINA
* --ascat_ploidy: null
* --ascat_min_base_qual: 20
* --ascat_min_counts: 10
* --ascat_min_map_qual: 35
* --ascat_purity: null
* --cf_ploidy: 2
* --cf_coeff: 0.05
* --cf_contamination: 0
* --cf_contamination_adjustment: false
* --cf_mincov: 0
* --cf_minqual: 0
* --cf_window: null
* --cnvkit_reference: null
* --concatenate_vcfs: false
* --ignore_soft_clipped_bases: false
* --joint_germline: false
* --joint_mutect2: false
* --only_paired_variant_calling: false
* --sentieon_dnascope_emit_mode: variant
* --sentieon_dnascope_pcr_indel_model: CONSERVATIVE
* --sentieon_haplotyper_emit_mode: variant
* --wes: false
* --bcftools_annotations: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/sarscov2/illumina/vcf/test2.vcf.gz
* --bcftools_annotations_index: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/sarscov2/illumina/vcf/test2.vcf.gz.tbi
* --bcftools_header_lines: /home/webadm/.nextflow/assets/nf-core/sarek/tests/config/bcfann_test_header.txt
* --dbnsfp: null
* --dbnsfp_consequence: null
* --dbnsfp_fields: rs_dbSNP,HGVSc_VEP,HGVSp_VEP,1000Gp3_EAS_AF,1000Gp3_AMR_AF,LRT_score,GERP++_RS,gnomAD_exomes_AF
* --dbnsfp_tbi: null
* --outdir_cache: null
* --spliceai_indel: null
* --spliceai_indel_tbi: null
* --spliceai_snv: null
* --spliceai_snv_tbi: null
* --vep_custom_args: --everything --filter_common --per_gene --total_length --offline --format vcf
* --vep_dbnsfp: null
* --vep_include_fasta: false
* --vep_loftee: null
* --vep_out_format: vcf
* --vep_spliceai: null
* --vep_spliceregion: null
* --vep_version: 110.0-0
* --multiqc_config: null
* --multiqc_title: null
* --multiqc_logo: null
* --max_multiqc_email_size: 25.MB
* --multiqc_methods_description: null
* --outdir: null
* --publish_dir_mode: copy
* --email: null
* --email_on_fail: null
* --plaintext_email: false
* --hook_url: null
* --version: false
* --config_profile_name: Test profile
* --config_profile_description: Minimal test dataset to check pipeline function
* --custom_config_version: master
* --custom_config_base: https://raw.githubusercontent.com/nf-core/configs/master
* --config_profile_contact: null
* --config_profile_url: null
* --test_data_base: https://raw.githubusercontent.com/nf-core/test-datasets/sarek3
* --max_memory: 6.5GB
* --max_cpus: 2
* --max_time: 8.h
* --validate_params: true
* --dbsnp: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz
* --fasta: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta
* --germline_resource: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz
* --intervals: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list
* --known_indels: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz
* --ngscheckmate_bed: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/chr21/germlineresources/SNP_GRCh38_hg38_wChr.bed
* --snpeff_db: 105
* --snpeff_genome: WBcel235
* --vep_cache_version: 110
* --vep_genome: WBcel235
* --vep_species: caenorhabditis_elegans
* --sentieon_dnascope_model: s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Annotation/Sentieon/SentieonDNAscopeModel1.1.model
* --ascat_alleles: null
* --ascat_genome: null
* --ascat_loci: null
* --ascat_loci_gc: null
* --ascat_loci_rt: null
* --bwa: null
* --bwamem2: null
* --cf_chrom_len: null
* --chr_dir: null
* --dbsnp_tbi: null
* --dbsnp_vqsr: null
* --dict: null
* --dragmap: null
* --fasta_fai: null
* --germline_resource_tbi: null
* --known_indels_tbi: null
* --known_indels_vqsr: null
* --known_snps: null
* --known_snps_tbi: null
* --known_snps_vqsr: null
* --mappability: null
* --pon: null
* --pon_tbi: null
* --input_restart: null


ERROR ~ Failed to load the meta schema:
The used schema draft (http://json-schema.org/draft-07/schema) is not correct, please use ""https://json-schema.org/draft/2020-12/schema"" instead.
See here for more information: https://json-schema.org/specification#migrating-from-older-drafts


 -- Check '.nextflow.log' file for details
```


### Relevant files

Feb-20 16:04:04.226 [main] ERROR n.validation.JsonSchemaValidator - Failed to load the meta schema:
The used schema draft (http://json-schema.org/draft-07/schema) is not correct, please use ""https://json-schema.org/draft/2020-12/schema"" instead.
See here for more information: https://json-schema.org/specification#migrating-from-older-drafts

Feb-20 16:04:04.229 [main] DEBUG nextflow.script.ScriptRunner - Parsed script files:
  Script_3b23d717def1bedf: /home/webadm/.nextflow/assets/nf-core/sarek/main.nf
Feb-20 16:04:04.229 [main] DEBUG nextflow.Session - Session aborted -- Cause: nextflow.validation.SchemaValidationException: 
Feb-20 16:04:04.241 [main] DEBUG nextflow.cli.Launcher - Operation aborted
nextflow.validation.SchemaValidationException: 
	at nextflow.validation.JsonSchemaValidator.validateObject(JsonSchemaValidator.groovy:43)
	at nextflow.validation.JsonSchemaValidator.validate(JsonSchemaValidator.groovy:109)
	at nextflow.validation.SchemaValidator.validateParameters(SchemaValidator.groovy:344)
	at nextflow.validation.SchemaValidator.validateParameters(SchemaValidator.groovy)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at nextflow.script.FunctionDef.invoke_a(FunctionDef.groovy:64)
	at nextflow.script.ComponentDef.invoke_o(ComponentDef.groovy:40)
	at nextflow.script.WorkflowBinding.invokeMethod(WorkflowBinding.groovy:102)
	at groovy.lang.GroovyObject$invokeMethod.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:148)
	at nextflow.script.BaseScript.invokeMethod(BaseScript.groovy:140)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:68)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:51)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:176)
	at Script_3b23d717def1bedf.runScript(Script_3b23d717def1bedf:92)
	at nextflow.script.BaseScript.run0(BaseScript.groovy:144)
	at nextflow.script.BaseScript.run(BaseScript.groovy:192)
	at nextflow.script.ScriptParser.runScript(ScriptParser.groovy:236)
	at nextflow.script.ScriptRunner.run(ScriptRunner.groovy:242)
	at nextflow.script.ScriptRunner.execute(ScriptRunner.groovy:137)
	at nextflow.cli.CmdRun.run(CmdRun.groovy:372)
	at nextflow.cli.Launcher.run(Launcher.groovy:500)
	at nextflow.cli.Launcher.main(Launcher.groovy:672)


### System information

latest, ubuntu 22, local",AndyBlake,https://github.com/nf-core/sarek/issues/1415
I_kwDOCvwIC86ABYCF,Better error message for empty fastq files,OPEN,2024-02-21T22:47:50Z,2024-02-22T08:42:41Z,,"### Description of feature

I noticed that sometimes if an empty fastq is provided, it returns a slightly cryptic error message:

```
ERROR ~ Cannot invoke method startsWith() on null object

 -- Check script '/home/vajith/.nextflow/assets/nf-core/sarek/./workflows/../subworkflows/local/samplesheet_to_channel/main.nf' at line: 275 or see '/home/vajith/workspaces/2024-02-22_wes_jason_sarek/logs/.nextflow.log' file for more details
```

Of course best practice is to always md5 check your files, but it took awhile to debug a colleague's pipeline as it seemed like it was a samplesheet error. It would help users unfamiliar to nextflow/sarek if this error was more verbose:

```
ERROR ~ The following input files are empty:
file1
file2
```

Figured this test would be simple to implement, and would probably be useful to all nextflow pipelines as well (if its not already implemented in other pipelines).",SpikyClip,https://github.com/nf-core/sarek/issues/1416
I_kwDOCvwIC86AEweR,URGENT: pin nf-validation versions,CLOSED,2024-02-22T10:28:29Z,2024-02-22T10:48:32Z,2024-02-22T10:39:35Z,"### Description of the bug

To prevent breaking this pipeline in the near future, the nf-validation version should be pinned to version `1.1.3` like:
```
plugins {
    id 'nf-validation@1.1.3'
}
```

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",nvnieuwk,https://github.com/nf-core/sarek/issues/1417
I_kwDOCvwIC86AGUR5,SPRING format support,CLOSED,2024-02-22T13:59:13Z,2024-10-26T15:05:04Z,2024-10-26T15:05:03Z,"### Description of feature

Hi,
we are storing our sequencing data in spring-format to reduce disc space. Do you plan to support spring-format besides fastq for sequencing reads in the near future?
Best,
 Axel",kunstner,https://github.com/nf-core/sarek/issues/1418
I_kwDOCvwIC86AMg37,running Spark with option save_mapped fails,CLOSED,2024-02-23T10:06:31Z,2024-02-23T10:08:13Z,2024-02-23T10:08:13Z,"### Description of the bug

Original cmd below.

Here is my attempt at replicating the error:

`nextflow run nf-core/sarek -r 3.4.0 -profile test,conda --outdir results --split_fastq 0 --no_intervals true --trim_fastq true --aligner 'bwa-mem2' --save_mapped true --save_output_as_bam true --tools freebayes,strelka,deepvariant,manta,tiddit --use_gatk_spark baserecalibrator,markduplicates`

```
ERROR ~ Error executing process > 'NFCORE_SAREK:sarek:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM (test)'

Caused by:
  Process `NFCORE_SAREK:sarek:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM (test)` terminated with an error exit status (1)

Command executed:

  samtools \
      index \
      -@ 0 \
       \
      test.sorted.bam

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:sarek:BAM_MERGE_INDEX_SAMTOOLS:INDEX_MERGE_BAM"":
      samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  [E::hts_idx_push] NO_COOR reads not in a single block at the end 0 -1
  [E::sam_index] Read 'normal#21#6388#35' with ref_name='chr22', ref_length=40001, flags=69, pos=38086 cannot be indexed
  samtools index: failed to create index for ""test.sorted.bam""
  ```
  
The test.sorted.bam is queryname sorted (SO:queryname). When I coordinate sorted it (SO:coordinate), then there was no problem indexing it.

@maxulysse says one should not be allowed to use save_mapped and spark at the same time as MDspark needs a specific ordering.

https://nfcore.slack.com/archives/CGFUX04HZ/p1708664152514979

### Command used and terminal output

```console
command:

 nextflow run nf-core/sarek -r 3.4.0 -name ""ap_wes_01"" -profile ""conda"" -params-file ""nf-params.json""

params:
""input"": ""/home/user/Documents/samplesheet.csv"",
    ""outdir"": ""/home/user/Documents/sarek_output"",
    ""split_fastq"": 0,
    ""wes"": true,
    ""no_intervals"": true,
    ""tools"": ""freebayes,strelka,deepvariant,manta,tiddit,merge"",
    ""trim_fastq"": true,
    ""aligner"": ""bwa-mem2"",
    ""save_mapped"": true,
    ""save_output_as_bam"": true,
    ""use_gatk_spark"": ""baserecalibrator,markduplicates"",
    ""concatenate_vcfs"": true,
    ""only_paired_variant_calling"": true,
    ""multiqc_title"": ""multiqc""


error msg:
Command error:
  [E::hts_idx_push] Unsorted positions on sequence #16: 58528620 followed by 58528490
  [E::sam_index] Read 'LH00271:69:2237HHLT4:7:1101:1000:11758' with ref_name='chr16', ref_length=90338345, flags=163, pos=58528490 cannot be indexed  samtools index: failed to create index for ""sample_19.sorted.bam""
```


### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1421
I_kwDOCvwIC86ASwsJ,Allow cram in mapping step,OPEN,2024-02-24T16:53:43Z,2024-12-16T00:01:19Z,,"### Description of feature

Hi,
sometimes I find myself having to use unmapped cram files with sarek, and it will be useful to be able to map them directly without first having to convert them to ubam. At the moment step: mapping is not compatible with cram input.
I guess it should be not too complex to implement.
Anyone else would need this?",saulpierotti,https://github.com/nf-core/sarek/issues/1424
I_kwDOCvwIC86AXovX,Use modules_testdata_base_path instead of test_data in nf-tests,CLOSED,2024-02-26T09:27:53Z,2024-05-22T18:10:54Z,2024-05-22T18:10:54Z,"### Description of the bug

Need to update the following modules:

- [x] [bwa/index](https://github.com/nf-core/modules/pull/4987)
- [x] [bwa/mem](https://github.com/nf-core/modules/pull/4988)
- [x] [cat/cat](https://github.com/nf-core/modules/pull/4990)
- [x] cat/fastq (Already updated in modules, but needs to be updated in Sarek)
- [x] [dragmap/align](https://github.com/nf-core/modules/pull/4993)
- [x] [gatk4/applybqsr](https://github.com/nf-core/modules/pull/4995)
- [x] [gatk4/baserecalibrator](https://github.com/nf-core/modules/pull/4997)
- [x] untar (Already updated in modules, but needs to be updated in Sarek)
- [x] [gatk4/markduplicates](https://github.com/nf-core/modules/pull/5000)
- [x] [gatk4/genomicsdbimport](https://github.com/nf-core/modules/pull/5000)
- [x] [gatk4/mergevcfs](https://github.com/nf-core/modules/pull/5000)

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1425
I_kwDOCvwIC86As-rP,Update docs: Haplotypecaller is fine without BQSR (got nothing to do with dragmap),OPEN,2024-02-28T15:49:12Z,2024-02-28T16:01:28Z,,"### Description of the bug

I just found this forum entry, saying the reason why BQSR was dropped from the DRAGEN-GATK workflow was changes to Haplotypecaller and the addition of DragMap was not the reason. We should reflect this better in the docs/warning messages:

https://gatk.broadinstitute.org/hc/en-us/community/posts/4416632101275-DRAGMAP-and-Mutect

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1427
I_kwDOCvwIC86BMtOp,"Error: with the current options, either 'chrFiles' or 'GCcontentProfile' must be set",OPEN,2024-03-04T19:31:14Z,2024-03-05T14:24:17Z,,"### Description of the bug

Hello,

Trying to run controlfreec option in sarek pipeline on rat:

vi samplesheet.csv

patient,sex,status,sample,lane,bam,bai
rat1,XX,1,tumor_sample,lane_1,/data/johnsonko/Analysis_Projects/Sadhana_Jackson/WGS_20240226/preprocessing/recalibrated/tumor_sample/tumor_sample.recal.bam,/data/johnsonko/Analysis_Projects/Sadhana_Jackson/WGS_20240226/preprocessing/recalibrated/tumor_sample/tumor_sample.recal.bam.bai

:wq!

vi nf_core_sarek_go.swarm

nextflow run nf-core/sarek -r 3.4.0 --input samplesheet.csv --outdir . --step variant_calling --fasta Rattus_norvegicus.mRatBN7.2.dna.toplevel.fa --fasta_fai Rattus_norvegicus.mRatBN7.2.dna.toplevel.fa.fai --dbsnp rattus_norvegicus.vcf.gz --known_snps rattus_norvegicus.vcf.gz --tools controlfreec -profile biowulf --igenomes_ignore --genome null

:wq!

swarm -f nf_core_sarek_go.swarm -g 200 --gres=lscratch:200 --time 36:00:00 -t 24 --module nextflow,singularity

... getting error:

Error: with the current options, either 'chrFiles' or 'GCcontentProfile' must be set

... I do not see 'chrFiles' as a parameter option.

Need help/guidance please.

Thank you!


### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",drkoryjohns,https://github.com/nf-core/sarek/issues/1429
I_kwDOCvwIC86Bk-15,ASCAT WES reference resources,OPEN,2024-03-07T14:10:03Z,2024-03-07T14:10:03Z,,"### Description of feature

Hi Developers,

I'm trying to run the Sarek implemented ASCAT for CNV analysis on WES data. On the nfcore Sarek website, it's suggested to follow 5 steps, as specified in this doc https://nf-co.re/sarek/3.4.0/docs/usage#how-to-generate-ascat-resources-for-exome-or-targeted-sequencing, to generate reference information (allele.zip, loci.zip, GC.zip, and RT.zip) for exome data instead of using the default igenome directly. I noticed that the ASCAT author had also provided ref files for WES at https://github.com/VanLoo-lab/ascat/tree/master/ReferenceFiles/WES, which seemed to be a ready-to-use version when provided with an appropriate BED file. Would it be feasible to replace the default ignome ref with those for Sarek ASCAT? 

I'm now running Sarek with params (-- --ascat_alleles, --ascat_loci,  --ascat_loci_gc, --ascat_loci_rt) on the command line. The pipeline seems to work well. But, it would be great to hear advice from you. 

Thank you!",wlyucl,https://github.com/nf-core/sarek/issues/1432
I_kwDOCvwIC86BrwPK,Please pin the version of the `nf-validation` plugin!,CLOSED,2024-03-08T10:23:52Z,2024-03-08T10:26:03Z,2024-03-08T10:26:02Z,"### Description of feature

Hello!

⚠️ The `nf-validation` plugin will have soon a new release (v2.0) which has breaking changes. 
This will break the usage of all pipelines that don't pin the version of this plugin, as Nextflow tries to pull the latest version.

- ❓  How to pin the version:
Add this to your `nextflow.config` file (notice the **`@1.1.3`**):
```
plugins {
    nf-validation@1.1.3
}
```

- 🚨  Release a patch release as soon as possible!
Either if you already pinned the version on your dev branch or if you are going to add it now, please release this changes ASAP, to avoid breaking the usage of the latest version of your pipeline when the new version of `nf-validation` comes out.

- 🤓  How to use the new version of nf-validation?
If you want to be prepared for the new version of nf-validation, this will use the latest version of a JSON Schema draft. You will have to update the `nextflow_schema.json` and `input_schema.json` files (and any other JSON schemas that you use!).
You can find a migration guide [here](https://nextflow-io.github.io/nf-validation/latest/migration_guide/).",mirpedrol,https://github.com/nf-core/sarek/issues/1433
I_kwDOCvwIC86BsyIz,GenomicsDBImport fails when no_intervals is set to true,OPEN,2024-03-08T13:00:45Z,2024-08-03T12:39:14Z,,"### Description of the bug

This is likely due to the GenomicsDBImport module calling the tool with the flag `--intervals []` (with a literal `[]`). Attached the log.

### Command used and terminal output

```console
nextflow run nf-core/sarek \
        -c nextflow.config \
        -params-file params.yaml \
        -r 3.4.0 \
        -profile singularity,slurm
```


### Relevant files

[sarek_logs_and_inputs.tar.gz](https://github.com/nf-core/sarek/files/14538382/sarek_logs_and_inputs.tar.gz)


### System information

_No response_",saulpierotti,https://github.com/nf-core/sarek/issues/1434
I_kwDOCvwIC86BudZv,Add some docs crosslinking the warning about kubernetes and GLS,OPEN,2024-03-08T17:07:41Z,2024-03-08T17:07:41Z,,"              Can we add some docs crosslinking the warning about kubernetes and GLS?

_Originally posted by @FriederikeHanssen in https://github.com/nf-core/sarek/pull/1431#pullrequestreview-1925350178_
            ",maxulysse,https://github.com/nf-core/sarek/issues/1435
I_kwDOCvwIC86CUq1Z,Minor: empty output folder for fastp created (see aws ful size tests),OPEN,2024-03-14T13:57:49Z,2024-03-14T14:01:27Z,,"### Description of the bug

In the full size tests an empty output folder for fastp is created, but no reads are published. Seems to be mostly a cosmetic issue.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1437
I_kwDOCvwIC86ColLY,add parabricks modules to Sarek pipeline,OPEN,2024-03-18T09:31:19Z,2024-10-22T08:17:05Z,,"8 parabricks modules in total should be added to Sarek:

- [ ] fq2bam
- [ ] applybqsr
- [ ] haplotypecaller
- [ ] genotypegvcf
- [ ] indexgvcf
- [ ] dbsnp
- [ ] mutectcaller
- [ ] deepvariant

Documentation for each parabricks tool can be found here: https://docs.nvidia.com/clara/parabricks/4.2.0/documentation/tooldocs/standalonetools.html",Furentsu,https://github.com/nf-core/sarek/issues/1438
I_kwDOCvwIC86Ct_lQ,Add DeepSomatic to the Sarek Pipeline,OPEN,2024-03-18T19:30:45Z,2024-03-18T20:00:27Z,,DeepSomatic was recently released as an extension of DeepVariant that enables tumor-normal somatic calling. I would like to try adding this to Sarek.,danielecook,https://github.com/nf-core/sarek/issues/1440
I_kwDOCvwIC86CxfED,"TIDDIT_SV produces `*.tiddit.ploidies.tab` but not `*.tiddit.vcf`, timing out and failing to complete",OPEN,2024-03-19T04:45:28Z,2024-03-22T11:49:46Z,,"### Description of the bug

Spent the last few weeks trying to troubleshoot this bug where tiddit would consistently fail to complete for certain samples and not others. However, within the work process folder, `*.tiddit.ploidies.tab` is completed, and the process sits there until it times out with exit `140` (up to 8 hours later).

Coinciding with these issues is unusually high memory use for `ENSEMBLVEP_VEP` and `FREEBAYES`, which I have had to up to `144GB` and `65GB`, respectively, to avoid OOM issues. I don't know if this is related. I am working on a cluster running the nextflow head job in an interactive `smux`.

I run pretty much all the tools sarek has on my tumour-only WGS, and apart from the three processes above, everything else runs fine.

Things I've tried:
1. Checking md5s for fastq files and double checking for mismatched samples.
2. Downgrading nextflow (`23.10.1 -> 23.04.5`)
3. Downgrading sarek (`3.4.0 -> 3.3.2`)
4. Downgrading tiddit (`3.6.1 -> 3.3.2`)
5. Redownload igenomes, `snpEff`, and `VEP` caches (unrelated, but was also debugging the memory issues at the same time)
6. Providing more memory to the head job (`4GB -> 8GB`).
7. Providing more memory/time/cores to tiddit.
8. Running sarek `-profile test_full` (tiddit completes with no issues)

The two aspects of this problem that confuse me are:
- The outcome is deterministic, i.e. tiddit only fails consistently on certain samples, but the rest of the tools run fine and the output seems normal for these samples (to me at least)
- `*.tiddit.ploidies.tab` does get produced in the work folder correctly, its just the `vcf` that is empty.

Things I've yet to try:
1. Run sarek on an older dataset which has previously given me no issues (this is my next step, I just need to recover some space on the cluster).



I have attached the relevant `pipeline_info`, `.nextflow.log` and 3 examples of tiddit work folders that failed.

### Command used and terminal output

```console
Note: I have not reproduced the full script here, but all the `pipeline_info` parameters are in the attached zip file.

main() {
    cmdline ""$@""
    module load ""singularity/${singularity_module_version}""

    # export NXF_VER=""23.04.5""
    export NXF_VER=""23.10.1""

    nextflow -log ""${nxf_log}"" run ""${pipeline_name}"" \
        -revision ""${revision}"" \
        -profile ""${profile}"" \
        -config ""${config}"" \
        -params-file ""${params}"" \
        --input ""${samplesheet}"" \
        --email ""${nxf_mail}"" \
        --outdir ""${outdir}"" \
        ""${nxf_args[@]:-}""
}
```
```


### Relevant files

[tiddit_bug.zip](https://github.com/nf-core/sarek/files/14643142/tiddit_bug.zip)


### System information

- Nextflow: `23.10.1`, `23.04.5`
- Hardware: HPC
- Executor: slurm
- Container engine: Singularity
- OS: `Linux m3-login2 3.10.0-1160.71.1.el7.x86_64 #1 SMP Tue Jun 28 15:37:28 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux`
- nf-core/sarek: `3.4.0`, `3.3.2`",SpikyClip,https://github.com/nf-core/sarek/issues/1441
I_kwDOCvwIC86C1dD7,sarek 3.4.0 hang on vep ,CLOSED,2024-03-19T13:56:38Z,2024-03-20T10:55:35Z,2024-03-20T10:55:35Z,"### Description of the bug

sarek 3.4.0, nextflow 23.10.1.5893 hangs for me while (I guess, nextflow beginner here) waiting for input for the vep
step. The sample sheet contains one sample definition with forward/reverse fastq files.



### Command used and terminal output

```console
nextflow run nf-core/sarek -params-file params.yaml -profile docker --input samplesheet.csv --dbsnp clinvar.vcf.gz --tools mpileup,strelka,ngscheckmate,bcfann,snpeff,vep

Last status from terminal output:

executor >  local (137)                                                                                                                                                  
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                           -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                           -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                       -                                                                        
[d1/a209b0] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY (geno... [100%] 1 of 1 ✔                                                          
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                       -                                                                        
[5f/e4e4dc] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX (genome.fa)              [100%] 1 of 1 ✔                                                          
[a3/3caa82] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP (clinvar.vcf)               [100%] 1 of 1 ✔                                                          
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                 -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                        -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                      -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                               -                                                                        
[3f/27244e] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:BUILD_INTERVALS ([genome.fa])        [100%] 1 of 1 ✔                                                          
[4e/f94545] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED (genome.fa.bed) [100%] 1 of 1 ✔                                                          
[d2/abccac] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (... [100%] 18 of 18 ✔                                                        
[2b/737b77] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINE... [100%] 1 of 1 ✔                                                          
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP              -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP          -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_MAP            -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_UNMAP            -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_MERGE_UNMAP               -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_UNMAP                -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:COLLATE_FASTQ_MAP                  -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:CAT_FASTQ                          -                                                                        
[42/484b78] process > NFCORE_SAREK:SAREK:FASTQC (S1-L001)                                       [100%] 1 of 1 ✔                                                          
[c9/1bcc4f] process > NFCORE_SAREK:SAREK:FASTP (S1-L001)                                        [100%] 1 of 1 ✔                                                          
[33/ae9897] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_ME... [100%] 12 of 12 ✔                                                        
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM   -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_B... -                                                                        
[c3/f61e22] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (S1)           [100%] 1 of 1 ✔                                                          
[20/fc24f5] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOO... [100%] 1 of 1 ✔                                                          
[a1/009d39] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEP... [100%] 1 of 1 ✔                                                          
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM                                            -                                                                        
[ae/ea239d] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (S1)       [100%] 18 of 18 ✔                                                        
[38/599c1d] process > NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_GATHERBQSRREPORTS (S1)      [100%] 1 of 1 ✔                                                          
[1b/9b37e3] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:GATK4_APPLYBQSR (S1)                     [100%] 18 of 18 ✔                                                        
[4d/847fd2] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:MERGE_CRAM ... [100%] 1 of 1 ✔                                                          
[2e/f53970] process > NFCORE_SAREK:SAREK:BAM_APPLYBQSR:CRAM_MERGE_INDEX_SAMTOOLS:INDEX_CRAM ... [100%] 1 of 1 ✔                                                          
[88/2ccf2e] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:SAMTOOLS_STATS (S1)                      [100%] 1 of 1 ✔                                                          
[61/050050] process > NFCORE_SAREK:SAREK:CRAM_QC_RECAL:MOSDEPTH (S1)                            [100%] 1 of 1 ✔                                                          
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM_RECAL                                      -                                                                        
[3d/443597] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP (S1)   [100%] 1 of 1 ✔                                                          
[fc/5d347a] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM (1)    [100%] 1 of 1 ✔                                                          
[75/6e8a86] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLIN... [100%] 18 of 18 ✔                                                        
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLIN... -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLIN... -                                                                        
[57/318ccb] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLIN... [100%] 1 of 1 ✔                                                          
[32/c56963] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLIN... [100%] 18 of 18 ✔                                                        
[5c/5f9768] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLIN... [100%] 1 of 1 ✔                                                          
[ea/2d998f] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLIN... [100%] 1 of 1 ✔                                                          
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALL... -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALL... -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALL... -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALL... -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALL... -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALL... -                                                                        
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALL... -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING... -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING... -
[-        ] process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING... -
[04/1f1656] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS (S1)           [100%] 2 of 2 ✔
[77/965593] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT (S1)      [100%] 2 of 2 ✔
[e8/43ed74] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL (S1)       [100%] 2 of 2 ✔
[01/637370] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY (S1)         [100%] 2 of 2 ✔
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_BCFTOOLS:BCFTOOLS_ANN... -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_BCFTOOLS:TABIX_TABIX     -
[15/b50e52] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF ... [100%] 2 of 2 ✔
[c4/d4c02c] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:TABIX_BGZIPTAB... [100%] 2 of 2 ✔
[30/267b7a] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:ENSEMBLVEP... [  0%] 0 of 2
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:TABIX_TABIX   -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                            -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC                                                -

```
```


### Relevant files

```
$ cat params.yaml 
outdir: './results/'
genome: 'Ensembl.GRCh37'
```

Last lines of .nextflow.log
```
Mar-19 14:24:21.672 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > No more task to compute -- The following nodes are still active:
[process] NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:ENSEMBLVEP_VEP
  status=ACTIVE
  port 0: (queue) closed; channel: -
  port 1: (value) bound ; channel: genome
  port 2: (value) bound ; channel: species
  port 3: (value) bound ; channel: cache_version
  port 4: (value) bound ; channel: cache
  port 5: (value) bound ; channel: -
  port 6: (value) bound ; channel: extra_files
  port 7: (cntrl) -     ; channel: $

[process] NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:TABIX_TABIX
  status=ACTIVE
  port 0: (queue) OPEN  ; channel: -
  port 1: (cntrl) -     ; channel: $

[process] NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS
  status=ACTIVE
  port 0: (queue) OPEN  ; channel: versions
  port 1: (cntrl) -     ; channel: $

[process] NFCORE_SAREK:SAREK:MULTIQC
  status=ACTIVE
  port 0: (value) OPEN  ; channel: multiqc_files
  port 1: (value) bound ; channel: multiqc_config
  port 2: (value) bound ; channel: extra_multiqc_config
  port 3: (value) bound ; channel: multiqc_logo
  port 4: (cntrl) -     ; channel: $
```

### System information

Server with `AMD EPYC 7713 64-Core Processor`, 512GB memory running Ubuntu 22.04.4 LTS, local executor, using the Docker profile.",sjaenick,https://github.com/nf-core/sarek/issues/1442
I_kwDOCvwIC86C9Gdz,No reads for mitochondrial chromosome and additional contigs after recalibration,OPEN,2024-03-20T09:29:47Z,2024-03-21T07:01:19Z,,"### Description of the bug

We have run Sarek pipeline v3.1.2 on 3 WGS batches, including around 2500 samples, using the GATK.GRCh38 reference genome from iGenomes and mostly default parameters (see the command). 

In all of these samples, the alignment CRAM files from the mapping and markduplicate steps look fine, but in the CRAM file after the recalibration step, all reads outside canonical chromosomes (i.e., chr1-22, X, Y) are lost. 
This means that we have no reads on the mitochondrial genome or any other additional contig, and therefore, we can't call variants or perform other analyses on chrM, for example.

Is this a known behavior related to recalibration, or is something unexpected happening during the recalibration step?

### Command used and terminal output

```console
#variables
pipeline_name=""nf-core/sarek""
pipeline_revision=""3.1.2""
pipeline_hub=""github""
pipeline_work_dir=""/scratch/nextflow_works/primary_analysis""

#run nextflow
nextflow run \
	$pipeline_name -r $pipeline_revision -hub $pipeline_hub \
	-c nextflow.config \
	-profile genfac \
	-params-file params.yaml \
	-work-dir $pipeline_work_dir \
	-resume;

#Content of params.yml
#genome: GATK.GRCh38
#igenomes_base: /processing_data/reference_datasets/iGenomes/2023.1
#input: metadata.csv
#outdir: output_nf-core@sarek_3.1.2/
#save_mapped: true
```


### Relevant files

_No response_

### System information

- nextflow v23.10.0
- sarek v3.1.2
- system: HPC 
- executor: SLURM
- container engine: singularity v3.8.5",edg1983,https://github.com/nf-core/sarek/issues/1443
I_kwDOCvwIC86DAfdQ,DragMap fails if --trim_fastq flag is enabled ,OPEN,2024-03-20T16:12:50Z,2024-08-21T13:32:02Z,,"### Description of the bug

Good evening,
I am experiencing problems with Sarek's pipeline. When I try to remove the adapters from my WES data, the dragmap tool runs into ""Segmentation Fault"". 
Do you have any idea what the problem might be?

Command line:
nextflow run main.nf
		 --input sample_sheet_3.csv
		 --genome GATK.GRCh38
		 --aligner dragmap
		 --save_output_as_bam
		 --step mapping
		 --outdir './results'
		 --save_output_as_bam
		 --wes false
		 -resume
		 -profile aws
		 --trim_fastq


### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",Ilaria-Pirona,https://github.com/nf-core/sarek/issues/1445
I_kwDOCvwIC86DGS5e,meta-sarek: One pipeline to rule them all,OPEN,2024-03-21T08:04:05Z,2024-03-21T08:04:06Z,,"### Description of feature

One Pipeline to rule them all, One [place](nf-co.re) to find them; One pipeline to bring them all and let @maxulysse  bind them.

https://nfcore.slack.com/archives/C05V9FRJYMV/p1711007143994889
Create stand alone workflows for each large part of sarek and integrate them into one large scale meta-pipeline
",matthdsm,https://github.com/nf-core/sarek/issues/1446
I_kwDOCvwIC86DJyR5,ERROR Timeout waiting for connection from pool,OPEN,2024-03-21T14:41:05Z,2024-06-12T11:57:16Z,,"### Description of the bug

Hi it is my first time trying to use sarek pipeline and in general nexflow and nf-core. I was to run only the annotation part with input vcf. 
I am geting the following error and i have no clue or idea how to solve it
Do you know any solution ? 


### Command used and terminal output

```console
`
$ nextflow run nf-core/sarek -r 3.4.0 profile test --input AC1/vcf_single_AC1.csv --outdir testResutsAC1_new --step annotate --tools snpeff

ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_RT'

Caused by:
  Timeout waiting for connection from pool


 -- Check '.nextflow.log' file for details

`

```


### Relevant files

[vcf_single_AC1.csv](https://github.com/nf-core/sarek/files/14696725/vcf_single_AC1.csv)
[AC1_HC_S2_008_HC_ignore_softclips_hg38liftover_AF005.vcf.gz](https://github.com/nf-core/sarek/files/14696727/AC1_HC_S2_008_HC_ignore_softclips_hg38liftover_AF005.vcf.gz)


### System information

Mac book Pro M2 - macOS",zisis86,https://github.com/nf-core/sarek/issues/1447
I_kwDOCvwIC86DV1j2,Sarek possibly supplying the wrong number of args for CONTROLFREEC_ASSESSSIGNIFICANCE?,OPEN,2024-03-23T00:04:34Z,2024-03-23T00:04:35Z,,"### Description of the bug

Ran into this controlfreec error trying to run sarek in paired mode:
```
Error in `$<-.data.frame`(`*tmp*`, Ratio, value = logical(0)) : 
  replacement has 0 rows, data has 828
Calls: $<- -> $<-.data.frame
Execution halted
```

I noticed that 4 files were supplied to the R script in this order:
```
cat $(which assess_significance.R) | R --slave --args PPMP022120320ANGIO_CELL_vs_PPMP022120320ANGIO_BCL.tumor.mpileup.gz_CNVs PPMP022120320ANGIO_CELL_vs_PPMP022120320ANGIO_BCL.tumor.mpileup.gz_normal_CNVs PPMP022120320ANGIO_CELL_vs_PPMP022120320ANGIO_BCL.tumor.mpileup.gz_normal_ratio.txt PPMP022120320ANGIO_CELL_vs_PPMP022120320ANGIO_BCL.tumor.mpileup.gz_ratio.txt
```

I am a bit confused by the contents of the [R script](https://github.com/BoevaLab/FREEC/blob/master/scripts/assess_significance.R), it seems to only take two tables as args and for some reason is calling from args[5] and args[4]:
```
args <- commandArgs()

dataTable <-read.table(args[5], header=TRUE);
ratio<-data.frame(dataTable)

dataTable <- read.table(args[4], header=FALSE)
cnvs<- data.frame(dataTable)
```

I suspect what is happening is that `args[5]` is returning `NA` causing issues. However, if that was the case I don't think this script would be working for anybody. I'm clearing missing something with how this script was written. Will try investigate it further by looking at runs that work, but I've already cleared out my cache for previously successful tumor-only runs.

### Command used and terminal output

_No response_

### Relevant files

[controlfreec_assesssignificance_bug.zip](https://github.com/nf-core/sarek/files/14729431/controlfreec_assesssignificance_bug.zip)


### System information

### System information

- Nextflow: `23.10.1`, `23.04.5`
- Hardware: HPC
- Executor: slurm
- Container engine: Singularity
- OS: `Linux m3-login2 3.10.0-1160.71.1.el7.x86_64 #1 SMP Tue Jun 28 15:37:28 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux`
- nf-core/sarek: `3.4.0`, `3.3.2`",SpikyClip,https://github.com/nf-core/sarek/issues/1449
I_kwDOCvwIC86D4y3k,add nf-boost,OPEN,2024-03-28T09:06:11Z,2024-03-28T12:39:32Z,,"### Description of feature

Add [nf-boost](https://github.com/bentsherman/nf-boost) to improve the codebase, especially when dealing with the csv creation (`exec` operator)
",maxulysse,https://github.com/nf-core/sarek/issues/1450
I_kwDOCvwIC86EELIa,Non-unique of `sample` values across several patients causes name clashes,OPEN,2024-03-29T16:53:19Z,2024-12-02T10:36:50Z,,"### Description of the bug

Hello, I am very grateful for your development of the Sarek pipeline. This pipeline has been very helpful to me in handling WGS analysis. However, I encountered an error when testing the pipeline with the test dataset. I would like to ask what might have caused this error.

When I provide a pair of normal and tumor data, an error occurs when calling BAM_VARIANT_CALLING_SOMATIC_ALL in the variant_calling step. The error message is as follows:
```
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:MPILEUP_NORMAL:CAT_MPILEUP (1)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:MPILEUP_NORMAL:CAT_MPILEUP` input file name collision -- There are multiple input files for each of the following file names: HCC1395T_vs_HCC1395N.mpileup.gz


Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`

 -- Check '.nextflow.log' file for details
```
And this is the sample.stomatic.csv:
```
patient,sex,status,sample,lane,fastq_1,fastq_2
HCC1395,XX,0,HCC1395N,1,./SRR7890919.10M_1.fastq.gz,./SRR7890919.10M_2.fastq.gz
HCC1395,XX,1,HCC1395T,1,./SRR7890918.10M_1.fastq.gz,./SRR7890918.10M_2.fastq.gz
```
This is the configuration file that I set up, with other parameters kept at default values:
```
params {
    config_profile_name        = 'WES Demo'
    max_cpus   = 8

    input = '/mnt/disk0/01.nf-core-pipelines/demo/sarek_3.4.0/wes.demo/sample.stomatic.csv'

    // Other params
    tools       = 'controlfreec,vep'
    split_fastq = 20000000
    intervals   = '/mnt/disk0/01.nf-core-pipelines/demo/sarek_3.4.0/wes.demo/S07604624_Padded_Agilent_SureSelectXT_allexons_V6_UTR.bed'
    wes         = true
}
```
Could you please provide valuable suggestions for this runtime error? Thank you very much!

### Command used and terminal output

```console
nextflow run ${nfcorePath}/nf-core-sarek_3.4.0/3_4_0 -profile singularity -c wes.conf --outdir ./outdir --genome GATK.GRCh38
```


### Relevant files

[nextflow.log](https://github.com/nf-core/sarek/files/14806629/nextflow.log)


### System information

- Nextflow version: 23.10.1 build 5891
- System: Linux 3.10.0-1160.108.1.el7.x86_64
- Runtime: Groovy 3.0.19 on OpenJDK 64-Bit Server VM 11.0.22+7-LTS
- Encoding: UTF-8 (ANSI_X3.4-1968)
- Version of nf-core/sarek (3.4.0)",hsk6328,https://github.com/nf-core/sarek/issues/1451
I_kwDOCvwIC86EL6sL,bwa index can not build,OPEN,2024-04-01T07:29:32Z,2024-04-01T07:29:32Z,,"### Description of the bug


![微信图片_20240401152535](https://github.com/nf-core/sarek/assets/156302339/6508bce7-5357-4185-b238-1d2d6c9af5ba)



### Command used and terminal output

```console
Workflow execution completed unsuccessfully!
The exit status of the task that caused the workflow execution to fail was: 1.

The full error message was:

Error executing process > 'NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX (wheat_AK58v4MP.genome_part.fa)'

Caused by:
  Process `NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX (wheat_AK58v4MP.genome_part.fa)` terminated with an error exit status (1)

Command executed:

  mkdir bwamem2
  bwa-mem2 \
      index \
       \
      wheat_AK58v4MP.genome_part.fa -p bwamem2/wheat_AK58v4MP.genome_part.fa
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX"":
      bwamem2: $(echo $(bwa-mem2 version 2>&1) | sed 's/.* //')
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  WARNING: Skipping mount /var/apptainer/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container
  Looking to launch executable ""/usr/local/bin/bwa-mem2.avx512bw"", simd = .avx512bw
  Launching executable ""/usr/local/bin/bwa-mem2.avx512bw""
  [bwa_index] Pack FASTA... 326.78 sec
  * Entering FMI_search
  init ticks = 2314687594798
  ref seq len = 29505445308
  binary seq ticks = 1439253015648
  Allocation of 219.83 GB for suffix_array failed.
  Current Allocation = 247.31 GB

Work dir:
  /cluster/home/fanrong/projects/AK58/input/work/15/67742430e3c44550423e1622438e63

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out
```


### Relevant files

XXX@node6 67742430e3c44550423e1622438e63]$ cat .command.sh 
#!/bin/bash -euo pipefail
mkdir bwamem2
bwa-mem2 \
    index \
     \
    wheat_AK58v4MP.genome_part.fa -p bwamem2/wheat_AK58v4MP.genome_part.fa

cat <<-END_VERSIONS > versions.yml
""NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX"":
    bwamem2: $(echo $(bwa-mem2 version 2>&1) | sed 's/.* //')
END_VERSIONS

### System information

_No response_",xiangqing111,https://github.com/nf-core/sarek/issues/1452
I_kwDOCvwIC86EWdMr,java.net.SocketTimeoutException: Read timed out when downloading reference data,OPEN,2024-04-02T13:05:54Z,2024-07-08T14:45:33Z,,"### Description of the bug

Hi, I'm trying to use Sarek from the annotation step. I'm using Linux and Conda. Each time I try, it gives me an error like this:
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF (36)'

Caused by:
  java.net.SocketTimeoutException: Read timed out


the ouput tells me to check the  '.nextflow.log', but it dosen't include what i did today
### Command used and terminal output

```console
the input:
nextflow run nf-core/sarek -r 3.4.0 -profile conda --input ./sarek_vcf.csv --outdir ./output --genome GATK.GRCh38 --step annotate --tools merge

the output:

[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                          -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM1_INDEX                          -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:BWAMEM2_INDEX                          -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                      -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY         -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                      -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                         -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_DBSNP                            -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                       -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                     -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:TABIX_PON                              -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_ALLELES                          -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_LOCI                             -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_GC                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_GENOME:UNZIP_RT                               -
[-        ] process > NFCORE_SAREK:SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED  -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP       -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM       -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:BCFTOOLS_STATS               -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_COUNT          -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_TSTV_QUAL           -
[-        ] process > NFCORE_SAREK:SAREK:VCF_QC_BCFTOOLS_VCFTOOLS:VCFTOOLS_SUMMARY             -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF    -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:TABIX_BGZIPTABIX -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_MERGE:ENSEMBLVEP_VEP    -
[-        ] process > NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_MERGE:TABIX_TABIX       -
[-        ] process > NFCORE_SAREK:SAREK:CUSTOM_DUMPSOFTWAREVERSIONS                           -
[-        ] process > NFCORE_SAREK:SAREK:MULTIQC                                               -
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_SNPEFF:SNPEFF_SNPEFF (36)'

Caused by:
  java.net.SocketTimeoutException: Read timed out


 -- Check '.nextflow.log' file for details
```

UPTADE:
i tried the -profile test and this error pop out:

""Caused by:
  Failed to create Conda environment
  command: conda env create --prefix /media/victor/c1d5c312-b546-4d5e-b24f-72dbe9e6f18f/javier_CPTAC/WGS/raw_data/work/conda/gawk-f9b4f646bf1b67e52890543d27cb2f27 --file /home/victor/.nextflow/assets/nf-core/sarek/./workflows/../subworkflows/local/prepare_intervals/../../../modules/local/create_intervals_bed/environment.yml
  status : 1
  message:
    Collecting package metadata (repodata.json): ...working... done
    Solving environment: ...working... 
    Found conflicts! Looking for incompatible packages.
    This can take several minutes.  Press CTRL-C to abort.
    
    Building graph of deps:   0%|          | 0/5 [00:00<?, ?it/s]
    Examining libgcc-ng[version='>=7.3.0']:   0%|          | 0/5 [00:00<?, ?it/s]
    Examining @/linux-64::__unix==0=0:  20%|██        | 1/5 [00:00<00:00, 34.65it/s]
    Examining @/linux-64::__glibc==2.36=0:  40%|████      | 2/5 [00:00<00:00, 69.12it/s]
    Examining @/linux-64::__archspec==1=cascadelake:  60%|██████    | 3/5 [00:00<00:00, 103.50it/s]
    Examining @/linux-64::__linux==6.1.0=0:  80%|████████  | 4/5 [00:00<00:00, 137.77it/s]         
                                                                                          
    
    Determining conflicts:   0%|          | 0/5 [00:00<?, ?it/s]
    Examining conflict for __glibc libgcc-ng:   0%|          | 0/5 [00:00<?, ?it/s]
                                                                                   
    failed
    Solving environment: ...working... 
    Found conflicts! Looking for incompatible packages.
    This can take several minutes.  Press CTRL-C to abort.
    
    Building graph of deps:   0%|          | 0/5 [00:00<?, ?it/s]
    Examining libgcc-ng[version='>=7.3.0']:   0%|          | 0/5 [00:00<?, ?it/s]
    Examining @/linux-64::__unix==0=0:  20%|██        | 1/5 [00:00<00:00, 38.52it/s]
    Examining @/linux-64::__glibc==2.36=0:  40%|████      | 2/5 [00:00<00:00, 76.77it/s]
    Examining @/linux-64::__archspec==1=cascadelake:  60%|██████    | 3/5 [00:00<00:00, 114.93it/s]
    Examining @/linux-64::__linux==6.1.0=0:  80%|████████  | 4/5 [00:00<00:00, 152.95it/s]         
                                                                                          
    
    Determining conflicts:   0%|          | 0/5 [00:00<?, ?it/s]
    Examining conflict for __glibc libgcc-ng:   0%|          | 0/5 [00:00<?, ?it/s]
                                                                                   
    failed
    
    UnsatisfiableError: The following specifications were found to be incompatible with your system:
    
      - feature:/linux-64::__glibc==2.36=0
      - libgcc-ng[version='>=7.3.0'] -> __glibc[version='>=2.17']
    
    Your installed version is: 2.36
    
    Note that strict channel priority may have removed packages required for satisfiability.



 -- Check '.nextflow.log' file for details
""
### Relevant files

_No response_

### System information

Nextflow version: 23.10.1 build 5891
System: Linux 6.1.0-12-amd64 x86_64
Version of nf-core/sarek (3.4.0)
",javierAPC,https://github.com/nf-core/sarek/issues/1453
I_kwDOCvwIC86EWu5C,vep_cache versions should probably not be integers,CLOSED,2024-04-02T13:36:24Z,2024-07-08T14:50:28Z,2024-07-08T14:50:27Z,"### Description of the bug

Hello,
I am still seeing the `vep_cache_version` as naked integers in this config
https://github.com/nf-core/sarek/blob/master/conf/igenomes.config

This has caused [problems in the past](https://github.com/nf-core/sarek/issues/518), not to mention there are often point releases in vep e.g. [110.1](https://github.com/Ensembl/ensembl-vep/releases/tag/release%2F110.1)



### Command used and terminal output

_No response_

### Relevant files

https://github.com/nf-core/sarek/blob/master/conf/igenomes.config

### System information

_No response_",leipzig,https://github.com/nf-core/sarek/issues/1454
I_kwDOCvwIC86EkQkY,Additional documentation for specifying ext.args for Mutect2 to avoid join mismatch error,OPEN,2024-04-03T23:07:53Z,2024-07-08T14:38:43Z,,"### Description of feature

I was getting a join mismatch exception when running Mutect2 workflows and specifying additional parameters. The specific error I was seeing was as follows:
`nextflow.exception.AbortOperationException: Join mismatch for the following entries: key=[id:JTI018-T-01_vs_JTI018-B-01, normal_id:JTI018-B-01, patient:JTI018, sex:NA, tumor_id:JTI018-T-01] values=`

There didn’t appear to be much information in the logs that suggested an upstream error. I’m attempting to use two additional params to mutect2 (to set up a purecn analysis):` --genotype-germline-sites true` and `--genotype-pon-sites true`. The error seemed to have something to do with them, since the workflow completed without error when they were not utilized.

Based on a suggestion in the Slack channel, it was pointed out that additional parameters need to be added to tell Mutect2 to generate the f1r2 file: 
```
{ params.ignore_soft_clipped_bases ?
                                ""--dont-use-soft-clipped-bases true --f1r2-tar-gz ${task.ext.prefix}.f1r2.tar.gz --normal-sample ${meta.patient}_${meta.normal_id}"" :
                                ""--f1r2-tar-gz ${task.ext.prefix}.f1r2.tar.gz --normal-sample ${meta.patient}_${meta.normal_id}"" }
        }
```

This fixed the error, but it was suggested that perhaps additional documentation should be put in place to articulate the case since it seems to have tripped up multiple users. Upon request, I’m opening this issue as a reminder for the team to look into documenting this.

In my case, the final process block I provided that permitted the pipeline to run without error was as follows:
```
process {
  withName: 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED' {
    ext.args = {[
       ""--genotype-germline-sites true --genotype-pon-sites true"",
   
       (params.ignore_soft_clipped_bases) ? ""--dont-use-soft-clipped-bases true --f1r2-tar-gz ${task.ext.prefix}.f1r2.tar.gz --normal-sample ${meta.patient}_${meta.normal_id}"" :
                ""--f1r2-tar-gz ${task.ext.prefix}.f1r2.tar.gz --normal-sample ${meta.patient}_${meta.normal_id}"",
    ].join(' ').trim()} 
     
  }
}
```

The log file showing the original exception is attached:
[nextflow.log](https://github.com/nf-core/sarek/files/14859427/nextflow.log)

",pulintz-um,https://github.com/nf-core/sarek/issues/1455
I_kwDOCvwIC86E-CXH,Unable to run sarek workflow in AWS health omics ,OPEN,2024-04-08T10:39:55Z,2024-07-08T14:38:23Z,,"### Description of the bug

I was following the tutorial https://catalog.us-east-1.prod.workshops.aws/workshops/76d4a4ff-fe6f-436a-a1c2-f7ce44bc5d17/en-US/workshop/test-workflow and I managed to create my workflow in AWS Helathomics. Everything is correctly done but when I am trying to run the workflow in healthomics it is not working  
 I am receiving strange kinds of errors related to the input parameters or samplesheet.I changed the parameters and I removed the -- so now I am getting the following. Do you have any idea what can be wrong? 

### Command used and terminal output

```console
WARNING: Could not load nf-core/config profiles: https://raw.githubusercontent.com/nf-core/configs/master/nfcore_custom.config
WARNING: Could not load nf-core/config/sarek profiles: https://raw.githubusercontent.com/nf-core/configs/master/pipeline/sarek.config
Launching `sarek/main.nf` [prickly_mirzakhani] DSL2 - revision: 48ed053221
Module path must start with / or ./ prefix -- Offending module: plugin/nf-validation,  -- Check script 'sarek/main.nf' at line: 79 or see '.nextflow.log' file for more details
Nextflow engine failed
```


### Relevant files

_No response_

### System information

_No response_",DimitrisZisis,https://github.com/nf-core/sarek/issues/1458
I_kwDOCvwIC86FnLGu,This tool requires AVX instruction set support by default,CLOSED,2024-04-13T16:09:15Z,2024-04-18T14:15:55Z,2024-04-18T14:15:55Z,"### Description of the bug

Hi there,

I am trying to run sarek on a cluster using slurm, but keep getting the error below from a couple of tools like deepvariant and some others 

      
I am not sure which tensorflow is it refering to within the container or in the host machine, or whether or not this error message is related to tensorflow at all. How can we set disable-avc-check within a config file top bypass this ?

I have the most recent version of tensorflow installed on all my compute nodes in the cluster.
 
 Best,

Rad

  
  

### Command used and terminal output
Command error:
  WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.
  16:00:43.967 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
  16:00:44.063 INFO  CNNScoreVariants - ------------------------------------------------------------
  16:00:44.069 INFO  CNNScoreVariants - The Genome Analysis Toolkit (GATK) v4.4.0.0
  16:00:44.069 INFO  CNNScoreVariants - For support and documentation go to https://software.broadinstitute.org/gatk/
  16:00:44.069 INFO  CNNScoreVariants - Executing as ranbiolinks@31d54fce75d5 on Linux v5.4.0-176-generic amd64
  16:00:44.069 INFO  CNNScoreVariants - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10-Ubuntu-0ubuntu118.04.1
  16:00:44.070 INFO  CNNScoreVariants - Start Date/Time: April 13, 2024 at 4:00:43 PM GMT
  16:00:44.070 INFO  CNNScoreVariants - ------------------------------------------------------------
  16:00:44.070 INFO  CNNScoreVariants - ------------------------------------------------------------
  16:00:44.071 INFO  CNNScoreVariants - HTSJDK Version: 3.0.5
  16:00:44.072 INFO  CNNScoreVariants - Picard Version: 3.0.0
  16:00:44.072 INFO  CNNScoreVariants - Built for Spark Version: 3.3.1
  16:00:44.073 INFO  CNNScoreVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  16:00:44.073 INFO  CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  16:00:44.073 INFO  CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  16:00:44.074 INFO  CNNScoreVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  16:00:44.074 INFO  CNNScoreVariants - Deflater: IntelDeflater
  16:00:44.074 INFO  CNNScoreVariants - Inflater: IntelInflater
  16:00:44.075 INFO  CNNScoreVariants - GCS max retries/reopens: 20
  16:00:44.075 INFO  CNNScoreVariants - Requester pays: disabled
  16:00:44.076 INFO  CNNScoreVariants - Initializing engine
  16:00:44.443 INFO  FeatureManager - Using codec VCFCodec to read file file://074-24.haplotypecaller.vcf.gz
  16:00:44.724 INFO  FeatureManager - Using codec BEDCodec to read file file://Twist_Exome_RefSeq_targets_hg38_200-pad_wGenes.bed
  16:00:45.527 INFO  IntervalArgumentCollection - Processing 113689316 bp from intervals
  16:00:45.605 INFO  CNNScoreVariants - Done initializing engine
  16:00:45.607 INFO  NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so
  16:00:45.629 INFO  CNNScoreVariants - Done scoring variants with CNN.
  16:00:45.629 INFO  CNNScoreVariants - Shutting down engine
  [April 13, 2024 at 4:00:45 PM GMT] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.03 minutes.
  Runtime.totalMemory()=260046848
  ***********************************************************************
  
  A USER ERROR has occurred: This tool requires AVX instruction set support by default due to its dependency on recent versions of the TensorFlow library.
   If you have an older (pre-1.6) version of TensorFlow installed that does not require AVX you may attempt to re-run the tool with the disable-avx-check argument to bypass this check.
   Note that such configurations are not officially supported.
  
  ***********************************************************************
  Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.
  Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx9830M -XX:-UsePerfData -jar /gatk/gatk-package-4.4.0.0-local.jar CNNScoreVariants --variant 074-24.haplotypecaller.vcf.gz --output 074-24.cnn.vcf.gz --reference Homo_sapiens_assembly38.fasta --intervals Twist_Exome_RefSeq_targets_hg38_200-pad_wGenes.bed --tmp-dir .

### Relevant files

_No response_

### System information

sarek 3.4.0",radaniba,https://github.com/nf-core/sarek/issues/1464
I_kwDOCvwIC86F-MGO,Clarify how to use Mutect2 with custom bam files,CLOSED,2024-04-17T08:14:54Z,2024-07-08T14:39:40Z,2024-07-08T14:39:39Z,"### Description of the bug

Hi, I run into the following error with running mutect2 via sarek:

```
INFO:    Converting SIF file to temporary sandbox...
Using GATK jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar
Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx29491M -XX:-UsePerfData -jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar Mutect2 --input patient1-10A.converted.cram --input patient1-01A.converted.cram --output patient1-01A_vs_patient1-10A.mutect2.vcf.gz --reference GRCh38.d1.vd1.fa --intervals chr17_7651780-7697538.bed --tmp-dir . --f1r2-tar-gz patient1-01A_vs_patient1-10A.mutect2.f1r2.tar.gz --normal-sample patient1_patient1-10A
15:23:13.503 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
15:23:13.529 INFO  Mutect2 - ------------------------------------------------------------
15:23:13.531 INFO  Mutect2 - The Genome Analysis Toolkit (GATK) v4.4.0.0
15:23:13.531 INFO  Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/
15:23:13.531 INFO  Mutect2 - Executing as wij20zul@hilbert166 on Linux v3.10.0-1160.49.1.el7.x86_64 amd64
15:23:13.531 INFO  Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src
15:23:13.532 INFO  Mutect2 - Start Date/Time: April 15, 2024 at 3:23:13 PM GMT
15:23:13.532 INFO  Mutect2 - ------------------------------------------------------------
15:23:13.532 INFO  Mutect2 - ------------------------------------------------------------
15:23:13.532 INFO  Mutect2 - HTSJDK Version: 3.0.5
15:23:13.532 INFO  Mutect2 - Picard Version: 3.0.0
15:23:13.533 INFO  Mutect2 - Built for Spark Version: 3.3.1
15:23:13.533 INFO  Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2
15:23:13.533 INFO  Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
15:23:13.533 INFO  Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
15:23:13.533 INFO  Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
15:23:13.533 INFO  Mutect2 - Deflater: IntelDeflater
15:23:13.533 INFO  Mutect2 - Inflater: IntelInflater
15:23:13.533 INFO  Mutect2 - GCS max retries/reopens: 20
15:23:13.533 INFO  Mutect2 - Requester pays: disabled
15:23:13.534 INFO  Mutect2 - Initializing engine
15:23:14.107 INFO  FeatureManager - Using codec BEDCodec to read file chr17_7651780-7697538.bed
15:23:14.111 INFO  IntervalArgumentCollection - Processing 129382 bp from intervals
15:23:14.119 INFO  Mutect2 - Done initializing engine
15:23:14.149 INFO  Mutect2 - Shutting down engine
[April 15, 2024 at 3:23:14 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.01 minutes.
Runtime.totalMemory()=285212672
***********************************************************************

A USER ERROR has occurred: Bad input: Sample patient1_patient1-10A is not in BAM header: [patient1-01A-31W-A062-09, patient1-10A-01W-A062-09]

***********************************************************************
Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.
INFO:    Cleaning up image...
```

It seems that there is something wrong with the mutect2 command in this configuration.
It defines a normal sample which cannot be found by mutect2 (from the `.command.sh`):

```
#!/bin/bash -euo pipefail
gatk --java-options ""-Xmx29491M -XX:-UsePerfData"" \
    Mutect2 \
    --input patient1-10A.converted.cram --input patient1-01A.converted.cram \
    --output patient1-01A_vs_patient1-10A.mutect2.vcf.gz \
    --reference GRCh38.d1.vd1.fa \
     \
     \
    --intervals chr17_7651780-7697538.bed \
    --tmp-dir . \
    --f1r2-tar-gz patient1-01A_vs_patient1-10A.mutect2.f1r2.tar.gz --normal-sample patient1_patient1-10A

cat <<-END_VERSIONS > versions.yml
""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED"":
    gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
END_VERSIONS
```

Freebayes and strelka run fine without an error message.

I have the following configuration:
- Starting variant calling from aligned BAM files.
- always having tumor-normal pairs
- targeted sequencing

Can you help me fixing this problem?


### Command used and terminal output

```console
nf-params.json:

{
    ""step"": ""variant_calling"",
    ""outdir"": ""results/"",
    ""wes"": true,
    ""intervals"": ""targets.bed"",
    ""tools"": ""msisensorpro,mutect2,freebayes,strelka"",
    ""genome"": ""GRCh38"",
    ""fasta"": ""GRCh38.d1.vd1.fa"",
    ""fasta_fai"": ""GRCh38.d1.vd1.fa.fai"",
    ""save_reference"": true,
    ""save_mapped"": false,
    ""igenomes_ignore"": true,
}
```

samplesheet (example):
```
patient,status,sample,bam,bai
patient1,1,patient1-01A,sample1_wxs_gdc_realn.bam,sample1_wxs_gdc_realn.bai
patient1,0,patient1-10A,sample2_wxs_gdc_realn.bam,sample2_wxs_gdc_realn.bai
```

run command:
```
nextflow run nf-core-sarek_3.4.0/3_4_0 \
-profile singularity,hilbert \
-c config/run-sarek.config \
-params-file config/nf-params.json \
-plugins nf-validation@1.0.0 \
--input samplesheet.csv
```
```


### Relevant files

_No response_

### System information

nf-core sarek verison 3.4.0",sci-kai,https://github.com/nf-core/sarek/issues/1468
I_kwDOCvwIC86GdXpL," A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""[]"" is not valid for this input.",CLOSED,2024-04-22T07:53:36Z,2024-04-22T07:54:10Z,2024-04-22T07:54:10Z,"### Description of the bug

I don't have interval file , I hope use GATK CombineGVCFs options , I would like to ask how to use this feature , thank you!

### Command used and terminal output

```console
The exit status of the task that caused the workflow execution to fail was: 2.

The full error message was:

Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOMICSDBIMPORT (joint_variant_calling)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOMICSDBIMPORT (joint_variant_calling)` terminated with an error exit status (2)

Command executed:

  gatk --java-options ""-Xmx29491M -XX:-UsePerfData"" \
      GenomicsDBImport \
      --variant ZS.haplotypecaller.g.vcf.gz \
      --genomicsdb-workspace-path [].joint \
      --intervals [] \
      --tmp-dir . \
      --genomicsdb-shared-posixfs-optimizations true --bypass-feature-reader
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOMICSDBIMPORT"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  2

Command output:
  (empty)

Command error:
  INFO:    Environment variable SINGULARITYENV_TMPDIR is set, but APPTAINERENV_TMPDIR is preferred
  INFO:    Environment variable SINGULARITYENV_NXF_TASK_WORKDIR is set, but APPTAINERENV_NXF_TASK_WORKDIR is preferred
  INFO:    Environment variable SINGULARITYENV_NXF_DEBUG is set, but APPTAINERENV_NXF_DEBUG is preferred
  Using GATK jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx29491M -XX:-UsePerfData -jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport --variant ZS.haplotypecaller.g.vcf.gz --genomicsdb-workspace-path [].joint --intervals [] --tmp-dir . --genomicsdb-shared-posixfs-optimizations true --bypass-feature-reader
  17:50:53.308 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
  17:50:53.401 INFO  GenomicsDBImport - ------------------------------------------------------------
  17:50:53.404 INFO  GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0
  17:50:53.404 INFO  GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/
  17:50:53.405 INFO  GenomicsDBImport - Executing as fanrong@node8 on Linux v4.18.0-477.15.1.el8_8.x86_64 amd64
  17:50:53.405 INFO  GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src
  17:50:53.405 INFO  GenomicsDBImport - Start Date/Time: April 21, 2024 at 5:50:53 PM GMT
  17:50:53.405 INFO  GenomicsDBImport - ------------------------------------------------------------
  17:50:53.405 INFO  GenomicsDBImport - ------------------------------------------------------------
  17:50:53.405 INFO  GenomicsDBImport - HTSJDK Version: 3.0.5
  17:50:53.405 INFO  GenomicsDBImport - Picard Version: 3.0.0
  17:50:53.406 INFO  GenomicsDBImport - Built for Spark Version: 3.3.1
  17:50:53.406 INFO  GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  17:50:53.406 INFO  GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  17:50:53.406 INFO  GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  17:50:53.406 INFO  GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  17:50:53.406 INFO  GenomicsDBImport - Deflater: IntelDeflater
  17:50:53.406 INFO  GenomicsDBImport - Inflater: IntelInflater
  17:50:53.407 INFO  GenomicsDBImport - GCS max retries/reopens: 20
  17:50:53.407 INFO  GenomicsDBImport - Requester pays: disabled
  17:50:53.407 INFO  GenomicsDBImport - Initializing engine
  17:50:53.933 INFO  GenomicsDBImport - Shutting down engine
  [April 21, 2024 at 5:50:53 PM GMT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.
  Runtime.totalMemory()=671088640
  ***********************************************************************
  
  A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""[]"" is not valid for this input.
  
  ***********************************************************************
  Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.

Work dir:
  /cluster/home/fanrong/work_lei/BSA/02_ZS/02_mapping_1/work/84/8a56d8c07a501d01a91e23ba41c7e1

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`
```


### Relevant files

_No response_

### System information

_No response_",wheat2024,https://github.com/nf-core/sarek/issues/1474
I_kwDOCvwIC86GdY6p," A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""[]"" is not valid for this input.",CLOSED,2024-04-22T07:56:12Z,2024-05-29T01:09:42Z,2024-05-29T01:09:42Z,"### Description of the bug

I don't have interval file , I hope use GATK CombineGVCFs options , I would like to ask how to use this feature , thank you!

### Command used and terminal output

```console
The exit status of the task that caused the workflow execution to fail was: 2.

The full error message was:

Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOMICSDBIMPORT (joint_variant_calling)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOMICSDBIMPORT (joint_variant_calling)` terminated with an error exit status (2)

Command executed:

  gatk --java-options ""-Xmx29491M -XX:-UsePerfData"" \
      GenomicsDBImport \
      --variant ZS.haplotypecaller.g.vcf.gz \
      --genomicsdb-workspace-path [].joint \
      --intervals [] \
      --tmp-dir . \
      --genomicsdb-shared-posixfs-optimizations true --bypass-feature-reader
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOMICSDBIMPORT"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  2

Command output:
  (empty)

Command error:
  INFO:    Environment variable SINGULARITYENV_TMPDIR is set, but APPTAINERENV_TMPDIR is preferred
  INFO:    Environment variable SINGULARITYENV_NXF_TASK_WORKDIR is set, but APPTAINERENV_NXF_TASK_WORKDIR is preferred
  INFO:    Environment variable SINGULARITYENV_NXF_DEBUG is set, but APPTAINERENV_NXF_DEBUG is preferred
  Using GATK jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx29491M -XX:-UsePerfData -jar /usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar GenomicsDBImport --variant ZS.haplotypecaller.g.vcf.gz --genomicsdb-workspace-path [].joint --intervals [] --tmp-dir . --genomicsdb-shared-posixfs-optimizations true --bypass-feature-reader
  17:50:53.308 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.4.0.0-0/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
  17:50:53.401 INFO  GenomicsDBImport - ------------------------------------------------------------
  17:50:53.404 INFO  GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.4.0.0
  17:50:53.404 INFO  GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/
  17:50:53.405 INFO  GenomicsDBImport - Executing as fanrong@node8 on Linux v4.18.0-477.15.1.el8_8.x86_64 amd64
  17:50:53.405 INFO  GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v17.0.3-internal+0-adhoc..src
  17:50:53.405 INFO  GenomicsDBImport - Start Date/Time: April 21, 2024 at 5:50:53 PM GMT
  17:50:53.405 INFO  GenomicsDBImport - ------------------------------------------------------------
  17:50:53.405 INFO  GenomicsDBImport - ------------------------------------------------------------
  17:50:53.405 INFO  GenomicsDBImport - HTSJDK Version: 3.0.5
  17:50:53.405 INFO  GenomicsDBImport - Picard Version: 3.0.0
  17:50:53.406 INFO  GenomicsDBImport - Built for Spark Version: 3.3.1
  17:50:53.406 INFO  GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  17:50:53.406 INFO  GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  17:50:53.406 INFO  GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  17:50:53.406 INFO  GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  17:50:53.406 INFO  GenomicsDBImport - Deflater: IntelDeflater
  17:50:53.406 INFO  GenomicsDBImport - Inflater: IntelInflater
  17:50:53.407 INFO  GenomicsDBImport - GCS max retries/reopens: 20
  17:50:53.407 INFO  GenomicsDBImport - Requester pays: disabled
  17:50:53.407 INFO  GenomicsDBImport - Initializing engine
  17:50:53.933 INFO  GenomicsDBImport - Shutting down engine
  [April 21, 2024 at 5:50:53 PM GMT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.
  Runtime.totalMemory()=671088640
  ***********************************************************************
  
  A USER ERROR has occurred: Badly formed genome unclippedLoc: Query interval ""[]"" is not valid for this input.
  
  ***********************************************************************
  Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.

Work dir:
  /cluster/home/fanrong/work_lei/BSA/02_ZS/02_mapping_1/work/84/8a56d8c07a501d01a91e23ba41c7e1

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`
```


### Relevant files

_No response_

### System information

_No response_",fan040,https://github.com/nf-core/sarek/issues/1475
I_kwDOCvwIC86GdaPz,bwa index,CLOSED,2024-04-22T07:58:23Z,2024-04-22T08:39:55Z,2024-04-22T08:08:23Z,"### Description of feature

My reference genome is very large and it will take a long time for bwa to set up the index. I hope you can consider adding a parameter to specify the index file.",fan040,https://github.com/nf-core/sarek/issues/1476
I_kwDOCvwIC86GrwiS,Missing input validation: Error handle for sentieon-dedup with non-sentieon aligner,OPEN,2024-04-23T19:28:21Z,2024-12-09T16:22:16Z,,"### Description of the bug

There is nothing preventing the user from selecting the tool `sentieon_dedup` with a non-sentieon aligner, that combination however causes the pipeline to crash.

https://nfcore.slack.com/archives/CGFUX04HZ/p1713879506993449

No user has requested that it should be possible to run such a combination, but the ""bug"" did pop up during the test done by a user.

Unless the combination is requested by a user, let's just have the pipeline stop with a suitable error message on such a combination of tool and aligner.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1480
I_kwDOCvwIC86HQd7X,"GCP error in v.3.4.1 ""Cloud Storage objects are immutable""",CLOSED,2024-04-29T14:45:40Z,2024-07-08T14:32:09Z,2024-07-08T14:32:09Z,"### Description of the bug

Got the error:
```
Apr-24 16:07:17.072 [Actor Thread 40] ERROR nextflow.extension.OperatorImpl - @unknown
com.google.cloud.storage.contrib.nio.CloudStorageObjectImmutableException: Cloud Storage objects are immutable.
	at com.google.cloud.storage.contrib.nio.CloudStorageFileAttributeView.setTimes(CloudStorageFileAttributeView.java:65)
	at java.base/java.nio.file.CopyMoveHelper.copyToForeignTarget(CopyMoveHelper.java:135)
	at java.base/java.nio.file.CopyMoveHelper.moveToForeignTarget(CopyMoveHelper.java:157)
	at java.base/java.nio.file.Files.move(Files.java:1435)
	at nextflow.file.SimpleFileCollector.saveFile(SimpleFileCollector.groovy:102)
	at nextflow.file.FileCollector.saveTo0(FileCollector.groovy:228)
```
the pipeline has been launched with a very simple test command (I’ve hidden some parts of the identifiers).
conversation in Slack:
https://nfcore.slack.com/archives/C05V9FRJYMV/p1713976551898249

### Command used and terminal output

```console
nextflow run nf-core/sarek -c ../***_profile.config -profile gls,test --outdir .

profiles {
    gls {
        process.executor = 'google-batch'
        workDir = 'gs://****-data/work'
        google.location = 'europe-west4'
        google.region  = 'europe-west4'
        google.project = '*****'
        google.batch.usePrivateAddress = 'true'
        google.batch.spot = true
        fusion.enabled = true
        wave.enabled = true
        process.scratch = false
    }
}
process {
  errorStrategy = { task.exitStatus in [1,143,137,104,134,139,255,108] ? 'retry' : 'finish' }
  maxRetries = 4
  maxErrors = '-1'
}
```


### Relevant files

_No response_

### System information

Google Cloud Batch
nf-core/sarek v3.4.1-gea88402
nextflow 23.10.1
same error when launched from GCP console CLI or when launched from a small VM with CentOS 7",lescai,https://github.com/nf-core/sarek/issues/1491
I_kwDOCvwIC86HXZPZ,Error creating symlinks to BWAIndex,OPEN,2024-04-30T10:09:59Z,2024-04-30T10:10:00Z,,"### Description of the bug

An execution failed in the `FASTQ_CREATE_UMI_CONSENSUS_FGBIO:ALIGN_UMI:BWAMEM1_MEM` process since symlinks to BWAIndex where pointing to a different path from the path where this index was actually downloaded.

Symlinks in the workDir where pointing to: `/work/stage/8f/b099e803ad6802621e8d1e1fdd38c7/BWAIndex`
And this actually downloaded in: `/work/stage/a5/bb9f3e3f91928e18c24b2e256655e6/`

![image](https://github.com/nf-core/sarek/assets/60278027/297714f4-0d99-4fec-a1d0-cb41e978dd53)

It has happened twice on this HPC system. Relaunching the execution with `-resume` seems to solve the problem.

Issue #340 might be related??

### Command used and terminal output

```console
command:
nextflow run nf-core/sarek -r 3.1.2 -profile singularity -name tanda_completa -params-file /mnt/zonahpc/home/bioinformatica/comun/pruebas_borrar/test_sarek3/conf/nf-local-params_completa.yaml -c /mnt/zonahpc/home/bioinformatica/comun/pruebas_borrar/test_sarek3/conf/nextflow_fran.config

error output:
Workflow execution completed unsuccessfully!
The exit status of the task that caused the workflow execution to fail was: 1.

The full error message was:

Error executing process > 'NFCORE_SAREK:SAREK:FASTQ_CREATE_UMI_CONSENSUS_FGBIO:ALIGN_UMI:BWAMEM1_MEM (113-ffpe1-1)'

Caused by:
  Process `NFCORE_SAREK:SAREK:FASTQ_CREATE_UMI_CONSENSUS_FGBIO:ALIGN_UMI:BWAMEM1_MEM (113-ffpe1-1)` terminated with an error exit status (1)

Command executed:

  INDEX=`find -L ./ -name ""*.amb"" | sed 's/.amb//'`
  
  bwa mem \
      -K 100000000 -p -C -Y -R ""@RG\tID:HVTWYDSX3.113-ffpe1.1\tPU:1\tSM:113_113-ffpe1\tLB:113-ffpe1\tDS:s3://ngi-igenomes/igenomes/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta\tPL:ILLUMINA"" \
      -t 16 \
      $INDEX \
      113-ffpe1-1_interleaved.fq.gz \
      | samtools view -bS --threads 16 -o 113-ffpe1-1.umi_unsorted.bam -
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:FASTQ_CREATE_UMI_CONSENSUS_FGBIO:ALIGN_UMI:BWAMEM1_MEM"":
      bwa: $(echo $(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*$//')
      samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
         -y INT        seed occurrence for the 3rd round seeding [20]
         -c INT        skip seeds with more than INT occurrences [500]
         -D FLOAT      drop chains shorter than FLOAT fraction of the longest overlapping chain [0.50]
         -W INT        discard a chain if seeded bases shorter than INT [0]
         -m INT        perform at most INT rounds of mate rescues for each read [50]
         -S            skip mate rescue
         -P            skip pairing; mate rescue performed unless -S also in use
  
  Scoring options:
  
         -A INT        score for a sequence match, which scales options -TdBOELU unless overridden [1]
         -B INT        penalty for a mismatch [4]
         -O INT[,INT]  gap open penalties for deletions and insertions [6,6]
         -E INT[,INT]  gap extension penalty; a gap of size k cost '{-O} + {-E}*k' [1,1]
         -L INT[,INT]  penalty for 5'- and 3'-end clipping [5,5]
         -U INT        penalty for an unpaired read pair [17]
  
         -x STR        read type. Setting -x changes multiple parameters unless overridden [null]
                       pacbio: -k17 -W40 -r10 -A1 -B1 -O1 -E1 -L0  (PacBio reads to ref)
                       ont2d: -k14 -W20 -r10 -A1 -B1 -O1 -E1 -L0  (Oxford Nanopore 2D-reads to ref)
                       intractg: -B9 -O16 -L5  (intra-species contigs to ref)
  
  Input/output options:
  
         -p            smart pairing (ignoring in2.fq)
         -R STR        read group header line such as '@RG\tID:foo\tSM:bar' [null]
         -H STR/FILE   insert STR to header if it starts with @; or insert lines in FILE [null]
         -o FILE       sam file to output results to [stdout]
         -j            treat ALT contigs as part of the primary assembly (i.e. ignore .alt file)
         -5            for split alignment, take the alignment with the smallest coordinate as primary
         -q            don't modify mapQ of supplementary alignments
         -K INT        process INT input bases in each batch regardless of nThreads (for reproducibility) []
  
         -v INT        verbosity level: 1=error, 2=warning, 3=message, 4+=debugging [3]
         -T INT        minimum score to output [30]
         -h INT[,INT]  if there are 80% of the max score, output all in XA [5,200]
         -a            output all alignments for SE or unpaired PE
         -C            append FASTA/FASTQ comment to SAM output
         -V            output the reference FASTA header in the XR tag
         -Y            use soft clipping for supplementary alignments
         -M            mark shorter split hits as secondary
  
         -I FLOAT[,FLOAT[,INT[,INT]]]
                       specify the mean, standard deviation (10% of the mean if absent), max
                       (4 sigma from the mean if absent) and min of the insert size distribution.
                       FR orientation only. [inferred]
  
  Note: Please read the man page for detailed description of the command line and options.
  
  [main_samview] fail to read the header from ""-"".

Work dir:
  /mnt/zonahpc/home/bioinformatica/comun/pruebas_borrar/test_sarek3/log_completa/work/64/9eb7698aa6f87ca16ef989cd20792f

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`
```


### Relevant files

config file:
// Se explicita executor y cola para todos los procesos
process {
  executor   = ""slurm""
  queue      = ""cpu""
  maxRetries = 2

// Aumenta el tiempo de ejecución para procesos con label process_low
  withLabel:process_low {
    time = ""20h""
  }

  // Aumenta el tiempo de ejecución para procesos con label process_medium
  withLabel:process_medium {
    time = ""12h""
  }

// Aumenta la memoria para gatk baserecalibrator
  withName: GATK4_BASERECALIBRATOR {
    memory =  ""8GB""
  }

// Aumenta la memoria para gatk applybqsr
  withName: GATK4_APPLYBQSR {
    memory =  ""12GB""
  }

// Aumenta la memoria para samtools collatefastq
  withName: SAMTOOLS_COLLATEFASTQ {
    memory =  ""18GB""
  }

// Aumenta la memoria para fgbio callmolecularconsesus
  withName: FGBIO_CALLMOLECULARCONSENSUSREADS {
    memory =  ""90GB""
  }

// Aumenta la memoria para fgbio group reads by umi
  withName: FGBIO_GROUPREADSBYUMI {
    memory =  ""100GB""
  }
}

// Caché de singularity
singularity.cacheDir = ""/mnt/zonahpc/home/bioinformatica/comun/pruebas_borrar/test_sarek3/singularity_cacheDir""

params file:
input: /mnt/zonahpc/home/bioinformatica/comun/pruebas_borrar/test_sarek3/conf/samplesheet_completa.csv
outdir: /mnt/zonahpc/home/bioinformatica/comun/pruebas_borrar/test_sarek3/results_completa
genome: GATK.GRCh38
wes: true
intervals: /mnt/zonahpc/home/bioinformatica/comun/panel_designs/HyperExome/target.bed
trim_fastq: true
umi_read_structure: 9M151T 151T
vep_include_fasta: true


### System information

Nextflow version 22.10.1.5828
Hardware: HPC
Executor: slurm
Container engine: Singularity",nevinwu,https://github.com/nf-core/sarek/issues/1492
I_kwDOCvwIC86HpDwO,Sarek 3.4.1 GATK4_HAPLOTYPECALLER will only run first intervals. Join mismatch error when run the second,CLOSED,2024-05-02T14:28:48Z,2024-05-07T12:26:14Z,2024-05-07T12:25:45Z,"### Description of the bug

If run GATK4_HAPLOTYPECALLER in 3.4.1, you will get only first one intervals GVCF. Error will occur when process the second intervals GVCF when run GATK4_HAPLOTYPECALLER:
```
Join mismatch for the following entries: 
- key=[patient:sample01, sample: sample01, sex:XX, status:0, n_fastq:24, data_type:cram, id:sample01, interval_name:chrX_37285838-49348394, num_intervals:21, variantcaller:haplotypecaller] values=[/work/7f/fd03e6705a9ffa0f345db370a3ba8b/sample01.recal.cram, /work/a9/f29835a7281a84dded8bec1f5dedcf/sample01.recal.cram.crai, /work/da/bd30415f8dc9e4cde745fd7ca13282/chrX_37285838-49348394.bed, []] 
- key=[patient:sample02, sample: sample02, sex:XY, status:0, n_fastq:12, data_type:cram, id:sample02, interval_name:chr18_47019913-54536574, num_intervals:21, variantcaller:haplotypecaller] values=[/work/76/5a715b39c519cb83d8c3349d8b8fa2/sample02.recal.cram, //work/6b/f73a30e3bbf9f8a824ae1c59c9022d/sample02.recal.cram.crai, /work/da/bd30415f8dc9e4cde745fd7ca13282/chr18_47019913-54536574.bed, []] 
(more omitted)
```
This error don't occur in 3.4.0, I think it related to the change of GATK4_HAPLOTYPECALLER parameter structure.
3.4.1:
https://github.com/nf-core/sarek/blob/ea88402912c329b4fd234ad07294ce05bbd2590c/subworkflows/local/bam_variant_calling_haplotypecaller/main.nf#L33-L39
https://github.com/nf-core/sarek/blob/ea88402912c329b4fd234ad07294ce05bbd2590c/modules/nf-core/gatk4/haplotypecaller/main.nf#L10-L16
3.4.0
https://github.com/nf-core/sarek/blob/9d15b23ae4ed3cba6eeb95244df016391965b005/modules/nf-core/gatk4/haplotypecaller/main.nf#L10-L16
I change 3.4.1 code to the below, solving the join mismatch error and process all intervals GVCF
```
GATK4_HAPLOTYPECALLER(
        cram_intervals,
        fasta.first(),
        fasta_fai.first(),
        dict.first(),
        dbsnp.first(),
        dbsnp_tbi.first())
```",bnjvrjnke,https://github.com/nf-core/sarek/issues/1495
I_kwDOCvwIC86H25vB,Region 536904516..536917009 cannot be stored in a tbi index,CLOSED,2024-05-05T04:23:51Z,2024-05-20T12:40:06Z,2024-05-20T12:40:06Z,"### Description of the bug

I currently have an error that my reference genome is about 14Gb, causing my bed file to be too large to establish tbi index, I would like to know how to modify it to establish csi index, thank you for your help.

### Command used and terminal output

```console
Command error:
  INFO:    Environment variable SINGULARITYENV_TMPDIR is set, but APPTAINERENV_TMPDIR is preferred
  INFO:    Environment variable SINGULARITYENV_NXF_TASK_WORKDIR is set, but APPTAINERENV_NXF_TASK_WORKDIR is preferred
  INFO:    Environment variable SINGULARITYENV_NXF_DEBUG is set, but APPTAINERENV_NXF_DEBUG is preferred
  [E::hts_idx_check_range] Region 536904516..536917009 cannot be stored in a tbi index. Try using a csi index
  tbx_index_build failed: up_down_2k_promoters.all.bed.gz

Work dir:
  /cluster/home/fanrong/work_lei/BSA/02_ZS/02_mapping_1/work/f9/901b8ac1827e6196b65b21ba97e017

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`

 -- Check '.nextflow.log' file for details
```


### Relevant files

_No response_

### System information

_No response_",fan040,https://github.com/nf-core/sarek/issues/1500
I_kwDOCvwIC86H9Gl0,same sample name over multiple patient doest not fail input schema validation,OPEN,2024-05-06T13:55:01Z,2024-07-08T14:30:44Z,,"### Description of the bug

This is the output that is seen on the terminal once the pipeline has failed after `GATK4_MARKDUPLICATES`, my guess is that one of the later join operator is causing the subsequent failure:
```
Detected join operation duplicate emission on left channel -- offending element: key=[patient:test2, sample:test, sex:XX, status:0, n_fastq:1, data_type:bam, id:test]; value=/home/max/workspace/sarek/work/fe/2e8890cae572ee686c7475edd6e895/test.md.cram
```

We should really fail early for that.

> Issue reported by [Ist4lri](https://github.com/Ist4lri)

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1503
I_kwDOCvwIC86H9KRo,mispelled field in the samplesheet does not raise an issue,CLOSED,2024-05-06T14:02:17Z,2024-05-06T15:27:41Z,2024-05-06T15:07:11Z,"### Description of the bug

Using `satus` instead of `status` does not fail.
That could be problematic when one expect status to be used.

@mirpedrol any idea on how to catch that?

> Issue reported by [Ist4lri](https://github.com/Ist4lri)

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1504
I_kwDOCvwIC86IEfVH,Copy-Number Enhancements - Ascat penality,OPEN,2024-05-07T10:26:37Z,2024-08-19T13:12:39Z,,"### Description of feature

Hi there.
I think Sarek is in a good place with the current ASCAT implementation, but I'd like to point towards two things that I think would be helpful for the community.

- The function `ascat.aspcf()` (found in the [ascat module](https://github.com/nf-core/sarek/blob/3.4.1/modules/nf-core/ascat/main.nf)) accepts a parameter `penalty` which controls the sensitivity of the tool to potential segment breakpoints. The default is 70 which is _usually fine,_ but for individuals using panel/wes data being able to change this value might be helpful - any chance of exposing this as a `param`? 

Kind regards,
George

EDIT by @maxulysse - created https://github.com/nf-core/sarek/issues/1512",seedgeorge,https://github.com/nf-core/sarek/issues/1510
I_kwDOCvwIC86IE6te,primer trimming,CLOSED,2024-05-07T11:12:46Z,2024-07-08T14:30:27Z,2024-07-08T14:30:27Z,"### Description of feature

Hi, should i do the primer trimming before i run Sarek?
for example, 
cutadapt -a <primer_sequence> -o <output_file.fastq> <input_file.fastq>

Thank you,
All the best
Lin
",aihualin,https://github.com/nf-core/sarek/issues/1511
I_kwDOCvwIC86IFEQr,Copy-Number Enhancements - Ascat joint calling,OPEN,2024-05-07T11:33:09Z,2024-08-19T13:12:39Z,,"### Description of feature

From @seedgeorge

> Hi there.
> I think Sarek is in a good place with the current ASCAT implementation, but I'd like to point towards two things that I think would be helpful for the community.
> 
> - A bigger request here, but one that I think is an obvious next step for the pipeline. Sarek currently supports multi-sample calling with Mutect2 (hooray!) and ASCAT also supports this feature, with the paper [here](https://pubmed.ncbi.nlm.nih.gov/32449758/) and documentation [here](https://github.com/VanLoo-lab/ascat/blob/master/ASCAT/vignettes/asmultipcf-vignette.Rmd). It effectively uses both the logr and BAF tracks across all same-patient tumour samples to jointly find segments, helping a) breakpoint accuracy (particularly with smaller segments) and b) downstream interpretation of CNA data.
> 
> Kind regards,
> George",maxulysse,https://github.com/nf-core/sarek/issues/1512
I_kwDOCvwIC86IG32O,"Giving a user fasta file, but keeping all default fil path",CLOSED,2024-05-07T14:22:29Z,2024-07-08T14:45:03Z,2024-07-08T14:45:03Z,"### Description of the bug

I provide a fasta file for running Mutect2 and have this error :  

`  A USER ERROR has occurred: Fasta index file file://GRCh38_latest_genomic.fna.fai for reference file://GRCh38_latest_genomic.fna does not exist. Please see https://gatk.broadinstitute.org/hc/articles/360035531652-FASTA-Reference-genome-format for help creating it.`

from Mutect2 of GATK.

But my file is here, and exist.

### Command used and terminal output

```console
`nextflow run nf-core/sarek -r dev -profile singularity -c custom.config -params-file nf-params.json`
```

json :
```json
{
    ""input"": ""sample.csv"",
    ""outdir"": ""results"",
    ""wes"": ""true"",
    ""fasta"": ""/gpfs/home/plgouttebel/home/exomic/data/ref/GRCh38_latest_genomic.fna"",
    ""aligner"": ""bwa-mem2"",
    ""tools"": ""mutect2"",
    ""skip_tools"": ""baserecalibrator,markduplicates""
}
```
config :
`singularity.cacheDir = '/scratch/plgouttebel/data_Singula/nf-core-sarek_dev/singularity-images'`

Output from Log file : 

```shell
May-07 15:15:33.560 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
May-07 15:15:33.560 [Task submitter] INFO  nextflow.Session - [48/601cc5] Submitted process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:MUTECT2 (BR666F)
May-07 15:15:33.655 [Task monitor] DEBUG nextflow.processor.TaskProcessor - Handling unexpected condition for
  task: name=NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:GETPILEUPSUMMARIES (BR666F); work-dir=/scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/a0/84752509ea76ccd51c89f3b8af9c20
  error [nextflow.exception.ProcessFailedException]: Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:GETPILEUPSUMMARIES (BR666F)` terminated with an error exit status (2)
May-07 15:15:33.763 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:GETPILEUPSUMMARIES (BR666F)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:GETPILEUPSUMMARIES (BR666F)` terminated with an error exit status (2)

Command executed:

  gatk --java-options ""-Xmx9830M -XX:-UsePerfData"" \
      GetPileupSummaries \
      --input BR666F.sorted.cram \
      --variant af-only-gnomad.hg38.vcf.gz \
      --output BR666F.mutect2.chr2_16146120-32867130.pileups.table \
      --reference GRCh38_latest_genomic.fna \
      --intervals chr2_16146120-32867130.bed \
      --tmp-dir . \


  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_TUMOR_ONLY_ALL:BAM_VARIANT_CALLING_TUMOR_ONLY_MUTECT2:GETPILEUPSUMMARIES"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  2

Command output:
  (empty)

Command error:
  Using GATK jar /usr/local/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar
  Running:
      java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx9830M -XX:-UsePerfData -jar /usr/local/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar GetPileupSummaries --input BR666F.sorted.cram --variant af-only-gnomad.hg38.vcf.gz --output BR666F.mutect2.chr2_16146120-32867130.pileups.table --reference GRCh38_latest_genomic.fna --intervals chr2_16146120-32867130.bed --tmp-dir .
  13:15:32.947 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
  13:15:33.307 INFO  GetPileupSummaries - ------------------------------------------------------------
  13:15:33.313 INFO  GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.5.0.0
  13:15:33.314 INFO  GetPileupSummaries - For support and documentation go to https://software.broadinstitute.org/gatk/
  13:15:33.314 INFO  GetPileupSummaries - Executing as plgouttebel@n064 on Linux v3.10.0-1160.el7.x86_64 amd64
  13:15:33.314 INFO  GetPileupSummaries - Java runtime: OpenJDK 64-Bit Server VM v17.0.10-internal+0-adhoc..src
  13:15:33.314 INFO  GetPileupSummaries - Start Date/Time: May 7, 2024 at 1:15:32 PM GMT
  13:15:33.314 INFO  GetPileupSummaries - ------------------------------------------------------------
  13:15:33.315 INFO  GetPileupSummaries - ------------------------------------------------------------
  13:15:33.316 INFO  GetPileupSummaries - HTSJDK Version: 4.1.0
  13:15:33.316 INFO  GetPileupSummaries - Picard Version: 3.1.1
  13:15:33.316 INFO  GetPileupSummaries - Built for Spark Version: 3.5.0
  13:15:33.317 INFO  GetPileupSummaries - HTSJDK Defaults.COMPRESSION_LEVEL : 2
  13:15:33.317 INFO  GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
  13:15:33.317 INFO  GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
  13:15:33.317 INFO  GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
  13:15:33.318 INFO  GetPileupSummaries - Deflater: IntelDeflater
  13:15:33.318 INFO  GetPileupSummaries - Inflater: IntelInflater
  13:15:33.318 INFO  GetPileupSummaries - GCS max retries/reopens: 20
  13:15:33.318 INFO  GetPileupSummaries - Requester pays: disabled
  13:15:33.319 INFO  GetPileupSummaries - Initializing engine
  13:15:33.322 INFO  GetPileupSummaries - Shutting down engine
  [May 7, 2024 at 1:15:33 PM GMT] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 0.01 minutes.
  Runtime.totalMemory()=167772160
  ***********************************************************************

  A USER ERROR has occurred: Fasta index file file://GRCh38_latest_genomic.fna.fai for reference file://GRCh38_latest_genomic.fna does not exist. Please see https://gatk.broadinstitute.org/hc/articles/360035531652-FASTA-Reference-genome-format for help creating it.

  ***********************************************************************
  Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.

Work dir:
  /scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/a0/84752509ea76ccd51c89f3b8af9c20

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`
May-07 15:15:33.769 [Task monitor] INFO  nextflow.Session - Execution cancelled -- Finishing pending tasks before exit
May-07 15:15:33.795 [main] DEBUG nextflow.Session - Session await > all processes finished
```


### Relevant files

```shell
[plgouttebel@login01 nf-core-sarek_dev]$ ls -l /scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/a0/84752509ea76ccd51c89f3b8af9c20
total 4
lrwxrwxrwx 1 plgouttebel ubx2 160 May  7 15:15 af-only-gnomad.hg38.vcf.gz -> /scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/stage-afe03af1-e05d-4a93-af32-09b63a751b4a/3c/e686ef595583a185a5b7f2480f6f94/af-only-gnomad.hg38.vcf.gz
lrwxrwxrwx 1 plgouttebel ubx2 164 May  7 15:15 af-only-gnomad.hg38.vcf.gz.tbi -> /scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/stage-afe03af1-e05d-4a93-af32-09b63a751b4a/e9/bc174e86314d14b42fab79c5283b02/af-only-gnomad.hg38.vcf.gz.tbi
lrwxrwxrwx 1 plgouttebel ubx2 109 May  7 15:15 BR666F.sorted.cram -> /scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/1a/5ad95b654c06311dc198df39b7a33d/BR666F.sorted.cram
lrwxrwxrwx 1 plgouttebel ubx2 114 May  7 15:15 BR666F.sorted.cram.crai -> /scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/1a/5ad95b654c06311dc198df39b7a33d/BR666F.sorted.cram.crai
lrwxrwxrwx 1 plgouttebel ubx2 117 May  7 15:15 chr2_16146120-32867130.bed -> /scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/b0/fead63adc11db1d9353e4e666e6bf9/chr2_16146120-32867130.bed
lrwxrwxrwx 1 plgouttebel ubx2  69 May  7 15:15 GRCh38_latest_genomic.fna -> /gpfs/home/plgouttebel/home/exomic/data/ref/GRCh38_latest_genomic.fna
lrwxrwxrwx 1 plgouttebel ubx2 162 May  7 15:15 Homo_sapiens_assembly38.dict -> /scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/stage-afe03af1-e05d-4a93-af32-09b63a751b4a/0f/674a437a17df7ac9f50ac6d50c930c/Homo_sapiens_assembly38.dict
lrwxrwxrwx 1 plgouttebel ubx2 167 May  7 15:15 Homo_sapiens_assembly38.fasta.fai -> /scratch/plgouttebel/data_Singula/nf-core-sarek_dev/work/stage-afe03af1-e05d-4a93-af32-09b63a751b4a/01/63bf12053a02deb319a2f6ac4dbe47/Homo_sapiens_assembly38.fasta.fai
```

### System information

HPC on curta from MCIA (Mésocentre de calcul intensif aquitain)
sarek downloaded locally",Ist4lri,https://github.com/nf-core/sarek/issues/1514
I_kwDOCvwIC86IMOCQ,Missing file in iGenomes,CLOSED,2024-05-08T07:37:26Z,2024-05-23T08:54:27Z,2024-05-23T08:54:27Z,"### Description of the bug

Hi,

I'm trying to run sarek, and I'm getting the following error:

```

ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP (1)'

Caused by:
  Not Found (Service: Amazon S3; Status Code: 404; Error Code: 404 Not Found; Request ID: DWAFHYDZD0HB3HXB; S3 Extended Request ID: 9UnG6G4ZvIrq/mijFQxFBq5uJUwvZXWga5ogNegCaF2yXEQ4UgMlcuMwhj8udwpksh2uh3JsTg0=; Proxy: null)


 -- Check '.nextflow.log' file for details
-[nf-core/sarek] Sent summary e-mail to mazaltov@hadassah.org.il (mail)-
-[nf-core/sarek] Pipeline completed with errors-
WARN: Killing running tasks (2)

```

### Command used and terminal output

```console
$ nextflow run -resume -params-file OV3_params.yaml  -profile singularity nf-core/sarek

where params file is: 

input: OV3_sample_sheet.csv
outdir: output
step: variant_calling
wes: true
tools: 'mutect2,vep'
genome: GATK.GRCh37
save_reference: true
download_cache: true
```
```


### Relevant files

_No response_

### System information

nextflow version:  23.04.4
nf-core/sarek: 3.4.1",ekushele,https://github.com/nf-core/sarek/issues/1515
I_kwDOCvwIC86IcI2u,Sarek test run unable to find docker image ,OPEN,2024-05-10T06:56:12Z,2024-05-16T13:42:46Z,,"### Description of the bug

I am new to nextlfow sarek pipeline, I tried to use docker as profile to test run the sarek pipeline but it had error. is this the docker issue ? how can I download back that bwa version, I tried to write config file to implement the bwa images but it isn't work. Please advice. Thanks.

### Command used and terminal output

```console
command:
nextflow run nf-core/sarek -r 3.4.1 -profile test,docker --outdir test_result

Output:
[nf-core/sarek] Pipeline completed with errors-
ERROR ~ Error executing process > 'NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX (genome.fasta)'

Caused by:
  Process `NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX (genome.fasta)` terminated with an error exit status (125)

Command executed:

  mkdir bwa
  bwa \
      index \
       \
      -p bwa/genome \
      genome.fasta

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX"":
      bwa: $(echo $(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*$//')
  END_VERSIONS

Command exit status:
  125

Command output:
  (empty)

Command error:
  Unable to find image 'quay.io/biocontainers/bwa:0.7.17--hed695b0_7' locally
  0.7.17--hed695b0_7: Pulling from biocontainers/bwa
  docker: [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of quay.io/biocontainers/bwa:0.7.17--hed695b0_7 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/.
  See 'docker run --help'.

Work dir:
  /data2/peter/HHGPHK/bam_files/work/1e/6a22d245cdb00e6ceb0bd0edaa18d6

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`

 -- Check '.nextflow.log' file for details
```


### Relevant files

I don't have relevant files, I just followed the https://nf-co.re/docs/usage/introduction 
4. To test that everything is working properly, try running the tests for your pipeline of interest in the terminal:
nextflow run nf-core/sarek -r 3.4.1 -profile test,docker --outdir test_result


### System information

      N E X T F L O W
      version 23.10.1 build 5891
      created 12-01-2024 22:01 UTC (13-01-2024 06:01 HKST)
      cite doi:10.1038/nbt.3820
      http://nextflow.io

Liunx 5.15.0-106-generic 

Distributor ID: Ubuntu
Description:    Ubuntu 22.04.4 LTS
Release:        22.04
Codename:       jammy


",C2i-PeterChung,https://github.com/nf-core/sarek/issues/1519
I_kwDOCvwIC86IcZTm,joint_germline error,CLOSED,2024-05-10T07:32:30Z,2024-05-27T03:20:54Z,2024-05-27T03:20:54Z,"### Description of the bug

Hello, this is my error message, it says that the field Settings are not uniform, I would like to ask how to adjust the parameter Settings? Or is there something wrong with my file Settings?

(nextflow) cat .command.err 
INFO:    Environment variable SINGULARITYENV_TMPDIR is set, but APPTAINERENV_TMPDIR is preferred
INFO:    Environment variable SINGULARITYENV_NXF_TASK_WORKDIR is set, but APPTAINERENV_NXF_TASK_WORKDIR is preferred
INFO:    Environment variable SINGULARITYENV_NXF_DEBUG is set, but APPTAINERENV_NXF_DEBUG is preferred
WARNING: Skipping mount /var/apptainer/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container

VCFtools - 0.1.16
(C) Adam Auton and Anthony Marcketta 2009

Parameters as interpreted:
	--gzvcf joint_germline.vcf.gz
	--out joint_germline
	--TsTv-by-count

Using zlib version: 1.2.11
Warning: Expected at least 2 parts in FORMAT entry: ID=PGT,Number=1,Type=String,Description=""Physical phasing haplotype information, describing how the alternate alleles are phased in relation to one another; will always be heterozygous and is not intended to describe called alleles"">
Warning: Expected at least 2 parts in FORMAT entry: ID=PID,Number=1,Type=String,Description=""Physical phasing ID information, where each unique ID within a given sample (but not across samples) connects records within a phasing group"">
Warning: Expected at least 2 parts in FORMAT entry: ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">
Warning: Expected at least 2 parts in FORMAT entry: ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">
Warning: Expected at least 2 parts in INFO entry: ID=AC,Number=A,Type=Integer,Description=""Allele count in genotypes, for each ALT allele, in the same order as listed"">
Warning: Expected at least 2 parts in INFO entry: ID=AC,Number=A,Type=Integer,Description=""Allele count in genotypes, for each ALT allele, in the same order as listed"">
Warning: Expected at least 2 parts in INFO entry: ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">
Warning: Expected at least 2 parts in INFO entry: ID=AF,Number=A,Type=Float,Description=""Allele Frequency, for each ALT allele, in the same order as listed"">
Warning: Expected at least 2 parts in INFO entry: ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">
Warning: Expected at least 2 parts in INFO entry: ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">
Warning: Expected at least 2 parts in INFO entry: ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">
Warning: Expected at least 2 parts in INFO entry: ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">
After filtering, kept 3 out of 3 Individuals
Outputting Ts/Tv by Alternative Allele Count
After filtering, kept 827865 out of a possible 827865 Sites
Run Time = 3.00 seconds
/public/home/fanrong/projects/BSA/03_H/02_nf_core/work/f0/b3c91e3cf79362b5d579c62aa69ddc/.command.sh: line 7:    42 Segmentation fault      (core dumped) vcftools --gzvcf joint_germline.vcf.gz --out joint_germline --TsTv-by-count

### Command used and terminal output

```console
nextflow run /cluster/home/fanrong/biosofts/nextflow/nf-core/nf-core-sarek/3_4_0 -profile zwnj_2022 -offline --input /public/home/fanrong/projects/BSA/03_H/01_raw_data/samplesheet.csv --outdir ./result --step mapping --fasta /public/home/fanrong/projects/AK58/fasta/wheat_AK58v4MP.genome_part.fa --fasta_fai /public/home/fanrong/projects/AK58/fasta/wheat_AK58v4MP.genome_part.fa.fai --dict /public/home/fanrong/projects/AK58/fasta/wheat_AK58v4MP.genome_part.dict --trim_fastq --aligner bwa-mem2 --max_cpus 16 --max_memory 250.GB --max_time 720.h --tools haplotypecaller --save_mapped --save_output_as_bam --save_trimmed --bwamem2 /cluster/home/fanrong/work_lei/BSA/01_ZG/02_mapping_3/work/33/706371da5349b7b033e07f32ae9d58/bwamem2 --igenomes_ignore --genome null --skip_tools baserecalibrator haplotypecaller_filter --joint_germline --wes
```


### Relevant files

_No response_

### System information

_No response_",fan040,https://github.com/nf-core/sarek/issues/1520
I_kwDOCvwIC86I-7lp,port CREATE_INTERVALS_BED to nf-core/modules,OPEN,2024-05-15T15:11:52Z,2024-05-15T15:39:40Z,,"### Description of feature

That would be actually necessary to port all our subworkflows with intervals to nf-core",maxulysse,https://github.com/nf-core/sarek/issues/1523
I_kwDOCvwIC86I-8Jz,Find/create a nf-core/modules module to replace ADD_INFO_TO_VCF,CLOSED,2024-05-15T15:12:58Z,2024-05-15T15:30:43Z,2024-05-15T15:30:43Z,"### Description of feature

I'm fairly sure we can do better than what we are currently doing there",maxulysse,https://github.com/nf-core/sarek/issues/1524
I_kwDOCvwIC86JE_y6,Mutect2 run only on one intervals file,OPEN,2024-05-16T09:00:23Z,2024-08-19T14:50:06Z,,"### Description of the bug

Hi,

I'm trying to run sarek with ""mutect2"" for variant calling.
Pipeline completes successfully, but the folder ""output/variant_calling/"" is empty.
It seems like process `MUTECT2_PAIRED` run only on one interval file, the `.command.sh` is:
```
#!/bin/bash -euo pipefail
gatk --java-options ""-Xmx29491M -XX:-UsePerfData"" \
    Mutect2 \
    --input 1641_parp_ov.recal.cram --input FFPE_ovary.recal.cram \
    --output FFPE_ovary_vs_1641_parp_ov.mutect2.chr2_16146120-32867130.vcf.gz \
    --reference Homo_sapiens_assembly38.fasta \
    --panel-of-normals 1000g_pon.hg38.vcf.gz \
    --germline-resource af-only-gnomad.hg38.vcf.gz \
    --intervals chr2_16146120-32867130.bed \
    --tmp-dir . \
    --f1r2-tar-gz FFPE_ovary_vs_1641_parp_ov.mutect2.chr2_16146120-32867130.f1r2.tar.gz --normal-sample OV3_1641_parp_ov

cat <<-END_VERSIONS > versions.yml
""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED"":
    gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
END_VERSIONS
```

I think it should run on all bed files combined together or run on all bed files and then combine them together...

### Command used and terminal output

```console
nextflow -config config_sarek.conf run -resume -params-file new_OV3_params.yaml  -profile singularity nf-core/sarek

new_OV3_params.yaml file:
input: new_OV3_sample_sheet.csv
outdir: new_output
step: mapping
wes: true
tools: 'mutect2,vep'
genome: GATK.GRCh38
save_reference: true
download_cache: true


new_OV3_sample_sheet.csv:
patient,sex,status,sample,lane,fastq_1,fastq_2
OV3,XX,1,FFPE_ovary,lane_1,fastq/OV3_FFPE_ovary_R1.conc.fastq.gz,fastq/OV3_FFPE_ovary_R2.conc.fastq.gz
OV3,XX,0,1641_parp_ov,lane_1,fastq/1641_parp_ov_S4_R1_001.fastq.gz,fastq/1641_parp_ov_S4_R2_001.fastq.gz
```


### Relevant files

_No response_

### System information

nextflow version:  23.04.4
nf-core/sarek: 3.4.1",ekushele,https://github.com/nf-core/sarek/issues/1525
I_kwDOCvwIC86JGLUb,WES files provided by ascat author,OPEN,2024-05-16T11:05:10Z,2024-08-19T13:12:39Z,,"### Description of the bug

Hi Developers,

I'm trying to run the Sarek implemented ASCAT for CNV analysis on WES data. On the nfcore Sarek website, it's suggested to follow 5 steps, as specified in this doc https://nf-co.re/sarek/3.4.0/docs/usage#how-to-generate-ascat-resources-for-exome-or-targeted-sequencing, to generate reference information (allele.zip, loci.zip, GC.zip, and RT.zip) for exome data instead of using the default igenome directly. I noticed that the ASCAT author had also provided ref files for WES at https://github.com/VanLoo-lab/ascat/tree/master/ReferenceFiles/WES, which seemed to be a ready-to-use version when provided with an appropriate BED file. Would it be feasible to replace the default ignome ref with those for Sarek ASCAT?

I'm now running Sarek with params (-- --ascat_alleles, --ascat_loci, --ascat_loci_gc, --ascat_loci_rt) on the command line. The pipeline seems to work well. But, it would be great to hear advice from you.

Thank you!

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",wlyucl,https://github.com/nf-core/sarek/issues/1526
I_kwDOCvwIC86JKodd,Different sample name handling causes issues with merging,OPEN,2024-05-16T20:18:48Z,2024-10-30T10:49:36Z,,"### Description of the bug

It looks like there's a mix-up of the sample name somewhere along the variant calling process. Merging across `deepvariant,freebayes,haplotypecaller,mpileup,strelka` is unsuccessful because the sample names in the vcf are different. 

```
ls *.gz | while read line; do echo $line; zcat $line | head -n 500 | grep ""#CHROM"" | cut -f10; done
s02.bcftools.added_info.vcf.gz
s02
s02.deepvariant.added_info.vcf.gz
f1_03_s02
s02.freebayes.added_info.vcf.gz
f1_03_s02
s02.strelka.variants.added_info.vcf.gz
f1_03_s02
```

More specifically, it looks like the `patient` field is not getting added onto the bcftools file.



### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.2 -resume

-[nf-core/sarek] Pipeline completed with errors-
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:POST_VARIANTCALLING:CONCATENATE_GERMLINE_VCFS:GERMLINE_VCFS_CONCAT (s02)'

Caused by:
  Process `NFCORE_SAREK:SAREK:POST_VARIANTCALLING:CONCATENATE_GERMLINE_VCFS:GERMLINE_VCFS_CONCAT (s02)` terminated with an error exit status (255)

Command executed:

  bcftools concat \
      --output s02.vcf.gz \
      -a \
      --threads 1 \
      s02.bcftools.added_info.vcf.gz s02.deepvariant.added_info.vcf.gz s02.freebayes.added_info.vcf.gz s02.strelka.variants.added_info.vcf.gz

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:POST_VARIANTCALLING:CONCATENATE_GERMLINE_VCFS:GERMLINE_VCFS_CONCAT"":
      bcftools: $(bcftools --version 2>&1 | head -n1 | sed 's/^.*bcftools //; s/ .*$//')
  END_VERSIONS

Command exit status:
  255

Command output:
  (empty)

Command error:
  INFO:    Mounting image with FUSE.
  Checking the headers and starting positions of 4 files
  Different sample names in s02.deepvariant.added_info.vcf.gz. Perhaps ""bcftools merge"" is what you are looking for?

Work dir:
  work/fc/fb37d157b346a47f8cf7d1b9cf06b0

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

 -- Check '.nextflow.log' file for details
```


### Relevant files

`input.csv`:
```
patient,status,sample,lane,fastq_1,fastq_2
f1_02,0,s01,l01,LIB033270-DIL01_227KVNLT4_S392_L001_R1_001.fastq.gz,LIB033270-DIL01_227KVNLT4_S392_L001_R2_001.fastq.gz
f1_03,0,s02,l01,LIB033272-DIL01_227KVNLT4_S393_L001_R1_001.fastq.gz,LIB033272-DIL01_227KVNLT4_S393_L001_R2_001.fastq.gz
f1_04,0,s03,l01,LIB033273-DIL01_227KVNLT4_S384_L001_R1_001.fastq.gz,LIB033273-DIL01_227KVNLT4_S384_L001_R2_001.fastq.gz
f1_05,0,s04,l01,LIB033271-DIL01_227KVNLT4_S385_L001_R1_001.fastq.gz,LIB033271-DIL01_227KVNLT4_S385_L001_R2_001.fastq.gz
```

`nextflow.config`:
```
params {
    input = './input.csv'
    outdir = ""./results/""
    tools = ""deepvariant,freebayes,haplotypecaller,mpileup,strelka""
    trim_fastq = true
    save_trimmed = true
    aligner = 'bwa-mem2'
    save_mapped = true
    concatenate_vcfs = true
    joint_germline = true
    genome = ""GRCm38""
    save_reference = true
    max_cpus = 64
    max_memory = '512.GB'
}
```

### System information

_No response_",njspix,https://github.com/nf-core/sarek/issues/1528
I_kwDOCvwIC86JYNjO,new version,CLOSED,2024-05-19T23:20:52Z,2024-05-20T00:37:29Z,2024-05-20T00:36:19Z,,XikunHan,https://github.com/nf-core/sarek/issues/1529
I_kwDOCvwIC86JcYaC,Dynamic computing resources,CLOSED,2024-05-20T12:43:21Z,2024-05-20T13:12:24Z,2024-05-20T13:12:24Z,"### Description of the bug

I want to modify the memory, want to ask where the statement is not standardized, resulting in an error


### Command used and terminal output

```console
N E X T F L O W  ~  version 23.10.1
WARNING: Could not load nf-core/config profiles: /cluster/home/fanrong/biosofts/nextflow/nf-core/nf-core-sarek/3_4_0/../configs//nfcore_custom.config
ERROR ~ Plugins definition is only allowed in config top-most scope
process {
  executor = 'slurm'
  queue = 'cpu'
  clusterOptions = '--exclude=node10'
  // queue = { task.time <= 2.h ? 'nodeall' : task.time <= 24.h ? 'fat': 'cpu2' }
  // queue = { task.time <= 2.h ? 'cpu2' : task.time <= 24.h ? 'cpu2': 'cpu2' }
  maxRetries = 2
  beforeScript = 'export PATH=/usr/bin:${PATH}'
  // beforeScript = 'module load software/apptainer/v1.1.5'
  // beforeScript = 'mkdir -p ./tmp/ && export TMPDIR=./tmp/'
  withName: 'BWAMEM1_MEM|BWAMEM2_MEM' {
      cpus = 16
      memory = 100.GB
      errorStrategy = { task.exitStatus in 137..140 ? 'retry' : 'terminate' }
      maxRetries 3
  }
}
```


### Relevant files

_No response_

### System information

_No response_",fan040,https://github.com/nf-core/sarek/issues/1530
I_kwDOCvwIC86JcylB,don't use fastp and fastqc,CLOSED,2024-05-20T13:38:54Z,2024-05-21T01:14:55Z,2024-05-21T01:14:31Z,"### Description of feature

I would like to ask whether it is possible to directly talk about bwa comparison of fastq files without going through quality control steps",fan040,https://github.com/nf-core/sarek/issues/1531
I_kwDOCvwIC86JiVUy,Nextflow docker: Error response from daemon,CLOSED,2024-05-21T06:57:06Z,2024-08-19T14:49:14Z,2024-08-19T14:49:14Z,"### Description of the bug

hello I have an error when I ran the nextflow sarek pipeline:

`nextflow -bg run nf-core/sarek -r 3.4.0 -params-file params_samplesheet_F52_3.json -profile docker &

Workflow execution completed unsuccessfully!
The exit status of the task that caused the workflow execution to fail was: 125.

The full error message was:

Error executing process > 'NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (120236)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR (120236)` terminated with an error exit status 
(125)

Command executed:

  gatk --java-options ""-Xmx3276M -XX:-UsePerfData"" \
      BaseRecalibrator  \
      --input 120236.converted.cram \
      --output 120236_chr1_122026460-124977944.recal.table \
      --reference Homo_sapiens_assembly38.fasta \
      --intervals chr1_122026460-124977944.bed \
      --known-sites dbsnp_146.hg38.vcf.gz --known-sites Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites 
Homo_sapiens_assembly38.known_indels.vcf.gz \
      --tmp-dir . \


  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_BASERECALIBRATOR:GATK4_BASERECALIBRATOR"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  125

Command output:
  (empty)

Command error:
  docker: Error response from daemon: failed to create endpoint nxf-02MAUnCuusW7YHwLnsBbzWSA on network bridge: failed to add the host 
(vethe58dde8) <=> sandbox (veth83d3dac) pair interfaces: cannot allocate memory.
  time=""2024-05-21T11:53:49+08:00"" level=error msg=""error waiting for container: context canceled""

Work dir:
  /data/hhgphk/runF52/work/00/8748af7adb7ffb4cbacc79d0fedbca

Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh``

I  tried to add the memory limit to docker file:

```
sudo nano /etc/default/docker
DOCKER_OPTS=""--default-memory-limit=32768m --default-memory-swap=65536m""
```
My docker and nextflow version: 



But it still does not work. I am new to nextflow, can anyone give me advice. Thanks.


### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

Docker version 20.10.24, build 297e128
  
N E X T F L O W
  version 23.10.0 build 5889
  created 15-10-2023 15:07 UTC (23:07 HKST)
  cite doi:10.1038/nbt.3820
  http://nextflow.io",C2i-PeterChung,https://github.com/nf-core/sarek/issues/1532
I_kwDOCvwIC86JjWeg,ignore sample error,CLOSED,2024-05-21T08:53:23Z,2024-05-29T01:10:16Z,2024-05-29T01:10:15Z,"### Description of feature

Hello, I would like to ask whether it is possible to realize that if the fastq file of the sample is wrong, the sample can be ignored and the remaining samples can be tested for variation without any error being reported and the pipeline will stop running.",fan040,https://github.com/nf-core/sarek/issues/1533
I_kwDOCvwIC86Jpyn0,Dynamic computing resources,CLOSED,2024-05-22T02:19:14Z,2024-05-27T03:21:16Z,2024-05-27T03:21:16Z,"### Description of feature

![image](https://github.com/nf-core/sarek/assets/166574153/9a893a26-d421-4725-833c-197342f1484c)
![image](https://github.com/nf-core/sarek/assets/166574153/acb5cacf-11af-4ce3-bb0a-9c40113689c4)
I would like to ask, will setting this parameter cause my cpus to double as well, and if so, how should I set it to stay the same?
",fan040,https://github.com/nf-core/sarek/issues/1535
I_kwDOCvwIC86Ju_Cm,Check if any empty folders are created,OPEN,2024-05-22T15:06:16Z,2024-05-22T15:06:16Z,,"### Description of the bug

I think if we don't download cache there's still an empty folder created, so let's check that out

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1537
I_kwDOCvwIC86JvkOt,Add a parameter to pass in a genomedb that will be updated,CLOSED,2024-05-22T16:18:23Z,2024-05-22T16:20:15Z,2024-05-22T16:20:15Z,"### Description of feature

Currently, a new genomeDB is created in sarek in each run. But, there are times when I want to either update an existing genomeDB with new samples as new batches of samples are sequenced, or more interestingly to me currently, include a set of parent samples with all new batches of sequencing.

It would require that a parameter is added to the pipeline `nextlfow.config` to allow passing in an existing genomeDB. In this line:

```
            // meta is now a list of [meta1, meta2] but they are all the same. So take the first element.
            [ meta_list[0], gvcf, tbi, intervals, [], [] ]
```

https://github.com/nf-core/sarek/blob/b5b766d3b4ac89864f2fa07441cdc8844e70a79e/subworkflows/local/bam_joint_calling_germline_gatk/main.nf#L44C13-L44C59

And, the final input would be that `param.genomedb` path. If that parameter exists, then the third input of the GVCFS_GENOMICSDBIMPORT() should be `true` rather than `false`: https://github.com/nf-core/sarek/blob/b5b766d3b4ac89864f2fa07441cdc8844e70a79e/subworkflows/local/bam_joint_calling_germline_gatk/main.nf#L48",cmatKhan,https://github.com/nf-core/sarek/issues/1538
I_kwDOCvwIC86JvlUM,Add option to pass in existant genomeDB to sarek,CLOSED,2024-05-22T16:20:33Z,2024-05-23T11:18:02Z,2024-05-23T11:18:02Z,"### Description of feature

Currently, a new genomeDB is created in sarek in each run. But, there are times when I want to either update an existing genomeDB with new samples as new batches of samples are sequenced, or more interestingly to me currently, include a set of parent samples with all new batches of sequencing.

It would require that a parameter is added to the pipeline `nextlfow.config` to allow passing in an existing genomeDB. In this line:

```
            // meta is now a list of [meta1, meta2] but they are all the same. So take the first element.
            [ meta_list[0], gvcf, tbi, intervals, [], [] ]
```

https://github.com/nf-core/sarek/blob/b5b766d3b4ac89864f2fa07441cdc8844e70a79e/subworkflows/local/bam_joint_calling_germline_gatk/main.nf#L44C13-L44C59

And, the final input would be that `param.genomedb` path. If that parameter exists, then the third input of the GVCFS_GENOMICSDBIMPORT() should be `true` rather than `false`: https://github.com/nf-core/sarek/blob/b5b766d3b4ac89864f2fa07441cdc8844e70a79e/subworkflows/local/bam_joint_calling_germline_gatk/main.nf#L48",cmatKhan,https://github.com/nf-core/sarek/issues/1539
I_kwDOCvwIC86Jwg9o,Mapped bam and bai published in different folders,CLOSED,2024-05-22T18:19:05Z,2024-08-19T13:25:01Z,2024-08-19T13:25:01Z,"### Description of the bug

When running
```
nextflow run nf-core/sarek -profile test,alignment_to_fastq,docker -r 3.4.2 --outdir results --save_mapped --save_output_as_bam
```
I get
```
results/preprocessing/mapped/test/test-1.sorted.bam.bai
results/preprocessing/mapped/test-1/test-1.sorted.bam
```
I think the bam and bai sound be in the same folder.

### Command used and terminal output

```console
nextflow run nf-core/sarek -profile test,alignment_to_fastq,docker -r 3.4.2 --outdir results --save_mapped --save_output_as_bam

results/preprocessing/mapped/test/test-1.sorted.bam.bai
results/preprocessing/mapped/test-1/test-1.sorted.bam
```


### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1540
I_kwDOCvwIC86KKnf4,memory limit ,CLOSED,2024-05-27T01:14:43Z,2024-06-11T11:56:09Z,2024-06-11T11:56:09Z,"### Description of the bug

I would like to ask whether this configuration can dynamically adjust resources. I have many files and a large reference genome, which consumes more resources, but my initial setting resources can not be too large, so I want to set a dynamic adjustment resource configuration for all software

This is my settting:
![image](https://github.com/nf-core/sarek/assets/166574153/3b4835c0-344b-4ea7-b88b-87ed7fffe7f6)
This is my error：
INFO:    Environment variable SINGULARITYENV_TMPDIR is set, but APPTAINERENV_TMPDIR is preferred
INFO:    Environment variable SINGULARITYENV_NXF_TASK_WORKDIR is set, but APPTAINERENV_NXF_TASK_WORKDIR is preferred
INFO:    Environment variable SINGULARITYENV_NXF_DEBUG is set, but APPTAINERENV_NXF_DEBUG is preferred
slurmstepd: error: Job 518023 exceeded memory limit (4339056 > 4194304), being killed
slurmstepd: error: Exceeded job memory limit
slurmstepd: error: *** JOB 518023 ON node1 CANCELLED AT 2024-05-27T09:01:58 ***


### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",fan040,https://github.com/nf-core/sarek/issues/1543
I_kwDOCvwIC86KM4rW,Add more intricate CI tests to verify multiple/single lanes run well,OPEN,2024-05-27T09:00:44Z,2024-07-08T13:21:21Z,,"https://github.com/asp8200/sarek/blob/8d1592c242fbb2bd2cc7da66377032d55afc7816/workflows/sarek/main.nf#L270

https://github.com/asp8200/sarek/blob/8d1592c242fbb2bd2cc7da66377032d55afc7816/workflows/sarek/main.nf#L288

 I just hope we have CI-tests covering all this intricate stuff 🤞

_Originally posted by @asp8200 in https://github.com/nf-core/sarek/pull/1541#discussion_r1615662778_",maxulysse,https://github.com/nf-core/sarek/issues/1544
I_kwDOCvwIC86KRrR-,FILTERMUTECTCALLS is not run for tumor-only samples when there is no germline_resource,OPEN,2024-05-27T23:36:43Z,2024-07-08T13:20:17Z,,"### Description of the bug

When there is no `germline_resource` specified, FILTERMUTECTCALLS is not run in the tumor-only somatic variant calling mode of the MUTECT2 workflow. This appears to happen because in the absence of the germline resource, GETPILEUPSUMMARIES, GATHERPILEUPSUMMARIES, and CALCULATECONTAMINATION are not run, each of which generates an output that is passed to FILTERMUTECTCALLS. However, FILTERMUTECTCALLS does not actually require these inputs to run. Nevertheless, FILTERMUTECTCALLS is never triggered/started in this circumstance.

In my case I am using the GRCm38 genome, for which no standard germline resource exists. Let me know if you need any other information from me, and thank you for your help!

Best,
Branden

Edit: 
May I add that everything up to and including LEARNREADORIENTATIONMODEL completes without issue. The problem is with getting FILTERMUTECTCALLS to even attempt to run. I have tried providing various dummy values like empty paths, to no avail -- although perhaps I am not approaching that in the right way.

### Command used and terminal output

```console
nextflow run ../../pipelines/sarek -profile singularity -work-dir path/to/work/dir --input /path/to/input.csv --genome GRCm38 --tools ""mutect2,merge"" --outdir /path/to/output
```


### Relevant files

_No response_

### System information

Nextflow version 23.10.1.
Running on an HPC.
Slurm executor.
Singularity for containerization.
Sarek version 3.4.2.",brandenjlynch,https://github.com/nf-core/sarek/issues/1546
I_kwDOCvwIC86KnLNO,Remove duplicated config-snippet for SENTIEON_GVCFTYPER,OPEN,2024-05-30T12:10:21Z,2024-12-09T16:22:17Z,,"### Description of the bug

Not much of a bug, but we have a duplicated config-snippet for `SENTIEON_GVCFTYPER`:
https://github.com/nf-core/sarek/blob/f2852ff589fc30cbd2c3f838aed20df26735b49a/conf/modules/sentieon_dnascope_joint_germline.config#L18-L25
https://github.com/nf-core/sarek/blob/f2852ff589fc30cbd2c3f838aed20df26735b49a/conf/modules/sentieon_haplotyper_joint_germline.config#L18-L24

Maxime and I figured that that config-snippet should be placed in `conf/modules/sentieon.config`. Anyways, it certainly should be duplicated.

https://nfcore.slack.com/archives/C05V9FRJYMV/p1717059187193919

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1548
I_kwDOCvwIC86Kn4sV,Please add VEP plugins (Conservation and Condel),OPEN,2024-05-30T13:36:42Z,2024-06-04T14:50:53Z,,"### Description of feature

Dear Sarek team,
Please add the following two VEP plugins: 
- Conservation
-  Condel

Thx a lot,
Best, Andrea",andrew2k,https://github.com/nf-core/sarek/issues/1549
I_kwDOCvwIC86K3n3C,single sample haplotypecaller with no dbsnp causes error because of missing dbsnp_tbi,CLOSED,2024-06-02T19:54:38Z,2024-08-23T18:53:45Z,2024-08-23T18:53:45Z,"### Description of the bug

Using haplotypecaller without passing a `--dbsnp`, the following error is raised:

```
ERROR ~ Cannot get property 'baseName' on null object

 -- Check script '/scratch/mblab/chasem/nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/main.nf' at line: 132 or see '.nextflow.log' file for more details
ERROR ~ Cannot get property 'baseName' on null object

 -- Check script '/scratch/mblab/chasem/nextflow/assets/nf-core/sarek/./workflows/sarek/../../subworkflows/local/bam_variant_calling_germline_all/main.nf' at line: 133 or see '.nextflow.log' file for more details
-[nf-core/sarek] Pipeline completed with errors-
```
If I replace this line:

https://github.com/nf-core/sarek/blob/b5b766d3b4ac89864f2fa07441cdc8844e70a79e/subworkflows/local/bam_variant_calling_germline_all/main.nf#L133

with

```
[[:], []]
```
the subworkflow completes without error.

Sorry for the lack of a reproducible example for the time being -- will try to make one early next week.

### Command used and terminal output

```console
nextflow run nf-core/sarek -profile singularity -params-file bam_input_params.json -c /scratch/mblab/chasem/bsa/wustl_htcf.config -c kn99.config --input samplesheets/run_6991.csv --outdir run_6991_results
```


### Relevant files

I tried this with `haplotypecaller_filter` in ""skip_tools"", also, with the same result.

```json
{
    ""tools"": ""haplotypecaller,manta,cnvkit,snpeff"",
    ""skip_tools"": ""baserecalibrator"",
    ""joint_germline"": false,
    ""genome"": """",
    ""dbsnp_vqsr"": false,
    ""fasta"": ""\/ref\/mblab\/data\/KN99\/KN99_genome_fungidb.fasta"",
    ""fasta_fai"": ""\/ref\/mblab\/data\/KN99\/KN99_genome_fungidb.fasta.fai"",
    ""known_indels_vqsr"": false,
    ""known_snps"": false,
    ""known_snps_tbi"": false,
    ""known_snps_vqsr"": false,
    ""ngscheckmate_bed"": false,
    ""snpeff_db"": ""1"",
    ""snpeff_genome"": ""ASM221672v1"",
    ""igenomes_ignore"": true,
    ""vep_cache"": """",
    ""snpeff_cache"": ""\/ref\/mblab\/data\/KN99\/snpeff_db"",
    ""split_fastq"": 1000000,
    ""nucleotides_per_second"": 10000,
    ""save_output_as_bam"": true,
    ""step"": ""prepare_recalibration""
}

```

### System information

Nextflow: 24.04.2
Hardware: HPC
Executor: SLURM
Container engine: Singularityce
OS: Rocky linux v8.9
nf-core/sarek: 3.4.2",cmatKhan,https://github.com/nf-core/sarek/issues/1550
I_kwDOCvwIC86K6WZC,Alignment-only worfklow?,CLOSED,2024-06-03T08:48:21Z,2024-07-09T16:01:38Z,2024-07-09T16:01:38Z,"### Description of feature

As the subject says, it may be useful to perform just an alignment a first QC (and UMI adjustment) to evaluate the base metrics of an experiment before committing to variant calling etc.
From the Sarek docs this doesn't seem to be currently possible.
",lbeltrame,https://github.com/nf-core/sarek/issues/1551
I_kwDOCvwIC86LrZGu,Re enable all nf-test tests,OPEN,2024-06-10T09:33:59Z,2024-06-10T09:33:59Z,,"Not great having to disable more nf-tests :-/

_Originally posted by @asp8200 in https://github.com/nf-core/sarek/pull/1545#pullrequestreview-2104419382_
            ",maxulysse,https://github.com/nf-core/sarek/issues/1554
I_kwDOCvwIC86LtOiv,DRAGMAP_Alignment_error,OPEN,2024-06-10T13:10:32Z,2024-12-04T22:33:53Z,,"### Description of the bug

[my_param.json](https://github.com/user-attachments/files/15766958/my_param.json)


> >  error [nextflow.exception.ProcessFailedException]: Process `NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN (sample1-lane_1)` terminated with an error exit status (1)
> Jun-10 12:48:18.909 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run
> Jun-10 12:48:18.909 [Task submitter] INFO  nextflow.Session - [5d/c27009] Submitted process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN (sample1-lane_1)
> Jun-10 12:48:18.917 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN (sample1-lane_1)'
> 
> Caused by:
>   Process `NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN (sample1-lane_1)` terminated with an error exit status (1)
> 
> Command executed:
> 
>   dragen-os \
>       -r dragmap \
>       --RGSM patient1_sample1 --RGID ""@RG\tID:H8VDAADXX.sample1.lane_1\tPU:lane_1\tSM:patient1_sample1\tLB:sample1\tDS:s3://ngi-igenomes/igenomes//Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta\tPL:ILLUMINA"" \
>       --num-threads 12 \
>       -1 0002.sample1-lane_1_1.fastp.fastq.gz -2 0002.sample1-lane_1_2.fastp.fastq.gz \
>       2> >(tee sample1-lane_1.0002.dragmap.log >&2) \
>       | samtools sort -n --threads 12  -o sample1-lane_1.0002.bam -
>   
>   cat <<-END_VERSIONS > versions.yml
>   ""NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN"":
>       dragmap: $(echo $(dragen-os --version 2>&1))
>       samtools: $(echo $(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*$//')
>       pigz: $( pigz --version 2>&1 | sed 's/pigz //g' )
>   END_VERSIONS
> 
> Command exit status:
>   1
> 
> Command output:
>   (empty)
> 
> Command error:
>   When maskLen < 15, the function ssw_align doesn't return 2nd best alignment information.
>   When maskLen < 15, the function ssw_align doesn't return 2nd best alignment information.
>   When maskLen < 15, the function ssw_align doesn't return 2nd best alignment information.
>   When maskLen < 15, the function ssw_align doesn't return 2nd best alignment information.
>   When maskLen < 15, the function ssw_align doesn't return 2nd best alignment information.

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.2 \
-profile singularity \
-params-file my_param.json \
--outdir results_sarek_2 \
-resume
```


### Relevant files

[nextflow.log](https://github.com/user-attachments/files/15767160/nextflow.log)


### System information

_No response_",poddarharsh15,https://github.com/nf-core/sarek/issues/1555
I_kwDOCvwIC86LvVjg,Add spring-icon to subway-map,CLOSED,2024-06-10T17:15:00Z,2024-11-18T09:52:56Z,2024-11-18T09:52:55Z,"### Description of feature

After [this](https://github.com/nf-core/sarek/pull/1534) spring-PR has been merged, we also need to add a spring-icon to the [subway](https://github.com/nf-core/sarek/blob/master/docs/images/sarek_subway.png)-map.

![image](https://github.com/nf-core/sarek/assets/37172585/05063eeb-e3c4-4fef-9dc7-48441e9e80ad)
",asp8200,https://github.com/nf-core/sarek/issues/1558
I_kwDOCvwIC86L0GDi,ensembl-vep refseq/merged option broken,CLOSED,2024-06-11T08:03:33Z,2024-06-12T13:40:59Z,2024-06-12T13:40:58Z,"### Description of the bug

The code parsing vep related options can't deal with the `--refseq/--merged` options in the vep command.

If setting:
```yaml
vep_cache_version: ""111""
vep_genome: ""GRCh37""
vep_cache: ""path/to/vep-cache/""
vep_custom_args: ""--refseq --offline --everything""
vep_species: ""homo_sapiens_refseq""
```

then the error is:
```
  -------------------- EXCEPTION --------------------
  MSG: Should not use homo_sapiens_refseq as --species.
  Try using flags --refseq or --merged with --species homo_sapiens
```

the `command.sh`:
```bash
vep \
      -i sample.freebayes.vcf.gz \
      -o sample.freebayes_VEP.ann.vcf.gz \
      --stats_file sample.freebayes_VEP.ann.summary.html \
      --vcf --refseq --offline --hgvsg --allele_number --everything --format vcf \
      --compress_output bgzip \
      --fasta human_g1k_v37_decoy.fasta \
      --assembly GRCh37 \
      --species homo_sapiens_refseq \
      --cache \
      --cache_version 111 \
      --dir_cache ${PWD}/ensembl-vep \
      --fork 6
```

Notice that `--species homo_sapiens_refseq` is incorrect.
The correct way is to use `--species homo_sapiens` along with `--refseq` (latter is present because we include it in `vep_custom_args`.

The `--dir_cache ${PWD}/ensembl-vep` (corresponds to `vep_cache`) is organized as:
```
${PWD}/ensembl-vep
├── homo_sapiens_refseq
│   ├── 111_GRCh37
│   └── 111_GRCh38
...
```

which is the default structure if downloaded via ensembl-vep (these files are downloaded via docker myself). For homo_sapiens, there are two more versions, the default `homo_sapien` and the other one `homo_sapiens_merged`. If download them all, then the folder would look like:
```
${PWD}/ensembl-vep
├── homo_sapiens
│   ├── 111_GRCh37
│   └── 111_GRCh38
├── homo_sapiens_merged
│   ├── 111_GRCh37
│   └── 111_GRCh38
├── homo_sapiens_refseq
│   ├── 111_GRCh37
│   └── 111_GRCh38
```

VEP's annotation folder selection logic: 
- By default vep will use `${dir_cache}/${species}/${cache_version}_${assembly}`
- If `--refseq` is provided, it will use `${dir_cache}/${species}_refseq/${cache_version}_${assembly}`
- If `--merged` is provided, it will use `${dir_cache}/${species}_merged/${cache_version}_${assembly}`

Below are mappings between vep command options and sarek input params:
- `$dir_cache: $vep_cache`
- `$species: $vep_species`
- `$cache_version: $vep_cache_version`
- `$assembly: $vep_genome`

However, I'm not sure if this is the case for other species other than homo_sapiens.
For homo_sapiens, the correct logic to parse might be:
- check if `vep_custom_args` contains `--refseq/--merged`
- if both not exist, then validate annotation folder  existence: `${vep_cache}/${vep_species}/${vep_cache_version}_${vep_genome}`
- if any of them exists, should validate annotation folder existence: `${vep_cache}/${vep_species}_refseq/${vep_cache_version}_${vep_genome}` or `${vep_cache}/${vep_species}_merged/${vep_cache_version}_${vep_genome}` ; note that `$vep_species` should still be `homo_sapiens` (because we need `--species homo_sapiens` in the vep command)

A more consistent way might be to provide another sarek input param to deal with vep's `--refseq/--merged` option

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",YeHW,https://github.com/nf-core/sarek/issues/1559
I_kwDOCvwIC86L2JeA,too many genotypes in the combined VCF record,OPEN,2024-06-11T11:55:04Z,2024-08-19T14:48:29Z,,"### Description of the bug

hi,i find 
```
process > NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_JOINT_CALLING_GERMLINE_GATK:GATK4_GENOTYPEGVCFS (joint_variant_calling)"" going wrong.
I checked the log file , 
Sample/Callset D_D900002( TileDB row idx 0) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900005( TileDB row idx 1) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900006( TileDB row idx 2) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900035( TileDB row idx 3) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900036( TileDB row idx 4) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900038( TileDB row idx 5) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900039( TileDB row idx 6) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900049( TileDB row idx 7) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900053( TileDB row idx 8) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900054( TileDB row idx 9) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles, 
ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added         for this sample for this location.

Sample/Callset D_D900055( TileDB row idx 10) at Chromosome chr4D_part1 position 10150573 (TileDB column 7817502166) has too many genotypes in the combined VCF record : 1081 : current limit : 1024 (num_alleles,
 ploidy) = (46, 2). Fields, such as  PL, with length equal to the number of genotypes will NOT be added        for this sample for this location.
```

How can I solve this problem ,looking forward to a reply,thank you!

### Command used and terminal output

```console
nextflow run /cluster/home/fanrong/biosofts/nextflow/nf-core/nf-core-sarek/3_4_0 -profile zwnj_2022 -offline --input /public/home/fanrong/projects/AK58/03_nfcore/samplesheet.csv --outdir ./result --step mapping --fasta /public/home/fanrong/projects/AK58/fasta/wheat_AK58v4MP.genome_part.fa --fasta_fai /public/home/fanrong/projects/AK58/fasta/wheat_AK58v4MP.genome_part.fa.fai --dict /public/home/fanrong/projects/AK58/fasta/wheat_AK58v4MP.genome_part.dict --trim_fastq --aligner bwa-mem2 --max_cpus 16 --max_memory 100.GB --max_time 720.h --tools haplotypecaller --bwamem2 /cluster/home/fanrong/work_lei/BSA/01_ZG/02_mapping_3/work/33/706371da5349b7b033e07f32ae9d58/bwamem2 --igenomes_ignore --genome null --skip_tools baserecalibrator,haplotypecaller_filter --joint_germline --split_fastq 0
```


### Relevant files

_No response_

### System information

_No response_",fan040,https://github.com/nf-core/sarek/issues/1560
I_kwDOCvwIC86L7xw7,Pooled normals for CNVkit,OPEN,2024-06-12T03:08:51Z,2024-06-12T03:08:51Z,,"### Description of feature

Hi, CNVkit recommends to pool all normal samples, instead of using one matched normal sample for each tumor sample https://cnvkit.readthedocs.io/en/stable/pipeline.html#paired-or-pooled-normals.  
Could you please implement an option, similar to `--joint_germline` and `--joint_mutect2`, to use multiple or all normal samples as CNVkit reference? 

Thanks a lot!",HenriettaHolze,https://github.com/nf-core/sarek/issues/1562
I_kwDOCvwIC86MD5Np,Question for developping a Sarek/nf-core GUI,CLOSED,2024-06-12T23:21:43Z,2024-06-14T00:27:28Z,2024-06-13T07:44:21Z,"### Description of feature

 I'm interested in developing an web application with a graphical user interface (GUI) to streamline the use of nf-core pipelines(sarek) for biologists. The goal is to create a user-friendly tool that would allow users to easily specify analysis parameters, manage input and output data, and track the progress of analyses, all without needing to use the command line or master technical aspects of computing. If you have ideas on how to design and develop such an application, or if you have practical advice for approaching this project, I would greatly appreciate hearing them. Thanks in advance for your help!  ",Sadiki-Noureddine,https://github.com/nf-core/sarek/issues/1565
I_kwDOCvwIC86MGBU9,Missing import statements on error messages when starting without samplesheet,CLOSED,2024-06-13T07:35:33Z,2024-10-30T10:32:38Z,2024-10-30T10:32:36Z,"### Description of the bug

When not supplying `--input` (directly or indirectly through use of profile) in a cmd like
```
nextflow run nf-core/sarek -r 3.4.2 -profile docker --outdir results
```
on gets the not-so-informative error msg:
```
ERROR ~ No such variable: Nextflow

 -- Check script '/home/ubuntu/.nextflow/assets/nf-core/sarek/./subworkflows/local/utils_nfcore_sarek_pipeline/main.nf' at line: 344 or see '.nextflow.log' file for more details
```

It might not be so straightforward to improve on this, as not all commands for sarek require `--input`:

https://nfcore.slack.com/archives/C05V9FRJYMV/p1718263762323759?thread_ts=1718262962.010069&cid=C05V9FRJYMV



### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1566
I_kwDOCvwIC86Micd8,samplesheet issue,CLOSED,2024-06-17T17:18:16Z,2024-08-19T13:21:23Z,2024-08-19T13:21:23Z,"              Hey, this problem has not been entirely fixed... 
I am running with a local genome with` ignore igenomes` set and have a sample sheet with both status entered: 0 and 1. 
However, this error is still occurring making the workflow un-runnable for version: **nf-core/sarek v3.4.2**

`
nextflow.exception.WorkflowScriptErrorException: 
The sample-sheet only contains tumor-samples, but the following tools,  which were requested by the option ""tools"", expect at least one normal-sample : haplotypecaller
`

**COMMAND:**

`
nextflow \
    main.nf \
    -c ""pfr_profile.config"" \
    -profile pfr,singularity \
    -params-file ""pfr_params.json"" \
	--input ${INPUT} \
    --outdir ""${OUT_DIR}"" \
	--genome null \
    --igenomes_ignore \
	--wes true \
	--skip_tools baserecalibrator \
	--tools ""freebayes,haplotypecaller,mpileup"" \
    --fasta ${FASTA} \
    --fasta_fai ${FAI} \
	--dict ${DICT} \
	--trim_fastq true \
	--save_trimmed true \
    --save_reference \
	--save_mapped true \
	--save_output_as_bam true \
    -resume
`

**Note, the pfr profile only specifies SLURM as the executor.**

**pfr_params.jason:**

`
{
	""genome"": null,
    ""igenomes_ignore"": true,
    ""save_reference"": true,
    ""split_fastq"": 50000000,
    ""trim_fastq"": true,
    ""save_trimmed"": true,
    ""aligner"": ""bwa-mem"",
    ""save_mapped"": true,
    ""save_output_as_bam"": true
}
`

**Here is the error in the log file:**

`
Jun-14 23:58:10.815 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: slurm
Jun-14 23:58:10.815 [main] DEBUG nextflow.executor.ExecutorFactory - >> processorType: 'slurm'
Jun-14 23:58:10.831 [main] DEBUG nextflow.Session - Config process names validation disabled as requested
Jun-14 23:58:10.833 [main] DEBUG nextflow.Session - Igniting dataflow network (184)
Jun-14 23:58:10.844 [Actor Thread 5] ERROR nextflow.extension.OperatorImpl - @unknown
java.lang.NullPointerException: Cannot get property 'baseName' on null object
	at org.codehaus.groovy.runtime.NullObject.getProperty(NullObject.java:60)
	at org.codehaus.groovy.runtime.InvokerHelper.getProperty(InvokerHelper.java:190)
	at org.codehaus.groovy.runtime.callsite.NullCallSite.getProperty(NullCallSite.java:46)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callGetProperty(AbstractCallSite.java:329)
	at Script_48a2d276$_runScript_closure1$_closure2$_closure6.doCall(Script_48a2d276:132)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:38)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:53)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:139)
	at nextflow.extension.MapOp$_apply_closure1.doCall(MapOp.groovy:56)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:107)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:274)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1035)
	at groovy.lang.Closure.call(Closure.java:412)
	at groovyx.gpars.dataflow.operator.DataflowOperatorActor.startTask(DataflowOperatorActor.java:120)
	at groovyx.gpars.dataflow.operator.DataflowOperatorActor.onMessage(DataflowOperatorActor.java:108)
	at groovyx.gpars.actor.impl.SDAClosure$1.call(SDAClosure.java:43)
	at groovyx.gpars.actor.AbstractLoopingActor.runEnhancedWithoutRepliesOnMessages(AbstractLoopingActor.java:293)
	at groovyx.gpars.actor.AbstractLoopingActor.access$400(AbstractLoopingActor.java:30)
	at groovyx.gpars.actor.AbstractLoopingActor$1.handleMessage(AbstractLoopingActor.java:93)
	at groovyx.gpars.util.AsyncMessagingCore.run(AsyncMessagingCore.java:132)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
`

**Here is the sample sheet:**

`
patient,status,sample,lane,fastq_1,fastq_2

GC,1,GC_CAT13ANXX_TTAGGC,L001,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_A_GC_CAT13ANXX_TTAGGC_L001_R1.fastq.gz,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_A_GC_CAT13ANXX_TTAGGC_L001_R2.fastq.gz
GC,1,GC_CAT13ANXX_TTAGGC,L002,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_A_GC_CAT13ANXX_TTAGGC_L002_R1.fastq.gz,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_A_GC_CAT13ANXX_TTAGGC_L002_R2.fastq.gz
GC,1,GC_CAT13ANXX_TTAGGC,L003,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_A_GC_CAT13ANXX_TTAGGC_L003_R1.fastq.gz,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_A_GC_CAT13ANXX_TTAGGC_L003_R2.fastq.gz
SW,0,SW_CAT13ANXX_TGACCA,L001,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_B_SW_CAT13ANXX_TGACCA_L001_R1.fastq.gz,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_B_SW_CAT13ANXX_TGACCA_L001_R2.fastq.gz
SW,0,SW_CAT13ANXX_TGACCA,L002,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_B_SW_CAT13ANXX_TGACCA_L002_R1.fastq.gz,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_B_SW_CAT13ANXX_TGACCA_L002_R2.fastq.gz
SW,0,SW_CAT13ANXX_TGACCA,L003,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_B_SW_CAT13ANXX_TGACCA_L003_R1.fastq.gz,/WGS/AGRF_CAGRF14129_CAT13ANXX/DH_B_SW_CAT13ANXX_TGACCA_L003_R2.fastq.gz
`

_Originally posted by @charlesdavid in https://github.com/nf-core/sarek/issues/1260#issuecomment-2169063980_
            ",maxulysse,https://github.com/nf-core/sarek/issues/1567
I_kwDOCvwIC86MqETb,Sanitize path,CLOSED,2024-06-18T13:13:01Z,2024-08-19T13:19:45Z,2024-08-19T13:19:45Z,"### Description of the bug

params.outdir + ""/path/output/""

->

file(params.outdir + ""/path/output/"").toUriString()

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1568
I_kwDOCvwIC86Mqng0,snpeff capitalization in tower.yml,CLOSED,2024-06-18T14:16:22Z,2024-08-22T19:03:46Z,2024-08-22T19:03:46Z,"The snpEff reports were not present in the 'reports' section of platform until I changed the capitalization of `snpEff` to all lower case to match the subdir name

https://github.com/nf-core/sarek/blob/b5b766d3b4ac89864f2fa07441cdc8844e70a79e/tower.yml#L56

https://github.com/nf-core/sarek/blob/b5b766d3b4ac89864f2fa07441cdc8844e70a79e/tower.yml#L54",cmatKhan,https://github.com/nf-core/sarek/issues/1569
I_kwDOCvwIC86NDrw9,Error while loading shared library for BWA in Sarek pipeline with Singularity,OPEN,2024-06-21T13:27:51Z,2024-10-09T18:11:21Z,,"### Description of the bug

Running the Sarek pipeline while using Singularity results in the following error: bwa: error while loading shared lib. This issue occurs occasionally when running bwa to get the version information, producing the message: bwa: failed to load shared library. This error string is included instead of the version in the intermediate version.yml file generated by the module, leading to a corrupt YAML file (missing quotes around the string due to the presence of :). Consequently, the Nextflow job, that combines all these version.yml files into one, fails. Note that this error is occurring after 40 subsamples. 

### Command used and terminal output

```console
./nextflow run nf-core/sarek -r 3.4.2 -profile singularity --input ./input-sarek.csv --outdir /home/sarek-nf-results --genome null --igenomes_ignore --fasta ./GCF_002263795.3_ARS-UCD2.0_genomic.fna  --skip_tools baserecalibrator --tools deepvariant

[b9/3b06bd] process > NFCORE_SAREK:SAREK:FASTQC (zr8425-99-lane_99)                                                              [ 98%] 259 of 263, cached: 259
[d2/7b9351] process > NFCORE_SAREK:SAREK:FASTP (zr8425-595-lane_595)                                                             [ 56%] 149 of 263, cached: 149
[1f/2990a2] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (zr8425-105-lane_105)              [  2%] 40 of 1787
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM2_MEM                                    -
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:DRAGMAP_ALIGN                                  -
[-        ] process > NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:SENTIEON_BWAMEM                                -
[-        ] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES                                                 -
[-        ] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:SAMTOOLS_STATS                             -
[-        ] process > NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:CRAM_QC_MOSDEPTH_SAMTOOLS:MOSDEPTH                                   -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_TO_BAM                                                                             -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP                                         -
Plus 9 more processes waiting for tasks…
ERROR ~ mapping values are not allowed here
 in 'reader', line 2, column 13:
        bwa: bwa: error while loading shared lib ...
                ^
```


### Relevant files

_No response_

### System information

_No response_",Simonassaf,https://github.com/nf-core/sarek/issues/1575
I_kwDOCvwIC86NY4_E,Test and document that Sarek can handle input-samplesheets with mixed samples,OPEN,2024-06-25T09:00:31Z,2024-08-19T14:46:19Z,,"### Description of feature

Test and document that Sarek can handle input-samplesheets with ""mixed"" samples, that is, a csv-file containing fastq.gz, fastq.gz.spring, bam and cram.

We already have a csv [here](https://github.com/nf-core/sarek/blob/dev/tests/csv/3.0/bam_and_fastq_and_spring.csv) and corresponding test [here](https://github.com/nf-core/sarek/blob/dev/tests/test_alignment_from_everything.yml) with fastq.gz, fastq.gz.spring, bam and cram.

On https://github.com/nf-core/sarek/pull/1573, I tested that dev-Sarek is able to handle input-samples with ""mixed"" samples, and, AFAICT, it worked fine; see ""test-results"" below.

If it works, then we can perhaps also remove [this](https://github.com/nf-core/sarek/blob/b5b766d3b4ac89864f2fa07441cdc8844e70a79e/workflows/sarek/main.nf#L167-L169) comment in the source code.

--------------------------------------


I ran the following test:
```
nextflow run main.nf -profile test,alignment_from_everything,docker --outdir results
```
which takes this ""mixed"" [csv](https://github.com/nf-core/sarek/blob/dev/tests/csv/3.0/bam_and_fastq_and_spring.csv) as input.

I've taken a close look at the resulting execution_report.html-file and, AFAICT, the pipeline processes the input exactly as hoped. Below I have pasted the commands (from execution_report.html) which were used to process the samples from fastq.gz.spring-files to bam/cram-files:

--------------------------------------------------------------------------------------------------------
NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R1_FQ (test2-test2_L1)
```
spring \
        -d \
        -g \
        -t 2 \
         \
        -i test_1.fastq.gz.spring \
        -o test_1.fastq.gz
```
--------------------------------------------------------------------------------------------------------
NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R2_FQ (test2-test2_L1)
```
spring \
        -d \
        -g \
        -t 2 \
         \
        -i test_2.fastq.gz.spring \
        -o test_2.fastq.gz
```
--------------------------------------------------------------------------------------------------------
NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_FQ_PAIR (test3-test3_L1)
```
spring \
        -d \
        -g \
        -t 2 \
         \
        -i test_R1_R2.fastq.gz.spring \
        -o test_R1_R2_R1.fastq.gz test_R1_R2_R2.fastq.gz
```
--------------------------------------------------------------------------------------------------------
NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test)
```
    bwa mem \
        -K 100000000 -Y -R ""@RG\tID:null.test.test_L1\tPU:test_L1\tSM:test_test\tLB:test\tDS:s3://ngi-igenomes/testdata/nf-core/modules/genomics/homo_sapiens/genome/genome.fasta\tPL:ILLUMINA"" \
        -t 2 \
        $INDEX \
        test_1.fastq.gz test_2.fastq.gz \
        | samtools sort   --threads 2 -o test.sorted.bam -
```
--------------------------------------------------------------------------------------------------------

NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test2)
```
    bwa mem \
        -K 100000000 -Y -R ""@RG\tID:null.test2.test2_L1\tPU:test2_L1\tSM:test2_test2\tLB:test2\tDS:s3://ngi-igenomes/testdata/nf-core/modules/genomics/homo_sapiens/genome/genome.fasta\tPL:ILLUMINA"" \
        -t 2 \
        $INDEX \
        test_1.fastq.gz test_2.fastq.gz \
        | samtools sort   --threads 2 -o test2.sorted.bam -
```
--------------------------------------------------------------------------------------------------------
NFCORE_SAREK:SAREK:FASTQ_ALIGN_BWAMEM_MEM2_DRAGMAP_SENTIEON:BWAMEM1_MEM (test3)
```
    bwa mem \
        -K 100000000 -Y -R ""@RG\tID:null.test3.test3_L1\tPU:test3_L1\tSM:test3_test3\tLB:test3\tDS:s3://ngi-igenomes/testdata/nf-core/modules/genomics/homo_sapiens/genome/genome.fasta\tPL:ILLUMINA"" \
        -t 2 \
        $INDEX \
        test_R1_R2_R1.fastq.gz test_R1_R2_R2.fastq.gz \
        | samtools sort   --threads 2 -o test3.sorted.bam -
```
--------------------------------------------------------------------------------------------------------
NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (test_bam)
```
gatk --java-options ""-Xmx5324M -XX:-UsePerfData"" \
        MarkDuplicates \
        --INPUT test_bam.sorted.bam \
        --OUTPUT test_bam.md.bam \
        --METRICS_FILE test_bam.md.cram.metrics \
        --TMP_DIR . \
        --REFERENCE_SEQUENCE genome.fasta \
        -REMOVE_DUPLICATES false -VALIDATION_STRINGENCY LENIENT
```
--------------------------------------------------------------------------------------------------------
NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (test)
```
gatk --java-options ""-Xmx5324M -XX:-UsePerfData"" \
        MarkDuplicates \
        --INPUT test.sorted.bam \
        --OUTPUT test.md.bam \
        --METRICS_FILE test.md.cram.metrics \
        --TMP_DIR . \
        --REFERENCE_SEQUENCE genome.fasta \
        -REMOVE_DUPLICATES false -VALIDATION_STRINGENCY LENIENT
```
--------------------------------------------------------------------------------------------------------
NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (test2)
```
gatk --java-options ""-Xmx5324M -XX:-UsePerfData"" \
        MarkDuplicates \
        --INPUT test2.sorted.bam \
        --OUTPUT test2.md.bam \
        --METRICS_FILE test2.md.cram.metrics \
        --TMP_DIR . \
        --REFERENCE_SEQUENCE genome.fasta \
        -REMOVE_DUPLICATES false -VALIDATION_STRINGENCY LENIENT
```
--------------------------------------------------------------------------------------------------------
NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (test3)
```
gatk --java-options ""-Xmx5324M -XX:-UsePerfData"" \
        MarkDuplicates \
        --INPUT test3.sorted.bam \
        --OUTPUT test3.md.bam \
        --METRICS_FILE test3.md.cram.metrics \
        --TMP_DIR . \
        --REFERENCE_SEQUENCE genome.fasta \
        -REMOVE_DUPLICATES false -VALIDATION_STRINGENCY LENIENT
```
--------------------------------------------------------------------------------------------------------
output in `preprocessing/markduplicates`:
```
├── test
│   ├── test.md.cram
│   └── test.md.cram.crai
├── test2
│   ├── test2.md.cram
│   └── test2.md.cram.crai
├── test3
│   ├── test3.md.cram
│   └── test3.md.cram.crai
└── test_bam
    ├── test_bam.md.cram
    └── test_bam.md.cram.crai
```
",asp8200,https://github.com/nf-core/sarek/issues/1576
I_kwDOCvwIC86OAFTY,Human and mouse read disambiguation for PDX samples?,OPEN,2024-06-30T19:10:42Z,2024-07-08T18:24:00Z,,"### Description of feature

Hi,

Thank you for creating this awesome pipeline. I'm wondering if there are any modules within sarek that do disambiguation of mouse and human reads for PDX samples? For example like the disambiguate tool from Astra Zeneca:

https://github.com/AstraZeneca-NGS/disambiguate

Thanks for your time and help.

Best,
Asher",apsteinberg,https://github.com/nf-core/sarek/issues/1578
I_kwDOCvwIC86OMWjM,Pipeline is failing on GCP due to double slash when creating new samplesheets,CLOSED,2024-07-02T08:31:04Z,2024-08-19T14:46:01Z,2024-08-19T14:46:01Z,"### Description of the bug

If the `outdir` parameter ends on `/` the pipeline currently crashes on GCP with 

```
I/O not allowed on dot-dirs or extra slashes when !permitEmptyPathComponents: /<path>/<file>```
```

This likely needs a fix upstream. Reporting here for visibility. 

Current work around: set `--outdir` without trailing slash
",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1580
I_kwDOCvwIC86OxcUL,DeepVariant params addition request: ,OPEN,2024-07-08T11:04:19Z,2024-07-23T07:35:57Z,,"### Description of feature

**Description**:
I am writing to request the addition of parameters for specifying haploid contigs and regions when detecting SNPs and Indels using DeepVariant in the nf-core/sarek pipeline. These parameters are essential for our benchmarking and analysis using GIAB data.

**Proposed Parameters**:
```
--haploid_contigs ""${HAPLOID_CONTIGS}""
--regions ""${REGION}""
```
Use Case:
Including these parameters will allow users to define specific regions and haploid contigs for their analysis, improving the flexibility and accuracy of the SNP and Indel detection process.

**Example Usage**:

  ```
""haploid_contigs"": ""chrX,chrY"",
  ""regions"": ""chr20:10,000,000-10,500,000""
```

This example demonstrates how users can specify the haploid contigs and regions in the params.json file.

**Benefits**:
Enhanced control over the genomic regions being analyzed.
Improved accuracy for SNP and Indel detection, especially in specialized cases like haploid genomes.
Thank you for considering this request. Your assistance in improving the nf-core/sarek pipeline is greatly appreciated. 
@maxulysse 

Docs for help
[1.](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-haploid-support.md)

[2.](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-xy-calling-case-study.md)

Best regards,
Harsh Poddar",poddarharsh15,https://github.com/nf-core/sarek/issues/1583
I_kwDOCvwIC86O-xjk,Add an option to optionally use the GATK VariantFiltration instead of VQSR,OPEN,2024-07-09T17:58:33Z,2024-08-19T13:12:41Z,,"### Description of feature

For haplotypecaller, if known snps or indels are not passed, then VQSR can't be used.

VQSR should be set by default. But, the user should be able to select VariantFiltration if they would rather set filter thresholds explicitly. If known snps or indels are not passed in, and haplotypecaller is selected, then VariantFiltration should be used automatically.

https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering#2",cmatKhan,https://github.com/nf-core/sarek/issues/1586
I_kwDOCvwIC86PNGQK,Add lofreq call-parallel (tumor-only) and lofreq somatic as variant callers,OPEN,2024-07-11T08:24:45Z,2024-07-11T08:24:45Z,,"### Description of feature

Hi all, this issue is just to let you know that @AitorPeseta and I are gonna try to add **lofreq** as a variant caller to Sarek. We will try to add the tumor only version (`lofreq call-parallel`) and the somatic/paired sample version (`lofreq somatic`).

Our idea is to work on different branches on his fork and then make only one PR, if that's okay.",nevinwu,https://github.com/nf-core/sarek/issues/1588
I_kwDOCvwIC86POpk4,base.conf does not override module's process.label for CNVKIT_BATCH,OPEN,2024-07-11T11:49:24Z,2024-08-19T14:45:27Z,,"### Description of the bug

Hello, and first of all thank you for this sophisticated and smart tool!

I was running a slightly modified parameter set (through CLI only) on a newly set up slurm cluster. A CNVKIT_BATCH processes was killed with `Process exceeded running time limit (4h)`, it does not look as expected behavior as to my understanding `/conf/base.config` [should](https://github.com/nf-core/sarek/blob/b5b766d3b4ac89864f2fa07441cdc8844e70a79e/conf/base.config#L73) assign `process_high` to it. 

Attaching the detailed log and the slurm job definition.

### Command used and terminal output

```console
sbatch run.slurm
```


### Relevant files

[run.slurm.txt](https://github.com/user-attachments/files/16176175/run.slurm.txt)
[nextflow.log.1.txt](https://github.com/user-attachments/files/16176177/nextflow.log.1.txt)


### System information


```
[]$ sinfo
PARTITION   AVAIL  TIMELIMIT  NODES  STATE NODELIST
cpu*           up 10-00:00:0      4  drain cpu[104,143-144,150]
cpu*           up 10-00:00:0      9    mix cpu[106-108,112,117-118,136-137,145]
cpu*           up 10-00:00:0     61   idle cpu[105,109-111,113-116,119-135,138-142,146-149,151-176,301]
gpu            up 10-00:00:0      1  down* gpu117
gpu            up 10-00:00:0      1  drain gpu107
gpu            up 10-00:00:0     13    mix gpu[102-106,108-110,115-116,118-120]
gpu            up 10-00:00:0      4   idle gpu[111-114]
gpua100        up 10-00:00:0      1   idle gpu201
interactive    up    4:00:00      4   idle cpu[101-103],gpu101",dimalvovs,https://github.com/nf-core/sarek/issues/1591
I_kwDOCvwIC86PvwBe,GATK4_MARKDUPLICATES : Not a valid path value type: java.util.ArrayList,OPEN,2024-07-16T17:11:31Z,2024-10-28T09:54:26Z,,"### Description of the bug

During running sarek on a tumor vs normal analysis, I get the following error message when BWAMEM2 step is completed and then the run halts. I cannot see any limiting resources (RAM, CPU, free disk space) on my side.

### Command used and terminal output

```console
$sudo nextflow run nf-core/sarek -r 3.4.2 -profile docker --input samplesheet_30K.csv -params-file ./nf-params.json --split_fastq 0 --outdir_cache ./cache --joint_mutect2 -c custom_config_somatic.txt --igenomes_base /workdir/ref/igenomes --step mapping --skip_tools fastqc

Execution cancelled -- Finishing pending tasks before exit
-[nf-core/sarek] Pipeline completed with errors-
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_MARKDUPLICATES:GATK4_MARKDUPLICATES (1)'

Caused by:
  Not a valid path value type: java.util.ArrayList ([/workdir/work/63/7e3a890853ae5d29db768ee2a54d67/tumor_sample-lane_1.sorted.bam, /workdir/work/63/7e3a890853ae5d29db768ee2a54d67/tumor_sample-lane_1.sorted.bam.tmp.0036.bam, /workdir/work/63/7e3a890853ae5d29db768ee2a54d67/tumor_sample-lane_1.sorted.bam.tmp.0041.bam, /workdir/work/63/7e3a890853ae5d29db768ee2a54d67/tumor_sample-lane_1.sorted.bam.tmp.0044.bam, /workdir/work/63/7e3a890853ae5d29db768ee2a54d67/tumor_sample-lane_1.sorted.bam.tmp.0056.bam])



Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

 -- Check '.nextflow.log' file for details
```


### Relevant files

[nf-params.json](https://github.com/user-attachments/files/16254250/nf-params.json)
[custom_config_somatic.txt](https://github.com/user-attachments/files/16254254/custom_config_somatic.txt)


### System information

  Version: 24.04.3 build 5916
  Created: 09-07-2024 19:35 UTC (12:35 PDT)
  System: Linux 5.15.0-112-generic
  Runtime: Groovy 4.0.21 on OpenJDK 64-Bit Server VM 21.0.3+9-Ubuntu-1ubuntu122.04.1
  Encoding: UTF-8 (UTF-8)



    Nextflow version (eg. 23.04.3)
    Hardware (eg. HPC, Desktop, Cloud): HPC
    Executor (eg. slurm, local, awsbatch): local
    Container engine: (e.g. Docker, Singularity, Conda, Podman, Shifter, Charliecloud, or Apptainer): docker
    OS (eg. CentOS Linux, macOS, Linux Mint): Ubuntu 22.04
    Version of nf-core/sarek (eg. 1.1, 1.5, 1.8.2): 3.4.2

",farshadf,https://github.com/nf-core/sarek/issues/1598
I_kwDOCvwIC86Pyxzi,Add `msisensor-pro` tumor-only mode,OPEN,2024-07-17T02:08:17Z,2024-08-08T08:11:27Z,,"### Description of feature

Since `msisensor-pro` can now detect MSI from tumor only samples and [works](https://github.com/xjtu-omics/msisensor-pro/issues/31) with `cram`, should we [add](https://github.com/nf-core/sarek/issues/330) it to the pipeline?",bounlu,https://github.com/nf-core/sarek/issues/1599
I_kwDOCvwIC86QKBXA,Slurm crashes e.g. FASTP without informative error,CLOSED,2024-07-19T10:10:05Z,2024-07-23T10:12:54Z,2024-07-23T09:12:00Z,"### Description of the bug

Hey there. I'm trying to make the pipeline reliably run on our cluster with mostly default settings at the moment and I'm running into some weird issues.

In essence, some tasks --most notably FASTP at the moment-- seem to get killed almost immediately by slurm. Now, I tried to follow the troubleshooting guide and navigated to the work directory where things broke and submitted -- from the same node that I started the pipeline from and with the same versions of Singularity and Nextflow loaded -- the .command.run script and ... it works. But now I'm none the wiser in what caused the process to fail in the first place.

Any ideas what causes this or what I could do to trace the root cause?

### Command used and terminal output

```console
$ nextflow run nf-core/sarek -r 3.4.2 -profile slurm,singularity --outdir sarek_GATK_hg38_output/ --input ../samplesheet.csv --genome GATK.GRCh38

Something to the effect of:
(I replaced some info with #=#something#=#, paths were definitly valid before)

Jul-18 16:40:16.873 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'NFCORE_SAREK:SAREK:FASTP (#=#othersample#=#-4)'

Caused by:
  Process `NFCORE_SAREK:SAREK:FASTP (#=#othersample#=#-4)` terminated for an unknown reason -- Likely it has been terminated by the external system

Command executed:

  [ ! -f  #=#othersample#=#-4_1.fastq.gz ] && ln -sf #=#othersample#=#_S4_R1_001.fastq.gz #=#othersample#=#-4_1.fastq.gz
  [ ! -f  #=#othersample#=#-4_2.fastq.gz ] && ln -sf #=#othersample#=#_S4_R2_001.fastq.gz #=#othersample#=#-4_2.fastq.gz
  fastp \
      --in1 #=#othersample#=#-4_1.fastq.gz \
      --in2 #=#othersample#=#-4_2.fastq.gz \
      --out1 #=#othersample#=#-4_1.fastp.fastq.gz \
      --out2 #=#othersample#=#-4_2.fastp.fastq.gz \
      --json #=#othersample#=#-4.fastp.json \
      --html #=#othersample#=#-4.fastp.html \
       \
       \
       \
      --thread 12 \
      --detect_adapter_for_pe \
      --disable_adapter_trimming      --split_by_lines 200000000 \
      2> >(tee #=#othersample#=#-4.fastp.log >&2)

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:FASTP"":
      fastp: $(fastp --version 2>&1 | sed -e ""s/fastp //g"")
  END_VERSIONS

Command exit status:
  -

Command output:
  (empty)

Work dir:
  /mnt/#=#repo-path#=#/sarek/output/work/b0/4ae313db18b0198d97d76f5c99f045

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`
```

But the output/work/b0/4ae313db18b0198d97d76f5c99f045 workdir is mostly empty except for .command.run and .command.sh. I attached (sanitized versions of) everything below.
```


### Relevant files

Renamed and added a .txt so that github does not complain. I also removed maybe-sensitive information as per our guidelines and shortened that repetitive parts of the logfile. Comments and replacements are marked by `#=#something#=#`.

[b0_4a.workdir.command.sh.sanitized.txt](https://github.com/user-attachments/files/16310480/b0_4a.workdir.command.sh.sanitized.txt)
[nextflow.log.excerpt.sanitized.txt](https://github.com/user-attachments/files/16310481/nextflow.log.excerpt.sanitized.txt)
[b0_4a.workdir.command.run.sanitized.txt](https://github.com/user-attachments/files/16310482/b0_4a.workdir.command.run.sanitized.txt)


### System information

nextflow version 23.10.0.5891
Singularity version 3.8.7
Hardware: slurm cluster, started from an interactive node
Executor: Slurm
Container: Singularity
OS (on the cluster I presume): 20.04.6 LTS (GNU/Linux 5.4.0-182-generic x86_64)
Sarek Version: 3.4.2",FPGro,https://github.com/nf-core/sarek/issues/1600
I_kwDOCvwIC86QWTQp,ERROR ~ Error executing process > 'NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX',CLOSED,2024-07-22T02:24:31Z,2024-08-19T14:44:46Z,2024-08-19T14:44:46Z,"### Description of the bug

I have got the following error:
```
ERROR ~ Error executing process > 'NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX'

Caused by:
  The target server failed to respond
```

In the log file, I saw this error: 

```S3 download directory: s3://ngi-igenomes/igenomes/Homo_sapiens/NCBI/GRCh38/Sequence/BWAIndex/version0.6.0 interrupted```

I can see from the staging file that there are version0.6.0 files therre but I am not sure what is causing the issue. 


### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.2 -profile docker --input samplesheet.csv --outdir results --genome NCBI.GRCh38 --skip_tools baserecalibrator
```


### Relevant files

_No response_

### System information

_No response_",YasirKusay,https://github.com/nf-core/sarek/issues/1601
I_kwDOCvwIC86QzAm7,Mutect2 - PON - GRCh38 - Question,CLOSED,2024-07-25T07:54:40Z,2024-07-26T07:34:26Z,2024-07-26T07:34:04Z,"### Description of the bug

Hi,
I'm writing to ask about the PON panel when executing Mutect2 in tumor-only mode. If I’m using the GATK.GRCh38 genome, do I need to create a PON, or will the pipeline download it? I ask because I don’t have enough samples to create one. I've seen messages stating that I need to create one before using the pipeline, while others suggest this genome provides one (though the best way is to create one with your own normal samples).
Thanks for your support!

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",jbague,https://github.com/nf-core/sarek/issues/1603
I_kwDOCvwIC86Q_o8A,"how to link the information provided in the snpEff reports to detailed variant information in annotation vcf file such as rsID, chr, pos, etc",CLOSED,2024-07-26T16:49:06Z,2024-08-19T14:44:33Z,2024-08-19T14:44:33Z,"I'm unsure how to link the information provided in the snpEff reports to detailed variant information such as rsID, chromosome (chr), and position (pos) within the VCF files. I have snpEff reports *csv, *html, *txt files, and more detailed variant annotation files *vcf. However, if I want to focus on high impact variants and genes, how can I access this data in the *vcf file? ",slives-lab,https://github.com/nf-core/sarek/issues/1604
I_kwDOCvwIC86RKT3l,Samplesheet of sarek,CLOSED,2024-07-29T13:13:37Z,2024-11-13T10:10:27Z,2024-08-08T12:27:37Z,"### Description of the bug

I am getting an error in the samplesheet composition when running sarek pipelines. I have read many post about this kind of error when runned with different pipelines, but I didn't find anything about a possible solution for my issue.
Speciffically the error reports this message:
""The sample-sheet only contains normal-samples, but the following tools, which were requested with ""--tools"", expect at least one tumor-sample : mutect2""
Unfortunately, I don't understand what I missed since my samplesheet have both normal and tumor samples (also relapse tumor for some samples), as specified in the usage page.  

### Command used and terminal output

```console
source /share/data/apps/anaconda3/bin/activate pipelines

nextflow run nf-core/sarek -profile singularity \
--input /share/project3/home/perciostefano/sts/Sarcomics/WES/samplesheet_prova.csv \
--step mapping \
--outdir /share/project3/home/perciostefano/sts/Sarcomics/WES/results/ \
--wes \
--intervals /share/project3/home/perciostefano/reference/genome/hg38_Twist_ILMN_Exome_2.5_Panel_annotated.bed \
--tools cnvkit,mutect2,strelka,merge \
--trim_fastq \
--aligner bwa-mem \
--save_mapped \
--save_output_as_bam \
--only_paired_variant_calling \
--joint_mutect2 \
--genome hg38 \
--save_reference \
--multiqc_title ""sts_Sarcomics_wes_report""
```


### Relevant files

[samplesheet_prova.csv](https://github.com/user-attachments/files/16413729/samplesheet_prova.csv)
[nextflow.log](https://github.com/user-attachments/files/16413791/nextflow.log)



### System information

N E X T F L O W   ~  version 24.04.3
HPC
Slurm
Singularity
nf-core/sarek v3.4.2-gb5b766d",Poocee,https://github.com/nf-core/sarek/issues/1605
I_kwDOCvwIC86RZVrv,cnvkit reference titling broken,OPEN,2024-07-31T07:27:50Z,2024-10-29T14:27:31Z,,"### Description of the bug

I looked at the CNVKit reference file. It looks like it's meant to have the prefix in the title.  That doesn't seem to work as all of my reference files are just called  reference.cnn

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",SethMagnusJarvis,https://github.com/nf-core/sarek/issues/1606
I_kwDOCvwIC86RZqFd,Allow specifying `--fasta` for VEP,CLOSED,2024-07-31T08:14:23Z,2024-07-31T12:28:04Z,2024-07-31T10:03:26Z,"### Description of feature

This is required to allow generation of HGVSc and HGVSp nomenclature when running offline (which is the default for sarek).

From the VEP docs:

> To generate HGVS identifiers when using [--cache](http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html#opt_cache) or [--offline](http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html#opt_offline) you must use a FASTA file and [--fasta](http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html#opt_fasta).",lbeltrame,https://github.com/nf-core/sarek/issues/1607
I_kwDOCvwIC86Rk_--,RFE: Variant decomposition,OPEN,2024-08-01T12:58:53Z,2024-08-02T05:44:03Z,,"### Description of feature

This time I tried to look through issues and code. ;)

As far as I can see, sarek does not do any kind of variant decomposition, which (if coupled with normalization) can simplify processing of the produced VCF file by downstream tools, prior to annotation.  ""bcbio"" back in the day did this, although if this is implemented in sarek, it should be definitely optional and turned off by default. ",lbeltrame,https://github.com/nf-core/sarek/issues/1609
I_kwDOCvwIC86RwvBr,CNVKIT doesn't accept *.list format interval files,OPEN,2024-08-02T17:40:21Z,2024-08-19T14:38:07Z,,"### Description of the bug

when passing a .list format file, eg

```raw
Chr1:1-1000
```

to `--intervals`,  the CNVKIT step fails

```
ERROR ~ Error executing process > 'NFCORE_SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_ANTITARGET (intervals)'

Caused by:
  Process `NFCORE_SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_ANTITARGET (intervals)` terminated with an error exit status (1)


Command executed:

  cnvkit.py \
      antitarget \
      intervals.list \
      --output intervals.antitarget.bed \
  
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:PREPARE_REFERENCE_CNVKIT:CNVKIT_ANTITARGET"":
      cnvkit: $(cnvkit.py version | sed -e ""s/cnvkit v//g"")
  END_VERSIONS

Command exit status:
  1

Command output:
  (empty)

Command error:
  INFO:    Converting SIF file to temporary sandbox...
  Matplotlib created a temporary config/cache directory at /tmp/matplotlib-c_o_2ofd because the default path (/home/chasem/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
  Fontconfig error: No writable cache directories
  Traceback (most recent call last):
    File ""/usr/local/bin/cnvkit.py"", line 10, in <module>
      sys.exit(main())
    File ""/usr/local/lib/python3.10/site-packages/cnvlib/cnvkit.py"", line 10, in main
      args.func(args)
    File ""/usr/local/lib/python3.10/site-packages/cnvlib/commands.py"", line 479, in _cmd_antitarget
      targets = tabio.read_auto(args.targets)
    File ""/usr/local/lib/python3.10/site-packages/skgenome/tabio/__init__.py"", line 103, in read_auto
      fmt = sniff_region_format(infile)
    File ""/usr/local/lib/python3.10/site-packages/skgenome/tabio/__init__.py"", line 255, in sniff_region_format
      raise ValueError(""File %r does not appear to be a recognized ""
  ValueError: File 'intervals.list' does not appear to be a recognized format! (Any of: text, tab, interval, refflat, gff, bed)
  First non-blank line:
  CP022325.1:69348-69349
```

### Command used and terminal output

```console
nextflow run nf-core/sarek -r latest --intervals intervals.list ...
```
```


### Relevant files

_No response_

### System information

- Nextflow version: 24.04.2
- Hardware: HPC
- Executor: slurm
- Container engine: Singularity
- OS: Rocky linux
- Version nf-core/sarek: 3.4.2",cmatKhan,https://github.com/nf-core/sarek/issues/1610
I_kwDOCvwIC86R06fq,ERROR > SAREK_BAM_VARIANT_CALLING_GERMLINE (MANTA),OPEN,2024-08-03T19:55:30Z,2024-08-05T15:14:46Z,,"### Description of the bug

I've run into an issue running the Sarek pipeline with Sea Urchin genome data. I would like to run Manta to find all variants from short read DNA sequencing data. I have already ran this same data with GATK Haplotypecaller successfully, however, I'm running into an issue that I do not fully understand. 

I made a custom config file as I had 8hr time limit issues running GATK, creating that custom config file fixed that so I decided to use it again here for Manta.

Any help is appreciated.

### Command used and terminal output

```console
Launching Pipeline:
nextflow run nf-core/sarek -r 3.3.2 -name Urchin_manta_data_1 -profile singularity -params-file nf-params.json --skip_tools baserecalibrator --genome null --igenomes_ignore --fasta /path/to/my/file/ragtag.HiFi.polished.simplified.filtered.fasta -c /path/to/my/file/customsarek.config

Output Error:
ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_GERMLINE_MANTA:MANTA_GERMLINE (illumina)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_GERMLINE_MANTA:MANTA_GERMLINE (illumina)` terminated with an error exit status (1)

Command executed:

  configManta.py         --bam illumina.md.cram         --reference ragtag.HiFi.polished.simplified.filtered.fasta         --runDir manta         --callRegions [ragtag.HiFi.polished.simplified.filtered.fasta].bed.gz

  python manta/runWorkflow.py -m local -j 30

  mv manta/results/variants/candidateSmallIndels.vcf.gz         illumina.manta.candidate_small_indels.vcf.gz
  mv manta/results/variants/candidateSmallIndels.vcf.gz.tbi         illumina.manta.candidate_small_indels.vcf.gz.tbi
  mv manta/results/variants/candidateSV.vcf.gz         illumina.manta.candidate_sv.vcf.gz
  mv manta/results/variants/candidateSV.vcf.gz.tbi         illumina.manta.candidate_sv.vcf.gz.tbi
  mv manta/results/variants/diploidSV.vcf.gz         illumina.manta.diploid_sv.vcf.gz
  mv manta/results/variants/diploidSV.vcf.gz.tbi         illumina.manta.diploid_sv.vcf.gz.tbi

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_GERMLINE_MANTA:MANTA_GERMLINE"":
      manta: $( configManta.py --version )
  END_VERSIONS

Command exit status:
  1

Command output:

  Successfully created workflow run script.
  To execute the workflow, run the following script and set appropriate options:

  manta/runWorkflow.py
Command error:
  [2024-08-02T04:59:34.017008Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeLocusGraph_chromId_069_scf00000069-1-4126316_0000' launched from master workflow
  [2024-08-02T04:59:34.245234Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeLocusGraph_chromId_065_scf00000065-1-4189768_0000' launched from master workflow
  [2024-08-02T04:59:34.373047Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeLocusGraph_chromId_253_seq00000024_RagTag-1-1873092_0000' launched from master workflow
  [2024-08-02T04:59:34.500506Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeLocusGraph_chromId_556_seq00000481_RagTag-1-4291023_0000' launched from master workflow
  [2024-08-02T04:59:34.777749Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeLocusGraph_chromId_071_scf00000071-1-14326224_0000' launched from master workflow
  [2024-08-02T04:59:34.777817Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeLocusGraph_chromId_107_scf00000107-1-3365541_0000' launched from master workflow
  [2024-08-02T04:59:35.004529Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeLocusGraph_chromId_045_scf00000045-1-4371124_0000' launched from master workflow
  [2024-08-02T04:59:35.887639Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeLocusGraph_chromId_279_seq00000069_RagTag-1-3676942_0000' launched from master workflow
  [2024-08-02T04:59:36.871557Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeLocusGraph_chromId_028_scf00000028-1-7379030_0000' launched from master workflow
  [2024-08-02T04:59:36.881890Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Launching sub-workflow task: 'mergeLocusGraphInputList' from master workflow
  [2024-08-02T04:59:36.882175Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskRunner:mergeLocusGraphInputList] Starting task specification for sub-workflow
  [2024-08-02T04:59:36.892291Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskRunner:mergeLocusGraphInputList] Finished task specification for sub-workflow
  [2024-08-02T04:59:36.898523Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed sub-workflow task: 'mergeLocusGraphInputList' launched from master workflow
  [2024-08-02T04:59:36.898938Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Launching command task: 'mergeLocusGraph' from master workflow
  [2024-08-02T04:59:36.917906Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskRunner:mergeLocusGraph] Task initiated on local node
  [2024-08-02T05:00:08.376843Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'mergeLocusGraph' launched from master workflow
  [2024-08-02T05:00:08.401917Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Launching command task: 'removeTmpDir' from master workflow
  [2024-08-02T05:00:08.402227Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Launching command task: 'checkLocusGraph' from master workflow
  [2024-08-02T05:00:08.402413Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Launching command task: 'locusGraphStats' from master workflow
  [2024-08-02T05:00:08.422485Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskRunner:removeTmpDir] Task initiated on local node
  [2024-08-02T05:00:08.424033Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskRunner:checkLocusGraph] Task initiated on local node
  [2024-08-02T05:00:08.424707Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskRunner:locusGraphStats] Task initiated on local node
  [2024-08-02T05:00:11.628345Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'locusGraphStats' launched from master workflow
  [2024-08-02T05:00:12.746512Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'checkLocusGraph' launched from master workflow
  [2024-08-02T05:00:12.746982Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Launching command task: 'makeHyGenDir' from master workflow
  [2024-08-02T05:00:12.761186Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskRunner:makeHyGenDir] Task initiated on local node
  [2024-08-02T05:00:12.862868Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'makeHyGenDir' launched from master workflow
  [2024-08-02T05:00:16.037506Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Completed command task: 'removeTmpDir' launched from master workflow
  [2024-08-02T05:00:16.037941Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] Launching command task: 'generateCandidateSV_0000' from master workflow
  [2024-08-02T05:00:16.053038Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskRunner:generateCandidateSV_0000] Task initiated on local node
  [2024-08-02T05:07:12.656332Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] Failed to complete command task: 'generateCandidateSV_0000' launched from master workflow, error code: 1, command: '/usr/local/share/manta-1.6.0-1/libexec/Genera$
  [2024-08-02T05:07:12.658080Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] [generateCandidateSV_0000] Error Message:
  [2024-08-02T05:07:12.658130Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] [generateCandidateSV_0000] Anomalous task wrapper stderr output. Wrapper signal file: 'manta/workspace/pyflow.data/logs/tmp/taskWrapperLogs/004/513/pyflowTaskWra$
  [2024-08-02T05:07:12.658174Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] [generateCandidateSV_0000] Logging 5 line(s) of task wrapper log output below:
  [2024-08-02T05:07:12.658207Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] [2024-08-02T05:00:16.109282Z] [hpc-compute-p27.cm.cluster] [4159_1] [pyflowTaskWrapper:generateCandidateSV_0000] $
  [2024-08-02T05:07:12.658249Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] [2024-08-02T05:00:16.130044Z] [hpc-compute-p27.cm.cluster] [4159_1] [pyflowTaskWrapper:generateCandidateSV_0000] $
  [2024-08-02T05:07:12.658283Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] [2024-08-02T05:07:12.580347Z] [hpc-compute-p27.cm.cluster] [4159_1] [pyflowTaskWrapper:generateCandidateSV_0000] $
  [2024-08-02T05:07:12.658312Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] [2024-08-02T05:07:12.589605Z] [hpc-compute-p27.cm.cluster] [4159_1] [pyflowTaskWrapper:generateCandidateSV_0000] $
  [2024-08-02T05:07:12.658342Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] Last 0 stderr lines from task (of 0 total lines):
  [2024-08-02T05:07:12.658377Z] [hpc-compute-p27.cm.cluster] [4159_1] [TaskManager] [ERROR] Shutting down task submission. Waiting for remaining tasks to complete.
  [2024-08-02T05:07:22.358650Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] Workflow terminated due to the following task errors:
  [2024-08-02T05:07:22.358699Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] Failed to complete command task: 'generateCandidateSV_0000' launched from master workflow, error code: 1, command: '/usr/local/share/manta-1.6.0-1/libexec/Gen$
  [2024-08-02T05:07:22.358716Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] Error Message:
  [2024-08-02T05:07:22.358740Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] Anomalous task wrapper stderr output. Wrapper signal file: 'manta/workspace/pyflow.data/logs/tmp/taskWrapperLogs/004/513/pyflowTask$
  [2024-08-02T05:07:22.358749Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] Logging 5 line(s) of task wrapper log output below:
  [2024-08-02T05:07:22.358769Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] [2024-08-02T05:00:16.109282Z] [hpc-compute-p27.cm.cluster] [4159_1] [pyflowTaskWrapper:generateCandidateSV_000$
  [2024-08-02T05:07:22.358778Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] [2024-08-02T05:00:16.130044Z] [hpc-compute-p27.cm.cluster] [4159_1] [pyflowTaskWrapper:generateCandidateSV_000$
  [2024-08-02T05:07:22.358789Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] [2024-08-02T05:07:12.580347Z] [hpc-compute-p27.cm.cluster] [4159_1] [pyflowTaskWrapper:generateCandidateSV_000$
  [2024-08-02T05:07:22.358797Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] [2024-08-02T05:07:12.589605Z] [hpc-compute-p27.cm.cluster] [4159_1] [pyflowTaskWrapper:generateCandidateSV_000$
  [2024-08-02T05:07:22.358806Z] [hpc-compute-p27.cm.cluster] [4159_1] [WorkflowRunner] [ERROR] [generateCandidateSV_0000] [taskWrapper-stderr] Last 0 stderr lines from task (of 0 total lines):
```


### Relevant files

params {
    max_memory = 128.GB
    max_cpus = 32
    max_time = 144.h
}

process {
   withName:'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:GATK4_HAPLOTYPECALLER' {
        time = 24.h
   }
}


### System information

Nextflow: 23.10.1
Hardward: HPC
Executor: Slurm
Container: Singularity
OS: macOS
nf-core/sarek: 3.3.2",kchilkert,https://github.com/nf-core/sarek/issues/1611
I_kwDOCvwIC86SZXev,Restart from step variantcalling fails on NGScheckmate,OPEN,2024-08-08T15:44:09Z,2024-09-24T18:45:31Z,,"### Description of the bug

```
ERROR nextflow.processor.TaskProcessor - Error executing process > 'NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP (1)'

Caused by:
  Not a valid path value: '../data/references/Homo_sapiens/GATK/GRCh38/Annotation/NGSCheckMate/SNP_GRCh38_hg38_wChr.bed'
  
```

NGSCHeckmate should not even be running

```
ae/f697b0] process > NFCORE_SAREK:SAREK:BAM_TO_CRAM (sample_01X)                                                                                      [100%] 1 of 1, cached: 1 ✔
[-        ] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:SAMTOOLS_STATS                                                                    [  0%] 0 of 1
[-        ] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH                                                                          [  0%] 0 of 1
[-        ] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP                                                               -
[-        ] process > NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM                                                               -
```

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.2 -latest -profile docker --input data/markduplicates_no_table_local.csv --outdir ./test_single_sample --igenomes_base ../data/references --step variant_calling --tools cnvkit,
 strelka,deepvariant,haplotypecaller -with-tower --save_output_as_bam  --max_memory 4.GB --max_cpus 2 -resume
```


### Relevant files

_No response_

### System information

sarek: 3.4.2",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1615
I_kwDOCvwIC86SZZbF,Restart from step variantcalling fails on haplotypecaller,OPEN,2024-08-08T15:48:07Z,2024-09-02T08:06:35Z,,"### Description of the bug

q

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.2 -latest -profile docker --input data/markduplicates_no_table_local.csv --outdir ./test_single_sample --igenomes_base ../data/references --step variant_calling --tools cnvkit,
 strelka,deepvariant,haplotypecaller -with-tower --save_output_as_bam  --max_memory 4.GB --max_cpus 2 -resume --ngscheckmate_bed false
```

```
  ***********************************************************************

  A USER ERROR has occurred: Illegal argument value: Positional arguments were provided ',{chr8_44033745-45877265.bed}' but no positional argument is defined for this tool.

  ***********************************************************************
  Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.

Work dir:
```
```


### Relevant files

q

### System information

sarek 3.4.2",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1616
I_kwDOCvwIC86SeqjR,GATK4_HAPLOTYPECALLER not running in parallel,OPEN,2024-08-09T09:30:55Z,2024-09-28T10:16:06Z,,"### Description of the bug

As a minimal example, I am locally (on a system with 24 cores and 128 GB RAM) running joint germline with just two WES samples with this nextflow.config file

process {
    withName: 'FASTP' {cpus = 16 }
    withName: 'BWAMEM1_MEM|BWAMEM2_MEM' {
        cpus   = { cpus = 22 }
        memory = 100.GB
    }
    withName: 'GATK4_HAPLOTYPECALLER' {  
        cpus   = 20
        memory = 120.GB     
}
}

For simplicity, as --intervals, I am using igenomes WGS wgs_calling_regions_noseconds.hg38.bed bed file (same file as sarek is using). I know that I should use special exon intervals, but If I understand it, this WGS intervals should also provide some kind of parallelisation (and when I tried exon intervals before, the parallelisation problem was the same). 

When checking htop, FASTP and BWA are utilizing multiple cores, but haplotypecaller not (just 1-2 cores). Why?

### Command used and terminal output

```console
nextflow run nf-core/sarek -r 3.4.3 -profile docker -config $INPUT/nextflow.config -work-dir $OUT/workdir --joint_germline --wes --intervals $INPUT/wgs_calling_regions_noseconds.hg38.bed --trim_fastq --genome GATK.GRCh38 --input $INPUT/test_samplesheet.csv --outdir $OUT/output --tools haplotypecaller --max_memory 130.GB --max_cpus 23 --skip_tools bcftools,fastqc,haplotypecaller_filter,haplotyper_filter,markduplicates,markduplicates_report,mosdepth,multiqc --aligner bwa-mem2
```


### Relevant files


[nextflow.zip](https://github.com/user-attachments/files/16559210/nextflow.zip)



### System information

Nextflow version: 24.04.4.5917
Hardware: Desktop PC, 24 cores, 128GB RAM
Executor: local
Container engine: Docker
OS: Ubuntu 22.04
Version of nf-core/sarek: 3.4.{2,3}",sitems,https://github.com/nf-core/sarek/issues/1617
I_kwDOCvwIC86SpmDj,"FASTP tooling error "" ERROR: sequence and quality have different length""",CLOSED,2024-08-12T08:39:40Z,2024-08-27T06:51:26Z,2024-08-27T06:51:26Z,"### Description of the bug

Hi,
I am running 40 samples on Sarek pipeline. A little number of samples are producing an error
during the FASTP stage but other samples run completely the pipeline.
All samples are sequenced using the same methodology.










### Command used and terminal output

```console
SCRIPT:

#!/bin/bash
#SBATCH --qos=standard
#SBATCH --job-name=sarek_array_3_4_2    # Job name
#SBATCH --tasks=1
#SBATCH --cpus-per-task=40
#SBATCH --tasks-per-node=1
#SBATCH --nodes=1
#SBATCH --output=/slgpfs/projects/cli20/cli20901/soft/prova_nf_sarek_3.4.2/output/sarek_somatic_%j.out
#SBATCH --chdir=/slgpfs/projects/cli20/cli20901/soft/prova_nf_sarek_3.4.2
#SBATCH --error=/slgpfs/projects/cli20/cli20901/soft/prova_nf_sarek_3.4.2/output/sarek_somatic_%j.err
#SBATCH --time=2-12:00:00

module load java singularity/3.8.7 nextflow
export NXF_OFFLINE='true'
export NXF_SINGULARITY_CACHEDIR=/slgpfs/projects/cli20/cli20901/soft/nf-core-sarek_3.4.2/singularity-images

nextflow run /slgpfs/projects/cli20/cli20901/soft/prova_nf_sarek_3.4.2/3_4_2/main.nf -profile singularity --input batch_2_modified_firstpart.csv --outdir ./results_batch_2 --max_cpus 40 --genome GATK.GRCh38 --ngscheckmate_bed 'false' --igenomes_base /slgpfs/projects/cli20/cli20901/soft/prova_nf_sarek_3.4.2/references --wes --tools mutect2 --joint_mutect2

OUTPUT:

ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:FASTP (qG17029053-1)'

Caused by:
  Process `NFCORE_SAREK:SAREK:FASTP (qG17029053-1)` terminated with an error exit status (255)

Command executed:

  [ ! -f  qG17029053-1_1.fastq.gz ] && ln -sf qG17029053.R1.fq.gz qG17029053-1_1.fastq.gz
  [ ! -f  qG17029053-1_2.fastq.gz ] && ln -sf qG17029053.R2.fq.gz qG17029053-1_2.fastq.gz
  fastp \
      --in1 qG17029053-1_1.fastq.gz \
      --in2 qG17029053-1_2.fastq.gz \
      --out1 qG17029053-1_1.fastp.fastq.gz \
      --out2 qG17029053-1_2.fastp.fastq.gz \
      --json qG17029053-1.fastp.json \
      --html qG17029053-1.fastp.html \
       \
       \
       \
      --thread 12 \
      --detect_adapter_for_pe \
      --disable_adapter_trimming      --split_by_lines 200000000 \
      2> >(tee qG17029053-1.fastp.log >&2)

  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:FASTP"":
      fastp: $(fastp --version 2>&1 | sed -e ""s/fastp //g"")
  END_VERSIONS

Command exit status:
  255

Command output:
  (empty)

Command error:
  ERROR: sequence and quality have different length:
  @NB501979:93:HKNGNBGX3:1:13301:5679:14867/2
  ATCCACACGGCCAACCCCATGGAACACGCCAACCACATGGCTGCCCAGCCACAGTTCGTGCACCCGGAACACCGCTCCTTTGTTGACCTGTCAGGCCACAACCTGGCCAACCCCCACCCGTTCGCAGGTAGGACATGGGGAGGG
  +
  <=>>ABAB=@CBBBCBBBBBE@BBCBC>DBBBCBBCBBE@DCEDBBBDDBBCBD@BD>@EDBCB>>@B?CBCB>DCDBC@BE@BE@CBCA@BBD@DBBCB?CBBE@DBBBCBBB:>CBB>>BD>DBD@@@D@BCBCE@@@BD@@B?+B>@0E@/BCE/BC+B>ABBE@0/B>ATGGAGCATCTCCGCTTGGTCTCCCTCCCCATGTCCT
  ERROR: sequence and quality have different length

Work dir:
  /slgpfs/projects/cli20/cli20901/soft/prova_nf_sarek_3.4.2/work/11/07ae6641c0b1e04dea5dde3d73bd2c
```


### Relevant files

_No response_

### System information

Summary software and hardware

Nextflow 23.10.0
Singularity 3.8.7
java 12.0.2
Hardware: HPC
Executor: slurm
nf-core/sarek 3.4.2
",jbague,https://github.com/nf-core/sarek/issues/1618
I_kwDOCvwIC86S6WDD,Misleading info on Documentation,CLOSED,2024-08-14T02:32:54Z,2024-08-19T14:08:21Z,2024-08-19T14:08:21Z,"The [doc](https://nf-co.re/sarek/docs/usage/#specify-the-cache-location) says the cache location for VEP should be:
`${vep_species}/${vep_genome}_${vep_cache_version}`

But the given example shows it in a different way:
```
/data/
├─ vep_cache/
│  ├─ homo_sapiens/
│  │  ├─ 110_GRCh38/
```

Which one is correct?",bounlu,https://github.com/nf-core/sarek/issues/1621
I_kwDOCvwIC86S65zo,Bug introduced in `3.4.1` causes premature stop of the pipeline,CLOSED,2024-08-14T05:10:44Z,2024-12-05T20:34:53Z,2024-11-18T09:53:32Z,"### Description of the bug

The pipeline prematurely stops with the error:

`Missing process or function Channel.empty([[]])`

The log file says:
`Caused by: groovy.lang.MissingMethodException: No signature of method: java.lang.Object.Channel.empty() is applicable for argument types: (ArrayList) values: [[]]`

It's hard to debug where it comes from but I confirm it was introduced in bfec02fcda0bfcdb3ce1ced5792bcc9552d8b26f

When I run the pipeline with the sha before than that it works fine.

### Command used and terminal output

```console
#!/bin/bash

nextflow run nf-core/sarek \
-latest \
-profile docker \
--aligner 'bwa-mem2' \
--step mapping \
--tools freebayes,mutect2,strelka,manta,tiddit,cnvkit,controlfreec,snpeff,vep,merge \
--wes \
--intervals 'chip.hg38.bed' \
--igenomes_base 's3://ngi-igenomes/igenomes' \
--snpeff_cache '/data/snpeff_cache' \
--snpeff_genome 'GRCh38' \
--snpeff_db '105' \
--vep_cache '/data/vep_cache' \
--vep_species 'homo_sapiens' \
--vep_genome 'GRCh38' \
--vep_cache_version '111' \
--input 'samplesheet.csv' \
--outdir 'results/' \
-work-dir 'work/' \
-c 'custom_local.config' \
-r bfec02fcda0bfcdb3ce1ced5792bcc9552d8b26f \
-resume
```


### Relevant files

_No response_

### System information

Nextflow version 24.07.0-edge
Ubuntu Server 22.04
local executor
Docker",bounlu,https://github.com/nf-core/sarek/issues/1622
I_kwDOCvwIC86TX-Pi,`cnvkit` output files are missing,OPEN,2024-08-19T06:09:26Z,2024-09-28T08:41:31Z,,`cnvkit` output files are missing. There is no segmentation file `.cns` and scatter and diagram plot files `.pdf` in the output folder. They are not generated by `cnvkit batch` module.,bounlu,https://github.com/nf-core/sarek/issues/1625
I_kwDOCvwIC86TZodA,Challenge with VEP Annotation in Offline Mode - --hgvs Disabled Due to Missing FASTA File,CLOSED,2024-08-19T10:05:41Z,2024-08-30T16:09:20Z,2024-08-30T16:09:20Z,,chumawinnie,https://github.com/nf-core/sarek/issues/1626
I_kwDOCvwIC86UM_HP,Avoid misleading error messages concerning sample-sheets,OPEN,2024-08-26T09:45:18Z,2024-11-06T10:49:11Z,,"### Description of the bug

This bug has been around for a while now. When Sarek encounters an error is will often issue the correct error message along with a couple of wrong error messages. The wrong error messages are related to the sample-sheets.

Here is an example where Sarek v3.4.3 was given an incorrect path to the VEP-cache, and Sarek gave the following error msg:
```
The sample-sheet only contains tumor-samples, but the following tools, which were requested by the option ""tools"", expect at least one normal-sample : ascat
The sample-sheet only contains normal-samples, but the following tools, which were requested with ""--tools"", expect at least one tumor-sample : ascat, controlfreec, mutect2
Path provided with VEP cache is invalid.
Make sure there is a directory named homo_sapiens/111_GRCh38 in /ngc/shared/nf_tools/references/vep_cache./nPlease refer to https://nf-co.re/sarek/docs/usage/#how-to-customise-snpeff-and-vep-annotation for more information.
```
The error messages concerning the sample-sheet are wrong and shouldn't be issued in this scenario. (The error message concerning the VEP-cache is fine.)

The error isn't caused by `nf-validation`, but by code in Sarek:

https://github.com/nf-core/sarek/blob/e92242ead3dff8e24e13adbbd81bfbc0b6862e4c/subworkflows/local/samplesheet_to_channel/main.nf#L162

https://github.com/nf-core/sarek/blob/e92242ead3dff8e24e13adbbd81bfbc0b6862e4c/subworkflows/local/annotation_cache_initialisation/main.nf#L50

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",asp8200,https://github.com/nf-core/sarek/issues/1629
I_kwDOCvwIC86UP29T,AGeNT,OPEN,2024-08-26T15:51:43Z,2024-08-26T15:51:43Z,,"### Description of feature

Hi guys!

I'm a researcher dealing with the secondary analysis of targeted and whole-exome data. Since we use the Agilent SureSelect XT HS2 kits, which adds to sequencing libraries​ the duplex molecular barcodes, we need AGeNT tool (Agilent) for trimming and deduplication of reads. Is there any way to incorporate this tool into sarek? Thanks in advance for your reply.

Best wishes.

Riccardo",RP-Bioinfo,https://github.com/nf-core/sarek/issues/1630
I_kwDOCvwIC86UQicy,some reference files seem to be missing,CLOSED,2024-08-26T17:19:23Z,2024-08-29T23:02:06Z,2024-08-29T23:02:06Z,"### Description of the bug

Some of the files described in the `igenomes.config` file seem to no longer be present. Am I doing something wrong or were these files moved?

These all seem to be missing;

```bash
aws s3 ls s3://ngi-igenomes/Homo_sapiens/GATK/GRCh37/Sequence/BWAIndex/
aws s3 ls s3://ngi-igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAmem2Index/
aws s3 ls s3://ngi-igenomes/Homo_sapiens/GATK/GRCh38/Sequence/BWAIndex/
```

I found similar files under a path like this

```bash
$ aws s3 ls s3://ngi-igenomes/igenomes/Homo_sapiens/NCBI/GRCh38/Sequence/BWAIndex/
                           PRE version0.5.x/
                           PRE version0.6.0/
2017-04-13 07:05:20 3144230986 genome.fa
2017-04-13 07:05:21      17478 genome.fa.amb
2017-04-13 07:05:21      27715 genome.fa.ann
2017-04-13 07:05:22 3099922624 genome.fa.bwt
2017-04-13 07:05:30  774980637 genome.fa.pac
2017-04-13 07:05:52 1549961320 genome.fa.sa
```

Are these the exact same files that were meant to be at those locations listed in the config file?

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",stevekm,https://github.com/nf-core/sarek/issues/1631
I_kwDOCvwIC86URYm2,Can BAM files be set as the input for the analysis?,CLOSED,2024-08-26T19:16:14Z,2024-09-02T08:05:33Z,2024-09-02T08:05:33Z,"### Description of feature

I have handreds of BAM files for WGS data. I love your pipeline. So can you please add the option to set BAM files as input  also?
Thanks",martinyanglyu,https://github.com/nf-core/sarek/issues/1632
I_kwDOCvwIC86UYKn4,Process execution failed due to exceeding the allocated runtime limit I do get the following error,OPEN,2024-08-27T13:24:42Z,2024-09-25T09:59:47Z,,"Hello,

I am running nf-core/sarek on Ubuntu 22.04.3 LTS (GNU/Linux 5.15.153.1-microsoft-standard-WSL2 x86_64). I encountered the error: ""Process exceeded running time limit (8h)."" To address this, I tried using a custom.config file as discussed in #932. However, I am still encountering the same error.

Could you please provide further guidance on resolving this issue?


**Command:** 
nextflow -log /mnt/f/ARIA/sarek_output/log/M14TIL55.log  
run nf-core/sarek -r 3.4.2 -profile docker  --input  M14TIL55_sarekinput.csv    --genome GATK.GRCh38    --outdir  /pt55/out/ --max_cpus 8 --tools mutect2,strelka,vep,snpeff,controlfreec,ascat  --only_paired_variant_calling  -c custom.config

Execution_report error:
**Workflow execution completed unsuccessfully!**
The exit status of the task that caused the workflow execution to fail was: null.

The full error message was:

Error executing process > 'NFCORE_SAREK:SAREK:FASTQC (55_healthy-1)'

Caused by:
  Process exceeded running time limit (8h)


Command executed:

  printf ""%s %s\n"" noinfo-pt55healthy_R1_001.fastq.gz 55_healthy-1_1.gz noinfo-pt55healthy_R2_001.fastq.gz 55_healthy-1_2.gz | while read old_name new_name; do
      [ -f ""${new_name}"" ] || ln -s $old_name $new_name
  done
  
  fastqc \
      --quiet \
      --threads 4 \
      --memory 4096 \
      55_healthy-1_1.gz 55_healthy-1_2.gz
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:FASTQC"":
      fastqc: $( fastqc --version | sed '/FastQC v/!d; s/.*v//' )
  END_VERSIONS

Command exit status:
  -

Command output:
  application/gzip
  application/gzip

Command wrapper:
  application/gzip
  application/gzip

Work dir:
  /mnt/f/ARIA/work/0b/66d2170cbbe48f92750fc33fc0ca1e

Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line

**Please note that:** My custom.config is
params {
    max_memory = 128.GB
    max_time = 144.h
}
process {

     withName: 'NFCORE_SAREK:SAREK:FASTQC (55_healthy-1)' {
        time = 48.h // Specific time limit for this process
   }
}
process {

     withName: 'NFCORE_SAREK:SAREK:FASTQC (55_baseline-1)' {
        time = 48.h // Specific time limit for this process
   } 
}
process {

     withName: 'NFCORE_SAREK:SAREK:FASTQC (55_progression-1)' {
        time = 48.h // Specific time limit for this process
   }
}

**### .command.log from**
Work dir:
  /mnt/f/ARIA/work/0b/66d2170cbbe48f92750fc33fc0ca1e

application/gzip
application/gzip
Error response from daemon: cannot stop container: nxf-WF6UjjdSRFxhxbOdAIedosOd: tried to kill container, but did not receive an exit event",TILccit,https://github.com/nf-core/sarek/issues/1634
I_kwDOCvwIC86UoYss,A question with hg19,OPEN,2024-08-29T07:01:59Z,2024-08-29T07:01:59Z,,"### Description of the bug

Hi,

I wanted to ask if it's recommended, when using the UCSC hg19 genome, to run the BaseRecalibrator step using the required files for this step from the GATK.hg19 genome (manually specifying the path), or if it’s better to skip the step with --skip_tools BaseRecalibrator. The data comes from a WES, and due to protocol limitations, it's necessary to compare somatic data using this reference genome.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",jbague,https://github.com/nf-core/sarek/issues/1637
I_kwDOCvwIC86VPsWQ,Sarek How Tos,OPEN,2024-09-03T22:28:50Z,2024-09-03T22:28:51Z,,"Doc push for ""How To do X in Sarek"" to show off all the tools in the swiss-army knife. Maybe a blog post or a bytesized series? Maybe a series of configs?

```[tasklist]
### How-Tos
- [ ] Adding a new variant caller
- [ ] By default sarek only runs the preprocessing
```

Just thinking it would be cool to link people in #new-pipelines to a blog post showing how to that in Sarek.",edmundmiller,https://github.com/nf-core/sarek/issues/1643
I_kwDOCvwIC86VR6Y7,Mutect2 error,OPEN,2024-09-04T07:15:07Z,2024-09-05T10:36:43Z,,"### Description of the bug

Hello,

I am running the ""standard"" pipeline with many variant callers, but suddenly got an error with Mutect2. The same command has worked previously for me for before. The command I ran is:
`nextflow run nf-core/sarek    -profile docker    --input samplesheet.csv    --outdir EMB95_01-009_sarek --igenomes_base ~/references/ --tools ""deepvariant,mpileup,haplotypecaller,mutect2,freebayes,strelka"" -resume`

I have attached the nextflow log.

### Command used and terminal output

```console
nextflow run nf-core/sarek    -profile docker    --input samplesheet.csv    --outdir EMB95_01-009_sarek --igenomes_base ~/references/ --tools ""deepvariant,mpileup,haplotypecaller,mutect2,freebayes,strelka"" -resume
```


### Relevant files

[nextflow.log](https://github.com/user-attachments/files/16862213/nextflow.log)


### System information

_No response_",yannic-chen,https://github.com/nf-core/sarek/issues/1644
I_kwDOCvwIC86VU5LN,error running BQSR: cannot remove tmp file ,OPEN,2024-09-04T12:52:50Z,2024-09-04T12:52:50Z,,"### Description of the bug

I'm running into an error that I can't seem to address. I'm running the Mutect2 -> VEP pipelines. Once the pipeline gets to the apply BQSR step, it seems to crash after it can't remove files from the temporary directory. I've tried changing the tmp directory to a directory I own, but it has not worked. Any insight as to what the problem might be would be appreciated. I've included the nextflow.log and .out files. I have also included one command.log instance. 

For what it is worth, I also am having to run the pipeline with no_intervals as the pipeline would crash while trying to create intervals with awk. 

### Command used and terminal output

```console
#!/bin/bash
#SBATCH --job-name=sarek_mutect_nextflow
#SBATCH --partition=bigmem
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4 
#SBATCH --time=24:0:0
#SBATCH --output=sarek_mutect_nextflow_log%j.log
#SBATCH --error=sarek_mutect_nextflow_log%j.err
#SBATCH --mem=16G 
#SBATCH --signal=2

# Initialize Conda
source /PHShome/jyd97/mambaforge/etc/profile.d/conda.sh

export PATH=$PATH:/PHShome/jyd97/mambaforge/envs/nextflow/bin

conda activate nextflow
module load singularity/3.7.0

NXF_OPTS='-Xms1g -Xmx4g'

nextflow run /data/cteu/2_users/jon/software/sarek_download/3_4_3/ -profile singularity \
-c ./slurm.config \
-params-file ./nf-params.json \
-resume

## nf-params.json
{
    ""input"": "".\/samplesheet.csv"",
    ""step"": ""prepare_recalibration"",
    ""outdir"": "".\/results"",
    ""genome"": ""GATK.GRCh38"",
    ""no_intervals"": true,
    ""tools"": ""mutect2,vep"",
    ""vep_custom_args"": ""--everything --filter_common --per_gene --total_length --offline --format vcf --check_existing"",
    ""igenomes_base"": ""\/data\/cteu\/2_users\/jon\/genome_reference\/igenomes\/"",
    ""vep_cache"": ""/data/cteu/2_users/jon/genome_reference/cache/vep_cache/""
}

## slurm.config
process {
    executor = 'slurm'
    queue = 'bigmem'
    scratch = true
    clusterOptions = ''
    jobName = { ""$task.hash"" }
    
}

singularity {
    enabled = true
    autoMounts = true
    runOptions = '-B /data/cteu/2_users/jon/tmp'
}

env {
  TMPDIR= ""/data/cteu/2_users/jon/tmp""
}
```


### Relevant files

[command.log](https://github.com/user-attachments/files/16870784/command.log)
[nextflow.log](https://github.com/user-attachments/files/16870788/nextflow.log)
[sarek_mutect_nextflow_log5354346.log](https://github.com/user-attachments/files/16870793/sarek_mutect_nextflow_log5354346.log)



### System information

Nextflow version: 24.04.2
Hardware: HPC
Executor: slurm
Container engine: Singularity
OS: Linux CentOS 7
Version of nf-core/sarek: 3.4.3",jdownie2389,https://github.com/nf-core/sarek/issues/1645
I_kwDOCvwIC86WR5v_,variant_calling DO NOT PROCESS,OPEN,2024-09-12T03:52:25Z,2024-09-12T07:39:25Z,,"### Description of the bug

Hi am working with the pipeline, in the test working fine. with samples do not report the VARIANT_CALLING, there is no error in the run.
How I can make that processes?

### Command used and terminal output

```console
nf-core/sarek v3.4.4-g5cc3049
------------------------------------------------------
Core Nextflow options
  revision             : 3.4.4
  runName              : maniac_pasteur
  containerEngine      : singularity
  launchDir            : /mnt/lustre/koa/scratch/torres91/mw
  workDir              : /mnt/lustre/koa/scratch/torres91/mw/work
  projectDir           : /home/torres91/.nextflow/assets/nf-core/sarek
  userName             : torres91
  profile              : singularity
  configFiles          : 

Input/output options
  input                : samplesheet.csv
  outdir               : /home/torres91/koa_scratch/mw/tes_results

Main options
  intervals            : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/Annotation/intervals/GRCm38_calling_list.bed

Reference genome options
  genome               : GRCm38
  bwa                  : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/Sequence/BWAIndex/version0.6.0/
  chr_dir              : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/Sequence/Chromosomes
  dbsnp                : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/MouseGenomeProject/mgp.v5.merged.snps_all.dbSNP142.vcf.gz
  dbsnp_tbi            : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/MouseGenomeProject/mgp.v5.merged.snps_all.dbSNP142.vcf.gz.tbi
  dict                 : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/Sequence/WholeGenomeFasta/genome.dict
  fasta                : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/Sequence/WholeGenomeFasta/genome.fa
  fasta_fai            : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/Sequence/WholeGenomeFasta/genome.fa.fai
  known_indels         : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/MouseGenomeProject/mgp.v5.merged.indels.dbSNP142.normed.vcf.gz
  known_indels_tbi     : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/MouseGenomeProject/mgp.v5.merged.indels.dbSNP142.normed.vcf.gz.tbi
  mappability          : s3://ngi-igenomes/igenomes//Mus_musculus/Ensembl/GRCm38/Annotation/Control-FREEC/GRCm38_68_mm10.gem
  snpeff_db            : 99
  snpeff_genome        : GRCm38
  vep_genome           : GRCm38
  vep_species          : mus_musculus
  vep_cache_version    : 102
  save_reference       : true

.
.
.
Plus 13 more processes waiting for tasks…
-[nf-core/sarek] Pipeline completed successfully-
Completed at: 11-Sep-2024 22:40:04
Duration    : 3h 18m 27s
CPU hours   : 49.6 (0% cached)
Succeeded   : 233
Cached      : 20
```


### Relevant files

ls -lth tes_results 
total 1488
drwxr-x---   6 alikam  staff   192B Sep 11 17:19 preprocessing
drwxr-x---   8 alikam  staff   256B Sep 11 17:12 reports
drwxr-x---   4 alikam  staff   128B Sep 11 17:07 reference
drwxr-x---  18 alikam  staff   576B Sep 11 12:40 pipeline_info
-rw-r-----@  1 alikam  staff   635K Sep 11 12:40 sarek_3528528_4294967294_error.out
drwxr-x---   5 alikam  staff   160B Sep 11 12:37 multiqc
drwxr-x---   5 alikam  staff   160B Sep 11 12:37 csv
-rw-r-----   1 alikam  staff   826B Sep 11 09:19 samplesheet.csv

### System information

_No response_",torres-HI,https://github.com/nf-core/sarek/issues/1648
I_kwDOCvwIC86WSFyt,error when use custom reference,OPEN,2024-09-12T04:39:05Z,2024-09-13T20:56:01Z,,"### Description of the bug

When I use my custom reference, error always show: This path is not available within annotation-cache. Please check https://annotation-cache.github.io/ to create a request for it.

My command is : nextflow run ./sarek -profile singularity --input samplesheet.csv --outdir ./ --tools 'freebayes,snpeff' --genome null --igenomes_ignore --fasta ./ref/hs37d5.fa.gz --skip_tools baserecalibrator

The log:

N E X T F L O W   ~  version 24.04.4

Launching `./sarek/main.nf` [distraught_edison] DSL2 - revision: e3d6110e17

WARN: Access to undefined parameter `monochromeLogs` -- Initialise it to a default value eg. `params.monochromeLogs = some_value`


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .Â´ _  `.
   /  |\`-_ \      __        __   ___     
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /Â¯Â¯\ |  \ |___ |  \
    `|____\Â´

  nf-core/sarek v3.4.4
....
* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md
------------------------------------------------------
[-        ] NFCâ€¦EPARE_GENOME:BWAMEM1_INDEX -
[-        ] NFCâ€¦EPARE_GENOME:BWAMEM2_INDEX -
[-        ] NFCâ€¦E_GENOME:DRAGMAP_HASHTABLE -
[-        ] NFCâ€¦4_CREATESEQUENCEDICTIONARY -
[-        ] NFCâ€¦E_GENOME:MSISENSORPRO_SCAN -
[-        ] NFCâ€¦PARE_GENOME:SAMTOOLS_FAIDX -

[-        ] NFCâ€¦EPARE_GENOME:BWAMEM1_INDEX -
[-        ] NFCâ€¦EPARE_GENOME:BWAMEM2_INDEX -
[-        ] NFCâ€¦E_GENOME:DRAGMAP_HASHTABLE -
[-        ] NFCâ€¦4_CREATESEQUENCEDICTIONARY -
[-        ] NFCâ€¦E_GENOME:MSISENSORPRO_SCAN -
[-        ] NFCâ€¦PARE_GENOME:SAMTOOLS_FAIDX -
[-        ] NFCâ€¦TABIX_BCFTOOLS_ANNOTATIONS -
[-        ] NFCâ€¦PREPARE_GENOME:TABIX_DBSNP -
[-        ] NFCâ€¦ME:TABIX_GERMLINE_RESOURCE -
[-        ] NFCâ€¦RE_GENOME:TABIX_KNOWN_SNPS -
[-        ] NFCâ€¦_GENOME:TABIX_KNOWN_INDELS -
[-        ] NFCâ€¦K:PREPARE_GENOME:TABIX_PON -
[-        ] NFCâ€¦_INTERVALS:BUILD_INTERVALS -
[-        ] NFCâ€¦RVALS:CREATE_INTERVALS_BED -
[-        ] NFCâ€¦_BGZIPTABIX_INTERVAL_SPLIT -
[-        ] NFCâ€¦ZIPTABIX_INTERVAL_COMBINED -
This path is not available within annotation-cache.
Please check https://annotation-cache.github.io/ to create a request for it.






### Command used and terminal output

```console
$nextflow run ./sarek -profile singularity --input samplesheet.csv --outdir ./ --tools 'freebayes,snpeff' --genome null --igenomes_ignore --fasta ./ref/hs37d5.fa.gz --skip_tools baserecalibrator

terminal output:

N E X T F L O W   ~  version 24.04.4

Launching `./sarek/main.nf` [distraught_edison] DSL2 - revision: e3d6110e17

WARN: Access to undefined parameter `monochromeLogs` -- Initialise it to a default value eg. `params.monochromeLogs = some_value`


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .Â´ _  `.
   /  |\`-_ \      __        __   ___     
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /Â¯Â¯\ |  \ |___ |  \
    `|____\Â´

  nf-core/sarek v3.4.4
....
* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md
------------------------------------------------------
[-        ] NFCâ€¦EPARE_GENOME:BWAMEM1_INDEX -
[-        ] NFCâ€¦EPARE_GENOME:BWAMEM2_INDEX -
[-        ] NFCâ€¦E_GENOME:DRAGMAP_HASHTABLE -
[-        ] NFCâ€¦4_CREATESEQUENCEDICTIONARY -
[-        ] NFCâ€¦E_GENOME:MSISENSORPRO_SCAN -
[-        ] NFCâ€¦PARE_GENOME:SAMTOOLS_FAIDX -

[-        ] NFCâ€¦EPARE_GENOME:BWAMEM1_INDEX -
[-        ] NFCâ€¦EPARE_GENOME:BWAMEM2_INDEX -
[-        ] NFCâ€¦E_GENOME:DRAGMAP_HASHTABLE -
[-        ] NFCâ€¦4_CREATESEQUENCEDICTIONARY -
[-        ] NFCâ€¦E_GENOME:MSISENSORPRO_SCAN -
[-        ] NFCâ€¦PARE_GENOME:SAMTOOLS_FAIDX -
[-        ] NFCâ€¦TABIX_BCFTOOLS_ANNOTATIONS -
[-        ] NFCâ€¦PREPARE_GENOME:TABIX_DBSNP -
[-        ] NFCâ€¦ME:TABIX_GERMLINE_RESOURCE -
[-        ] NFCâ€¦RE_GENOME:TABIX_KNOWN_SNPS -
[-        ] NFCâ€¦_GENOME:TABIX_KNOWN_INDELS -
[-        ] NFCâ€¦K:PREPARE_GENOME:TABIX_PON -
[-        ] NFCâ€¦_INTERVALS:BUILD_INTERVALS -
[-        ] NFCâ€¦RVALS:CREATE_INTERVALS_BED -
[-        ] NFCâ€¦_BGZIPTABIX_INTERVAL_SPLIT -
[-        ] NFCâ€¦ZIPTABIX_INTERVAL_COMBINED -
This path is not available within annotation-cache.
Please check https://annotation-cache.github.io/ to create a request for it.
```


### Relevant files

_No response_

### System information

_No response_",ybdong919,https://github.com/nf-core/sarek/issues/1649
I_kwDOCvwIC86WTvQx,Add pattern for all reference files,OPEN,2024-09-12T08:45:28Z,2024-09-12T08:45:28Z,,"### Description of feature

We don't have patterns for most of the reference files, and that is a source of issue and confusion:

https://github.com/nf-core/sarek/blob/5cc30494a6b8e7e53be64d308b582190ca7d2585/nextflow_schema.json#L674-L679",maxulysse,https://github.com/nf-core/sarek/issues/1650
I_kwDOCvwIC86WYCkK,Update the metro map with lofreq before next release,OPEN,2024-09-12T16:45:14Z,2024-09-12T16:45:14Z,,"I'll create an issue, and I'll update the metro map at a later date (before the release).
I noticed some weird changes with the new version of the workflow, but don't worry about it, I'll have a look at the same time.

_Originally posted by @maxulysse in https://github.com/nf-core/sarek/issues/1620#issuecomment-2346779589_
            ",maxulysse,https://github.com/nf-core/sarek/issues/1651
I_kwDOCvwIC86WpJ0u,"use custom snpeff database, such as Candian.aruis",OPEN,2024-09-16T02:14:15Z,2024-10-25T19:05:31Z,,"### Description of the bug

How can I use a custom snpeff reference database, such as candida.auris, in Sarek. This snpeff reference did not included in Sarek. I tried --snpeff_cache, but not work.
Can you show me how to do it?

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",ybdong919,https://github.com/nf-core/sarek/issues/1654
I_kwDOCvwIC86XADS6,A USER ERROR has occurred: Bad input: Sample $name is not in BAM header: [...] ,OPEN,2024-09-18T10:39:57Z,2024-09-18T15:37:17Z,,"### Description of the bug

`A USER ERROR has occurred: Bad input: Sample P-*_P-*-REF-100X is not in BAM header: `[*_P-*-REF-100X,*_*-TMN-150X]````
I have the same error as the issue : [https://github.com/nf-core/sarek/issues/732](https://github.com/nf-core/sarek/issues/73)2, I added  in my .config: 
```
 withName: 'MUTECT2_PAIRED' {
      ext.args = { ""<copy ext.args from modules.config to avoid overwritting>  --normal-sample ${meta.normal_id}"" }
   }
```
, nothing changes. 

### Command used and terminal output

```console
Error executing process > 'NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED (P-*-TMN-150X_vs_P-*-REF-100X)'

Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED (P-*-TMN-150X_vs_*-REF-100X)` terminated with an error exit status (2)


Command executed:

  gatk --java-options ""-Xmx32768M -XX:-UsePerfData"" \
      Mutect2 \
      --input P-*.recal.cram --input P-*.recal.cram \
      --output P-*-TMN-150X_vs_P-*-REF-100X.mutect2.vcf.gz \
      --reference Homo_sapiens_assembly38.fasta \
      --panel-of-normals 1000g_pon.hg38.vcf.gz \
      --germline-resource af-only-gnomad.hg38.vcf.gz \
      --intervals chr1_69066-70033.bed \
      --tmp-dir . \
      --f1r2-tar-gz P-NI62112-TMN-150X_vs_P-NI62112-REF-100X.mutect2.f1r2.tar.gz --normal-sample P-*_P-*-REF-100X
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_SOMATIC_MUTECT2:MUTECT2_PAIRED"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

------------------------------------------------
nextflow run nf-core/sarek \
    -c config/nf_core_sarek.config \
    -profile singularity \
    --input  config/samplesheet_wes_mapped.csv \
    --outdir results/nfcore_sarek/WES/ \
    --intervals resources/Homo_sapiens/target_files/twist_human_core_exome_and_integragen_custom_v2.bed \
    --wes \
    --monochrome_logs \
    -ansi-log false \
    -params-file config/nf_core_sarek_params.yaml  \
    -resume \
    -with-report  results/nfcore_sarek/WES/report.html/ \
    -with-timeline results/nfcore_sarek/WES/timeline.html \
    -process.errorStrategy 'retry' 
    -process.maxRetries 5
```


### Relevant files

Using GATK jar /usr/local/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar
Running:
    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx32768M -XX:-UsePerfData -jar /usr/local/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar Mutect2 --input P-*-REF-100X.recal.cram --input P-MA2121-TMN-150X.recal.cram --output P-*-TMN-150X_vs_P-*-REF-100X.mutect2.vcf.gz --reference Homo_sapiens_assembly38.fasta --panel-of-normals 1000g_pon.hg38.vcf.gz --germline-resource af-only-gnomad.hg38.vcf.gz --intervals chr1_69066-70033.bed --tmp-dir . --f1r2-tar-gz P-*-TMN-150X_vs_P-*-REF-100X.mutect2.f1r2.tar.gz --normal-sample P-*_P-*-REF-100X
10:17:51.899 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so
10:17:52.259 INFO  Mutect2 - ------------------------------------------------------------
10:17:52.267 INFO  Mutect2 - The Genome Analysis Toolkit (GATK) v4.5.0.0
10:17:52.267 INFO  Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/
10:17:52.268 INFO  Mutect2 - Executing as v_robin@n11 on Linux v3.10.0-957.27.2.el7.x86_64 amd64
10:17:52.268 INFO  Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v17.0.10-internal+0-adhoc..src
10:17:52.268 INFO  Mutect2 - Start Date/Time: September 18, 2024 at 10:17:51 AM GMT
10:17:52.268 INFO  Mutect2 - ------------------------------------------------------------
10:17:52.269 INFO  Mutect2 - ------------------------------------------------------------
10:17:52.270 INFO  Mutect2 - HTSJDK Version: 4.1.0
10:17:52.270 INFO  Mutect2 - Picard Version: 3.1.1
10:17:52.270 INFO  Mutect2 - Built for Spark Version: 3.5.0
10:17:52.271 INFO  Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2
10:17:52.271 INFO  Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false
10:17:52.272 INFO  Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true
10:17:52.272 INFO  Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false
10:17:52.272 INFO  Mutect2 - Deflater: IntelDeflater
10:17:52.273 INFO  Mutect2 - Inflater: IntelInflater
10:17:52.273 INFO  Mutect2 - GCS max retries/reopens: 20
10:17:52.273 INFO  Mutect2 - Requester pays: disabled
10:17:52.274 INFO  Mutect2 - Initializing engine
10:17:54.653 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/beegfs/scratch/v_robin/.tmp/nxf.PVfwj5aNd0/1000g_pon.hg38.vcf.gz
10:17:55.230 INFO  FeatureManager - Using codec VCFCodec to read file file:///mnt/beegfs/scratch/v_robin/.tmp/nxf.PVfwj5aNd0/af-only-gnomad.hg38.vcf.gz
10:17:55.540 INFO  FeatureManager - Using codec BEDCodec to read file file:///mnt/beegfs/scratch/v_robin/.tmp/nxf.PVfwj5aNd0/chr1_69066-70033.bed
10:17:56.168 INFO  IntervalArgumentCollection - Processing 43826111 bp from intervals
10:17:56.400 INFO  Mutect2 - Done initializing engine
10:17:56.519 INFO  NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/usr/local/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_utils.so
10:17:56.548 INFO  NativeLibraryLoader - Loading libgkl_smithwaterman.so from jar:file:/usr/local/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar!/com/intel/gkl/native/libgkl_smithwaterman.so
10:17:56.553 INFO  SmithWatermanAligner - Using AVX accelerated SmithWaterman implementation
10:17:56.577 INFO  Mutect2 - Shutting down engine
[September 18, 2024 at 10:17:56 AM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.08 minutes.
Runtime.totalMemory()=1157627904
***********************************************************************

A USER ERROR has occurred: Bad input: Sample P-*_P-*-REF-100X is not in BAM header: [*_P-*-REF-100X, *_P-*-TMN-150X]

### System information

Version nf-core sarek 3.4.3
Container Singularity
Executor slurm
Hardware HPC
version nextflow 24.04.4",VivianRobin,https://github.com/nf-core/sarek/issues/1655
I_kwDOCvwIC86XI6ts,Wrong sample names in multiqc general table,OPEN,2024-09-19T08:44:42Z,2024-09-25T09:43:29Z,,"### Description of the bug

Hi,
When using sarek for the mapping steps the names in the multiqc general statistics table are a bit strange
![image](https://github.com/user-attachments/assets/b679f999-d2c0-4d0d-bc87-8201ed787d02)
In the image above I think that `1-ADN13260_1.md` and `1-ADN13260_1-lane1` should both be named `1-ADN13260_1` and that `1-ADN13260_1.1-ADN13260_1-lane1_1` should simply be `ADN13260_1-lane1_1`.

### Command used and terminal output

```console
nextflow run nf-core/sarek -profile singularity -r 3.4.4 --step ""mapping"" --skip_tools : ""mosdepth,baserecalibrator"" --outdir results --input file.csv
```


### Relevant files

_No response_

### System information

_No response_",LouisLeNezet,https://github.com/nf-core/sarek/issues/1658
I_kwDOCvwIC86XI-Qr,Coverage and Depth calculations,OPEN,2024-09-19T08:51:09Z,2024-09-19T10:39:30Z,,"### Description of the bug

Hello,

I hope you're doing well. I’m a bit confused with a question. I ran the pipeline using the GRCh38 reference genome and Mutect2 in paired tumor-normal files. The entire pipeline completed successfully using nf-sarek version 3.4.3.

However, I’m unsure where I can find the folder or file that contains information about the coverage and depth of the sequences.

I apologize if this is a basic question, and I really appreciate your help.

Thank you!


### Command used and terminal output

```console
nextflow run /slgpfs/projects/cli20/cli20901/soft/nf-core-sarek_3.4.3/3_4_3/main.nf -profile singularity --input ./ID091.csv --outdir ./results_ID091 --max_cpus 40 --genome GRCh38 --ngscheckmate_bed false --igenomes_base ./references --wes --tools mutect2
```


### Relevant files

_No response_

### System information

Nextflow v.23.04
Hardware HPC
Executor Slurm
Container engine Singularity
Version of nf-core/sarek 3.4.3",jbague,https://github.com/nf-core/sarek/issues/1659
I_kwDOCvwIC86YHKsp,Implementation of Indel and Multiallelic Variant Normalization in Sarek Pipeline,OPEN,2024-09-27T05:14:53Z,2024-09-27T05:14:53Z,,"### Description of feature

Dear Sarek Development Team,

I am writing on behalf of our research group, which uses the Sarek pipeline for WGS data analysis. We are currently exploring ways to optimize our workflow and have identified the need to include a normalization step for indels and multiallelic variants directly within the pipeline.

In our research, we compare data generated from different pipelines and have encountered discrepancies in how indels are represented. This complicates downstream analysis, as the unnormalized indels often lack proper annotations. To address this, we use bcftools for normalization, but after this process, the annotations are lost, requiring a reannotation step.

To streamline our workflow, we believe it would be highly beneficial to include the normalization step immediately after variant calling and before annotation with VEP. This would ensure consistent variant representation without needing reannotation after normalization.

We have had great experiences running Sarek on our cluster, and it has become an integral part of our work. We have no intention of switching pipelines, and we plan to continue using Sarek in our upcoming projects. For these reasons, we believe adding this normalization step would further improve the efficiency of our research process.

We would like to inquire whether this feature could be integrated into Sarek, or if you might have suggestions on how to implement it. We would be happy to collaborate or provide feedback as necessary.

Thank you for your attention, and we look forward to your response.

Best regards,

Patricie",Patricie34,https://github.com/nf-core/sarek/issues/1666
I_kwDOCvwIC86YtA0d,Read data length does not match expected size - bytes read: 2756; expected: 16013759,OPEN,2024-10-02T15:02:43Z,2024-10-03T09:31:45Z,,"### Description of the bug

Issue with latest Nextflow release.
This was a silent message before, it's now an error and fails the pipeline.

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1672
I_kwDOCvwIC86Y0M38,"Pipeline does not stop immediately, when non-unique samples are provided in the samplesheet",OPEN,2024-10-03T11:32:23Z,2024-10-03T11:33:50Z,,"### Description of the bug

When running the `nf-core/sarek` pipeline with a sample sheet in which samples are not unique, the pipeline runs and fails only after the alignment step (I believe, at `GATK4_MARKDUPLICATES ` process). 

When carefully checking the documentation at [`nf-core/sarek`](https://nf-co.re/sarek/3.4.4/docs/usage/), I realised that samples should be unique:

> Custom sample ID for each tumor and normal sample; more than one tumor sample for each subject is possible, i.e. a tumor and a relapse; samples can have multiple lanes for which the same ID must be used to merge them later (see also lane). Sample IDs must be unique for unique biological samples

Would be useful to add a check to stop the pipeline in case non-unique sample IDs are provided.

Samplesheet used: 

```
patient,status,sample,lane,fastq_1,fastq_2
F2407718,0,monolayer,L1,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407718_R1_combined.fastq.gz,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407718_R2_combined.fastq.gz
F2407719,0,monolayer,L1,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407719_R1_combined.fastq.gz,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407719_R2_combined.fastq.gz
F2407720,0,monolayer,L1,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407720_R1_combined.fastq.gz,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407720_R2_combined.fastq.gz
F2407721,1,monolayer,L1,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407721_R1_combined.fastq.gz,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407721_R2_combined.fastq.gz
F2407722,1,monolayer,L1,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407722_R1_combined.fastq.gz,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407722_R2_combined.fastq.gz
F2407723,1,monolayer,L1,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407723_R1_combined.fastq.gz,/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/F2407723_R2_combined.fastq.gz
```


Parameters file used:

```
{
    ""input"": ""/mnt/titan/mcasanova/ctDNA/01_samples/CHUgeneve/CHUgeneve_samplelist.csv"",
    ""wes"": true,
    ""tools"": ""mutect2,vep,snpeff"",
    ""aligner"": ""bwa-mem2"",
    ""vep_custom_args"": ""everything"",
    ""save_mapped"": true,
    ""save_output_as_bam"": true,
    ""genome"": ""GATK.GRCh38"",
    ""save_reference"": true
}
```


### Command used and terminal output

`$ nextflow run nf-core/sarek -profile docker --outdir ./results -resume -params-file nf-param.json -with-report -with-trace -with-dag flowchart.png -with-timeline timeline.html`


```
nextflow.exception.AbortOperationException: Detected join operation duplicate emission on left channel -- offending element: key=[patient:F2407719, sample:monolayer, sex:NA, status:0, n_fastq:12, data_type:bam, id:monolayer]; value=/mnt/titan/mcasanova/ctDNA/03_analysis_CHU/work/7f/5a1152a34ee73b2a2262a119788472/monolayer.sorted.bam
	at nextflow.extension.JoinOp.checkForDuplicate(JoinOp.groovy:266)
	at nextflow.extension.JoinOp.join0(JoinOp.groovy:177)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:343)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:328)
	at groovy.lang.MetaClassImpl.doInvokeMethod(MetaClassImpl.java:1333)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1088)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1007)
	at org.codehaus.groovy.runtime.InvokerHelper.invokePogoMethod(InvokerHelper.java:645)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:628)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeMethodSafe(InvokerHelper.java:82)
	at nextflow.extension.JoinOp$_handler_closure1.doCall(JoinOp.groovy:117)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:343)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:328)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:279)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1007)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.extension.DataflowHelper$_subscribeImpl_closure2.doCall(DataflowHelper.groovy:287)
	at jdk.internal.reflect.GeneratedMethodAccessor134.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:343)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:328)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:279)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1007)
	at groovy.lang.Closure.call(Closure.java:433)
	at groovyx.gpars.dataflow.operator.DataflowOperatorActor.startTask(DataflowOperatorActor.java:120)
	at groovyx.gpars.dataflow.operator.DataflowOperatorActor.onMessage(DataflowOperatorActor.java:108)
	at groovyx.gpars.actor.impl.SDAClosure$1.call(SDAClosure.java:43)
	at groovyx.gpars.actor.AbstractLoopingActor.runEnhancedWithoutRepliesOnMessages(AbstractLoopingActor.java:293)
	at groovyx.gpars.actor.AbstractLoopingActor.access$400(AbstractLoopingActor.java:30)
	at groovyx.gpars.actor.AbstractLoopingActor$1.handleMessage(AbstractLoopingActor.java:93)
	at groovyx.gpars.util.AsyncMessagingCore.run(AsyncMessagingCore.java:132)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
```


### Relevant files

_No response_

### System information

- Nextflow version 24.04.4
- Workstation
- Local
- Docker
- Ubuntu Linux
- nf-core/sarek v3.4.4-g5cc3049
",milcs40,https://github.com/nf-core/sarek/issues/1674
I_kwDOCvwIC86ZLiQY,terminated for an unknown reason -- Likely it has been terminated by the external system,CLOSED,2024-10-07T09:55:48Z,2024-11-11T09:18:36Z,2024-11-11T09:18:36Z,"### Description of the bug

 i don not know why there are such errors

### Command used and terminal output

```
Caused by:
  Process `NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:GATK4_HAPLOTYPECALLER (AK58_C_1)` terminated for an unknown reason -- Likely it has been terminated by the external system

Command executed:

  gatk --java-options ""-Xmx163840M -XX:-UsePerfData"" \
      HaplotypeCaller \
      --input AK58_C_1.md.cram \
      --output AK58_C_1.haplotypecaller.chr2A_part1_1-384157900.g.vcf.gz \
      --reference wheat_AK58v4MP.genome_part.fa \
       \
      --intervals chr2A_part1_1-384157900.bed \
       \
       \
      --tmp-dir . \
      -ERC GVCF
  
  cat <<-END_VERSIONS > versions.yml
  ""NFCORE_SAREK:SAREK:BAM_VARIANT_CALLING_GERMLINE_ALL:BAM_VARIANT_CALLING_HAPLOTYPECALLER:GATK4_HAPLOTYPECALLER"":
      gatk4: $(echo $(gatk --version 2>&1) | sed 's/^.*(GATK) v//; s/ .*$//')
  END_VERSIONS

Command exit status:
  -

Command output:
  (empty)

Work dir:
  /public/home/fanrong/work_lei/01_htt/01_240927_well_bse_bsr/01_bsr/02_sarek/02_vcf/work/bd/a8da3f055c7df258c4271504de32d7

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`

 -- Check '.nextflow.log' file for details

```

### Relevant files

_No response_

### System information

_No response_",fan040,https://github.com/nf-core/sarek/issues/1676
I_kwDOCvwIC86ZW9Y2,Mutations called by Mutect2 in paired mode are not annotated by VEP,OPEN,2024-10-08T11:34:26Z,2024-10-08T11:34:26Z,,"### Description of the bug

When using paired mode with Mutect2 to call mutations from both normal and tumor samples of the same patient, the resulting mutations are not annotated by VEP, despite VEP being specified in the tools parameters. It seems that the mutations are skipped during the annotation process. I would like to clarify whether this is intended behavior or if it constitutes a bug.

### Command used and terminal output

nextflow run /data/cache/nextflow/nf-core-sarek_3.4.2/3_4_2 -resume -offline -profile singularity -params-file configs/wes2_paired.yaml

### Relevant files

the used param file in above command:
```yaml
input:  'configs/wes2_paired.csv'
outdir: 'data/processed/WES2_20240816_paired'
tools:  'mutect2,vep'
wes:    true
step:   'variant_calling'

# reference genome
genome: null
igenomes_ignore: true
save_reference: true
fasta: ""/data/genomes/alias/GRCm39_ensembl/fasta/110.20230421-sm-primary_assembly/GRCm39_ensembl.fa""
fasta_fai: ""/data/genomes/alias/GRCm39_ensembl/fasta/110.20230421-sm-primary_assembly/GRCm39_ensembl.fa.fai""
# bwa: '/data/genomes/alias/GRCm39_ensembl/bwa_index/110.20230421-sm-primary_assembly-bwa_0.7.17'
vep_cache: '/data/cache/vep_cache'
vep_genome: 'GRCm39'
vep_species: 'mus_musculus'
vep_cache_version: 110
download_cache: false

snpeff_cache: '/data/cache/snpeff_cache'
snpeff_genome: 'GRCm38'
snpeff_db: 99

trim_fastq: true
save_mapped: true
save_output_as_bam: true

# resource
max_cpus: 32
max_memory: ""192.GB""
```

### System information

_No response_",Janzulene,https://github.com/nf-core/sarek/issues/1679
I_kwDOCvwIC86ZpfhW,Issue: FASTQC Process Fails with Exit Code 140 in nf-core/sarek Pipeline Using Singularity,OPEN,2024-10-10T06:42:35Z,2024-10-10T07:59:38Z,,"### Description of the bug

### **FASTQC Process Fails with Exit Code 140 in nf-core/sarek Pipeline Using Singularity:**  

---

### **Description of the bug:**  
The `nf-core/sarek` pipeline is consistently failing during the FASTQC process with exit code `140` when executed on a Slurm-based HPC cluster using Singularity. The same issue occurs when using Docker as the container runtime.

---


### **Additional context and observations:**

- The error occurs during the `FASTQC` process in both Singularity and Docker executions.
- Other pipelines such as `nf-core/rnaseq` run without issues in the same environment.
- Running the pipeline with root privileges also fails.
- The warning `Skipping mount /usr/local/var/singularity/mnt/session/etc/resolv.conf` appears but may not be directly related to the issue.
- Java I/O error `java.io.IOException: Bad file descriptor` suggests possible file handling issues within the container.
- The error persists even when Singularity is correctly configured and verified with other workflows.

--- 

### **Request for assistance:**

I am seeking help to resolve this issue with the FASTQC process in the nf-core/sarek pipeline. Any guidance on addressing the exit code 140 error would be greatly appreciated, particularly:

- Is this a known issue with FASTQC in nf-core/sarek?
- Could the `java.io.IOException: Bad file descriptor` indicate an underlying issue in the pipeline or the environment?
- Are there specific settings or configurations required for running this pipeline with Singularity on SLURM?


---

Posted as well at the NF-CORE Slack channel - https://nfcore.slack.com/archives/CE6SDBX2A/p1728540986179659

_Added command and terminal output and relevant files below._


### Command used and terminal output


### **Command used and terminal output:**  

#### **Command Executed:**
```bash
nextflow run nf-core/sarek -profile singularity --input samplesheet.csv --genome hg38 -r 3.2.3 -c nextflow.conf --outdir results_output --wes --known_indels Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --tools mutect2,snpeff --resume
```

#### **Error Output:**
```bash
Failed Process: NFCORE_SAREK:SAREK:FASTQC (Sample-1)

Command Executed:

printf ""%s %s\n"" DNA_Sample-1.R1.fastq.gz Sample-1_1.gz DNA_Sample-1.R2.fastq.gz Sample-1_2.gz | while read old_name new_name; do
    [ -f ""${new_name}"" ] || ln -s $old_name $new_name
done
fastqc --quiet --threads 8 Sample-1_1.gz Sample-1_2.gz

cat <<-END_VERSIONS > versions.yml
""NFCORE_SAREK:SAREK:FASTQC"":
    fastqc: $( fastqc --version | sed -e ""s/FastQC v//g"" )
END_VERSIONS

Exit Code: 140

Command Error:
WARNING: Skipping mount /usr/local/var/singularity/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container
java.io.IOException: Bad file descriptor
```

### Relevant files


---

### **Relevant files:**  
The following is the script used to launch the job (paths and personal information generalized for privacy):

```bash
#!/bin/bash
#SBATCH --job-name=CARIS_singularity # Job name
#SBATCH -p long
#SBATCH --mail-type=END,FAIL          # Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --ntasks=1                    # Run on a single CPU
#SBATCH --mem=10G                     # Job memory request
#SBATCH --cpus-per-task=1
#SBATCH --output=%x_%j_nobed.log   # Standard output and error log
#SBATCH --error=%x_%j_nobed.err

samples=""samplesheet.csv""
sarekoutput=""results_$SLURM_JOB_NAME""
logdir=""/path/to/log/""
logfile=""$SLURM_JOB_NAME.txt""
pon=""/path/to/required/1000g_pon.hg38.vcf.gz""
pon_tbi=""/path/to/required/1000g_pon.hg38.vcf.gz.tbi""
known_indels=""/path/to/required/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz""
other="" --germline_resource /path/to/required/af-only-gnomad.raw.sites_mod.vcf.gz --germline_resource_tbi /path/to/required/af-only-gnomad.raw.sites_mod.vcf.gz.tbi --pon $pon --pon_tbi $pon_tbi""
maxmem=""256.GB""
igenomes=""/path/to/required/""
max_cpu=""48""
max_time=""600.h""
tools=""mutect2,snpeff""

cmd=""nextflow run nf-core/sarek -profile singularity --input $samples --genome hg38 -r 3.2.3 -c nextflow.conf --outdir $sarekoutput --wes --known_indels $known_indels --trim_fastq --resume --tools $tools $other""

# Create cache directory if it doesn't exist
if [ ! -d ""cache"" ]; then
    mkdir cache
fi

# Create nextflow config file
read -r -d '' config <<- EOM
params {
  config_profile_description = 'bioinfo config'
  config_profile_contact = '$SLURM_JOB_USER $SLURM_JOB_USER@domain.org'
}

singularity {
  enabled = true
  autoMounts = true
  cacheDir ='./cache/'
}

executor {
  name = 'slurm'
  queueSize = 12
}

process {
  executor = 'slurm'
  queue  = { task.time <= 5.h && task.memory <= 10.GB ? 'short': (task.memory <= 95.GB ? 'long' : 'highmem')}
  queueSize = 12
}

params {
  max_memory = '$maxmem'
  max_cpus = $max_cpu
  max_time = '$max_time'
}
EOM

echo ""$config"" > nextflow.conf

# Create log file
message=$(date +""%D %T"")""        ""$(whoami)""     ""$SLURM_JOB_NAME""       ""$cmd
echo  $message >> $logdir$logfile

# Execute nextflow command
nextflow run nf-core/sarek -profile singularity --input $samples --genome GATK.GRCh38 -r 3.2.3 -c nextflow.conf --outdir $sarekoutput --wes --known_indels $known_indels --trim_fastq --resume --tools mutect2,snpeff
```

---

### System information

---

### **System information:**  
- **Nextflow version:** `24.04.3`
- **Hardware:** HPC
- **Executor:** Slurm
- **Container engine:** Singularity `3.11.0`, Docker 24.0.7
- **OS:** Ubuntu 22.04 (Jammy Jellyfish)
- **Version of nf-core/sarek:** `3.2.3`

---

I have attached the stdout final message and the stderr. The output logs show the message described ealier at the screenshot, and as for the .err, I am having a ""missing txt file"" which I do not recognize.

The job ran for 24 hours, with a couple of failed jobs, it still produced an output of 2 TB at the work directories.

Script Location: The entire script and more details are available on GitHub at this issue link.![Image](https://github.com/user-attachments/assets/90589a48-f39b-4ea6-9d3a-04228e0d5e6c)
![Image](https://github.com/user-attachments/assets/b03c9b6c-c3e2-44e3-a966-36f467d684c9)
",SirAymane,https://github.com/nf-core/sarek/issues/1683
I_kwDOCvwIC86Z0X2i,No status is 0 - but contains tumor only sample error message,OPEN,2024-10-11T07:27:23Z,2024-11-12T13:56:44Z,,"### Description of the bug

The sample-sheet only contains tumor-samples, but the following tools, which were requested by the option ""tools"", expect at least one normal-sample : deepvariant, haplotypecaller

while running with a samplesheet with no status and these tools

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1685
I_kwDOCvwIC86aJevl,Haplotypecaller exit status 402,OPEN,2024-10-14T14:11:48Z,2024-10-15T16:29:30Z,,"Hi! I am new in NF-Sarek Pipeline. I have been trying to run this for multiple times but I always get the same error.
The pipeline crashes in haplotypecaller step:

![Image](https://github.com/user-attachments/assets/75e7e3e2-25b3-49c2-ba94-150ba22c5273)
![Image](https://github.com/user-attachments/assets/8539246a-aade-4bd3-88eb-60c49ed2c0f0)


I also tried -resume option but it still crashes. I also increase time and memory but still failed.",janejajajuana,https://github.com/nf-core/sarek/issues/1687
I_kwDOCvwIC86a8t7c,"--skip_tools ""markduplicates,baserecalibrator"" in combination terminates run after mapping",CLOSED,2024-10-19T21:15:21Z,2024-10-19T21:37:45Z,2024-10-19T21:37:45Z,"### Description of the bug

Running the test profile with --skip_tools ""markduplicates,baserecalibrator"" skips MD and BQSR. However it also skips QC. Expected would be to run `CRAM_QC_NO_MD` 



### Command used and terminal output

nextflow run . -profile test,docker --outdir results -stub-run --split_fastq 0 --skip_tools ""markduplicates,baserecalibrator"" -resume

### Relevant files

_No response_

### System information

_No response_",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1689
I_kwDOCvwIC86a_woK,Problem with intervals file,OPEN,2024-10-20T12:15:18Z,2024-10-27T22:35:08Z,,"### Description of the bug

I think the problem might be the format (using .list or .bed formats) that I used to use in intervals parameter and I do not know the correct format (even after reading the help in sarek page). I need to use these intervals because my data are WES and I created an bed file from Illumina Exome Targeted Regions bed file. I will upload the files (my google drive link) that I used as intervals (exome_intervals.list, exome_intervals.bed) and the Illumina Exome Targeted Regions bed file (Illumina_Exome_TargetedRegions_v1.2.hg38.bed). Could someone help me?

### Command used 
`nextflow run nf-core/sarek -r 3.4.4 -profile conda --input /home/eduardo/gargantua/samplesheet.csv -work-dir /home/eduardo/gargantua/temp --step mapping --aligner bwa-mem2 --concatenate_vcfs false --joint_germline false --fasta /home/eduardo/sequencing_resources/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --fasta_fai /home/eduardo/sequencing_resources/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai --wes --intervals /home/eduardo/gargantua/exome_intervals.list --dbsnp /home/eduardo/sequencing_resources/GRCh38.dbSNP156_chr_included.vcf.gz --dbsnp_tbi /home/eduardo/sequencing_resources/GRCh38.dbSNP156_chr_included.vcf.gz.tbi --known_indels /home/eduardo/sequencing_resources/Homo_sapiens_assembly38.known_indels.vcf.gz,/home/eduardo/sequencing_resources/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known_indels_tbi /home/eduardo/sequencing_resources/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi,/home/eduardo/sequencing_resources/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi --skip_tools bcftools,multiqc,fastqc --genome null --igenomes_ignore --outdir home/eduardo/gargantua/results/`

### Terminal output
**This example is for only .list file and the same error happens with .bed file**

```
[](N E X T F L O W   ~  version 24.04.4

Launching `https://github.com/nf-core/sarek` [exotic_solvay] DSL2 - revision: 5cc30494a6 [3.4.4]

WARN: Access to undefined parameter `monochromeLogs` -- Initialise it to a default value eg. `params.monochromeLogs = some_value`


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .´ _  `.
   /  |\`-_ \      __        __   ___     
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /¯¯\ |  \ |___ |  \
    `|____\´

  nf-core/sarek v3.4.4-g5cc3049
------------------------------------------------------
Core Nextflow options
  revision             : 3.4.4
  runName              : exotic_solvay
  launchDir            : /home/eduardo/gargantua
  workDir              : /home/eduardo/gargantua/temp
  projectDir           : /home/eduardo/.nextflow/assets/nf-core/sarek
  userName             : eduardo
  profile              : conda
  configFiles          : 

Input/output options
  input                : /home/eduardo/gargantua/samplesheet.csv
  outdir               : home/eduardo/gargantua/results/

Main options
  wes                  : true
  intervals            : /home/eduardo/gargantua/exome_intervals.list
  skip_tools           : bcftools,multiqc,fastqc

Preprocessing
  aligner              : bwa-mem2

Reference genome options
  genome               : null
  dbsnp                : /home/eduardo/sequencing_resources/GRCh38.dbSNP156_chr_included.vcf.gz
  dbsnp_tbi            : /home/eduardo/sequencing_resources/GRCh38.dbSNP156_chr_included.vcf.gz.tbi
  fasta                : /home/eduardo/sequencing_resources/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna
  fasta_fai            : /home/eduardo/sequencing_resources/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai
  known_indels         : /home/eduardo/sequencing_resources/Homo_sapiens_assembly38.known_indels.vcf.gz,/home/eduardo/sequencing_resources/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz
  known_indels_tbi     : /home/eduardo/sequencing_resources/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi,/home/eduardo/sequencing_resources/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi
  igenomes_ignore      : true

Generic options
  validationLenientMode: true

!! Only displaying parameters that differ from the pipeline defaults !!
------------------------------------------------------
If you use nf-core/sarek for your analysis please cite:

* The pipeline
  https://doi.org/10.12688/f1000research.16665.2
  https://doi.org/10.1093/nargab/lqae031
  https://doi.org/10.5281/zenodo.3476425

* The nf-core framework
  https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md
------------------------------------------------------
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                  -
[-        ] NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                     -
[-        ] NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                  -
[-        ] NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                     -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS                         -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP                                        -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                            -
executor >  local (1)
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                  -
[-        ] NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                     -
[-        ] NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                  -
[-        ] NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                     -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS                         -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP                                        -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                            -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                   -
executor >  local (3)
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                  -
[-        ] NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                     -
[-        ] NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                  -
[-        ] NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                     -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS                         -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP                                        -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                            -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                   -
executor >  local (4)
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                  -
[-        ] NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                     -
[-        ] NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                  -
executor >  local (4)
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                  -
[-        ] NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY                     -
[-        ] NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                  -
executor >  local (4)
executor >  local (5)
executor >  local (5)
executor >  local (5)
executor >  local (5)
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                                                    -
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                                                    -
[-        ] NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                                                -
[6c/b0ee11] NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY (GCA_000001405.15_GRCh38_no_alt_analysis_set.fna) [100%] 1 of 1 ✔
[-        ] NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                                                -
[-        ] NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                                                   -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS                                                       -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP                                                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                                                          -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                                                 -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                                                               -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_PON                                                                        -
[-        ] NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED                                                          -
[-        ] NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT                                               -
[-        ] NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED                                            -
[-        ] NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_FQ_PAIR                                                              -
[-        ] NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R1_FQ                                                                -
[-        ] NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R2_FQ                                                                -
[-        ] NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP                                                 -
[-        ] NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP                                             -
[4f/3e1b79] NFCORE_SAREK:SAREK:FASTP (DAC021-L2)                                                                         [100%] 4 of 4 ✔
Plus 24 more processes waiting for tasks…
ERROR ~ Error executing process > 'NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED'

Caused by:
  Failed to create Conda environment
    command: conda env create --prefix /home/eduardo/gargantua/temp/conda/gawk-f9b4f646bf1b67e52890543d27cb2f27 --file /home/eduardo/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_intervals/../../../modules/local/create_intervals_bed/environment.yml
    status : 1
    message:
      Channels:
executor >  local (5)
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                                                                    -
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                                                                    -
[-        ] NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                                                                -
[6c/b0ee11] NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY (GCA_000001405.15_GRCh38_no_alt_analysis_set.fna) [100%] 1 of 1 ✔
[-        ] NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                                                                -
[-        ] NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                                                                   -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS                                                       -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP                                                                      -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                                                          -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                                                                 -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                                                               -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_PON                                                                        -
[-        ] NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED                                                          -
[-        ] NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT                                               -
[-        ] NFCORE_SAREK:PREPARE_INTERVALS:TABIX_BGZIPTABIX_INTERVAL_COMBINED                                            -
[-        ] NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_FQ_PAIR                                                              -
[-        ] NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R1_FQ                                                                -
[-        ] NFCORE_SAREK:SAREK:SPRING_DECOMPRESS_TO_R2_FQ                                                                -
[-        ] NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_MAP_MAP                                                 -
[-        ] NFCORE_SAREK:SAREK:CONVERT_FASTQ_INPUT:SAMTOOLS_VIEW_UNMAP_UNMAP                                             -
[4f/3e1b79] NFCORE_SAREK:SAREK:FASTP (DAC021-L2)                                                                         [100%] 4 of 4 ✔
Plus 24 more processes waiting for tasks…
ERROR ~ Error executing process > 'NFCORE_SAREK:PREPARE_INTERVALS:CREATE_INTERVALS_BED'

Caused by:
  Failed to create Conda environment
    command: conda env create --prefix /home/eduardo/gargantua/temp/conda/gawk-f9b4f646bf1b67e52890543d27cb2f27 --file /home/eduardo/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_intervals/../../../modules/local/create_intervals_bed/environment.yml
    status : 1
    message:
      Channels:
       - conda-forge
       - bioconda
       - defaults
       - anaconda
      Platform: linux-64
      Collecting package metadata (repodata.json): ...working... done
      Solving environment: ...working... failed
      Channels:
       - conda-forge
       - bioconda
       - defaults
       - anaconda
      Platform: linux-64
      Collecting package metadata (repodata.json): ...working... done
      Solving environment: ...working... failed
      
      LibMambaUnsatisfiableError: Encountered problems while solving:
        - package gawk-5.1.0-h7b6447c_0 is excluded by strict repo priority



 -- Check '.nextflow.log' file for details
ERROR ~ Pipeline failed. Please refer to troubleshooting docs: https://nf-co.re/docs/usage/troubleshooting

 -- Check '.nextflow.log' file for details
-[nf-core/sarek] Pipeline completed with errors-)
```
### Relevant files

[Google drive files](https://drive.google.com/drive/folders/1kRS5mF5PjUfdgR0v9OvOM-xX8kiOOLOi?usp=sharing)

### System information

Ubuntu 24.04.1 LTS
mamba 1.5.8
conda 24.3.0
nextflow 24.04.4
sarek v3.4.4-g5cc3049
",Eduardo-Auer,https://github.com/nf-core/sarek/issues/1690
I_kwDOCvwIC86bGQU8,`vep_loftee` doesn't add the expected columns to `INFO/CSQ`,OPEN,2024-10-21T10:37:24Z,2024-10-28T11:00:33Z,,"Related to https://github.com/nf-core/sarek/issues/687, which is marked complete but `loftee` still do not run and generates the same warning.

### Description of the bug

I am trying to annotate a vcf file using sarek (3.2.3 or 3.4.2) with `--step annotate`.

The pipeline starts with the option correctly truned on 
```bash
Core Nextflow options
  revision.    : 3.4.2
<...>
Annotation
  vep_loftee   : true
  vep_spliceai : true
```

- `vep` seems to work fine
- I could use `--vep_spliceai` fine.
**- Adding `--vep_loftee` doesn't seem to add any annotations to `INFO/CSQ` and produces a warning**

### Command used and terminal output

$ # nextflow run nf-core/sarek -r 3.2.3 \
$ nextflow run nf-core/sarek -r 3.4.2 \
	-profile singularity \
        -params-file params.yml \
        --vep_loftee true \
        --vep_spliceai true

# The pipeline completes successfully, and I get this warning
WARNING: Failed to compile plugin LoF: Can't locate Bio/Perl.pm in @INC (you may need to inst
all the Bio::Perl module) (@INC contains: /home/mahmed03/.vep/Plugins /usr/local/share/ensemb
l-vep-111.0-0/modules /usr/local/share/ensembl-vep-111.0-0 /usr/local/lib/perl5/5.32/site_per
l /usr/local/lib/perl5/site_perl /usr/local/lib/perl5/5.32/vendor_perl /usr/local/lib/perl5/v
endor_perl /usr/local/lib/perl5/5.32/core_perl /usr/local/lib/perl5/core_perl .) at /usr/loca
l/share/ensembl-vep-111.0-0/LoF.pm line 46.
BEGIN failed--compilation aborted at /usr/local/share/ensembl-vep-111.0-0/LoF.pm line 46.
Compilation failed in require at (eval 43) line 2.
BEGIN failed--compilation aborted at (eval 43) line 2.

### Relevant files

[nextflow.log](https://github.com/user-attachments/files/17459333/nextflow.log)


### System information

**System information**

- Nextflow version: 24.04.4
- Hardware: HPC 
- Executor: slurm and local
- Container engine: Singularity 
- OS: Linux
- Version of nf-core/sarek: 3.2.3 and 3.4.2",MahShaaban,https://github.com/nf-core/sarek/issues/1691
I_kwDOCvwIC86cCmWm,transfer variant calling tutorial on nf-core/sarek repo,CLOSED,2024-10-28T10:02:46Z,2024-10-28T16:18:45Z,2024-10-28T16:18:45Z,"This issue is to track the work needed to add the tutorial on Sarek variant calling under /usage on Sarek repo during the hackathon October 2024

For documentation:
the material in .md files should be added under /docs by creating a /usage folder in
https://github.com/nf-core/sarek/tree/dev/docs
each .md page under /usage will be linked on the left hand side on the rendered nf-core website

see here:
https://github.com/nf-core/crisprseq/tree/master/docs/usage
rendered here:
https://nf-co.re/crisprseq/2.2.1/docs/usage/",lescai,https://github.com/nf-core/sarek/issues/1696
I_kwDOCvwIC86cCyXb,Hackathon Items,OPEN,2024-10-28T10:22:23Z,2024-10-28T11:12:42Z,,,edmundmiller,https://github.com/nf-core/sarek/issues/1697
I_kwDOCvwIC86cCywC,Fix nf-schema bug,CLOSED,2024-10-28T10:23:04Z,2024-10-28T11:07:24Z,2024-10-28T11:01:02Z,,edmundmiller,https://github.com/nf-core/sarek/issues/1698
I_kwDOCvwIC86cCzJj,Get a full nf-test report in cis,OPEN,2024-10-28T10:23:35Z,2024-10-28T10:23:35Z,,,edmundmiller,https://github.com/nf-core/sarek/issues/1699
I_kwDOCvwIC86cCzV7,Remove pytest-workflow,OPEN,2024-10-28T10:23:47Z,2024-10-28T10:23:47Z,,,edmundmiller,https://github.com/nf-core/sarek/issues/1700
I_kwDOCvwIC86cDSKY,Shard 5 : Error: no valid tests found,OPEN,2024-10-28T11:12:42Z,2024-10-28T11:12:43Z,,"### Description of the bug

https://github.com/nf-core/sarek/actions/runs/11552623812/job/32152106306?pr=1702

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",maxulysse,https://github.com/nf-core/sarek/issues/1703
I_kwDOCvwIC86cEadW,Proposals for Sarek Tutorials on use-cases,OPEN,2024-10-28T13:17:20Z,2024-10-30T21:10:36Z,,"@edmundmiller and friends :)
this is a placeholder to collect ideas to realise additional tutorials on use-cases of sarek
Variant Calling is currently being integrated (http://lescai-teaching.github.io/sarek-tutorial/) into sarek repo usage docs.
Similar things could be realised for:
- CNV calling
- UMI somatic variant calling
[ propose others here]",lescai,https://github.com/nf-core/sarek/issues/1704
I_kwDOCvwIC86cX81X,Tutorial: CNV calling,OPEN,2024-10-30T10:07:04Z,2024-10-30T10:47:54Z,,"## Topic

CNVs analysis using Sarek

## Tasks to realise the tutorial

1) identify appropriate software
2) simulate CNV data (max size 100Mb each)
3) prepare gitpod / codespaces environment and build code
4) prepare theoretical intro
5) write down hands-on tutorial",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1713
I_kwDOCvwIC86cX9Tl,Tutorial: UMI,OPEN,2024-10-30T10:07:29Z,2024-10-30T10:07:30Z,,"### Description of feature

sss",FriederikeHanssen,https://github.com/nf-core/sarek/issues/1714
I_kwDOCvwIC86cYH51,Add library metadata and extend read group info,OPEN,2024-10-30T10:19:32Z,2024-10-30T10:19:32Z,,"### Description of feature

Based on the read group recommendations of the Sentieon docs (https://support.sentieon.com/appnotes/read_groups/), the library info can be added to the read group data so that reads from the same sample created in the same library prep step can be grouped together for Picard PCR duplicate detection and merging of reads (I believe currently only the same sample across different lanes are merged: `LB:${meta.sample}`).

This might occur when a specific library is re-sequenced at a later point (e.g., because the read output was too low). A sample like this (= same sample name, same library, optionally across multiple lanes and/or flowcells) needs to be distinguishable from a sample that underwent separate library preparations (= same sample name, different library, different flowcell, optionally across multiple lanes and/or flowcells). 

The library info could perhaps be added as an optional field in the input samplesheet.

Secondly, the read group info could be extended so that the PU field contains ` flowcell.lane` (currently `PU:${meta.lane}`). This is the info that  BQSR modelling looks at. Could it be that currently lanes from different flowcells are considered the same by BQSR?",pmoris,https://github.com/nf-core/sarek/issues/1715
I_kwDOCvwIC86cYH76,Check if flowcell id is uniform in a fastq file,OPEN,2024-10-30T10:19:35Z,2024-10-30T10:19:35Z,,"### Description of feature

This is an extension of https://github.com/nf-core/sarek/pull/1664

The proposal above adds a check to determine whether the first read of a paired fastq sample shares its flowcell id. However, if a user were to supply a fastq file that was merged from different fastq files of the same sample (e.g., across lanes or sequence runs), the flowcell id would not be the same for all reads in the fastq file.

Ideally we'd add a check to ensure that this is the case (seqkit?), or if possible, use something like `fastp` to split the files into separte fastq files with uniform flowcell ids?",pmoris,https://github.com/nf-core/sarek/issues/1716
I_kwDOCvwIC86cYgeC,Tutorial: long reads analysis,OPEN,2024-10-30T10:50:23Z,2024-10-30T10:50:24Z,,"### Description of feature

## Topic

Long Reads (nanopore, PacBio) analysis using Sarek

## Tasks to realise the tutorial

1) check whether https://github.com/nf-core/readsimulator can simulate long-reads or identify appropriate alternative
2) simulate sequence data (max size 100Mb each)
3) prepare gitpod / codespaces environment and build code
4) prepare theoretical intro
5) write down hands-on tutorial",lescai,https://github.com/nf-core/sarek/issues/1717
I_kwDOCvwIC86cephb,The target server failed to respond,OPEN,2024-10-30T20:33:42Z,2024-10-30T20:48:00Z,,"### Description of the bug

Hi there

Thank you for the pipeline, it was working good using just frebayes. now add sarek and mutec. 
I need to find structural mutations, and de novo mutation, however still missing the parent to do all the process.


the pipeline was working fine, but today I have this error:

ERROR ~ Error executing process > 'NFCORE_SAREK:SAREK:VCF_ANNOTATE_ALL:VCF_ANNOTATE_ENSEMBLVEP:ENSEMBLVEP_VEP (1)'
Caused by:
  The target server failed to respond

someone, know what is about???

2.- someone know how iI can filter all the SNV reported, and just keep the one that are in the samples???

thank you 

### Command used and terminal output

 N E X T F L O W   ~  version 24.04.4
  nf-core/sarek v3.4.4-g5cc3049
------------------------------------------------------
Core Nextflow options
  revision             : 3.4.4
  runName              : boring_lavoisier
  containerEngine      : singularity



### Relevant files

_No response_

### System information

HPC, 
serum
Singularity
",torres-HI,https://github.com/nf-core/sarek/issues/1718
I_kwDOCvwIC86clw20,Provide default UMI/trimming library specific values,OPEN,2024-10-31T15:27:12Z,2024-10-31T15:27:12Z,,"### Description of feature

That would be amazing",maxulysse,https://github.com/nf-core/sarek/issues/1719
I_kwDOCvwIC86coEtj,"how to use custom vep database for microbe, such as Candian.aruis",OPEN,2024-10-31T20:20:56Z,2024-10-31T20:20:56Z,,"### Description of the bug

custom vep annotation seems not working for microbe, such as Candian.aruis, sars_cov2, etc.

### Command used and terminal output

N E X T F L O W   ~  version 24.04.4

Launching `./sarek/main.nf` [nostalgic_wescoff] DSL2 - revision: 1eeed174dc

WARN: Access to undefined parameter `monochromeLogs` -- Initialise it to a default value eg. `params.monochromeLogs = some_value`


------------------------------------------------------
                                        ,--./,-.
        ___     __   __   __   ___     /,-._.--~'
  |\ | |__  __ /  ` /  \ |__) |__         }  {
  | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                        `._,._,'
      ____
    .Â´ _  `.
   /  |\`-_ \      __        __   ___     
  |   | \  `-|    |__`  /\  |__) |__  |__/
   \ |   \  /     .__| /Â¯Â¯\ |  \ |___ |  \
    `|____\Â´

  nf-core/sarek v3.5.0dev
------------------------------------------------------
Core Nextflow options
  runName                    : nostalgic_wescoff
  containerEngine            : singularity
  launchDir                  : /blue/bphl-florida/dongyibo/nf-core/sarek3
  workDir                    : /blue/bphl-florida/dongyibo/nf-core/sarek3/work
  projectDir                 : /blue/bphl-florida/dongyibo/nf-core/sarek3/sarek
  userName                   : dongyibo
  profile                    : singularity
  configFiles                : 

Input/output options
  input                      : ./samplesheet.csv
  outdir                     : ./results/

Main options
  tools                      : freebayes,strelka,snpeff,vep
  skip_tools                 : baserecalibrator,baserecalibrator_report,dnascope_filter

Variant Calling
  only_paired_variant_calling: true

Reference genome options
  fasta                      : /blue/bphl-florida/dongyibo/nf-core/sarek3/data/reference/covid/nCoV-2019.reference.fasta
  fasta_fai                  : /blue/bphl-florida/dongyibo/nf-core/sarek3/data/reference/covid/nCoV-2019.reference.fasta.fai
  snpeff_db                  : MN908947.3
  vep_genome                 : MN908947.3
  vep_species                : sars_cov2
  igenomes_ignore            : true
  vep_cache                  : /blue/bphl-florida/dongyibo/nf-core/sarek3/data/vep_cache
  snpeff_cache               : /blue/bphl-florida/dongyibo/nf-core/sarek3/data/snpeff_cache

Generic options
  validationLenientMode      : true

!! Only displaying parameters that differ from the pipeline defaults !!
------------------------------------------------------
If you use nf-core/sarek for your analysis please cite:

* The pipeline
  https://doi.org/10.12688/f1000research.16665.2
  https://doi.org/10.1093/nargab/lqae031
  https://doi.org/10.5281/zenodo.3476425

* The nf-core framework
  https://doi.org/10.1038/s41587-020-0439-x

* Software dependencies
  https://github.com/nf-core/sarek/blob/master/CITATIONS.md
------------------------------------------------------
[-        ] NFCâ€¦EPARE_GENOME:BWAMEM1_INDEX -
[-        ] NFCâ€¦EPARE_GENOME:BWAMEM2_INDEX -
[-        ] NFCâ€¦E_GENOME:DRAGMAP_HASHTABLE -
[-        ] NFCâ€¦4_CREATESEQUENCEDICTIONARY -

[-        ] NFCâ€¦EPARE_GENOME:BWAMEM1_INDEX -
[-        ] NFCâ€¦EPARE_GENOME:BWAMEM2_INDEX -
[-        ] NFCâ€¦E_GENOME:DRAGMAP_HASHTABLE -
[-        ] NFCâ€¦4_CREATESEQUENCEDICTIONARY -
[-        ] NFCâ€¦E_GENOME:MSISENSORPRO_SCAN -
[-        ] NFCâ€¦PARE_GENOME:SAMTOOLS_FAIDX -
[-        ] NFCâ€¦TABIX_BCFTOOLS_ANNOTATIONS -
[-        ] NFCâ€¦PREPARE_GENOME:TABIX_DBSNP -
[-        ] NFCâ€¦ME:TABIX_GERMLINE_RESOURCE -
[-        ] NFCâ€¦RE_GENOME:TABIX_KNOWN_SNPS -
[-        ] NFCâ€¦_GENOME:TABIX_KNOWN_INDELS -
[-        ] NFCâ€¦K:PREPARE_GENOME:TABIX_PON -
[-        ] NFCâ€¦_INTERVALS:BUILD_INTERVALS -
[-        ] NFCâ€¦RVALS:CREATE_INTERVALS_BED -
Path provided with VEP cache is invalid.
Make sure there is a directory named sars_cov2/null_MN908947.3 in /blue/bphl-florida/dongyibo/nf-core/sarek3/data/vep_cache./nPlease refer to https://nf-co.re/sarek/docs/usage/#how-to-customise-snpeff-and-vep-annotation for more information.


### Relevant files

_No response_

### System information

_No response_",ybdong919,https://github.com/nf-core/sarek/issues/1720
I_kwDOCvwIC86cxIRL,the vcf file from  haplotypecaller was not annotated by snpeff and vep,OPEN,2024-11-02T02:16:54Z,2024-11-04T14:56:06Z,,"### Description of the bug

After I got snp vcf files from strelka, freebayes, mpileup, and haplotypecaller, I tried to annotate them by snpeff and vep. Finally, the vcf file from  haplotypecaller was not annotated by snpeff and vep, but other vcf files from strelka, freebayes and mpileup are all successfully annotated by snpeff  and vep. 
Do you know why?

### Command used and terminal output

_No response_

### Relevant files

_No response_

### System information

_No response_",ybdong919,https://github.com/nf-core/sarek/issues/1722
I_kwDOCvwIC86czq7c,No such property: enable for class: java.lang.String - nextflow.enable.configProcessNamesValidation problem,CLOSED,2024-11-02T21:02:42Z,2024-11-04T16:29:36Z,2024-11-04T16:29:34Z,"### Description of the bug

Hi sarekers,

I am getting problem with:

`No such property: enable for class: java.lang.String`

Solution:

I commented this debug line in `nextflow.config` and it worked:

```
// Disable process selector warnings by default. Use debug profile to enable warnings.
// nextflow.enable.configProcessNamesValidation = false
```

In some way the change in [nextflow/#2700](https://github.com/nextflow-io/nextflow/issues/2700) is affected by java version.


### Command used and terminal output

nextflow run nf-core/sarek -r 3.4.4 -profile test

### Relevant files

Nov-02 17:10:34.806 [main] DEBUG nextflow.config.ConfigBuilder - Applying config profile: `test`
Nov-02 17:10:36.629 [main] ERROR nextflow.cli.Launcher - Unable to parse config file: '~/.nextflow/assets/nf-core/sarek/nextflow.config'

  No such property: enable for class: java.lang.String

groovy.lang.MissingPropertyException: No such property: enable for class: java.lang.String
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:67)
	at org.codehaus.groovy.vmplugin.v8.IndyGuardsFiltersAndSignatures.unwrap(IndyGuardsFiltersAndSignatures.java:163)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at _nf_config_6dcc87d5.run(_nf_config_6dcc87d5:364)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:343)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.config.ConfigParser$_parse_closure5.doCall(ConfigParser.groovy:457)
	at jdk.internal.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:343)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaMethod.invoke(ClosureMetaMethod.java:88)
	at groovy.lang.ExpandoMetaClass.invokeMethod(ExpandoMetaClass.java:1140)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1007)
	at groovy.lang.DelegatingMetaClass.invokeMethod(DelegatingMetaClass.java:165)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.selectMethod(IndyInterface.java:355)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.config.ConfigParser.parse(ConfigParser.groovy:486)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.config.ConfigParser.parse(ConfigParser.groovy:312)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.config.ConfigBuilder.parse0(ConfigBuilder.groovy:439)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.config.ConfigBuilder.merge0(ConfigBuilder.groovy:425)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.config.ConfigBuilder.buildConfig0(ConfigBuilder.groovy:375)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.config.ConfigBuilder.buildGivenFiles(ConfigBuilder.groovy:320)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.config.ConfigBuilder.buildConfigObject(ConfigBuilder.groovy:818)
	at org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)
	at nextflow.config.ConfigBuilder.build(ConfigBuilder.groovy:831)
	at nextflow.cli.CmdRun.run(CmdRun.groovy:332)
	at nextflow.cli.Launcher.run(Launcher.groovy:503)
	at nextflow.cli.Launcher.main(Launcher.groovy:658)

### System information

info
  
```
  Version: 24.10.0 build 5928
  Created: 27-10-2024 18:36 UTC (15:36 BRST)
  System: Linux 4.18.0-477.10.1.el8_8.x86_64
  Runtime: Groovy 4.0.23 on OpenJDK 64-Bit Server VM 17.0.10+7
  Encoding: UTF-8 (UTF-8)
```

HPC - slurm:

```
Linux 4.18.0-477.10.1.el8_8.x86_64 #1 SMP Tue May 16 11:38:37 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```",allyssonallan,https://github.com/nf-core/sarek/issues/1723
I_kwDOCvwIC86c6hoT,question regarding computational power,OPEN,2024-11-04T11:42:38Z,2024-11-05T21:26:06Z,,"I have a question regarding computational power. I'd like to know how much RAM is sufficient to run each tool in sarek pipeline.
I am analyzing exome derived from cancerous cell lines using typical command running in a docker. I was only able to run preprocessing and strelka with 34 GB RAM, any other tool employed for a job was halted due to insufficient amount of RAM.

Sincerely,
Michał",mggrami,https://github.com/nf-core/sarek/issues/1724
I_kwDOCvwIC86dFtDA,Unable to start,OPEN,2024-11-05T13:47:33Z,2024-11-06T00:14:33Z,,"### Description of the bug

Dear sarek developers,

First of all, congrats and thanks for your work.

I am new to the Nextflow/nf-core field and I am trying to run the pipeline with my own MOUSE dataset but I get the following error:

------------------------------------------------------
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                  -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                  -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE              -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN              -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                 -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS     -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP                    -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE        -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS               -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS             -
[-        ] process > NFCORE_SAREK:PREPARE_GENOME:TABIX_PON                      -
Missing process or function Channel.empty([[]])

 -- Check script '/home/localadmin/.nextflow/assets/nf-core/sarek/main.nf' at line: 342 or see '.nextflow.log' file for more details

------------------------------------------------------

Command line: nextflow run nf-core/sarek \
                                           -profile singularity \
                                           --genome GRCm38 \
                                           --input /media/localadmin/DATA/dbarras/Nextflow/WES_mouse_BRCA1/samplesheet.csv \
                                          --outdir /media/localadmin/DATA/dbarras/Nextflow/WES_mouse_BRCA1/


The dataset comes from an AVITI paired-end machine. (Twist exome mouse library)

I was able to run the test dataset using singularity.

Thanks so much in advance for your help! 

David

### Command used and terminal output

Command line: nextflow run nf-core/sarek \
                                           -profile singularity \
                                           --genome GRCm38 \
                                           --input /media/localadmin/DATA/dbarras/Nextflow/WES_mouse_BRCA1/samplesheet.csv \
                                          --outdir /media/localadmin/DATA/dbarras/Nextflow/WES_mouse_BRCA1/

### Relevant files

[samplesheet.csv](https://github.com/user-attachments/files/17633375/samplesheet.csv)
[nextflow.log](https://github.com/user-attachments/files/17633374/nextflow.log)

### System information

Hardware: Desktop
Executor: local
OS: Ubuntu 22.04.3 LTS
Nextflow version: 24.10.0.5928
Engine: singularity
nf-core version: 3.0.2",dbarras2,https://github.com/nf-core/sarek/issues/1725
I_kwDOCvwIC86dSBpZ,Error running Control-FREEC on WXS data,OPEN,2024-11-06T17:25:26Z,2024-11-09T13:03:05Z,,"### Description of the bug

Dear @FriederikeHanssen,

I am experiencing problems running Control-FREEC on my WXS data from matched normal/tumor samples.
I have used sarek on the same samples to perform mutations and structural variants calling and everything works fine.
This is the full command I am using: nextflow run nf-core/sarek -profile conda --input samplesheet.csv --outdir results --wes true --tools controlfreec --trim_fastq true --save_mapped true --save_output_as_bam true --genome GATK.GRCh38 --cf_chrom_len Homo_sapiens_assembly38.len
The error with Control-FREEC is the following: ERROR: there was a problem in the initial guess of the polynomial. Please contact the support team of change your input parameters. Exit. I am attaching a screenshot: ![Image](https://github.com/user-attachments/assets/1e33a391-4a9b-4693-8305-c8bf533eb41e)

It seems that the problem might be with the bed file used by sarek, as only a small number of SNPs are considered, similarly to what reported here: https://github.com/BoevaLab/FREEC/issues/123
I am also attaching a zip file with the Homo_sapiens_assembly38.len file and the file used in the analysis. [additional_files.zip](https://github.com/user-attachments/files/17650785/additional_files.zip)

Any advice on how to fix this issue?

Thank you,
Daniele

### Command used and terminal output

nextflow run nf-core/sarek -profile conda --input samplesheet.csv --outdir results --wes true --tools controlfreec --trim_fastq true --save_mapped true --save_output_as_bam true --genome GATK.GRCh38 --cf_chrom_len Homo_sapiens_assembly38.len

### Relevant files

_No response_

### System information

_No response_",danro9685,https://github.com/nf-core/sarek/issues/1726
I_kwDOCvwIC86ekwRB,"strange bug, two reverse conclusion are shown out at the same run",OPEN,2024-11-15T00:43:13Z,2024-11-16T11:09:48Z,,"### Description of the bug

Idk why, If I try to delete the two tools which report error, it will report for another problem
 -- Check script '/root/.nextflow/assets/nf-core/sarek/main.nf' at line: 342 or see '.nextflow.log' file for more details
I alter my samplesheet for many version. I have tried the sample sheet which only contain normal, tumour, and both. However I still face the same situation whatever I put in the sample sheet. I think I wont because the sample sheet.
[samplesheet.csv](https://github.com/user-attachments/files/17759884/samplesheet.csv)


### Command used and terminal output

```console
/mnt/c/Users/zbjia/Documents/GitHub/sarek# nextflow run nf-core/sarek    -profile docker    --input samplesheet.csv    --outdir /mnt/c/Users/zbjia/Documents/GitHub/sarek/work --wes --intervals /mnt/e/yiming/working/files/bed_file/V3_IDT_03861023_CML_Ph_v3_hg38.bed --tools ascat,cnvkit,controlfreec --fasta /mnt/e/yiming/working/fil
es/reference/GRCh38_masked_v2_decoy_gene.fasta --download_cache -r 3.4.4
 -- Check script '/root/.nextflow/assets/nf-core/sarek/main.nf' at line: 342 or see '.nextflow.log' file for more details
The sample-sheet only contains normal-samples, but the following tools, which were requested with ""--tools"", expect at least one tumor-sample : ascat, controlfreec
The sample-sheet only contains tumor-samples, but the following tools, which were requested by the option ""tools"", expect at least one normal-sample : ascat
```

### Relevant files

_No response_

### System information

_No response_",dazauzai,https://github.com/nf-core/sarek/issues/1732
I_kwDOCvwIC86fK4PU,No output and no run for variant calling,OPEN,2024-11-19T01:11:07Z,2024-11-19T01:11:07Z,,"### Description of the bug

I run the controlfreec
I done know why.[sheet6_with_gender.csv](https://github.com/user-attachments/files/17807887/sheet6_with_gender.csv)
[nf-params.json](https://github.com/user-attachments/files/17807891/nf-params.json)


### Command used and terminal output

```console
executor >  local (1)
executor >  local (1)
executor >  local (1)
executor >  local (1)
executor >  local (1)
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                              -
executor >  local (1)
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM1_INDEX                              -
[-        ] NFCORE_SAREK:PREPARE_GENOME:BWAMEM2_INDEX                              -
[-        ] NFCORE_SAREK:PREPARE_GENOME:DRAGMAP_HASHTABLE                          -
[-        ] NFCORE_SAREK:PREPARE_GENOME:GATK4_CREATESEQUENCEDICTIONARY             -
[-        ] NFCORE_SAREK:PREPARE_GENOME:MSISENSORPRO_SCAN                          -
[-        ] NFCORE_SAREK:PREPARE_GENOME:SAMTOOLS_FAIDX                             -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_BCFTOOLS_ANNOTATIONS                 -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_DBSNP                                -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_GERMLINE_RESOURCE                    -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_SNPS                           -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_KNOWN_INDELS                         -
[-        ] NFCORE_SAREK:PREPARE_GENOME:TABIX_PON                                  -
[-        ] NFCORE_SAREK:PREPARE_GENOME:UNZIP_ALLELES                              -
[-        ] NFCORE_SAREK:PREPARE_GENOME:UNZIP_LOCI                                 -
[-        ] NFCORE_SAREK:PREPARE_GENOME:UNZIP_GC                                   -
[-        ] NFCORE_SAREK:PREPARE_GENOME:UNZIP_RT                                   -
[31/c7686f] NFC…NTERVALS:CREATE_INTERVALS_BED (V3_IDT_03861023_CML_Ph_v3_hg38.bed) | 1 of 1, cached: 1 ✔
[71/044c11] NFC…INTERVALS:TABIX_BGZIPTABIX_INTERVAL_SPLIT (chr9_36859613-36859732) | 19 of 19, cached: 19 ✔
[7b/06569e] NFC…ABIX_BGZIPTABIX_INTERVAL_COMBINED (V3_IDT_03861023_CML_Ph_v3_hg38) | 1 of 1, cached: 1 ✔
[-        ] NFCORE_SAREK:DOWNLOAD_CACHE_SNPEFF_VEP:ENSEMBLVEP_DOWNLOAD             -
[-        ] NFCORE_SAREK:DOWNLOAD_CACHE_SNPEFF_VEP:SNPEFF_DOWNLOAD                 -
[-        ] NFCORE_SAREK:SAREK:BAM_TO_CRAM                                         -
[-        ] NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:SAMTOOLS_STATS          -
[-        ] NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:CRAM_QC_RECAL:MOSDEPTH                -
[-        ] NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:BCFTOOLS_MPILEUP     -
[-        ] NFCORE_SAREK:SAREK:CRAM_SAMPLEQC:BAM_NGSCHECKMATE:NGSCHECKMATE_NCM     -
[c4/407068] NFCORE_SAREK:SAREK:MULTIQC                                             | 1 of 1 ✔
Plus 26 more processes waiting for tasks…
-[nf-core/sarek] Pipeline completed successfully-
the running just looks like this, no error report, but no variant calling result or run exsist.
```

### Relevant files

_No response_

### System information

_No response_",dazauzai,https://github.com/nf-core/sarek/issues/1735
I_kwDOCvwIC86fMpU1,How to implement sarek for plant variant calling？,OPEN,2024-11-19T06:06:39Z,2024-11-20T16:44:20Z,,"### Description of feature

Can sarek be used for variant calling in plant genome? And if possible, what should be done?",meng-hhh,https://github.com/nf-core/sarek/issues/1736
I_kwDOCvwIC86fqPQR,"Error with Index Out of Bounds When Running Sarek WES Pipeline with Tools Enabled (e.g., Mutect2)",CLOSED,2024-11-21T09:09:52Z,2024-11-21T12:01:13Z,2024-11-21T12:01:11Z,"### Description of the bug

When running the Sarek pipeline on WES data without specifying any tools, the pipeline executes without issues. However, when adding one or more tools, such as --tools mutect2, the pipeline fails with an error:

```
ERROR ~ Index 1 out of bounds for length 1

 -- Check script '/home/kunger/.nextflow/assets/nf-core/sarek/./subworkflows/local/prepare_intervals/main.nf' at line: 80 or see '.nextflow.log' file for more details
ERROR ~ Pipeline failed. Please refer to troubleshooting docs: https://nf-co.re/docs/usage/troubleshooting

 -- Check '.nextflow.log' file for details
```

Environment Details:

	•	Sarek version: 3.4.4
	•	Nextflow version: 24.04.4
	•	Singularity profile
	•	Reference genome: GATK.GRCh38
	•	Intervals file: S07604514_Covered.bed
        •	java 17.0.12 2024-07-16 LTS


### Command used and terminal output

```console
runs smoothly:

NXF_VER=24.04.4 nextflow run nf-core/sarek -r 3.4.4 -profile singularity --input ./sample_sheet.csv --outdir ./results --genome GATK.GRCh38 --wes TRUE --intervals /train_data/kunger/Agilent_intervals/S07604514_Covered.bed

results in error:

NXF_VER=24.04.4 nextflow run nf-core/sarek -r 3.4.4 -profile singularity --input ./sample_sheet.csv --outdir ./results --genome GATK.GRCh38 --wes TRUE --intervals /train_data/kunger/Agilent_intervals/S07604514_Covered.bed --tools mutect2
```

### Relevant files

[nextflow.log](https://github.com/user-attachments/files/17842909/nextflow.log)


### System information

_No response_",kristianunger,https://github.com/nf-core/sarek/issues/1739
I_kwDOCvwIC86gWGUr,The sample-sheet only contains normal-samples error,OPEN,2024-11-25T10:24:14Z,2024-12-03T04:18:07Z,,"### Description of the bug

I have been processing tumor normal samples using sarek and I have results for 6/14 samples
The pipeline stopped and I tried re-running it but no luck. What I am trying now is to run just one sample and I am getting this error:

The sample-sheet only contains normal-samples, but the following tools, which were requested with ""--tools"", expect at least one tumor-sample : mutect2
Missing process or function Channel.empty([[]])

Please note that this sample was already processed using sarek and I have annotated vcf (in another directory), I was doing this exercise as a part of debugging.

There was a previous issue reported: https://github.com/nf-core/sarek/issues/1260 (I tried the solutions there but it isnt working for me)

### Command used and terminal output

```console
My NF code:

nextflow run nf-core/sarek \
   -profile singularity \
   --use_annotation_cache_keys \
   --input path/to/my/pwd/samplesheet.csv \
   --outdir path/to/my/pwd/folder/YS-8-14/ \
   --tools mutect2,vep

This is my samplesheet:

patient,sex,status,sample,lane,fastq_1,fastq_2
patient01,XY,0,N01,lane_1,/home/project/11003581/Data/YS-analysis/raw_data/N01_1.fq.gz,/home/project/11003581/Data/YS-analysis/raw_data/N01_2.fq.gz
patient01,XY,1,T01,lane_1,/home/project/11003581/Data/YS-analysis/raw_data/T01_1.fq.gz,/home/project/11003581/Data/YS-analysis/raw_data/T01_2.fq.gz
```

### Relevant files

This is my .nextflow.log
[nextflow.log](https://github.com/user-attachments/files/17901259/nextflow.log)


### System information

Nextflow version: 24.10.1 build 5930
I am running it in HPC- Linux
Version of nf-core/sarek: 3.4.4
container: singularity",ashpsnair,https://github.com/nf-core/sarek/issues/1740
I_kwDOCvwIC86hMJ-u,Check that GZI index provided if using a gzipped FASTA,OPEN,2024-11-29T08:24:27Z,2024-11-29T10:46:55Z,,"### Description of feature

I started a sarek run (`v3.4.2`) providing a custom reference in the form of a bgzipped FASTA. The run from FASTQs started normally and did not run into any errors until the MarkDuplicates step. I had missed copying an index file (*.fasta.gz.gzi) to the same folder as the FASTA which caused the step to fail just before finishing 🤦, see the error message below.

```
  [Thu Nov 28 20:40:47 GMT 2024] picard.sam.markduplicates.MarkDuplicates done. Elapsed time: 134.77 minutes.
  Runtime.totalMemory()=285212672
  [E::bgzf_index_load] Error opening GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz.gzi : No such file or directory
  [E::bgzf_open_ref] Unable to load .gzi index 'GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz.gzi'
  [E::refs_load_fai] Failed to open reference file 'GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz'
  [E::hts_open_format] Failed to open file ""OPM2.md.cram"" : Invalid argument
  samtools view: failed to open ""OPM2.md.cram"" for writing: Invalid argument
```

This is the relevant part of my parameter file
```
fasta: /proj/ngi2016004/private/strategic_proj/SR_23_02_Element_vs_Illumina/resources/GRCh38_GIABv3/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz
fasta_fai: /proj/ngi2016004/private/strategic_proj/SR_23_02_Element_vs_Illumina/resources/GRCh38_GIABv3/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz.fai
igenomes_ignore: true
bwa: /proj/ngi2016004/private/strategic_proj/SR_23_02_Element_vs_Illumina/resources/GRCh38_GIABv3/BWAIndex
```


It would be great if there could be a parameter check on start so that when a bgzipped fasta `*.fasta.gz` is provided a corresponding index `*.fasta.gz.gzi` should be present. 

Some other considerations if this is hard to implement:
- This `.gzi` file could be generated automatically if it is missing as part of the `build` process. 
- The MarkDuplicates step should have a check that if a bgzipped FASTA is provided an index file is also present before starting the step, making the run fail earlier.

Edit: forgot to add info about sarek version
Edit2: gzip --> bgzip ",pontushojer,https://github.com/nf-core/sarek/issues/1741
I_kwDOCvwIC86h0Xcw,new variant caller: MuSE,OPEN,2024-12-03T12:11:01Z,2024-12-03T12:11:35Z,,"### Description of feature

[MuSE](https://github.com/wwylab/MuSE) is a somatic point mutation caller which works with whole-genome sequencing (WGS) and whole-exome sequencing (WES) data.

> This tool is unique in accounting for tumor heterogeneity using a sample-specific error model that improves sensitivity and specificity in mutation calling from sequencing data.

MuSE was used in the TCGA PanCanAtlas project2 and the ICGC Pan-Cancer Analysis of Whole Genomes (PCAWG) initiative. I would like to use it with sarek to do some follow-up analysis comparison of TCGA processed data and raw data.

The modules are already created and ready in nf-core/modules. I would try and do the integration myself.",famosab,https://github.com/nf-core/sarek/issues/1742
I_kwDOCvwIC86jmsEW,Need help! Sifting through mutations in other cell lines (parameters and tools),OPEN,2024-12-17T12:37:36Z,2024-12-17T12:37:36Z,,"Hi!

I am looking for mutations (snp, indels ...) before and after cultivation in cancerous cell lines. 

However I am not sure how can i set up parameters to omit mutations in hg.38, and additionally mutations in healthy PBMC.

I got three short reads data sets (before after and PBMC all in fasta) and i would like to obtain:

mutations private to cell lines before cultivation **-** hg.38 mutations **-** PBMC

mutations private to cell lines after cultivation **-** hg.38 mutations **-** PBMC

and common mutations for both cell lines after and before cultivation **-** hg.38 mutations **-** PBMC

Previously I was doing it utilizing bcftools isec (working on vcf files)

Is there a way to do this in one command in one sample sheet? What programs shall I use?

Sincerely,
Michał",mggrami,https://github.com/nf-core/sarek/issues/1762
