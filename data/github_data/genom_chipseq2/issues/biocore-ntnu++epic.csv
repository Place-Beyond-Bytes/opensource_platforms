id,title,state,created_at,updated_at,closed_at,body,user,url
MDU6SXNzdWUxNDgyOTc1MTU=,Epic-effective: Automatically remove bad chromos.,CLOSED,2016-04-14T09:06:12Z,2016-05-12T13:03:14Z,2016-05-10T16:36:55Z,"Will do it sometime üëç
",endrebak,https://github.com/biocore-ntnu/epic/issues/1
MDU6SXNzdWUxNTEwNDg5Mjc=,Make it possible to enter fa/fai file on command line instead of genome,CLOSED,2016-04-26T06:07:27Z,2016-05-12T13:03:09Z,2016-05-10T18:33:00Z,"Hello,
instead of hard coding chromosome name and lengths. Could you for example use FASTA index created by samtools:

```
def addGenomeData(input_filename):
    genomeData = {}

    with open(input_filename) as i:
        for line in i:
            try:
                parts = line.rstrip().split('\t')
            except IndexError:
                continue
            genomeData[parts[0]] = parts[1]

    print genomeData
    print genomeData.keys()

if __name__ == ""__main__"":

    addGenomeData(""/data//Bactrocera_tryoni/Bac.fasta.fai"")
```

or to use http://nullege.com/codes/search/pysam.faidx so the user only has to provide FASTA file and pysam will create the FASTA index.

If Epic could be used without hard coding genome sizes than it will be possible to use it in galaxy (use galaxy.org).

Thank you in advance.

Mic
",mictadlo,https://github.com/biocore-ntnu/epic/issues/2
MDU6SXNzdWUxNTM4OTcwNDk=,Paired-end read support,CLOSED,2016-05-10T00:06:47Z,2016-06-15T11:48:28Z,2016-06-15T11:48:14Z,"Hi,
HiChip paper ( http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-280 ) describes how they added paired-end support to Sicer:

_For paired-end reads, the HiChIP pipeline keeps the first end and extends by the fragment length estimated from mapping positions of the two ends, rather than by the average fragment length of the library. Given the variability of fragment lengths across a complex genome like human genome, the use of actual coordinates of mapped pairs is expected to achieve better resolution in signal visualization. The bed file is then used to generate a bedGraph file by the genomeCoverageBed command from BEDTools._

Best wishes,

Michal
",mictadlo,https://github.com/biocore-ntnu/epic/issues/4
MDU6SXNzdWUxNTM5MzE0NDM=,Create bigwig files for UCSC genome browser display?,CLOSED,2016-05-10T06:17:09Z,2017-01-16T11:46:41Z,2017-01-16T11:46:41Z,"I'd like to create files for displaying in the genome browser. 

Need to find out:

1) Should I create one track with pooled data from all the ChIP files or one bigwig file per ChIP file?

I have never used the GB for displaying data so pinging you @daler. Do you have an opinion/other related suggestions? 
",endrebak,https://github.com/biocore-ntnu/epic/issues/5
MDU6SXNzdWUxNTQwNzg4ODQ=,bedtools needs genome,CLOSED,2016-05-10T18:48:52Z,2016-07-29T18:04:19Z,2016-05-12T12:37:17Z,"`bedtools bamtobed` needs a genome file on recent (within the last few years?) versions of bedtools. Using BAM files as input is not possible with the bedtools versions available in bioconda due to this [call to bedtools](https://github.com/endrebak/epic/blob/master/epic/windows/count/count_reads_in_windows.py#L62).
",daler,https://github.com/biocore-ntnu/epic/issues/6
MDU6SXNzdWUxNTQ0NzQ0ODI=,Add integration test that tests with bam file.,CLOSED,2016-05-12T12:50:36Z,2016-05-20T12:17:04Z,2016-05-20T12:17:00Z,"Will create sam files and add to repo example files. In the integration test I'll call bedtools to convert into bam, then run the test. This is to avoid adding binary files to the repo.
",endrebak,https://github.com/biocore-ntnu/epic/issues/8
MDU6SXNzdWUxNTQ0NzU5MTI=,Allow samtools filter flags + quality threshold on the command line?,CLOSED,2016-05-12T12:57:55Z,2016-06-15T11:49:41Z,2016-06-15T11:49:38Z,"Would allowing users to enter samtools flags or a fasta min quality threshold on the command line be helpful?

Opinions welcome!
",endrebak,https://github.com/biocore-ntnu/epic/issues/9
MDU6SXNzdWUxNTU3MjY4Mzc=,Fix p-value/FDR output,CLOSED,2016-05-19T12:39:39Z,2016-05-20T12:41:22Z,2016-05-20T12:41:22Z,,endrebak,https://github.com/biocore-ntnu/epic/issues/10
MDU6SXNzdWUxNTU5NTQ3Njc=,Automatically find appropriate effective genome size,CLOSED,2016-05-20T12:43:55Z,2016-05-24T13:45:28Z,2016-05-24T13:45:24Z,"Using the files kindly added by daler.
",endrebak,https://github.com/biocore-ntnu/epic/issues/12
MDU6SXNzdWUxNTU5NjIwMDg=,Remove debug info from log,OPEN,2016-05-20T13:22:33Z,2016-05-20T13:22:59Z,,"Make logmsgs much cleaner
",endrebak,https://github.com/biocore-ntnu/epic/issues/13
MDU6SXNzdWUxNTY3NTQ2MTc=,Question about paired-end reads,CLOSED,2016-05-25T13:45:16Z,2016-06-20T08:32:54Z,2016-06-15T11:49:06Z,"Hi,

I would like to know if EPIC is able to manage paired-end ChIP-seq reads.
Thanks a lot for your answer,

J. Fourquet
",JFourquet,https://github.com/biocore-ntnu/epic/issues/15
MDU6SXNzdWUxNTg4NzQ4OTg=,Test that chip/input gives same result as chip/2*input,CLOSED,2016-06-07T09:11:44Z,2016-06-07T09:41:40Z,2016-06-07T09:41:28Z,,endrebak,https://github.com/biocore-ntnu/epic/issues/16
MDU6SXNzdWUxNTg4NzQ5NTA=,Add file logger option,CLOSED,2016-06-07T09:12:01Z,2017-12-12T11:45:02Z,2017-12-12T11:45:02Z,,endrebak,https://github.com/biocore-ntnu/epic/issues/17
MDU6SXNzdWUxNTkwODc2ODA=,Add instructions on how to filter files in README.md,OPEN,2016-06-08T06:20:08Z,2016-06-08T06:20:08Z,,,endrebak,https://github.com/biocore-ntnu/epic/issues/18
MDU6SXNzdWUxNTkzOTM4Nzc=,Write matrix of counts to file,CLOSED,2016-06-09T12:19:14Z,2016-06-10T08:41:07Z,2016-06-10T08:41:07Z,,endrebak,https://github.com/biocore-ntnu/epic/issues/19
MDU6SXNzdWUxNjA2NDU5NzQ=,Galaxy (usegalaxy.org) wrapper for epic,CLOSED,2016-06-16T12:08:29Z,2017-01-16T11:46:03Z,2017-01-16T11:46:03Z,"Hi,
Any plans to develop a Galaxy (usegalaxy.org) wrapper for epic?

Mic
",mictadlo,https://github.com/biocore-ntnu/epic/issues/20
MDU6SXNzdWUxNjEyNDY1MjU=,printmatrix not working for bepde,CLOSED,2016-06-20T17:12:11Z,2016-08-15T10:03:20Z,2016-08-15T10:03:16Z,"Matrices are already merged to just contain ""Count"", not cols per infile.
",endrebak,https://github.com/biocore-ntnu/epic/issues/21
MDU6SXNzdWUxNjQxMjE0OTI=,Add instructions how to add a new genome,CLOSED,2016-07-06T16:42:07Z,2016-08-09T06:23:22Z,2016-08-09T06:23:22Z,"I tried running epic on zebrafish data. For this I created a file:
/usr/local/lib/python2.7/dist-packages/epic/scripts/chromsizes/danRer7.chromsizes
containing chormosome sizes and run epic on paired-end data:

```
epic --treatment myReads.bedpe.bz2 \
    --control myInput.bedpe.bz2 \
    --number-cores 8 \
    --genome danRer7 \
    --effective_genome_length 0.9 \
    --paired-end
```

But I ran into problems, which I don't understand. 

```

Merging ChIP and Input data. (File: helper_functions, Log level: INFO, Time: Tue, 05 Jul 2016 17:32:56 )
0.9effective_genome_length (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 05 Jul 2016 17:33:01 )
200 window size (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 05 Jul 2016 17:33:01 )
0 total chip count (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 05 Jul 2016 17:33:01 )
0.0average_window_readcount (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 05 Jul 2016 17:33:01 )
1island_enriched_threshold (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 05 Jul 2016 17:33:01 )
4.0gap_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 05 Jul 2016 17:33:01 )
1.0boundary_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 05 Jul 2016 17:33:01 )
Traceback (most recent call last):
  File ""/usr/local/bin/epic"", line 164, in <module>
    run_epic(args)
  File ""/usr/local/lib/python2.7/dist-packages/epic/run/run_epic.py"", line 45, in run_epic
    compute_background_probabilities(nb_chip_reads, args)
  File ""/usr/local/lib/python2.7/dist-packages/epic/statistics/compute_background_probabilites.py"", line 48, in compute_background_probabilities
    boundary_contribution, genome_length_in_bins)
  File ""/usr/local/lib/python2.7/dist-packages/epic/statistics/compute_score_threshold.py"", line 24, in compute_score_threshold
    current_scaled_score = int(round(score / BIN_SIZE))
OverflowError: cannot convert float infinity to integer

```

Could you please let me know how to proceed.

Piotr
",balwierz,https://github.com/biocore-ntnu/epic/issues/22
MDU6SXNzdWUxNjQxMjk3OTY=,Add cli option to list the genomes available?,OPEN,2016-07-06T17:24:39Z,2016-07-06T17:24:39Z,,,endrebak,https://github.com/biocore-ntnu/epic/issues/23
MDU6SXNzdWUxNjUzOTA5MjQ=,Using dm3 makes epic crash,CLOSED,2016-07-13T18:35:08Z,2017-01-16T11:47:14Z,2017-01-16T11:47:14Z,"Hi , I am very new to epic (I have installed it today) and when I am try to run it with multiple cores I get the following error (Running it at a single core appears unaffected until now - still running) . Please advice
## Pantelis Topalis

epic -t ../sorted_H3_APAA.bed -c ../sorted_H3_BiB.bed --number-cores 16 -gn dm3 -w 200 -g 3 -fs 150 -fdr 0.05 -egs 0.72 -sm APAA_BiB_matrix
# epic -t ../sorted_H3_APAA.bed -c ../sorted_H3_BiB.bed --number-cores 16 -gn dm3 -w 200 -g 3 -fs 150 -fdr 0.05 -egs 0.72 -sm APAA_BiB_matrix
# epic -t ../sorted_H3_APAA.bed -c ../sorted_H3_BiB.bed --number-cores 16 -gn dm3 -w 200 -g 3 -fs 150 -fdr 0.05 -egs 0.72 -sm APAA_BiB_matrix (File: epic, Log level: INFO, Time: Wed, 13 Jul 2016 21:22:54 )

Binning ../sorted_H3_APAA.bed (File: run_epic, Log level: INFO, Time: Wed, 13 Jul 2016 21:22:54 )
Binning chromosomes 2L, 2LHet, 2R, 2RHet, 3L, 3LHet, 3R, 3RHet, 4, M, U, Uextra, X, XHet, YHet (File: count_reads_in_windows, Log level: INFO, Time: Wed, 13 Jul 2016 21:22:54 )
Merging the bins on both strands per chromosome. (File: count_reads_in_windows, Log level: INFO, Time: Wed, 13 Jul 2016 21:23:05 )
Binning ../sorted_H3_BiB.bed (File: run_epic, Log level: INFO, Time: Wed, 13 Jul 2016 21:23:07 )
Binning chromosomes 2L, 2LHet, 2R, 2RHet, 3L, 3LHet, 3R, 3RHet, 4, M, U, Uextra, X, XHet, YHet (File: count_reads_in_windows, Log level: INFO, Time: Wed, 13 Jul 2016 21:23:07 )
Merging the bins on both strands per chromosome. (File: count_reads_in_windows, Log level: INFO, Time: Wed, 13 Jul 2016 21:23:19 )
Merging ChIP and Input data. (File: helper_functions, Log level: INFO, Time: Wed, 13 Jul 2016 21:23:22 )
Traceback (most recent call last):
  File ""/usr/local/bin/epic"", line 165, in <module>
    run_epic(args)
  File ""/usr/local/lib/python2.7/dist-packages/epic/run/run_epic.py"", line 42, in run_epic
    args.number_cores)
  File ""/usr/local/lib/python2.7/dist-packages/epic/utils/helper_functions.py"", line 37, in merge_chip_and_input
    for chip_df, input_df in zip(chip_dfs, input_dfs))
  File ""/usr/local/lib/python2.7/dist-packages/joblib/parallel.py"", line 764, in **call**
    self.retrieve()
  File ""/usr/local/lib/python2.7/dist-packages/joblib/parallel.py"", line 715, in retrieve
    raise exception
joblib.my_exceptions.JoblibValueError: JoblibValueError

---

Multiprocessing exception:
...........................................................................
/usr/local/bin/epic in <module>()
    160     elif not args.effective_genome_length and args.paired_end:
    161         logging.info(""Using paired end so setting readlength to 100."")
    162         args.effective_genome_length = get_effective_genome_length(args.genome,
    163                                                                    100)
    164
--> 165     run_epic(args)
    166
    167
    168
    169

...........................................................................
/usr/local/lib/python2.7/dist-packages/epic/run/run_epic.py in run_epic(args=Namespace(control=['../sorted_H3_BiB.bed'], effe...tment=['../sorted_H3_APAA.bed'], window_size=200))
     37
     38     nb_chip_reads = get_total_number_of_reads(chip_merged_sum)
     39     nb_input_reads = get_total_number_of_reads(input_merged_sum)
     40
     41     merged_dfs = merge_chip_and_input(chip_merged_sum, input_merged_sum,
---> 42                                       args.number_cores)
        args.number_cores = 16
     43
     44     score_threshold, island_enriched_threshold, average_window_readcount = \
     45         compute_background_probabilities(nb_chip_reads, args)
     46

...........................................................................
/usr/local/lib/python2.7/dist-packages/epic/utils/helper_functions.py in merge_chip_and_input(chip_dfs=[       Chromosome       Bin  Count
0           c...chr2L  22986200      3

[107160 rows x 3 columns],     Chromosome     Bin  Count
0     chr2LHet   1...  chr2LHet  367800      2

[539 rows x 3 columns],       Chromosome       Bin  Count
0          chr... chr2R  21145600      3

[98361 rows x 3 columns],      Chromosome      Bin  Count
0      chr2RHet ...chr2RHet  3277200      2

[5745 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3L  24537000      1

[113832 rows x 3 columns],      Chromosome      Bin  Count
0      chr3LHet ...chr3LHet  2547800      5

[5234 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134035 rows x 3 columns],      Chromosome      Bin  Count
0      chr3RHet ...chr3RHet  2517400      8

[4774 rows x 3 columns],      Chromosome      Bin  Count
0          chr4 ...    chr4  1285200      1

[5699 rows x 3 columns],   Chromosome    Bin  Count
0       chrM   1800  ...    chrM   8800      2
3       chrM  12000      1,      Chromosome       Bin  Count
0          chrU...   chrU  10047200      1

[8182 rows x 3 columns],      Chromosome       Bin  Count
0     chrUextra...rUextra  29003800      1

[4023 rows x 3 columns],        Chromosome       Bin  Count
0            ... chrX  22422000     17

[102257 rows x 3 columns],     Chromosome     Bin  Count
0      chrXHet    ...   chrXHet  190000      3

[522 rows x 3 columns],     Chromosome     Bin  Count
0      chrYHet    ...   chrYHet  341400      2

[471 rows x 3 columns]], input_dfs=[       Chromosome       Bin  Count
0           c...chr2L  22997600      1

[107540 rows x 3 columns],     Chromosome     Bin  Count
0     chr2LHet   1...  chr2LHet  367800      2

[577 rows x 3 columns],       Chromosome       Bin  Count
0          chr... chr2R  21146200      1

[98668 rows x 3 columns],      Chromosome      Bin  Count
0      chr2RHet ...chr2RHet  3277200      2

[5981 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3L  24535600     18

[114448 rows x 3 columns],      Chromosome      Bin  Count
0      chr3LHet ...chr3LHet  2547800      4

[5631 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134479 rows x 3 columns],      Chromosome      Bin  Count
0      chr3RHet ...chr3RHet  2517400      6

[5181 rows x 3 columns],      Chromosome      Bin  Count
0          chr4 ...    chr4  1318000      1

[5701 rows x 3 columns],   Chromosome    Bin  Count
0       chrM    600  ...    chrM  12000      2
4       chrM  12200      1,       Chromosome       Bin  Count
0           ch...  chrU  10043000      2

[10082 rows x 3 columns],      Chromosome       Bin  Count
0     chrUextra...rUextra  29000400      1

[5706 rows x 3 columns],        Chromosome       Bin  Count
0            ... chrX  22422200      3

[103662 rows x 3 columns],     Chromosome     Bin  Count
0      chrXHet    ...   chrXHet  197200      1

[544 rows x 3 columns],     Chromosome     Bin  Count
0      chrYHet    ...   chrYHet  341400      2

[574 rows x 3 columns]], nb_cpu=16)
     32     assert len(chip_dfs) == len(input_dfs)
     33
     34     logging.info(""Merging ChIP and Input data."")
     35     merged_chromosome_dfs = Parallel(n_jobs=nb_cpu)(
     36         delayed(_merge_chip_and_input)(chip_df, input_df)
---> 37         for chip_df, input_df in zip(chip_dfs, input_dfs))
        chip_dfs = [       Chromosome       Bin  Count
0           c...chr2L  22986200      3

[107160 rows x 3 columns],     Chromosome     Bin  Count
0     chr2LHet   1...  chr2LHet  367800      2

[539 rows x 3 columns],       Chromosome       Bin  Count
0          chr... chr2R  21145600      3

[98361 rows x 3 columns],      Chromosome      Bin  Count
0      chr2RHet ...chr2RHet  3277200      2

[5745 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3L  24537000      1

[113832 rows x 3 columns],      Chromosome      Bin  Count
0      chr3LHet ...chr3LHet  2547800      5

[5234 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134035 rows x 3 columns],      Chromosome      Bin  Count
0      chr3RHet ...chr3RHet  2517400      8

[4774 rows x 3 columns],      Chromosome      Bin  Count
0          chr4 ...    chr4  1285200      1

[5699 rows x 3 columns],   Chromosome    Bin  Count
0       chrM   1800  ...    chrM   8800      2
3       chrM  12000      1,      Chromosome       Bin  Count
0          chrU...   chrU  10047200      1

[8182 rows x 3 columns],      Chromosome       Bin  Count
0     chrUextra...rUextra  29003800      1

[4023 rows x 3 columns],        Chromosome       Bin  Count
0            ... chrX  22422000     17

[102257 rows x 3 columns],     Chromosome     Bin  Count
0      chrXHet    ...   chrXHet  190000      3

[522 rows x 3 columns],     Chromosome     Bin  Count
0      chrYHet    ...   chrYHet  341400      2

[471 rows x 3 columns]]
        input_dfs = [       Chromosome       Bin  Count
0           c...chr2L  22997600      1

[107540 rows x 3 columns],     Chromosome     Bin  Count
0     chr2LHet   1...  chr2LHet  367800      2

[577 rows x 3 columns],       Chromosome       Bin  Count
0          chr... chr2R  21146200      1

[98668 rows x 3 columns],      Chromosome      Bin  Count
0      chr2RHet ...chr2RHet  3277200      2

[5981 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3L  24535600     18

[114448 rows x 3 columns],      Chromosome      Bin  Count
0      chr3LHet ...chr3LHet  2547800      4

[5631 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134479 rows x 3 columns],      Chromosome      Bin  Count
0      chr3RHet ...chr3RHet  2517400      6

[5181 rows x 3 columns],      Chromosome      Bin  Count
0          chr4 ...    chr4  1318000      1

[5701 rows x 3 columns],   Chromosome    Bin  Count
0       chrM    600  ...    chrM  12000      2
4       chrM  12200      1,       Chromosome       Bin  Count
0           ch...  chrU  10043000      2

[10082 rows x 3 columns],      Chromosome       Bin  Count
0     chrUextra...rUextra  29000400      1

[5706 rows x 3 columns],        Chromosome       Bin  Count
0            ... chrX  22422200      3

[103662 rows x 3 columns],     Chromosome     Bin  Count
0      chrXHet    ...   chrXHet  197200      1

[544 rows x 3 columns],     Chromosome     Bin  Count
0      chrYHet    ...   chrYHet  341400      2

[574 rows x 3 columns]]
     38     return merged_chromosome_dfs
     39
     40
     41 def get_total_number_of_reads(dfs):

...........................................................................
/usr/local/lib/python2.7/dist-packages/joblib/parallel.py in **call**(self=Parallel(n_jobs=16), iterable=<generator object <genexpr>>)
    759             if pre_dispatch == ""all"" or n_jobs == 1:
    760                 # The iterable was consumed all at once by the above for loop.
    761                 # No need to wait for async callbacks to trigger to
    762                 # consumption.
    763                 self._iterating = False
--> 764             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=16)>
    765             # Make sure that we get a last message telling us we are done
    766             elapsed_time = time.time() - self._start_time
    767             self._print('Done %3i out of %3i | elapsed: %s finished',
    768                         (len(self._output), len(self._output),

---
## Sub-process traceback:

ValueError                                         Wed Jul 13 21:23:22 2016
PID: 33319                                  Python 2.7.11+: /usr/bin/python
...........................................................................
/usr/local/lib/python2.7/dist-packages/joblib/parallel.py in **call**(self=<joblib.parallel.BatchedCalls object>)
    122     def **init**(self, iterator_slice):
    123         self.items = list(iterator_slice)
    124         self._size = len(self.items)
    125
    126     def __call__(self):
--> 127         return [func(_args, *_kwargs) for func, args, kwargs in self.items]
        func = <function _merge_chip_and_input>
        args = (       Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134035 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134479 rows x 3 columns])
        kwargs = {}
        self.items = [(<function _merge_chip_and_input>, (       Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134035 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134479 rows x 3 columns]), {})]
    128
    129     def **len**(self):
    130         return self._size
    131

...........................................................................
/usr/local/lib/python2.7/dist-packages/epic/utils/helper_functions.py in _merge_chip_and_input(chip_df=       Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134035 rows x 3 columns], input_df=       Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134479 rows x 3 columns])
     13
     14     chip_df_nb_bins = len(chip_df)
     15     merged_df = chip_df.merge(input_df,
     16                               how=""left"",
     17                               on=[""Chromosome"", ""Bin""],
---> 18                               suffixes=["" ChIP"", "" Input""])
     19     merged_df = merged_df[[""Chromosome"", ""Bin"", ""Count ChIP"", ""Count Input""]]
     20     merged_df.columns = [""Chromosome"", ""Bin"", ""ChIP"", ""Input""]
     21
     22     merged_df = merged_df.fillna(0)

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas-0.18.1-py2.7-linux-x86_64.egg/pandas/core/frame.py in merge(self=       Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134035 rows x 3 columns], right=       Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134479 rows x 3 columns], how='left', on=['Chromosome', 'Bin'], left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=[' ChIP', ' Input'], copy=True, indicator=False)
   4432               suffixes=('_x', '_y'), copy=True, indicator=False):
   4433         from pandas.tools.merge import merge
   4434         return merge(self, right, how=how, on=on, left_on=left_on,
   4435                      right_on=right_on, left_index=left_index,
   4436                      right_index=right_index, sort=sort, suffixes=suffixes,
-> 4437                      copy=copy, indicator=indicator)
        copy = True
        indicator = False
   4438
   4439     def round(self, decimals=0, _args, *_kwargs):
   4440         """"""
   4441         Round a DataFrame to a variable number of decimal places.

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas-0.18.1-py2.7-linux-x86_64.egg/pandas/tools/merge.py in merge(left=       Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134035 rows x 3 columns], right=       Chromosome       Bin  Count
0           c...chr3R  27898600      4

[134479 rows x 3 columns], how='left', on=['Chromosome', 'Bin'], left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=[' ChIP', ' Input'], copy=True, indicator=False)
     34           suffixes=('_x', '_y'), copy=True, indicator=False):
     35     op = _MergeOperation(left, right, how=how, on=on, left_on=left_on,
     36                          right_on=right_on, left_index=left_index,
     37                          right_index=right_index, sort=sort, suffixes=suffixes,
     38                          copy=copy, indicator=indicator)
---> 39     return op.get_result()
        op.get_result = <bound method _MergeOperation.get_result of <pandas.tools.merge._MergeOperation object>>
     40 if __debug__:
     41     merge.**doc** = _merge_doc % '\nleft : DataFrame'
     42
     43

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas-0.18.1-py2.7-linux-x86_64.egg/pandas/tools/merge.py in get_result(self=<pandas.tools.merge._MergeOperation object>)
    212     def get_result(self):
    213         if self.indicator:
    214             self.left, self.right = self._indicator_pre_merge(
    215                 self.left, self.right)
    216
--> 217         join_index, left_indexer, right_indexer = self._get_join_info()
        join_index = undefined
        left_indexer = undefined
        right_indexer = undefined
        self._get_join_info = <bound method _MergeOperation._get_join_info of <pandas.tools.merge._MergeOperation object>>
    218
    219         ldata, rdata = self.left._data, self.right._data
    220         lsuf, rsuf = self.suffixes
    221

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas-0.18.1-py2.7-linux-x86_64.egg/pandas/tools/merge.py in _get_join_info(self=<pandas.tools.merge._MergeOperation object>)
    348                                     sort=self.sort)
    349         else:
    350             (left_indexer,
    351              right_indexer) = _get_join_indexers(self.left_join_keys,
    352                                                  self.right_join_keys,
--> 353                                                  sort=self.sort, how=self.how)
        self.sort = False
        self.how = 'left'
    354             if self.right_index:
    355                 if len(self.left) > 0:
    356                     join_index = self.left.index.take(left_indexer)
    357                 else:

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas-0.18.1-py2.7-linux-x86_64.egg/pandas/tools/merge.py in _get_join_indexers(left_keys=[array(['chr3R', 'chr3R', 'chr3R', ..., 'chr3R', 'chr3R', 'chr3R'], dtype=object), memmap([       0,      200,      400, ..., 27897200, 27898400, 27898600])], right_keys=[array(['chr3R', 'chr3R', 'chr3R', ..., 'chr3R', 'chr3R', 'chr3R'], dtype=object), memmap([       0,      200,      400, ..., 27897200, 27898400, 27898600])], sort=False, how='left')
    541
    542     # bind `sort` arg. of _factorize_keys
    543     fkeys = partial(_factorize_keys, sort=sort)
    544
    545     # get left & right join labels and num. of levels at each location
--> 546     llab, rlab, shape = map(list, zip(\* map(fkeys, left_keys, right_keys)))
        llab = undefined
        rlab = undefined
        shape = undefined
        fkeys = <functools.partial object>
        left_keys = [array(['chr3R', 'chr3R', 'chr3R', ..., 'chr3R', 'chr3R', 'chr3R'], dtype=object), memmap([       0,      200,      400, ..., 27897200, 27898400, 27898600])]
        right_keys = [array(['chr3R', 'chr3R', 'chr3R', ..., 'chr3R', 'chr3R', 'chr3R'], dtype=object), memmap([       0,      200,      400, ..., 27897200, 27898400, 27898600])]
    547
    548     # get flat i8 keys from label lists
    549     lkey, rkey = _get_join_keys(llab, rlab, shape, sort)
    550

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas-0.18.1-py2.7-linux-x86_64.egg/pandas/tools/merge.py in _factorize_keys(lk=memmap([       0,      200,      400, ..., 27897200, 27898400, 27898600]), rk=memmap([       0,      200,      400, ..., 27897200, 27898400, 27898600]), sort=False)
    708         lk = com._ensure_object(lk)
    709         rk = com._ensure_object(rk)
    710
    711     rizer = klass(max(len(lk), len(rk)))
    712
--> 713     llab = rizer.factorize(lk)
        llab = undefined
        rizer.factorize = <built-in method factorize of pandas.hashtable.Int64Factorizer object>
        lk = memmap([       0,      200,      400, ..., 27897200, 27898400, 27898600])
    714     rlab = rizer.factorize(rk)
    715
    716     count = rizer.get_count()
    717

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas-0.18.1-py2.7-linux-x86_64.egg/pandas/hashtable.so in pandas.hashtable.Int64Factorizer.factorize (pandas/hashtable.c:15827)()
    854
    855
    856
    857
    858
--> 859
    860
    861
    862
    863

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas-0.18.1-py2.7-linux-x86_64.egg/pandas/hashtable.so in View.MemoryView.memoryview_cwrapper (pandas/hashtable.c:29882)()
    611
    612
    613
    614
    615
--> 616
    617
    618
    619
    620

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas-0.18.1-py2.7-linux-x86_64.egg/pandas/hashtable.so in View.MemoryView.memoryview.**cinit** (pandas/hashtable.c:26251)()
    318
    319
    320
    321
    322
--> 323
    324
    325
    326
    327

ValueError: buffer source array is read-only
",topalis,https://github.com/biocore-ntnu/epic/issues/24
MDU6SXNzdWUxNjU2MjAwMzQ=,Make script to print system and library versions,CLOSED,2016-07-14T17:52:22Z,2017-01-16T11:47:22Z,2017-01-16T11:47:22Z,"Will be useful for debugging.
",endrebak,https://github.com/biocore-ntnu/epic/issues/25
MDU6SXNzdWUxNjYxMTcxMjY=,OverflowError: cannot convert float infinity to integer,CLOSED,2016-07-18T15:10:41Z,2017-12-12T11:45:30Z,2017-12-12T11:45:30Z,"Hi,

I'm using SICER in one of my projects, and I wanted to give epic a try. I'm using epic 0.0.6 (conda install). I downloaded the test.bed and control.bed files.

`epic --treatment test.bed --control control.bed`

And I'm getting the following error message:

> Merging ChIP and Input data. (File: helper_functions, Log level: INFO, Time: Mon, 18 Jul 2016 17:07:42 )
> 2290813547.42effective_genome_length (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 18 Jul 2016 17:07:42 )
> 200 window size (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 18 Jul 2016 17:07:42 )
> 0 total chip count (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 18 Jul 2016 17:07:42 )
> 0.0average_window_readcount (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 18 Jul 2016 17:07:42 )
> 1island_enriched_threshold (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 18 Jul 2016 17:07:42 )
> 4.0gap_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 18 Jul 2016 17:07:42 )
> 1.0boundary_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 18 Jul 2016 17:07:42 )
> Traceback (most recent call last):
>   File ""/home/estelle/miniconda2/bin/epic"", line 4, in <module>
>     **import**('pkg_resources').run_script('bioepic==0.0.6', 'epic')
>   File ""/home/estelle/miniconda2/lib/python2.7/site-packages/setuptools-20.3-py2.7.egg/pkg_resources/**init**.py"", line 726, in run_script
>   File ""/home/estelle/miniconda2/lib/python2.7/site-packages/setuptools-20.3-py2.7.egg/pkg_resources/**init**.py"", line 1491, in run_script
>   File ""/home/estelle/miniconda2/lib/python2.7/site-packages/bioepic-0.0.6-py2.7.egg/EGG-INFO/scripts/epic"", line 130, in <module>
> 
>   File ""/pica/h1/estelle/miniconda2/lib/python2.7/site-packages/bioepic-0.0.6-py2.7.egg/epic/run/run_epic.py"", line 38, in run_epic
>   File ""/pica/h1/estelle/miniconda2/lib/python2.7/site-packages/bioepic-0.0.6-py2.7.egg/epic/statistics/compute_background_probabilites.py"", line 59, in compute_background_probabilities
>   File ""/pica/h1/estelle/miniconda2/lib/python2.7/site-packages/bioepic-0.0.6-py2.7.egg/epic/statistics/compute_score_threshold.py"", line 24, in compute_score_threshold
> OverflowError: cannot convert float infinity to integer

/Estelle
",Estel-Kitsune,https://github.com/biocore-ntnu/epic/issues/26
MDU6SXNzdWUxNjYxNzc2NDY=,Add contribution guidelines ,OPEN,2016-07-18T19:49:29Z,2016-07-18T19:49:29Z,,,endrebak,https://github.com/biocore-ntnu/epic/issues/27
MDU6SXNzdWUxNjY5OTczMjY=,Default parameters for gap,CLOSED,2016-07-22T08:41:22Z,2017-01-16T11:47:37Z,2017-01-16T11:47:37Z,"Hi,

In the doc, the default parameter for the ""window size"" is 200, and the default parameter for ""gap"" is 3.

However, ""gap"" is supposed to be a multiple of ""window size"". Should you change the default gap value for 400, 600..? Or does that mean that by default the gap is 3 \* 200 = 600?

Thanks,
/Estelle
",Estel-Kitsune,https://github.com/biocore-ntnu/epic/issues/28
MDU6SXNzdWUxNjgzOTU3NDI=,Latest version of bioepic on anaconda is 0.0.6,CLOSED,2016-07-29T19:22:09Z,2016-08-10T10:15:10Z,2016-08-10T10:15:10Z,"Installed bioepic through bioconda and latest version is 0.0.6.
",c-guzman,https://github.com/biocore-ntnu/epic/issues/29
MDU6SXNzdWUxNjk5MTE5MzA=,Multiprocessing exception at merging ChIP and Input,CLOSED,2016-08-08T12:33:23Z,2016-08-09T12:49:35Z,2016-08-09T06:33:32Z,"Unfortunately I need to report another problem:

```
Binning input.bedpe.bz2 (File: run_epic, Log level: INFO, Time: Sun, 07 Aug 2016 00:50:10 )
Binning chromosomes 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, M (File: count_reads_in_windows, Log level: INFO, Time: Sun, 07 Aug 2016 00:50:10 )
Merging ChIP and Input data. (File: helper_functions, Log level: INFO, Time: Sun, 07 Aug 2016 02:31:24 )
Traceback (most recent call last):
  File ""/usr/local/bin/epic"", line 165, in <module>
    run_epic(args)
  File ""/usr/local/lib/python2.7/dist-packages/epic/run/run_epic.py"", line 42, in run_epic
    args.number_cores)
  File ""/usr/local/lib/python2.7/dist-packages/epic/utils/helper_functions.py"", line 37, in merge_chip_and_input
    for chip_df, input_df in zip(chip_dfs, input_dfs))
  File ""/usr/local/lib/python2.7/dist-packages/joblib/parallel.py"", line 764, in __call__
    self.retrieve()
  File ""/usr/local/lib/python2.7/dist-packages/joblib/parallel.py"", line 715, in retrieve
    raise exception
joblib.my_exceptions.JoblibValueError: JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/usr/local/bin/epic in <module>()
    160     elif not args.effective_genome_length and args.paired_end:
    161         logging.info(""Using paired end so setting readlength to 100."")
    162         args.effective_genome_length = get_effective_genome_length(args.genome,
    163                                                                    100)
    164 
--> 165     run_epic(args)
    166 
    167 
    168 
    169 

...........................................................................
/usr/local/lib/python2.7/dist-packages/epic/run/run_epic.py in run_epic(args=Namespace(control=['/mnt/biggles/csc_home/piotr/...Rer7.goodChr.sorted.bedpe.bz2'], window_size=200))
     37 
     38     nb_chip_reads = get_total_number_of_reads(chip_merged_sum)
     39     nb_input_reads = get_total_number_of_reads(input_merged_sum)
     40 
     41     merged_dfs = merge_chip_and_input(chip_merged_sum, input_merged_sum,
---> 42                                       args.number_cores)
        args.number_cores = 8
     43 
     44     score_threshold, island_enriched_threshold, average_window_readcount = \
     45         compute_background_probabilities(nb_chip_reads, args)
     46 

...........................................................................
/usr/local/lib/python2.7/dist-packages/epic/utils/helper_functions.py in merge_chip_and_input(chip_dfs=[       Chromosome       Bin  Count
0            ... chr1  60348200      1

[412411 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr2  60300400      4

[421692 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr3  63268800     18

[435867 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr4  62094400      5

[316130 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr5  75682000      1

[535305 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr6  59938600      6

[422058 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr7  77275200      1

[527395 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr8  56184600      8

[388639 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr9  58231800      7

[432048 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr10  46591000      3

[328844 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr11  46661200      3

[323983 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr12  50697000      1

[340626 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr13  54093600      9

[398247 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr14  53733400      2

[392001 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr15  47442200      2

[322950 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr16  58780600      3

[409774 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr17  53983600      2

[393334 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr18  49877000      1

[344906 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr19  50254400      1

[366537 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr20  55951800      1

[395725 rows x 3 columns], ...], input_dfs=[        Chromosome       Bin  Count
0           ...chr1  60348200      7

[1119107 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr2  60300400      8

[1134549 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr3  63268800     24

[1122376 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr4  62094400      6

[805058 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr5  75682000      1

[1402420 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr6  59938600      1

[1149075 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr7  77275600      1

[1461920 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr8  56184600     10

[1047948 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr9  58232000      7

[1109349 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr10  46591000      8

[866951 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr11  46661200      3

[887944 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr12  50697000      1

[920637 rows x 3 columns],         Chromosome       Bin  Count
0           ...hr13  54093600      1

[1032122 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr14  53733600      2

[983984 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr15  47442200      5

[871981 rows x 3 columns],         Chromosome       Bin  Count
0           ...hr16  58780600      3

[1068652 rows x 3 columns],         Chromosome       Bin  Count
0           ...hr17  53983600      7

[1039873 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr18  49877000      1

[977370 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr19  50254400     12

[973110 rows x 3 columns],         Chromosome       Bin  Count
0           ...hr20  55951800      3

[1054667 rows x 3 columns], ...], nb_cpu=8)
     32     assert len(chip_dfs) == len(input_dfs)
     33 
     34     logging.info(""Merging ChIP and Input data."")
     35     merged_chromosome_dfs = Parallel(n_jobs=nb_cpu)(
     36         delayed(_merge_chip_and_input)(chip_df, input_df)
---> 37         for chip_df, input_df in zip(chip_dfs, input_dfs))
        chip_dfs = [       Chromosome       Bin  Count
0            ... chr1  60348200      1

[412411 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr2  60300400      4

[421692 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr3  63268800     18

[435867 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr4  62094400      5

[316130 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr5  75682000      1

[535305 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr6  59938600      6

[422058 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr7  77275200      1

[527395 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr8  56184600      8

[388639 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr9  58231800      7

[432048 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr10  46591000      3

[328844 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr11  46661200      3

[323983 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr12  50697000      1

[340626 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr13  54093600      9

[398247 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr14  53733400      2

[392001 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr15  47442200      2

[322950 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr16  58780600      3

[409774 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr17  53983600      2

[393334 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr18  49877000      1

[344906 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr19  50254400      1

[366537 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr20  55951800      1

[395725 rows x 3 columns], ...]
        input_dfs = [        Chromosome       Bin  Count
0           ...chr1  60348200      7

[1119107 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr2  60300400      8

[1134549 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr3  63268800     24

[1122376 rows x 3 columns],        Chromosome       Bin  Count
0            ... chr4  62094400      6

[805058 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr5  75682000      1

[1402420 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr6  59938600      1

[1149075 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr7  77275600      1

[1461920 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr8  56184600     10

[1047948 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr9  58232000      7

[1109349 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr10  46591000      8

[866951 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr11  46661200      3

[887944 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr12  50697000      1

[920637 rows x 3 columns],         Chromosome       Bin  Count
0           ...hr13  54093600      1

[1032122 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr14  53733600      2

[983984 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr15  47442200      5

[871981 rows x 3 columns],         Chromosome       Bin  Count
0           ...hr16  58780600      3

[1068652 rows x 3 columns],         Chromosome       Bin  Count
0           ...hr17  53983600      7

[1039873 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr18  49877000      1

[977370 rows x 3 columns],        Chromosome       Bin  Count
0           c...chr19  50254400     12

[973110 rows x 3 columns],         Chromosome       Bin  Count
0           ...hr20  55951800      3

[1054667 rows x 3 columns], ...]
     38     return merged_chromosome_dfs
     39 
     40 
     41 def get_total_number_of_reads(dfs):

...........................................................................
/usr/local/lib/python2.7/dist-packages/joblib/parallel.py in __call__(self=Parallel(n_jobs=8), iterable=<generator object <genexpr>>)
    759             if pre_dispatch == ""all"" or n_jobs == 1:
    760                 # The iterable was consumed all at once by the above for loop.
    761                 # No need to wait for async callbacks to trigger to
    762                 # consumption.
    763                 self._iterating = False
--> 764             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=8)>
    765             # Make sure that we get a last message telling us we are done
    766             elapsed_time = time.time() - self._start_time
    767             self._print('Done %3i out of %3i | elapsed: %s finished',
    768                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Sun Aug  7 02:31:27 2016
PID: 10981                                   Python 2.7.12: /usr/bin/python
...........................................................................
/usr/local/lib/python2.7/dist-packages/joblib/parallel.py in __call__(self=<joblib.parallel.BatchedCalls object>)
    122     def __init__(self, iterator_slice):
    123         self.items = list(iterator_slice)
    124         self._size = len(self.items)
    125 
    126     def __call__(self):
--> 127         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _merge_chip_and_input>
        args = (       Chromosome       Bin  Count
0            ... chr1  60348200      1

[412411 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr1  60348200      7

[1119107 rows x 3 columns])
        kwargs = {}
        self.items = [(<function _merge_chip_and_input>, (       Chromosome       Bin  Count
0            ... chr1  60348200      1

[412411 rows x 3 columns],         Chromosome       Bin  Count
0           ...chr1  60348200      7

[1119107 rows x 3 columns]), {})]
    128 
    129     def __len__(self):
    130         return self._size
    131 

...........................................................................
/usr/local/lib/python2.7/dist-packages/epic/utils/helper_functions.py in _merge_chip_and_input(chip_df=       Chromosome       Bin  Count
0            ... chr1  60348200      1

[412411 rows x 3 columns], input_df=        Chromosome       Bin  Count
0           ...chr1  60348200      7

[1119107 rows x 3 columns])
     13 
     14     chip_df_nb_bins = len(chip_df)
     15     merged_df = chip_df.merge(input_df,
     16                               how=""left"",
     17                               on=[""Chromosome"", ""Bin""],
---> 18                               suffixes=["" ChIP"", "" Input""])
     19     merged_df = merged_df[[""Chromosome"", ""Bin"", ""Count ChIP"", ""Count Input""]]
     20     merged_df.columns = [""Chromosome"", ""Bin"", ""ChIP"", ""Input""]
     21 
     22     merged_df = merged_df.fillna(0)

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py in merge(self=       Chromosome       Bin  Count
0            ... chr1  60348200      1

[412411 rows x 3 columns], right=        Chromosome       Bin  Count
0           ...chr1  60348200      7

[1119107 rows x 3 columns], how='left', on=['Chromosome', 'Bin'], left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=[' ChIP', ' Input'], copy=True, indicator=False)
   4432               suffixes=('_x', '_y'), copy=True, indicator=False):
   4433         from pandas.tools.merge import merge
   4434         return merge(self, right, how=how, on=on, left_on=left_on,
   4435                      right_on=right_on, left_index=left_index,
   4436                      right_index=right_index, sort=sort, suffixes=suffixes,
-> 4437                      copy=copy, indicator=indicator)
        copy = True
        indicator = False
   4438 
   4439     def round(self, decimals=0, *args, **kwargs):
   4440         """"""
   4441         Round a DataFrame to a variable number of decimal places.

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py in merge(left=       Chromosome       Bin  Count
0            ... chr1  60348200      1

[412411 rows x 3 columns], right=        Chromosome       Bin  Count
0           ...chr1  60348200      7

[1119107 rows x 3 columns], how='left', on=['Chromosome', 'Bin'], left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=[' ChIP', ' Input'], copy=True, indicator=False)
     34           suffixes=('_x', '_y'), copy=True, indicator=False):
     35     op = _MergeOperation(left, right, how=how, on=on, left_on=left_on,
     36                          right_on=right_on, left_index=left_index,
     37                          right_index=right_index, sort=sort, suffixes=suffixes,
     38                          copy=copy, indicator=indicator)
---> 39     return op.get_result()
        op.get_result = <bound method _MergeOperation.get_result of <pandas.tools.merge._MergeOperation object>>
     40 if __debug__:
     41     merge.__doc__ = _merge_doc % '\nleft : DataFrame'
     42 
     43 

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py in get_result(self=<pandas.tools.merge._MergeOperation object>)
    212     def get_result(self):
    213         if self.indicator:
    214             self.left, self.right = self._indicator_pre_merge(
    215                 self.left, self.right)
    216 
--> 217         join_index, left_indexer, right_indexer = self._get_join_info()
        join_index = undefined
        left_indexer = undefined
        right_indexer = undefined
        self._get_join_info = <bound method _MergeOperation._get_join_info of <pandas.tools.merge._MergeOperation object>>
    218 
    219         ldata, rdata = self.left._data, self.right._data
    220         lsuf, rsuf = self.suffixes
    221 

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py in _get_join_info(self=<pandas.tools.merge._MergeOperation object>)
    348                                     sort=self.sort)
    349         else:
    350             (left_indexer,
    351              right_indexer) = _get_join_indexers(self.left_join_keys,
    352                                                  self.right_join_keys,
--> 353                                                  sort=self.sort, how=self.how)
        self.sort = False
        self.how = 'left'
    354             if self.right_index:
    355                 if len(self.left) > 0:
    356                     join_index = self.left.index.take(left_indexer)
    357                 else:

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py in _get_join_indexers(left_keys=[array(['chr1', 'chr1', 'chr1', ..., 'chr1', 'chr1', 'chr1'], dtype=object), memmap([    1400,     1600,     1800, ..., 60347800, 60348000, 60348200])], right_keys=[array(['chr1', 'chr1', 'chr1', ..., 'chr1', 'chr1', 'chr1'], dtype=object), memmap([    1400,     1600,     1400, ..., 60347800, 60348000, 60348200])], sort=False, how='left')
    541 
    542     # bind `sort` arg. of _factorize_keys
    543     fkeys = partial(_factorize_keys, sort=sort)
    544 
    545     # get left & right join labels and num. of levels at each location
--> 546     llab, rlab, shape = map(list, zip(* map(fkeys, left_keys, right_keys)))
        llab = undefined
        rlab = undefined
        shape = undefined
        fkeys = <functools.partial object>
        left_keys = [array(['chr1', 'chr1', 'chr1', ..., 'chr1', 'chr1', 'chr1'], dtype=object), memmap([    1400,     1600,     1800, ..., 60347800, 60348000, 60348200])]
        right_keys = [array(['chr1', 'chr1', 'chr1', ..., 'chr1', 'chr1', 'chr1'], dtype=object), memmap([    1400,     1600,     1400, ..., 60347800, 60348000, 60348200])]
    547 
    548     # get flat i8 keys from label lists
    549     lkey, rkey = _get_join_keys(llab, rlab, shape, sort)
    550 

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py in _factorize_keys(lk=memmap([    1400,     1600,     1800, ..., 60347800, 60348000, 60348200]), rk=memmap([    1400,     1600,     1400, ..., 60347800, 60348000, 60348200]), sort=False)
    708         lk = com._ensure_object(lk)
    709         rk = com._ensure_object(rk)
    710 
    711     rizer = klass(max(len(lk), len(rk)))
    712 
--> 713     llab = rizer.factorize(lk)
        llab = undefined
        rizer.factorize = <built-in method factorize of pandas.hashtable.Int64Factorizer object>
        lk = memmap([    1400,     1600,     1800, ..., 60347800, 60348000, 60348200])
    714     rlab = rizer.factorize(rk)
    715 
    716     count = rizer.get_count()
    717 

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas/hashtable.so in pandas.hashtable.Int64Factorizer.factorize (pandas/hashtable.c:15715)()
    854 
    855 
    856 
    857 
    858 
--> 859 
    860 
    861 
    862 
    863 

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas/hashtable.so in View.MemoryView.memoryview_cwrapper (pandas/hashtable.c:29784)()
    639 
    640 
    641 
    642 
    643 
--> 644 
    645 
    646 
    647 
    648 

...........................................................................
/usr/local/lib/python2.7/dist-packages/pandas/hashtable.so in View.MemoryView.memoryview.__cinit__ (pandas/hashtable.c:26059)()
    340 
    341 
    342 
    343 
    344 
--> 345 
    346 
    347 
    348 
    349 

ValueError: buffer source array is read-only
___________________________________________________________________________
```
",balwierz,https://github.com/biocore-ntnu/epic/issues/30
MDU6SXNzdWUxNzAwOTIwMDA=,AssertionError: assert len(merged_df) == chip_df_nb_bins,CLOSED,2016-08-09T06:33:07Z,2016-08-11T13:01:42Z,2016-08-11T12:50:59Z,"```
Traceback (most recent call last):
  File ""/usr/local/bin/epic"", line 165, in <module>
    run_epic(args)
  File ""/usr/local/lib/python2.7/dist-packages/epic/run/run_epic.py"", line 42, in run_epic
    args.number_cores)
  File ""/usr/local/lib/python2.7/dist-packages/epic/utils/helper_functions.py"", line 37, in merge_chip_and_input
    for chip_df, input_df in zip(chip_dfs, input_dfs))
  File ""/usr/local/lib/python2.7/dist-packages/joblib/parallel.py"", line 754, in __call__
    while self.dispatch_one_batch(iterator):
  File ""/usr/local/lib/python2.7/dist-packages/joblib/parallel.py"", line 604, in dispatch_one_batch
    self._dispatch(tasks)
  File ""/usr/local/lib/python2.7/dist-packages/joblib/parallel.py"", line 567, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File ""/usr/local/lib/python2.7/dist-packages/joblib/_parallel_backends.py"", line 109, in apply_async
    result = ImmediateResult(func)
  File ""/usr/local/lib/python2.7/dist-packages/joblib/_parallel_backends.py"", line 322, in __init__
    self.results = batch()
  File ""/usr/local/lib/python2.7/dist-packages/joblib/parallel.py"", line 127, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/usr/local/lib/python2.7/dist-packages/epic/utils/helper_functions.py"", line 24, in _merge_chip_and_input
    assert len(merged_df) == chip_df_nb_bins
AssertionError
```

(from #30).
",endrebak,https://github.com/biocore-ntnu/epic/issues/31
MDU6SXNzdWUxNzAxNzYwMTY=,"Write something about epic not being suitable for single-chromosome files (yet, if ever)",CLOSED,2016-08-09T14:09:39Z,2017-12-12T11:46:36Z,2017-12-12T11:46:36Z,"See #31 
",endrebak,https://github.com/biocore-ntnu/epic/issues/32
MDU6SXNzdWUxNzA2MzkzMDQ=,Bug in multiprocessing paired end data:,CLOSED,2016-08-11T12:57:17Z,2017-12-12T11:46:47Z,2017-12-12T11:46:47Z,"```
epic -pe -t examples/chr19_sample.bedpe   -c examples/chr19_input.bedpe
```

works, but 

```
epic -cpu 25 -pe -t examples/chr19_sample.bedpe   -c examples/chr19_input.bedpe  --store-matrix H3K27me3.matrix
```

fails! I'll try to get to the bottom of this, but the error is in a different library, not epic.

```
# epic -cpu 25 -pe -t examples/chr19_sample.bedpe -c examples/chr19_input.bedpe --store-matrix H3K27me3.matrix
# epic -cpu 25 -pe -t examples/chr19_sample.bedpe -c examples/chr19_input.bedpe --store-matrix H3K27me3.matrix (File: epic, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:40 )
Using paired end so setting readlength to 100. (File: epic, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:41 )
Using an effective genome fraction of 0.901962701202. (File: genomes, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:41 )
Binning examples/chr19_sample.bedpe (File: run_epic, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:41 )
Binning chromosomes 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, M, X, Y (File: count_reads_in_windows, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:41 )
Making duplicated bins unique by summing them. (File: count_reads_in_windows, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:48 )
Binning examples/chr19_input.bedpe (File: run_epic, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:48 )
Binning chromosomes 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, M, X, Y (File: count_reads_in_windows, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:48 )
Making duplicated bins unique by summing them. (File: count_reads_in_windows, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:54 )
Merging ChIP and Input data. (File: helper_functions, Log level: INFO, Time: Thu, 11 Aug 2016 14:55:55 )
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/joblib-0.9.4-py3.5.egg/joblib/parallel.py"", line 130, in __call__
    return self.func(*args, **kwargs)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/joblib-0.9.4-py3.5.egg/joblib/parallel.py"", line 72, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/joblib-0.9.4-py3.5.egg/joblib/parallel.py"", line 72, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/bioepic-0.1.8-py3.5.egg/epic/utils/helper_functions.py"", line 19, in _merge_chip_and_input
    suffixes=["" ChIP"", "" Input""])
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py"", line 4437, in merge
    copy=copy, indicator=indicator)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/pandas/tools/merge.py"", line 39, in merge
    return op.get_result()
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/pandas/tools/merge.py"", line 217, in get_result
    join_index, left_indexer, right_indexer = self._get_join_info()
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/pandas/tools/merge.py"", line 353, in _get_join_info
    sort=self.sort, how=self.how)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/pandas/tools/merge.py"", line 546, in _get_join_indexers
    llab, rlab, shape = map(list, zip(* map(fkeys, left_keys, right_keys)))
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/pandas/tools/merge.py"", line 713, in _factorize_keys
    llab = rizer.factorize(lk)
  File ""pandas/hashtable.pyx"", line 859, in pandas.hashtable.Int64Factorizer.factorize (pandas/hashtable.c:15715)
  File ""stringsource"", line 644, in View.MemoryView.memoryview_cwrapper (pandas/hashtable.c:29784)
  File ""stringsource"", line 345, in View.MemoryView.memoryview.__cinit__ (pandas/hashtable.c:26059)
ValueError: buffer source array is read-only

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/local/home/endrebak/anaconda3/lib/python3.5/tokenize.py"", line 392, in find_cookie
    line_string = line.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd0 in position 24: invalid continuation byte

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/local/home/endrebak/anaconda3/lib/python3.5/multiprocessing/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/joblib-0.9.4-py3.5.egg/joblib/parallel.py"", line 139, in __call__
    tb_offset=1)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/joblib-0.9.4-py3.5.egg/joblib/format_stack.py"", line 373, in format_exc
    frames = format_records(records)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/joblib-0.9.4-py3.5.egg/joblib/format_stack.py"", line 274, in format_records
    for token in generate_tokens(linereader):
  File ""/local/home/endrebak/anaconda3/lib/python3.5/tokenize.py"", line 514, in _tokenize
    line = readline()
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/joblib-0.9.4-py3.5.egg/joblib/format_stack.py"", line 265, in linereader
    line = getline(file, lnum[0])
  File ""/local/home/endrebak/anaconda3/lib/python3.5/linecache.py"", line 16, in getline
    lines = getlines(filename, module_globals)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/linecache.py"", line 47, in getlines
    return updatecache(filename, module_globals)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/linecache.py"", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File ""/local/home/endrebak/anaconda3/lib/python3.5/tokenize.py"", line 456, in open
    encoding, lines = detect_encoding(buffer.readline)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/tokenize.py"", line 433, in detect_encoding
    encoding = find_cookie(first)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/tokenize.py"", line 397, in find_cookie
    raise SyntaxError(msg)
  File ""<string>"", line None
SyntaxError: invalid or missing encoding declaration for '/local/home/endrebak/anaconda3/lib/python3.5/site-packages/pandas/hashtable.cpython-35m-x86_64-linux-gnu.so'
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/local/home/endrebak/anaconda3/bin/epic"", line 4, in <module>
    __import__('pkg_resources').run_script('bioepic==0.1.8', 'epic')
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/setuptools-20.7.0-py3.5.egg/pkg_resources/__init__.py"", line 719, in run_script
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/setuptools-20.7.0-py3.5.egg/pkg_resources/__init__.py"", line 1504, in run_script
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/bioepic-0.1.8-py3.5.egg/EGG-INFO/scripts/epic"", line 165, in <module>
    run_epic(args)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/bioepic-0.1.8-py3.5.egg/epic/run/run_epic.py"", line 42, in run_epic
    args.number_cores)
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/bioepic-0.1.8-py3.5.egg/epic/utils/helper_functions.py"", line 55, in merge_chip_and_input
    for chip_df, input_df in zip(chip_dfs, input_dfs))
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/joblib-0.9.4-py3.5.egg/joblib/parallel.py"", line 810, in __call__
    self.retrieve()
  File ""/local/home/endrebak/anaconda3/lib/python3.5/site-packages/joblib-0.9.4-py3.5.egg/joblib/parallel.py"", line 727, in retrieve
    self._output.extend(job.get())
  File ""/local/home/endrebak/anaconda3/lib/python3.5/multiprocessing/pool.py"", line 608, in get
    raise self._value
SyntaxError: invalid or missing encoding declaration for '/local/home/endrebak/anaconda3/lib/python3.5/site-packages/pandas/hashtable.cpython-35m-x86_64-linux-gnu.so
```
",endrebak,https://github.com/biocore-ntnu/epic/issues/33
MDU6SXNzdWUxNzA2NTI5Mjc=,Effective genome size for Rnor6,CLOSED,2016-08-11T14:00:32Z,2016-08-15T10:08:45Z,2016-08-15T10:08:45Z,"Hi 
Could you please let me know, how I can numbers for effective genome size files in the for rat and pig?

And also for broad peaks like K9me2 what settings shall I use?

Thank you
",abhisheksinghnl,https://github.com/biocore-ntnu/epic/issues/34
MDU6SXNzdWUxNzA2NTQyMzM=,Usage with multiple treatments and controls,CLOSED,2016-08-11T14:06:22Z,2016-08-12T11:15:16Z,2016-08-12T11:14:52Z,"If I have ChIP experiments on multiple different treatment groups  (lets say dosage A B C D and a control group) how should I input into epic? Do I list the different treatment in a single command and epic treats them separately? Or should I run epic for each treatment vs a control. 

Also that brings me to confusion in wordage for the control flag. Is it reads from an actual ChIP input (i.e no antibody chromatin DNA) or the IP of an untreated control group?  
",ksiklenka,https://github.com/biocore-ntnu/epic/issues/35
MDU6SXNzdWUxNzA3MTk4MTU=,EPIC stops if treatment and control are both input files,CLOSED,2016-08-11T18:57:52Z,2016-08-16T09:10:38Z,2016-08-15T10:01:36Z,"Because the way my pipeline is set up, it will typically run Input bed vs Input bed ... in MACS2 this is fine ... no peaks are called. But EPIC gets a command exit status 1:

Here is the output:

```
Command executed:

  epic --treatment Input_treatment.bed --control Input_control.bed --number-cores 3 --genome hg19 --fragment-size 150 > Input_epic.bed

Command exit status:
  1

Command output:
  (empty)

Command error:
      710 
      711     rizer = klass(max(len(lk), len(rk)))
      712 
  --> 713     llab = rizer.factorize(lk)
      714     rlab = rizer.factorize(rk)
      715 
      716     count = rizer.get_count()
      717 

  ...........................................................................
  /usr/local/anaconda2/lib/python2.7/site-packages/pandas/hashtable.so in pandas.hashtable.Int64Factorizer.factorize (pandas/hashtable.c:15715)()
      854 
      855 
      856 
      857 
      858 
  --> 859 
      860 
      861 
      862 
      863 

  ...........................................................................
  /usr/local/anaconda2/lib/python2.7/site-packages/pandas/hashtable.so in View.MemoryView.memoryview_cwrapper (pandas/hashtable.c:29784)()
      639 
      640 
      641 
      642 
      643 
  --> 644 
      645 
      646 
      647 
      648 

  ...........................................................................
  /usr/local/anaconda2/lib/python2.7/site-packages/pandas/hashtable.so in View.MemoryView.memoryview.__cinit__ (pandas/hashtable.c:26059)()
      340 
      341 
      342 
      343 
      344 
  --> 345 
      346 
      347 
      348 
      349 

  ValueError: buffer source array is read-only
  ___________________________________________________________________________

```
",c-guzman,https://github.com/biocore-ntnu/epic/issues/36
MDU6SXNzdWUxNzA4NjI1Nzk=,How to direct output to a file,CLOSED,2016-08-12T12:41:14Z,2017-12-12T11:46:09Z,2017-12-12T11:46:09Z,"Hi,

Strangely, I cannot save the output file. 

I cannot even find a parameter that says something on naming output file  :(

The output just displays on screen

How can I direct the output to files.

The command that I am using is 

 epic -t ChIPseq_H3K9me2.bed -c ChIPseq_input.bed -gn rn6

Thank you in advance.
",abhisheksinghnl,https://github.com/biocore-ntnu/epic/issues/37
MDU6SXNzdWUxNzU4MjM0ODA=,OverflowError: Cannot convert float infinity to integer,CLOSED,2016-09-08T18:11:40Z,2016-10-18T08:43:49Z,2016-10-18T08:43:49Z,"Command used:

```
Command executed:

  epic --treatment Control2_treatment.bed.gz --control Control2_control.bed.gz --genome hg19 --fragment-size 150 > Control2_epic.bed
```

Resulting error:

```
Command error:
  # epic --treatment Control2_treatment.bed.gz --control Control2_control.bed.gz --genome hg19 --fragment-size 150 (File: epic, Log level: INFO, Time: Thu, 08 Sep 2016 11:56:47 )

  gzip: stdout: Broken pipe
  Used first 10000 reads of Control2_treatment.bed.gz to estimate a median read length of 34.0
  Mean readlength: 33.3074, max readlength: 37, min readlength: 20. (File: find_readlength, Log level: INFO, Time: Thu, 08 Sep 2016 11:56:47 )
  Using an effective genome fraction of 0.810858412293. (File: genomes, Log level: INFO, Time: Thu, 08 Sep 2016 11:56:47 )
  Binning Control2_treatment.bed.gz (File: run_epic, Log level: INFO, Time: Thu, 08 Sep 2016 11:56:47 )
  Binning chromosomes 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, M, X, Y (File: count_reads_in_windows, Log level: INFO, Time: Thu, 08 Sep 2016 11:56:47 )
  Merging the bins on both strands per chromosome. (File: count_reads_in_windows, Log level: INFO, Time: Thu, 08 Sep 2016 12:00:12 )
  Binning Control2_control.bed.gz (File: run_epic, Log level: INFO, Time: Thu, 08 Sep 2016 12:00:13 )
  Binning chromosomes 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, M, X, Y (File: count_reads_in_windows, Log level: INFO, Time: Thu, 08 Sep 2016 12:00:13 )
  Merging the bins on both strands per chromosome. (File: count_reads_in_windows, Log level: INFO, Time: Thu, 08 Sep 2016 12:03:40 )
  Merging ChIP and Input data. (File: helper_functions, Log level: INFO, Time: Thu, 08 Sep 2016 12:03:40 )
  2510169508.0 effective_genome_length (File: compute_background_probabilites, Log level: DEBUG, Time: Thu, 08 Sep 2016 12:03:40 )
  200 window size (File: compute_background_probabilites, Log level: DEBUG, Time: Thu, 08 Sep 2016 12:03:40 )
  0 total chip count (File: compute_background_probabilites, Log level: DEBUG, Time: Thu, 08 Sep 2016 12:03:40 )
  0.0 average_window_readcount (File: compute_background_probabilites, Log level: DEBUG, Time: Thu, 08 Sep 2016 12:03:40 )
  1 island_enriched_threshold (File: compute_background_probabilites, Log level: DEBUG, Time: Thu, 08 Sep 2016 12:03:40 )
  4.0 gap_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Thu, 08 Sep 2016 12:03:40 )
  1.0 boundary_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Thu, 08 Sep 2016 12:03:40 )
  Traceback (most recent call last):
    File ""/usr/local/anaconda2/bin/epic"", line 4, in <module>
      __import__('pkg_resources').run_script('bioepic==0.1.6', 'epic')
    File ""/usr/local/anaconda2/lib/python2.7/site-packages/pkg_resources.py"", line 461, in run_script
      self.require(requires)[0].run_script(script_name, ns)
    File ""/usr/local/anaconda2/lib/python2.7/site-packages/pkg_resources.py"", line 1194, in run_script
      execfile(script_filename, namespace, namespace)
    File ""/usr/local/anaconda2/lib/python2.7/site-packages/bioepic-0.1.6-py2.7.egg-info/scripts/epic"", line 165, in <module>
      run_epic(args)
    File ""/usr/local/anaconda2/lib/python2.7/site-packages/epic/run/run_epic.py"", line 45, in run_epic
      compute_background_probabilities(nb_chip_reads, args)
    File ""/usr/local/anaconda2/lib/python2.7/site-packages/epic/statistics/compute_background_probabilites.py"", line 49, in compute_background_probabilities
      boundary_contribution, genome_length_in_bins)
    File ""/usr/local/anaconda2/lib/python2.7/site-packages/epic/statistics/compute_score_threshold.py"", line 24, in compute_score_threshold
      current_scaled_score = int(round(score / BIN_SIZE))
  OverflowError: cannot convert float infinity to integer
```

Any ideas?
",c-guzman,https://github.com/biocore-ntnu/epic/issues/38
MDU6SXNzdWUxNzcwMTQxMTQ=,Temporary Directory,CLOSED,2016-09-14T20:37:24Z,2016-10-05T12:25:02Z,2016-10-05T12:25:02Z,"I'm running to a problem where my /tmp folder is running out of space during a pipeline run. I was wondering if there was an option to choose where temporary files are stored for epic?
",c-guzman,https://github.com/biocore-ntnu/epic/issues/39
MDU6SXNzdWUxNzkzMDM1NTU=,Google Groups,CLOSED,2016-09-26T18:31:33Z,2016-09-27T06:17:52Z,2016-09-27T06:17:02Z,"Would it be possible to create a google groups for questions?

I'd like to integrate automatic effective genome size estimation into my chip-seq pipeline, but am having a bit of difficulty and can't find an appropriate place to post this (besides maybe biostars, but it's more of a programming question than a bioinformatic question).
",c-guzman,https://github.com/biocore-ntnu/epic/issues/40
MDU6SXNzdWUxODA5ODA1NDg=,IndexError: list index out of range,CLOSED,2016-10-04T19:19:09Z,2016-10-05T08:59:43Z,2016-10-05T08:59:43Z,"I installed epic ver. 0.1.18 and no issues were reported. However, when I try running it on any dataset including the example dataset shipped with the software, I am getting the following error.

mnrusimh@bioserv:/nfs/analysis/epic$ epic --treatment test.bed --control control.bed
# epic --treatment test.bed --control control.bed
# epic --treatment test.bed --control control.bed # epic_version: 0.1.18, pandas_version: 0.12.0 (File: epic, Log level: INFO, Time: Tue, 04 Oct 2016 09:45:41 )
Traceback (most recent call last):
  File ""/home/mnrusimh/Applications/epic/bin/epic"", line 5, in <module>
    pkg_resources.run_script('bioepic==0.1.18', 'epic')
  File ""/nfs/bio/sw/lib/python2.7/site-packages/distribute-0.6.34-py2.7.egg/pkg_resources.py"", line 505, in run_script
    self.require(requires)[0].run_script(script_name, ns)
  File ""/nfs/bio/sw/lib/python2.7/site-packages/distribute-0.6.34-py2.7.egg/pkg_resources.py"", line 1245, in run_script
    execfile(script_filename, namespace, namespace)
  File ""/home/mnrusimh/Applications/epic/lib/python2.7/site-packages/bioepic-0.1.18-py2.7.egg/EGG-INFO/scripts/epic"", line 193, in <module>
    estimated_readlength = find_readlength(args)
  File ""/home/mnrusimh/Applications/epic/lib/python2.7/site-packages/bioepic-0.1.18-py2.7.egg/epic/utils/find_readlength.py"", line 36, in find_readlength
    names=[""Start"", ""End""])
  File ""/nfs/bio/sw/lib/python2.7/site-packages/pandas-0.12.0-py2.7-linux-x86_64.egg/pandas/io/parsers.py"", line 400, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""/nfs/bio/sw/lib/python2.7/site-packages/pandas-0.12.0-py2.7-linux-x86_64.egg/pandas/io/parsers.py"", line 205, in _read
    return parser.read()
  File ""/nfs/bio/sw/lib/python2.7/site-packages/pandas-0.12.0-py2.7-linux-x86_64.egg/pandas/io/parsers.py"", line 608, in read
    ret = self._engine.read(nrows)
  File ""/nfs/bio/sw/lib/python2.7/site-packages/pandas-0.12.0-py2.7-linux-x86_64.egg/pandas/io/parsers.py"", line 1028, in read
    data = self._reader.read(nrows)
  File ""parser.pyx"", line 706, in pandas.parser.TextReader.read (pandas/parser.c:6745)
  File ""parser.pyx"", line 728, in pandas.parser.TextReader._read_low_memory (pandas/parser.c:6964)
  File ""parser.pyx"", line 804, in pandas.parser.TextReader._read_rows (pandas/parser.c:7780)
  File ""parser.pyx"", line 865, in pandas.parser.TextReader._convert_column_data (pandas/parser.c:8512)
  File ""parser.pyx"", line 1105, in pandas.parser.TextReader._get_column_name (pandas/parser.c:11684)
IndexError: list index out of range
",mnrusimh,https://github.com/biocore-ntnu/epic/issues/42
MDU6SXNzdWUxODE0MjUwMjM=,Error calculating readlength,CLOSED,2016-10-06T14:07:52Z,2017-01-16T07:55:47Z,2016-10-06T14:46:21Z,"I'm getting

```
# epic -t chip_A-sortsam.bam -c input_A-sortsam.bam -fs 300 (File: epic, Log level: INFO, Time: Thu, 06 Oct 2016 06:56:24 )
cat: write error: Broken pipe
Used first 10000 reads of chip_A-sortsam.bam to estimate a median read length of nan
Mean readlength: nan, max readlength: nan, min readlength: nan. (File: find_readlength, Log level: INFO, Time: Thu, 06 Oct 2016 06:56:25 )
# epic -t chip_A-sortsam.bam -c input_A-sortsam.bam -fs 300
Traceback (most recent call last):
  File ""/home/users/balter/miniconda2/bin/epic"", line 4, in <module>
    __import__('pkg_resources').run_script('bioepic==0.1.17', 'epic')
  File ""/home/users/balter/miniconda2/lib/python2.7/site-packages/setuptools-23.0.0-py2.7.egg/pkg_resources/__init__.py"", line 719, in run_script
  File ""/home/users/balter/miniconda2/lib/python2.7/site-packages/setuptools-23.0.0-py2.7.egg/pkg_resources/__init__.py"", line 1504, in run_script
  File ""/home/users/balter/miniconda2/lib/python2.7/site-packages/bioepic-0.1.17-py2.7.egg-info/scripts/epic"", line 190, in <module>
    closest_readlength = get_closest_readlength(estimated_readlength)
  File ""/home/users/balter/miniconda2/lib/python2.7/site-packages/epic/utils/find_readlength.py"", line 63, in get_closest_readlength
    if d == min_difference][0]
IndexError: list index out of range
```

BAM files:

```
ls -l
total 6622033
-rw-rw-r-- 1 balter CompBio 3387571391 Oct  6 05:59 chip_A-sortsam.bam
-rw-rw-r-- 1 balter CompBio 3387571391 Oct  6 06:34 input_A-sortsam.bam
```
",abalter,https://github.com/biocore-ntnu/epic/issues/43
MDU6SXNzdWUxODE1ODQzMzQ=,Bam support,OPEN,2016-10-07T04:43:53Z,2016-10-31T19:19:04Z,,"I think it is a bad idea for the below reasons. Feel free to suggest solutions:

> You will probably rerun the analyses many times. Having to run a time-consuming conversion step (the most time-consuming one in the algorithm) each time would be silly. It is also IO-intensive so parallell execution would not help much.
> 
> I am not just writing epic but a lot of helper scripts for ChIP-Seq and differential ChIP Seq. Adding a conversion step to bed in all of these before running the scripts would be a waste.
> 
> Also, where should I store the temporary bed files? Overflowing /tmp/ dirs is an eternal issue.
> 
> If I were to stream the data to bed using pipes, epic would not be fast anymore. I get a massive speedup from multiple cores if I use text files, presumably because the system knows it has the file in memory already. This is not the case if I start the pipe with bamToBed blabla | ...
> 
> There are many things that can go wrong when converting bam to bed, due to wonky bam files. I would get a bunch of github issues about ""epic not being able to use my bam files"" if I were to silently convert to bed within my programs.
",endrebak,https://github.com/biocore-ntnu/epic/issues/44
MDU6SXNzdWUxODczMDkxMjA=,bioepic: OverflowError: cannot convert float infinity to integer,OPEN,2016-11-04T11:19:57Z,2016-11-07T07:17:57Z,,,anishdattani,https://github.com/biocore-ntnu/epic/issues/45
MDU6SXNzdWUxODg2NDUxNTg=,epic - is it usual that analyses take time?,CLOSED,2016-11-10T23:17:52Z,2016-11-15T10:44:42Z,2016-11-15T07:21:41Z,,anishdattani,https://github.com/biocore-ntnu/epic/issues/46
MDU6SXNzdWUxOTk5Mjc2MTE=,-k FLAG,CLOSED,2017-01-10T20:38:23Z,2017-01-11T14:36:24Z,2017-01-11T14:36:24Z,"Can you add more information on the keep duplicate flag? I get an error saying it requires an argument, but i'm not sure what the correct argument would be.",c-guzman,https://github.com/biocore-ntnu/epic/issues/48
MDU6SXNzdWUyMDAxMzI2ODg=,Andaconda Update,CLOSED,2017-01-11T16:00:14Z,2017-01-18T12:13:06Z,2017-01-18T12:13:06Z,"EPIC on Anaconda is still at 1.6. I'd like to include and cite it as part of a docker container, unfortunately I require the latest updated version of EPIC which has access to the ""-cs"" and ""-sbw"" options.

Thank you! (And sorry for posting so much here)",c-guzman,https://github.com/biocore-ntnu/epic/issues/49
MDU6SXNzdWUyMTQzNDkzNzk=,epic-effective cannot find jellyfish,OPEN,2017-03-15T10:42:24Z,2017-03-15T14:48:05Z,,"Hi,

I have installed epic and jellyfish in a virtualenv:
virtualenv venv.epic_0.1.25
. /path/venv.epic_0.1.25/bin/activate
pip install bioepic
pip install jellyfish

When I try to run epic-effective as:
epic-effective --read-length=76 --nb-cpu=12 oviAri3_BASE.fa

I get the following error:
Temporary directory: /tmp/86174.1.C6320-512-haswell.q (File: effective_genome_size, Log level: INFO, Time: Tue, 14 Mar 2017 16:40:11 )
File analyzed: oviAri3_BASE.fa (File: effective_genome_size, Log level: INFO, Time: Tue, 14 Mar 2017 16:40:11 )
Genome length: 2587507083 (File: effective_genome_size, Log level: INFO, Time: Tue, 14 Mar 2017 16:40:11 )
/bin/sh: jellyfish: command not found
/bin/sh: jellyfish: command not found
Traceback (most recent call last):
File ""/mnt/fls01-home01/user/virtualenvs/venv.epic_0.1.25/bin/epic-effective"", line 38, in
effective_genome_size(fasta, read_length, nb_cpu, tmpdir)
File ""/mnt/fls01-home01/user/virtualenvs/venv.epic_0.1.25/lib/python2.7/site-packages/epic/scripts/effective_genome_size.py"", line 56, in effective_genome_size
shell=True)
File ""/opt/gridware/depots/4baff5c5/el7/pkg/apps/python/2.7.8/gcc-4.8.5/lib/python2.7/subprocess.py"", line 573, in check_output
raise CalledProcessError(retcode, cmd, output=output)
subprocess.CalledProcessError: Command 'jellyfish stats /tmp/86174.1.C6320-512-haswell.q/oviAri3_BASE.fa.jf' returned non-zero exit status 127
rm: cannot remove ‚Äò/tmp/86174.1.C6320-512-haswell.q/oviAri3_BASE.fa.jf‚Äô: No such file or directory

Thanks.",IanCodes,https://github.com/biocore-ntnu/epic/issues/50
MDU6SXNzdWUyMjE5MjQ5NTg=,Effective Genome Size,CLOSED,2017-04-15T04:02:10Z,2018-01-16T09:43:32Z,2017-12-12T11:44:42Z,"As part of a pipeline I am using your epic-effective script to calculate EGS prior to downstream analysis. I was going through your arguments and noticed that your `-egs` parameter requires a EGS between 0 and 1. As opposed to the typical ~2.8 billion that is used by macs2 and deeptools for human hg38. I tried running epic with the 2.8 billion number and didn't run into any errors. 

Is this just because the parameter information is outdated or will this result in incorrect peak calling? I'm interested in knowing if i'll have to parse out extra information or if what I already have done is fine.

Thanks!",c-guzman,https://github.com/biocore-ntnu/epic/issues/51
MDU6SXNzdWUyMjkxMDMxNzE=,Clarify Read Inputs and Differential Peak Calling,OPEN,2017-05-16T17:01:32Z,2017-05-17T07:15:22Z,,"I hope that you can clarify the relationship of reads when supplying multiple treatment and control files to epic. For example, are multiple treatment files pooled (like MACS) or are they treated independently?

Also, I have interest in differential peak calling between treated and non-treated ChIP samples with corresponding input controls.  Does epic have this ability like SICER-df.sh, or is there a mechanism to reproduce this functionality?

Thanks for the great work, I am enjoying using epic so far.",quantumdot,https://github.com/biocore-ntnu/epic/issues/52
MDU6SXNzdWUyMjkxMDY4MjA=,Use unique filenames for --sum-bigwig option,CLOSED,2017-05-16T17:15:27Z,2017-07-21T12:08:45Z,2017-07-21T12:08:45Z,"Running epic on multiple datasets with the same parameter directory location for `--sum-bigwig` will overwrite output of a previous run on a different dataset. It looks like the output file just get called `chip_sum.bw` and `input_sum.bw`. Can you change this to have a more unique name, perhaps by taking a `--name` parameter on the command line for an experiment name?",quantumdot,https://github.com/biocore-ntnu/epic/issues/53
MDU6SXNzdWUyMzA4NTc3NzA=,custom chromosomes with . in name,CLOSED,2017-05-23T21:51:25Z,2017-05-24T16:27:19Z,2017-05-24T16:27:19Z,"Hi, 

My chromosome names unfortunately contain full stops (chr1_v2.1, chr_v2.1 etc). When I use --chromsizes option and a file containing chromosome names and length I get an error.  When I remove the full stops this is resolved, but the chromosome names are incorrect now. 

 In the Genome.py script:

chromosome_lengths = [l.split() for l in open(chromsizes).readlines()]

appears not just to be splitting with white space, the full stops are causing a problem here for some reason. 
Any chance of a solution? 

Thanks!",saycheeeeese,https://github.com/biocore-ntnu/epic/issues/54
MDU6SXNzdWUyMzEwNjkwOTQ=,Concating dfs pandas error,CLOSED,2017-05-24T14:55:37Z,2018-01-16T09:43:16Z,2017-12-12T11:43:52Z,"Hi 
 I have received the following error

```
Concating dfs. (File: run_epic, Log level: INFO, Time: Wed, 24 May 2017 14:50:32 )
Traceback (most recent call last):
  File ""/home/ubuntu/.local/bin/epic"", line 219, in <module>
    run_epic(args)
  File ""/usr/local/lib/python2.7/dist-packages/bioepic-0.1.25-py2.7.egg/epic/run/run_epic.py"", line 60, in run_epic
    df = pd.concat([df for df in dfs if not df.empty])
  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 845, in concat
    copy=copy)
  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 878, in __init__
    raise ValueError('No objects to concatenate')
ValueError: No objects to concatenate
```

I am unsure if this an error in pandas or epic? Any ideas of how to fix this?
Thanks",saycheeeeese,https://github.com/biocore-ntnu/epic/issues/55
MDU6SXNzdWUyMzQ3MjE4NDc=, Score & Fold_change? What do they mean and how to caculate them?,OPEN,2017-06-09T05:00:51Z,2017-06-09T08:44:54Z,,"Epic outputs the calculation result in delim format. The result columns include Score and Fold_change. I am not sure how they are derived from. Do you plan to explain them in details? I check the Sicer website and the software readme, it is not mentioned. It seems a common knowledge I missed.",zhenyisong,https://github.com/biocore-ntnu/epic/issues/56
MDU6SXNzdWUyNDM0MTU3Nzk=,OverflowError for paired-end reads bedpe files,CLOSED,2017-07-17T14:33:49Z,2017-07-19T09:48:18Z,2017-07-19T09:48:18Z,"running the fololwing command 
`epic --treatment wt.ip.0h.1.reads.bedpe,wt.ip.0h.2.reads.bedpe --control wt.input.0h.1.reads.bedpe,wt.input.0h.2.reads.bedpe -cpu 10 --genome sacCer3 -egs 0.961651807729 -cs Sc.R64.1.genome -bw bigWigFIles -sbw  sumBigWigFIles -pe
`
I get an error message as such:

```
# epic --treatment wt.ip.0h.1.reads.bedpe,wt.ip.0h.2.reads.bedpe --control wt.input.0h.1.reads.bedpe,wt.input.0h.2.reads.bedpe -cpu 10 --genome sacCer3 -egs 0.961651807729 -cs Sc.R64.1.genome -bw bigWigFIles -sbw sumBigWigFIles -pe
# epic --treatment wt.ip.0h.1.reads.bedpe,wt.ip.0h.2.reads.bedpe --control wt.input.0h.1.reads.bedpe,wt.input.0h.2.reads.bedpe -cpu 10 --genome sacCer3 -egs 0.961651807729 -cs Sc.R64.1.genome -bw bigWigFIles -sbw sumBigWigFIles -pe # epic_version: 0.1.25, pandas_version: 0.20.2 (File: epic, Log level: INFO, Time: Mon, 17 Jul 2017 16:24:41 )
Binning wt.ip.0h.1.reads.bedpe,wt.ip.0h.2.reads.bedpe (File: run_epic, Log level: INFO, Time: Mon, 17 Jul 2017 16:24:41 )
Binning chromosomes I, II, III, IV, IX, MT, V, VI, VII, VIII, X, XI, XII, XIII, XIV, XV, XVI (File: count_reads_in_windows, Log level: INFO, Time: Mon, 17 Jul 2017 16:24:41 )
Binning wt.input.0h.1.reads.bedpe,wt.input.0h.2.reads.bedpe (File: run_epic, Log level: INFO, Time: Mon, 17 Jul 2017 16:24:41 )
Binning chromosomes I, II, III, IV, IX, MT, V, VI, VII, VIII, X, XI, XII, XIII, XIV, XV, XVI (File: count_reads_in_windows, Log level: INFO, Time: Mon, 17 Jul 2017 16:24:41 )
Merging ChIP and Input data. (File: helper_functions, Log level: INFO, Time: Mon, 17 Jul 2017 16:24:42 )
11690902.0 effective_genome_size (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 17 Jul 2017 16:24:42 )
200 window size (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 17 Jul 2017 16:24:42 )
0 total chip count (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 17 Jul 2017 16:24:42 )
0.0 average_window_readcount (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 17 Jul 2017 16:24:42 )
1 island_enriched_threshold (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 17 Jul 2017 16:24:42 )
4.0 gap_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 17 Jul 2017 16:24:42 )
1.0 boundary_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Mon, 17 Jul 2017 16:24:42 )
/usr/local/lib/python2.7/dist-packages/epic/statistics/compute_score_threshold.py:24: **RuntimeWarning: divide by zero encountered in log**
  score = -log(required_p_value)
Traceback (most recent call last):
  File ""/usr/local/bin/epic"", line 219, in <module>
    run_epic(args)
  File ""/usr/local/lib/python2.7/dist-packages/epic/run/run_epic.py"", line 50, in run_epic
    compute_background_probabilities(nb_chip_reads, args)
  File ""/usr/local/lib/python2.7/dist-packages/epic/statistics/compute_background_probabilites.py"", line 51, in compute_background_probabilities
    boundary_contribution, genome_length_in_bins)
  File ""/usr/local/lib/python2.7/dist-packages/epic/statistics/compute_score_threshold.py"", line 26, in compute_score_threshold
    current_scaled_score = int(round(score / BIN_SIZE))
**OverflowError: cannot convert float infinity to integer**
```

There is also a warning about divion by zero, which I can't really pin-point.

The projects works with paired-end samples. to make my mapping results works with epic i have sorted the bam files by name, fixed the flags and convert them to bedpe format like that:
```
samtools sort -@ 8 -n ../oldAnalysis/Mapping.bowtie2/$base/$base.sorted.bam -o $base.reads.sorted
samtools fixmate $base.reads.sorted $base.reads.fixed.sorted
bedtools bamtobed -i $base.reads.fixed.sorted -bedpe > $base.reads.bedpe
```

the bedpe files looks like that now:

``` head wt.ip.0h.1.reads.bedpe
V       550986  551070  V       551119  551205  NB500982:16:HLHKMBGXX:1:11101:1047:9075 42      +       -
IV      425501  425587  IV      425535  425619  NB500982:16:HLHKMBGXX:1:11101:1050:11357        42      +       -
XVI     338212  338298  XVI     338464  338550  NB500982:16:HLHKMBGXX:1:11101:1052:16655        42      +       -
VII     865993  866078  VII     866073  866159  NB500982:16:HLHKMBGXX:1:11101:1053:2697 42      +       -
IX      32309   32395   IX      32462   32548   NB500982:16:HLHKMBGXX:1:11101:1053:9302 42      +       -
VIII    462923  463009  VIII    462991  463077  NB500982:16:HLHKMBGXX:1:11101:1053:14383        42      +       -```

Any ideas on to to analyse the data wit hepic?
Do I really need to convert the bam files of a paired-end data set to bedpe format? 
Maybe this causes the problem.
thanks
Assa",yeroslaviz,https://github.com/biocore-ntnu/epic/issues/57
MDU6SXNzdWUyNDQyNTM0NTM=,total_genome_length is not defined,CLOSED,2017-07-20T05:59:46Z,2017-07-25T18:15:08Z,2017-07-25T18:15:08Z,"Thanks for updating the log and output options so fast. 
Now I get a different error though. 
I am using the same command as before 
```
epic --control wt.input.4h.1.reads.bedpe wt.input.4h.2.reads.bedpe --treatment wt.ip.4h.1.reads.bedpe wt.ip.4h.2.reads.bedpe -cpu 10 --genome sacCer3 -egf 0.961651807729 -cs Sc.R64.1.genome -b wt.input.4h.bed -bw wt.input.4h -sbw  wt.input.4h
```
Iam using a ```chromsize``` file, as saccharomyces cerevisiae is not (yet?) defined in your list of genomes, (besides I am using the ensembl annotations and not UCSC)

When running the newest version now (0.2.0), I amgetting the following error

```
# epic --control wt.input.4h.1.reads.bedpe wt.input.4h.2.reads.bedpe --treatment wt.ip.4h.1.reads.bedpe wt.ip.4h.2.reads.bedpe -cpu 10 --genome sacCer3 -egf 0.961651807729 -cs Sc.R64.1.genome -b wt.input.4h.bed -bw wt.input.4h -sbw wt.input.4h # epic_version: 0.2.0, pandas_version: 0.20.2 (File: epic, Log level: INFO, Time: Thu, 20 Jul 2017 07:53:32 )
Traceback (most recent call last):
  File ""/usr/local/bin/epic"", line 257, in <module>
    total_genome_length *= sum(args.chromosome_sizes.values())
NameError: name 'total_genome_length' is not defined
```

Any Ideas, why is this happening?
Assa

PS 
Do I need to use the ```--genome``` option at all in this case, if I supply a ```chromsize``` file?",yeroslaviz,https://github.com/biocore-ntnu/epic/issues/58
MDU6SXNzdWUyNDQyNTQ4NDU=,adding saccer3 to the genome list,CLOSED,2017-07-20T06:09:02Z,2017-07-20T07:40:20Z,2017-07-20T06:25:32Z,"I was trying to add my own genome to the genome list as explained in the scripts folder. 
It was not a problem to create the environment and install the needed packages.
After starting it and running the dry-run, I get the following error though

```
snakemake -n -s genome.snakefile
Traceback (most recent call last):
  File ""/home/yeroslaviz/anaconda_ete/envs/effective-genome-size/bin/snakemake"", line 4, in <module>
    import snakemake
  File ""/home/yeroslaviz/anaconda_ete/envs/effective-genome-size/lib/python3.5/site-packages/snakemake/__init__.py"", line 20, in <module>
    from snakemake.workflow import Workflow
  File ""/home/yeroslaviz/anaconda_ete/envs/effective-genome-size/lib/python3.5/site-packages/snakemake/workflow.py"", line 23, in <module>
    from snakemake.dag import DAG
  File ""/home/yeroslaviz/anaconda_ete/envs/effective-genome-size/lib/python3.5/site-packages/snakemake/dag.py"", line 20, in <module>
    from snakemake.jobs import Job, Reason
  File ""/home/yeroslaviz/anaconda_ete/envs/effective-genome-size/lib/python3.5/site-packages/snakemake/jobs.py"", line 17, in <module>
    from urllib.request import urlopen
  File ""/home/yeroslaviz/anaconda_ete/envs/effective-genome-size/lib/python3.5/urllib/request.py"", line 88, in <module>
    import http.client
  File ""/usr/local/lib/python2.7/dist-packages/future-0.16.0-py2.7.egg/http/__init__.py"", line 7, in <module>
    raise ImportError('This package should not be accessible on Python 3. '
ImportError: This package should not be accessible on Python 3. Either you are trying to run from the python-future src folder or your installation of python-future is corrupted.
```

Is it some conflict of the two python version I have?

Thanks
Assa",yeroslaviz,https://github.com/biocore-ntnu/epic/issues/59
MDU6SXNzdWUyNDU0OTQwMDA=,bigwig parameter with epic,CLOSED,2017-07-25T18:35:13Z,2018-01-16T09:42:52Z,2017-12-12T11:44:28Z,"I don't really understand what it means here. In the help it says: 

```
--bigwig BIGWIG, -bw BIGWIG
                        For each file, store a bigwig of both enriched and
                        non-enriched files to folder <BIGWIG>. Requires
                        different basenames for each file.
```

Are these two separate files for enriched and non-enriched regions. Or this suppose to say 
 ```
                       ...
                       For each file, store a bigwig of both enriched and
                        non-enriched **regions** to folder <BIGWIG>. ...
```",yeroslaviz,https://github.com/biocore-ntnu/epic/issues/60
MDU6SXNzdWUyNDcwOTYyMDg=,i2bw correction in the doc,CLOSED,2017-08-01T14:51:45Z,2017-08-14T12:13:07Z,2017-08-14T12:13:07Z,"Hi,
in the [documentation for the i2bw](http://bioepic.readthedocs.io/en/latest/output_files.html#i2bw-individual-log2fc-bigwigs) you're writing
`**-i2bw/‚Äìindividual-log2fc-bigwigs**

This flag takes a folder to store bigwigs in. One bigwig file is created per bed/bedpe file given for epic to analyze.  ...`

I assume this was copied from the [bed file description](http://bioepic.readthedocs.io/en/latest/output_files.html#b-bed-optional). 

But don't you mean it creates one bigwig file for each Chip-file? 
It doesn't make sense to do it for the input files (and it doesn't create them anyway).

Assa",yeroslaviz,https://github.com/biocore-ntnu/epic/issues/61
MDU6SXNzdWUyNDc3NjkyMjU=,Can we run Epic without a control file?,CLOSED,2017-08-03T16:48:06Z,2018-01-16T09:42:30Z,2018-01-16T09:42:30Z,"Hi there,

I have ChIP data for H3K27me3, but the biologist did not do a ChIP control experiment.  So, I don't have a matching control file.  When I tried to run epic, it crashed and threw an error saying it requires a control file.

Is it not possible at all to run epic without a control file?

I searched online, and for histone marks, many people say epic (SICER) is better, although MACS 2 seems to also have the ability to process broad peaks, possibly histone marks.

Anyway, please let me know if I can still run epic without controls.  Otherwise, I think I have no choice but to go for MACS?

Thanks a lot!
",CodeInTheSkies,https://github.com/biocore-ntnu/epic/issues/62
MDU6SXNzdWUyNTE2MzIzMzA=,-w parameter,CLOSED,2017-08-21T11:37:43Z,2017-08-22T12:24:49Z,2017-08-22T12:24:49Z,"There is something weird about the results here. When I am reading the resulted bigwig file (i2bw file) into my R session, I am getting non-overlapping bins with one position skipped each time.
```
GRanges object with 5451 ranges and 1 metadata column:
         seqnames             ranges strand |              score
            <Rle>          <IRanges>  <Rle> |          <numeric>
     [1]       XV         [  1, 199]      * |  0.244747966527939
     [2]       XV         [201, 399]      * |  0.857724845409393
     [3]       XV         [401, 599]      * | -0.441242098808289
     [4]       XV         [601, 799]      * | -0.302922338247299
     [5]       XV         [801, 999]      * |  -1.30326700210571
```
It looks line each time one base is skipped from the counting (200,400,600,etc.)
It is even worse when the -w parameter is set to 150. 

```
GRanges object with 60747 ranges and 1 metadata column:
          seqnames           ranges strand |              score
             <Rle>        <IRanges>  <Rle> |          <numeric>
      [1]        I       [  1, 149]      * |  0.566456496715546
      [2]        I       [201, 349]      * |   0.72018426656723
      [3]        I       [401, 549]      * |  0.222972229123116
      [4]        I       [601, 749]      * | -0.661386787891388
      [5]        I       [801, 949]      * | -0.159120514988899
 ```

Here each time 51 bases are skipped.

Is this deliberate?
I hope it is not to confusing, that I show the R output here, as this is how I can visualize my bigwig results. 
",yeroslaviz,https://github.com/biocore-ntnu/epic/issues/63
MDU6SXNzdWUyNTU4NjY5ODM=,cat: write error: Broken pipe,CLOSED,2017-09-07T09:08:00Z,2017-09-07T12:24:08Z,2017-09-07T12:24:08Z,"I've doing some test runs with epic on our data and always received this error message as the first message after running epic -t ... -c ... > ...

`cat: write error: Broken pipe`

The job runs fine anyway and results are created without problems (as far as I can tell).

You probably need some additional information, but before knowing exactly what you need I'll give you some basic info. I'm working on a node of a computational cluster, running Debian 7 (wheezy) and cat version 8.13.

Let me know if you need any other info.",romanhaa,https://github.com/biocore-ntnu/epic/issues/64
MDU6SXNzdWUyNzAwMDQ0OTU=,Genome request,CLOSED,2017-10-31T15:31:53Z,2018-01-16T09:42:12Z,2017-12-12T11:44:05Z,"Hi There,

I am trying to use epic for ChiPSeq data for a genome that does not exist in the current list of the available genomes. It is possible to use epic in such cases?

```
Genome PbergheiANKA not found
If yours is not there, please request it at github.com/endrebak/epic . (File: genomes, Log level: ERROR, Time: Tue, 31 Oct 2017 15:21:05 )
```

I would like to request the [Plasmodium genome](http://plasmodb.org/common/downloads/Current_Release/PbergheiANKA/fasta/data/PlasmoDB-34_PbergheiANKA_Genome.fasta) be added to the list. 

Thanks",sejmodha,https://github.com/biocore-ntnu/epic/issues/65
MDU6SXNzdWUyNzMxNjExNDE=,epic-merge fails to run,CLOSED,2017-11-11T17:25:45Z,2018-01-07T11:33:21Z,2017-11-13T11:20:47Z,"Doesn't include a shebang line. If this is done (e.g. copying the way it is done in the epic program) - then I can't seem to avoid this error:

 File ""/home/n/Apps/anaconda2/bin/epic-merge"", line 73, in <module>
    from math import gcd
ImportError: cannot import name gcd
",nw11,https://github.com/biocore-ntnu/epic/issues/66
MDU6SXNzdWUyNzcwMDQwNjQ=,epic - bed formats,CLOSED,2017-11-27T12:37:59Z,2017-12-12T11:43:22Z,2017-12-12T11:43:22Z,"Hi There,

I have used epic for ChIP-Seq data and now I want to annotate peaks using peak annotation program e.g. annotatePeaks (from Homer), could you please explain the epic's default output format and the bed format generated using -b flag as the bed file does not have strand or peak ID info in it.",sejmodha,https://github.com/biocore-ntnu/epic/issues/67
MDU6SXNzdWUyODYwOTI4OTM=,Add C. elegans genome?,OPEN,2018-01-04T19:43:25Z,2018-01-16T09:34:21Z,,"Hello Endre,

Would it be possible for you to add the C. elegans genomes to your list? I would personally like you to add WS220/ce10, but it might also be useful to add the most recent assembly on UCSC, WBcel235/ce11.

Thank you for developing epic - I've found the documentation very easy to use thus far.
Teresa",teresawlee,https://github.com/biocore-ntnu/epic/issues/68
MDU6SXNzdWUyODg4MzkwMTk=,Create bigwigs in epic-cluster,OPEN,2018-01-16T09:34:09Z,2018-01-16T09:34:14Z,,,endrebak,https://github.com/biocore-ntnu/epic/issues/69
MDU6SXNzdWUyOTYxNjM4Njk=,"where ""chr1_before_find_islands.csv"" file comes from?",CLOSED,2018-02-11T05:57:21Z,2018-02-12T10:50:38Z,2018-02-12T10:50:38Z,"I installed the local version epic through bioconda. However, when task was competed, there was a file named ""chr1_before_find_islands.csv"" which was created along with expected cvs files. Here is my shell snippet:
```
    macs2 callpeak --treatment ${treat} --control ${control} \
          --format BAM --gsize mm --name ${base} --bdg --qvalue 0.01
```

here is the excerpt of this file (""chr1_before_find_islands.csv"")

```
 Chromosome Bin ChIP Input Score
28 chr1 3007400 6 5.0 2.43113976041
134 chr1 3035800 6 4.0 2.43113976041
174 chr1 3046800 6 1.0 2.43113976041
177 chr1 3047400 6 3.0 2.43113976041
```
I am confused about this unwanted file. It seems that the run_epic.py will output this file. (at line 60)

```
dfs[0].to_csv(""chr1_before_find_islands.csv"", sep="" "")
```

Will you please clarify this chr1 output? Or it is just the temporary file and can be neglected whatever. Or the error message we should take care and tune the parameter in the command line?",zhenyisong,https://github.com/biocore-ntnu/epic/issues/70
MDU6SXNzdWUzMDIxNDg2MTI=,epicR?,OPEN,2018-03-05T02:01:19Z,2018-03-08T14:50:28Z,,"Any plans to create an R/Bioconductor package (e.g., `epicR`)?",Bohdan-Khomtchouk,https://github.com/biocore-ntnu/epic/issues/71
MDU6SXNzdWUzMDI1MjgwMTg=,Recommend publication,OPEN,2018-03-06T01:25:45Z,2018-03-06T06:51:47Z,,"Re:

> When is your paper coming out?
> 
> Dunno. We do not want to write a methods paper, but rather just include a section about epic in an appropriate biology paper sometime.

In its current state, `epic` seems to be a body of work ideally suited as a publication in OUP's _Bioinformatics_, as they generally publish papers featuring extensive benchmark work (as you've done) that push the state-of-the-art further in important areas like this.  Thus, I would highly recommend considering publishing a standalone methods paper for `epic`, possibly also including an R implementation in the same manuscript ([`epicR`](https://github.com/biocore-ntnu/epic/issues/71)).  ",Bohdan-Khomtchouk,https://github.com/biocore-ntnu/epic/issues/72
MDU6SXNzdWUzMDYwNDEzMjg=,No Releases,OPEN,2018-03-16T18:56:05Z,2018-03-16T20:08:19Z,,"Can you guys start creating tagged releases.  I am wanting to download the latest version which I think is 0.2.9  but there is no release, and I am not sure which commit number it is.  I would rather do the releases instead of always grabbing the master.",dslater82,https://github.com/biocore-ntnu/epic/issues/73
MDU6SXNzdWUzMDcwMzU0MjY=,epic 0.2.9 compute_score_threshold.py:24 RuntimeWarning: divide by zero encountered in log,OPEN,2018-03-20T20:56:53Z,2018-03-20T22:51:41Z,,"I converted my BAM files to BED format with bamtobed as suggested in the tutorial.

Then I run the epic command like this:

> epic  --treatment Amy_C2_763_C1_IP.bed \
>       --control  Amy_C2_763_C1_input.bed  \
>       --genome rn6 \
>       --window-size 200 \
>       --gaps-allowed 3 \
>       --number-cores 4 \
>       --bed Amy_C2_763_C1.W200.G3.epicCalls.bed \
>       --bigwig Amy_C2_763_C1.W200.G3 \
>       --outfile Amy_C2_763_C1.W200.G3.epicCalls.csv


The command aborted. These are the last few lines with the error message

> 0 total chip count (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 20 Mar 2018 14:06:56 )
> 0.0 average_window_readcount (File: compute_background_probabilites, Log level:DEBUG, Time: Tue, 20 Mar 2018 14:06:56 )
> 1 island_enriched_threshold (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 20 Mar 2018 14:06:56 )
> 4.0 gap_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 20 Mar 2018 14:06:56 )
> 1.0 boundary_contribution (File: compute_background_probabilites, Log level: DEBUG, Time: Tue, 20 Mar 2018 14:06:56 )
> /home/apps/software/epic/0.2.9-IGB-gcc-4.9.4-Python-2.7.13/lib/python2.7/site-packages/bioepic-0.2.9-py2.7.egg/epic/statistics/compute_score_threshold.py:24: 
> RuntimeWarning: divide by zero encountered in log
> Traceback (most recent call last):
>   File ""/home/apps/software/epic/0.2.9-IGB-gcc-4.9.4-Python-2.7.13/bin/epic"", line 4, in <module>
>     __import__('pkg_resources').run_script('bioepic==0.2.9', 'epic')
>   File ""build/bdist.linux-x86_64/egg/pkg_resources/__init__.py"", line 738, in run_script
>   File ""build/bdist.linux-x86_64/egg/pkg_resources/__init__.py"", line 1506, in run_script
>   File ""/home/apps/software/epic/0.2.9-IGB-gcc-4.9.4-Python-2.7.13/lib/python2.7/site-packages/bioepic-0.2.9-py2.7.egg/EGG-INFO/scripts/epic"", line 285, in <module>
> 
>   File ""build/bdist.linux-x86_64/egg/epic/run/run_epic.py"", line 54, in run_epic
>   File ""build/bdist.linux-x86_64/egg/epic/statistics/compute_background_probabilites.py"", line 51, in compute_background_probabilities
>   File ""build/bdist.linux-x86_64/egg/epic/statistics/compute_score_threshold.py"", line 26, in compute_score_threshold OverflowError: cannot convert float infinity to integer
 
A snippet of one of the BED files looks like this:


NC_001665.2	16221	16313	K00363:111:HMW3HBBXX:7:2212:31913:7134	11	+
NC_001665.2	16222	16313	K00363:111:HMW3HBBXX:2:2118:13098:31189	11	+
NC_001665.2	16222	16313	K00363:111:HMW3HBBXX:4:1109:15747:9016	11	-
NC_001665.2	16222	16313	K00363:111:HMW3HBBXX:5:2120:7476:19795	11	-
NC_001665.2	16223	16313	K00363:111:HMW3HBBXX:2:1205:32096:24894	11	-
NC_001665.2	16223	16313	K00363:111:HMW3HBBXX:5:1107:14631:29624	11	-
NC_001665.2	16223	16313	K00363:111:HMW3HBBXX:8:2109:21379:42653	11	+
NC_001665.2	16224	16313	K00363:111:HMW3HBBXX:4:2114:8816:15399	11	-
NC_001665.2	16225	16313	K00363:111:HMW3HBBXX:7:1118:30442:5886	11	+
NC_001665.2	16226	16313	K00363:111:HMW3HBBXX:3:1215:17168:13060	11	+


Any suggestions to correct this error??

",grendon,https://github.com/biocore-ntnu/epic/issues/74
MDU6SXNzdWUzMTM0OTk5MDI=,salmo salar genome request,OPEN,2018-04-11T21:46:56Z,2018-04-16T17:26:27Z,,"Hello,
I am working with Chip-seq data (Histone modifications) from Salmo salar and I would like to ask you if it is possible to request the genome and genome size file for this organism?.  Or any help to create include the annotation I used for the mapping in the epic run?

The genome annotation I used for the mapping is the one found at NCBI (https://www.ncbi.nlm.nih.gov/genome/369?genome_assembly_id=248466), I also include the ""unplaced contigs"" in this process. 
Thanks for your help,
Cheers,

Estefania

",etarisal,https://github.com/biocore-ntnu/epic/issues/76
MDU6SXNzdWUzMjE2MzgwMjc=,run epic without control samples,CLOSED,2018-05-09T16:22:47Z,2018-05-09T17:29:09Z,2018-05-09T17:29:09Z,"Hello,

I'm interested in trying epic on some ChIP-exo and ChIP-nexus datasets. Unlike traditional ChIP-seq, these protocols don't generate an input library, which is a problem since epic requires control libraries to be specified in order to run.

Is it possible to make the control samples optional? [Section 2.1.5 of the 2009 SICER paper](https://academic.oup.com/bioinformatics/article/25/15/1952/212783) is about using a random background model in the absence of a control library.
 ",james-chuang,https://github.com/biocore-ntnu/epic/issues/77
MDU6SXNzdWUzMjE5NDM1MDM=,Best choice parameters,OPEN,2018-05-10T14:05:06Z,2018-05-10T16:33:28Z,,"Hello,
I'm using epic 0.2.9 to compare two mapping samples against a mapping reference, and I have some questions for you.
I'm looking for enriched regions of unknown size comparing two experimental groups (GROUP1: sample1 and sample2) (GROUP2: reference). But it's essential that no reads of one of the groups map on the enriched sequence.
At first, I mapped the 3 samples sequences (sample 1, sample 2 and reference) against the same reference genome using bowtie2. Then I used epic to compare sample1_mapping/reference_mapping and  sample1_mapping/reference_mapping. I created a chromosome-size-file.

When I compared the samples with default parametres, I get large enriched regions and logically a lot of reads of both groups mapped on each enriched region.

I extracted a little test_sample of each sample. Speciffically I extracted known enriched region from genome. Then I reproduce the same steps.
I make a lot of executions with high and low FDR, proving combinations of windows size and gap allowed through some loops.
When I checked the epic results I realized that in sample1/reference comparison, the program return the exactly region.

But in sample2/reference comparison it returns:
-One very large region whith high FDR and logically both samples reads mapped in it. 
-A lot of very short regions with no reads mapped from one of the samples, with very low FDR, but there are a lot of little gaps between them.
-No enriched regions.
When I graph the coverage map of these comparison I can see a clear enriched region in both comparisons. 
Here are the graphics:
![sample1_mapping-against-reference_mapping](https://user-images.githubusercontent.com/32848673/39870363-62af07b0-5461-11e8-998a-545d37d778d1.png)
![sample2_mapping-against-reference_mapping](https://user-images.githubusercontent.com/32848673/39870364-65818e90-5461-11e8-917e-f85fc5f111f7.png)
The most notable difference between sample1 and sample2 mappings is the coverage deep, how you can see in the graphs. 
I could test a combination of parametres for tune up the program for the test-files. But in the case of the real samples, I don't think it work because I'm looking for unknown size sequences, from 3 pb to largest possible. 
FIRST QUESTION: 
Which combination of parameters do you recommend for that kind of experiment?
SECOND QUESTION: 
In what degree does the coverage deep diference between mappings affect?
THIRD QUESTION:
In what degree does the definition of the samples as control (-c) or as treatment (-t) affect?

I'm waiting for an answer.
Thanks you very much,
Jose.

",JoseCorCab,https://github.com/biocore-ntnu/epic/issues/78
MDU6SXNzdWUzMzU4NDg5NjE=,epic-merge throws an error of ValueError: Cannot convert NA to integer,CLOSED,2018-06-26T14:32:49Z,2018-07-18T11:31:43Z,2018-07-18T11:31:43Z,"Dear developers,
I tried to run epic-merge using two matrices and bed files generated by epic and I got the following error:
```
/users/bi/lcozzuto/.pyenv/versions/2.7.13/lib/python2.7/site-packages/numpy/lib/arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
Calling H3K27me3_EWS-FLI_matrix.txt.gz Enriched_H3K27me3_EWS-FLI_matrix.txt.gz in matrix file. (File: merge, Log level: INFO, Time: Tue, 26 Jun 2018 16:19:28 )
Calling H3K27me3_hMSC_matrix.txt.gz Enriched_H3K27me3_hMSC_matrix.txt.gz in matrix file. (File: merge, Log level: INFO, Time: Tue, 26 Jun 2018 16:19:56 )
Traceback (most recent call last):
  File ""/users/bi/lcozzuto/.pyenv/versions/2.7.13/bin/epic-merge"", line 91, in <module>
    merged_df = main(files, regions, keep_nonenriched, enriched_per_file, nb_cpus)
  File ""/users/bi/lcozzuto/.pyenv/versions/2.7.13/lib/python2.7/site-packages/epic/merge/merge.py"", line 20, in main
    bin_size = compute_bin_size(dfs)
  File ""/users/bi/lcozzuto/.pyenv/versions/2.7.13/lib/python2.7/site-packages/epic/merge/merge_helpers.py"", line 20, in compute_bin_size
    bins = df.head(100000).index.get_level_values(""Bin"").astype(int)
  File ""/users/bi/lcozzuto/.pyenv/versions/2.7.13/lib/python2.7/site-packages/pandas/core/indexes/numeric.py"", line 318, in astype
    raise ValueError('Cannot convert NA to integer')
ValueError: Cannot convert NA to integer
```

I'm using epic 0.2.9

Best,
Luca",lucacozzuto,https://github.com/biocore-ntnu/epic/issues/79
MDU6SXNzdWUzMzgzNjgxNTA=,epic and conda,CLOSED,2018-07-04T20:14:05Z,2018-07-06T14:45:17Z,2018-07-06T14:45:17Z,"Hi,
I'm trying to use the epic conda environment, but have an issue with scioy.
I can reproduce the error by simply importing the epic function into the python in the conda environment.
Not sure why it seems to use my scipy in local ...
Any idea would be very useful.
Thanks
Nicolas

```
>>python
Python 2.7.15 | packaged by conda-forge | (default, May  8 2018, 14:46:53) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import scipy
>>> from epic.run.run_epic import run_epic
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/bioinfo/local/build/Centos/envs_conda/epic_0.2.9/lib/python2.7/site-packages/epic/run/run_epic.py"", line 25, in <module>
    from epic.statistics.compute_background_probabilites import compute_background_probabilities
  File ""/bioinfo/local/build/Centos/envs_conda/epic_0.2.9/lib/python2.7/site-packages/epic/statistics/compute_background_probabilites.py"", line 15, in <module>
    from epic.statistics.compute_values_needed_for_recurrence import (
  File ""/bioinfo/local/build/Centos/envs_conda/epic_0.2.9/lib/python2.7/site-packages/epic/statistics/compute_values_needed_for_recurrence.py"", line 2, in <module>
    from scipy.stats import poisson
  File ""/data/users/nservant/.local/lib/python2.7/site-packages/scipy/stats/__init__.py"", line 338, in <module>
    from .stats import *
  File ""/data/users/nservant/.local/lib/python2.7/site-packages/scipy/stats/stats.py"", line 184, in <module>
    import scipy.special as special
  File ""/data/users/nservant/.local/lib/python2.7/site-packages/scipy/special/__init__.py"", line 586, in <module>
    from ._ufuncs import *
ImportError: liblapack.so.3gf: cannot open shared object file: No such file or directory
>>> 
(/bioinfo/local/build/Centos/envs_conda/epic_0.2.9) 

```",nservant,https://github.com/biocore-ntnu/epic/issues/80
MDU6SXNzdWUzMzg5OTgyNDk=,trackline and recommanded parameters,OPEN,2018-07-06T16:24:12Z,2018-07-10T10:16:00Z,,"Hi,
Thanks for developing epic which is much more stable than SICER ! that's great.
I have two comments:
- If I'm correct, the output bed files are generated for UCSC visualization. Adding a trackline would be great in that case.
- I think that other users already ask it, but having recommended parameters for different histone mark would be extremely useful.
I guess --window-size, --gaps-allowed and --false-discovery-rate-cutoff can be updated.
Any advice is welcome !
",nservant,https://github.com/biocore-ntnu/epic/issues/81
MDU6SXNzdWUzNDIyODk5MzM=,problem with output,CLOSED,2018-07-18T11:33:47Z,2018-07-31T12:25:36Z,2018-07-31T12:25:36Z,"Dear developers,
I'm using epic on a custom genome and I got a strange way to report the chromosomes:

Chromosome Start End ChIP Input Score Log2FC P FDR
1.0 3444800 3448199 76 31 46.45753848688762 1.3234133686618605 1.0186276074296772e-12 1.6867722789083852e-12
1.0 4773200 4777399 251 59 215.2449786380842 2.1185826701940806 5.0498235300167793e-79 2.9157875286298807e-78
1.0 4786400 4787599 37 8 29.947537650660315 2.2391355312341 8.964265400850882e-15 1.6083926773604145e-14

 
The genome sizes used are instead 1, 2, 3 etc...

Luca",lucacozzuto,https://github.com/biocore-ntnu/epic/issues/82
MDU6SXNzdWUzNTA0NjQ3OTM=,cannot recognize genome,OPEN,2018-08-14T14:53:34Z,2018-08-15T13:20:54Z,,"Hi,

The program does not recognize the genome speTri2 with the -gn option, although it is on your genome list.
Thanks.",Wangcn81,https://github.com/biocore-ntnu/epic/issues/83
MDU6SXNzdWUzNTI4NzM3NzE=,conda still installs 0.2.9,CLOSED,2018-08-22T09:31:54Z,2018-08-30T06:57:58Z,2018-08-28T09:31:35Z,"I am trying to install EPIC on an Ubuntu 16.04 using MiniConda and the conda method described in the README, and it seems it's pulling version 0.2.9.

Is there a conda-aware way of installing the newer 0.2.11 version, which seems important given the speed-up in the Changelog:
```
# 0.2.11 (01.08.18)
- Massive speedup with Cython
- Fix bugs added by 0.2.10
```",avilella,https://github.com/biocore-ntnu/epic/issues/84
MDU6SXNzdWUzNTMyNTYzMDg=,are read ids necessary in SE mode?,CLOSED,2018-08-23T07:26:32Z,2018-08-23T07:35:35Z,2018-08-23T07:35:35Z,"In trying to lower the memory requirements of epic, I would like to know if the read ids of the 'bedtools bamtobed' step are necessary in SE mode (i.e. not bedpe)? Would converting the read ids into an incrementing number in column 4 make the running of EPIC less memory hungry?",avilella,https://github.com/biocore-ntnu/epic/issues/85
MDU6SXNzdWUzNTMyNTY4MzM=,calculating memory requirements based on bam file size,OPEN,2018-08-23T07:28:16Z,2018-10-21T11:10:58Z,,Is it possible to approximately calculate the memory requirements of EPIC based on the size or number of reads of the bam files given as input? This is in line with the efforts to try to give the smallest possible instance to the job that will not run out of memory.,avilella,https://github.com/biocore-ntnu/epic/issues/86
MDU6SXNzdWUzNTc5MjQ3ODM=,Epic effective for ,OPEN,2018-09-07T05:29:25Z,2018-09-11T10:40:24Z,,"I want to run the epic-effective for human genome hg38. I am using a 16 gb ram for it but after using the command it is showing memory error.
using the following command:
epic-effective -r 36 GRCh38.p10.genome.fa
getting the following error:
File analyzed: GRCh38.p10.genome.fa (File: effective_genome_size, Log level: INFO, Time: Thu, 06 Sep 2018 17:28:46 )
Genome length: 3236815040 (File: effective_genome_size, Log level: INFO, Time: Thu, 06 Sep 2018 17:28:46 )
File analyzed:  GRCh38.p10.genome.fa
Genome length:  3236815040
terminate called after throwing an instance of 'jellyfish::large_hash::array_base<jellyfish::mer_dna_ns::mer_base_static<unsigned long, 0>, unsigned long, atomic::gcc, jellyfish::large_hash::unbounded_array<jellyfish::mer_dna_ns::mer_base_static<unsigned long, 0>, unsigned long, atomic::gcc, allocators::mmap> >::ErrorAllocation'
  what():  Failed to allocate 20443042440 bytes of memory
Aborted (core dumped)
Failed to open input file '/tmp/GRCh38.p10.genome.fa.jf'
Traceback (most recent call last):
  File ""/usr/local/bin/epic-effective"", line 38, in <module>
    effective_genome_size(fasta, read_length, nb_cpu, tmpdir)
  File ""/usr/local/lib/python2.7/dist-packages/epic/scripts/effective_genome_size.py"", line 56, in effective_genome_size
    shell=True)
  File ""/usr/lib/python2.7/subprocess.py"", line 574, in check_output
    raise CalledProcessError(retcode, cmd, output=output)
subprocess.CalledProcessError: Command 'jellyfish stats /tmp/GRCh38.p10.genome.fa.jf' returned non-zero exit status 1
rm: cannot remove '/tmp/GRCh38.p10.genome.fa.jf': No such file or directory

But when I use a single chromosome it works fine.
epic-effective -r 36 gencode_chrY.fa
Temporary directory: /tmp/ (File: effective_genome_size, Log level: INFO, Time: Fri, 07 Sep 2018 11:00:26 )
File analyzed: gencode_chrY.fa (File: effective_genome_size, Log level: INFO, Time: Fri, 07 Sep 2018 11:00:26 )
Genome length: 57227415 (File: effective_genome_size, Log level: INFO, Time: Fri, 07 Sep 2018 11:00:26 )
File analyzed:  gencode_chrY.fa
Genome length:  57227415
Number unique 36-mers: 21608143 (File: effective_genome_size, Log level: INFO, Time: Fri, 07 Sep 2018 11:00:39 )
Effective genome size: 0.377583768199 (File: effective_genome_size, Log level: INFO, Time: Fri, 07 Sep 2018 11:00:39 )
Number unique 36-mers:  21608143
Effective genome size:  0.377583768199

So how can overcome this issue for effective genome size calculation for entire genome.

",parida007,https://github.com/biocore-ntnu/epic/issues/87
MDU6SXNzdWUzNjIwODQyOTU=,How does EPIC work ?,OPEN,2018-09-20T08:55:24Z,2018-09-21T07:16:40Z,,"Hi,
A short question about the EPIC output file.
For some samples, it seems that EPIC is working pretty well. However, for others (same histone marks), almost all regions are significant ... So I try to set up my own filter on from the results.out file.

Here is an example ;
  Chromosome   Start     End ChIP Input     Score    Log2FC            P
1       chr1   29600   77199  682  1024 2693.9908 0.3391119 1.478957e-09
2       chr1   78200   86199   63    77  373.0064 0.6359773 4.019591e-04
3       chr1  437000  474999  457   635 2127.5436 0.4509215 8.612973e-11
4       chr1 2480800 2494399   96   114  676.9330 0.6775564 7.038296e-06
5       chr1 2606400 2631799  217   256 1367.6917 0.6870352 2.883622e-11
6       chr1 2746400 2863199 1544  2279 6962.8744 0.3637558 8.185924e-22

In the manual ;
The log2_fold change is the number of ChIP reads divided by the number of Input reads in the region (where a pseudocount is computed for regions with no input-reads.)
But for instance, in line 1, the input has more reads than the ChIP ...
And I would therefore expect a Log2FC = log2(682/1024)=-0.586 which is not the case.
Could you explain me why please ?
Thanks",nservant,https://github.com/biocore-ntnu/epic/issues/88
MDU6SXNzdWUzODEwMzQ0Njc=,Epic depricated,OPEN,2018-11-15T07:33:37Z,2018-11-15T12:05:45Z,,"Hi,
I found that SICER2 come out. So the advantage of epic like considering multiple input files in encompassed in SICER2 or not. And Shall I use epic for my analysis or not as it is showing deprecated??",parida007,https://github.com/biocore-ntnu/epic/issues/90
MDU6SXNzdWU1MzcxNDU5OTA=,Epic-effective: TypeError: cannot concatenate 'str' and 'NoneType' objects,OPEN,2019-12-12T18:35:47Z,2019-12-31T10:10:46Z,,"Hi. I am attempting to use epic effective to estimate the genome size of a custom assembly for a eukaryotic microbe prior to peak calling. 
`time epic-effective -r 75 /rhome/aboyd003/bigdata/oldGenome/Pinfestans.fa`

I receive the following error: 
`Traceback (most recent call last):
  File ""/rhome/aboyd003/.conda/envs/BIOEPIC/bin/epic-effective"", line 4, in <module>
    __import__('pkg_resources').run_script('bioepic==0.1.6', 'epic-effective')
  File ""/rhome/aboyd003/.conda/envs/BIOEPIC/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 666, in run_script
    self.require(requires)[0].run_script(script_name, ns)
  File ""/rhome/aboyd003/.conda/envs/BIOEPIC/lib/python2.7/site-packages/pkg_resources/__init__.py"", line 1462, in run_script
    exec(code, namespace, namespace)
  File ""/rhome/aboyd003/.conda/envs/BIOEPIC/lib/python2.7/site-packages/bioepic-0.1.6-py2.7.egg-info/scripts/epic-effective"", line 35, in <module>
    effective_genome_size(fasta, read_length, nb_cpu, tmpdir)
  File ""/rhome/aboyd003/.conda/envs/BIOEPIC/lib/python2.7/site-packages/epic/scripts/effective_genome_size.py"", line 22, in effective_genome_size
    logging.info(""Temporary directory: "" + tmpdir)
TypeError: cannot concatenate 'str' and 'NoneType' objects

real	0m0.766s
user	0m0.332s
sys	0m0.079s`

Sample of definition lines from genome file are provided below. The sequences include Ns.
>7000000037419696
>7000000037419352
>7000000037419663
>7000000037419515
",aboyd003,https://github.com/biocore-ntnu/epic/issues/91
MDU6SXNzdWU3ODM4ODA1NjU=,Genomes cannot find,OPEN,2021-01-12T03:38:30Z,2021-01-12T03:38:30Z,,The susScr11 cannot be find in the genomes list. Please add it. I suggest the genomes should be more flexible and not be limited in  your  list. Thanks advanse!,tan5251,https://github.com/biocore-ntnu/epic/issues/92
MDU6SXNzdWU4MjI0MTE1NTY=,ChIP-seq WT vs mutants comparison,OPEN,2021-03-04T18:59:40Z,2021-03-04T19:27:29Z,,"Hi, I really appreciate your efforts making these tools.
I'm a quite new to this area and have a question rather than an issue.

I successfully completed jobs with epic and generated lists of peaks.
For simplicity, let's say I have WT H3K27me3 peaks and mutant H3K27me3 peaks. 
WT H3K27me3 peaks were generated from epic using -t H3K27me3 -c H3 and greylist was already removed from the two lists.
Mutant H3K27me3 peaks were made in the same way. 

Now the question is, if I wanna compare H3K27me3 of WT vs mutants, can I just re-run epic with these epic-generated result?

Or for this purpose, do you recommend using other programs such as Diffbind or DESeq maybe?
   





",nermah1,https://github.com/biocore-ntnu/epic/issues/93
MDU6SXNzdWU4Nzg4NTc4NzU=,Arabidopsis thaliana,OPEN,2021-05-07T11:53:02Z,2021-05-07T12:52:53Z,,"Hello!
is ir possible to add arabidopsis thaliana genome?

Thank you!",mtrejo96,https://github.com/biocore-ntnu/epic/issues/94
